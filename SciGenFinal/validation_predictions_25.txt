table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , both approaches give good gains in performance in terms of both inference and training .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . balanced datasets also exhibit lower performance than the linear ones , indicating that the size of the balanced dataset does not impact the model performance .
the max pooling strategy consistently performs better in all model variations . hyperparametrization results for each model with different representation is shown in table 2 . we find that the dropout probability and the activation func . are the most important factors in the model performance , and when using softplus instead of softplus , the model performs better than both softplus and sb ( see x4 ) . when using conll08 as the representation , we find that softplus significantly outperforms softplus ( p < 0 . 05 ) and sb , but do not outperform softplus except for in the rare case of cases where softplus is used in combination with neural network features . we notice that using the sigmoid representation gives better performance than softplus in all but the one case when using only one of the four hyperparametrized parameters . finally , when using f1 scores as the f1 score , we see that adding softplus features also improves results .
table 1 shows the effect of using the shortest dependency path on each relation type . our model significantly improves the results for relation prediction in the 5 - fold test set without sdp . it achieves the best f1 score ever ( 59 . 59 % ) and the best ranking ever ( 71 . 63 % ) on topic .
the results are shown in table 3 . in general terms , the performance of the second generation model compared to the first generation is slightly better . the performance gap between the two sets is narrower with respect to f1 and r - f1 scores , but still suggests some performance variation . for example , in f1 50 % and f1 100 % both sets achieve better performance than expected by random chance . in comparison , in y - 3 , the accuracy gap is much narrower , with the f1 score improving from 51 . 59 % to 62 . 63 % on average .
the results are shown in table 1 . we show the precision numbers for each paragraph as well as the goal accuracy in question . for brevity we show only the results for english , german , dutch , russian and turkish .
we compare the performances of our system with the best performing lstm - parser parser ( 60 . 40 % c - f1 ) on the essay and paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between the average and average performances between the two indicated systems is less pronounced for the paragraph level , showing that lstmp - parser can learn to reason over more than sentences .
the results are shown in table 5 . the original and the cleaned tgen models perform comparably to the original ones , but the results show that tgen + has been significantly better at removing errors and replacing them with new ones when trained with the correct ones . when the original tgen model is cleaned , it achieves the best results ( 39 . 59 % ) on the bleu test set , while the cleaned model ( 37 . 63 % ) is slightly better than the original . meteor and rouge - l are both better than tgen − but do not exceed the upper boundary of the cleaned range , which indicates the extent to which tgen can be improved with a reasonable amount of data cleaning . we notice that sc - lstm is comparable , but does not outperform tgen , likely in part due to the smaller size of the training data set and the high overlap between training and test set . finally , we see that ser is comparable with the original model , but requires significantly less data and time to clean , indicating that it has been trained and tested on a larger corpus . table 5 shows the results of re - training the models after removing the wrong ones .
table 1 compares the original e2e dataset with the cleaned version . the difference in terms of mrs is minimal , however we see significant difference in ser as measured by a large percentage of textual references ( 17 . 69 % ) compared to the original data . also , the number of slots in our slot matching script is significantly less than the original dataset ( 4 . 299 vs . 42 , 061 ) indicating that our model can rely less on superficial cues .
table 3 shows the results for the original and the add models . the results show that tgen + improves upon the original model by 3 . 83 points in the bleu metric , but does not improve significantly over tgen − . by further adding parameters , the model achieves gains of 2 . 59 points over the original tgen model . rouge - l is still inferior to sc - lstm in terms of generalization , however it performs better than tgen and ser . adding parameters improves the generalization ability of the model , improving it by 2 . 36 points over tgn − .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the first set of errors we found was caused by errors caused by adding , removing , and replacing correct values with wrong values ( see table 4 ) . the second set we found caused errors due to the presence of errors in the training data ( which we labeled as missing , wrong values , and slightly disfluencies ) as shown in table 4 , cleaning the data after removing these errors did not improve the performance ,
table 3 compares our model against previous state - of - the - art approaches on the hidden test set of gcnseq , tree2str , tsp and pbmt . our model achieves the best results , outperforming all the base the performance gap between the two approaches is modest , with the exception of the gap between dcgcn and graphlstm ( 28 . 9 % vs . 25 . 2 % ) , however , if a model has access to the shared vocabulary , it performs much worse than our model .
the results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art . by comparison , ggnn2seq ( beck et al . , 2018 ) achieves 27 . 4 points , and seq2seqb achieves a whopping 142 . 5 points . gcnseq ( damonte and cohen , 2019 ) achieves a mere 27 . 6 points , a slight improvement over the performance of the previous approaches . though the number of parameters in our model is small , we achieve a considerable improvement over previous work on ensemble performance , showing the advantage of finetuning word embeddings .
table 2 compares the models for english - german , czech and french , compared to the previous state - of - the - art on both datasets . the results are presented in bold . for english , we see that the model using bow + gcn achieves gains on par with the best previous models on both languages . on the other hand , ggnn2seq ( beck et al . , 2018 ) achieves a gain of 2 . 8 points over the strong baselines on the german and czech datasets , and a drop of 4 . 6 points on the french one .
table 5 shows the effect of the number of layers inside our model on the performance improvement in dc . our model achieves state - of - the - art results , outperforming previous models across all three domains with a gap of 10 . 2 points in the performance gap .
table 6 compares the baselines of rcn with residual connections and baselines . adding rc information improves the results for both gcns , rcn models outperform both the base and the strong baselines indicating that residual connections do not harm the model performance . the difference between rc and la is less pronounced for dcgcn , but still suggests some reliance on residual connections .
the performance of our model compared to previous models on the 20 % held - out validation data is presented in table 4 . in general terms , we see that our dcgcn model ( dcgcn ) outperforms all the other models that do not use word embeddings with ellipsis , in that the size and type of similarity are the most important factors .
in table 8 , we show the ablation study results for the density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block degrades the model , and consequently , reduces the performance of dcgcn4 .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that , under the " - linear combination " and " - direction aggregation " models , the global nodes and the hierarchical clustering of the nodes achieve better performance than those using the global node . also , under " - coverage mechanism " we see that the decoder gets worse performance than the original embeddings . we notice that the domain - aware approach given the language modeling objective achieves better results overall , but still requires more data and time to encode .
table 7 shows the performance of our initialization strategies for different probing tasks . our framework establishes a new state - of - the - art on all three high - level metrics , and on all subtasks except for subjnum , where it performs relatively better than the glorot model .
the results are shown in table 5 . we observe that the h - cbow model outperforms both the cbow / 400 and the cmow model , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . the h - cmow model achieves state - of - the - art results , outperforming the original cbow model by 4 . 8 points in the semantic subtasks and surpassing the strong bshift baseline by 3 points .
the results are shown in table 5 . we observe that the cbow / 784 model significantly outperforms the cmow model and the cmp . model , indicating that cbow embedding cross - input models improves the interpretability of subj and mpqa . moreover , the model achieves state - of - the - art results on three of the four sub - criteria , outperforming the previous state of the art models on all metrics except for sst2 .
table 3 shows the relative improvements on the four downstream tasks that our models perform over the strong baselines . cmow shows a considerable performance gain of 26 . 2 % compared to cbow , while hybrid shows a gain of 25 . 6 % . the difference between cbow and cmow is less pronounced for sts13 and sts16 , but still suggests some performance improvement . when we switch from cbow to hybrid , we see a smaller performance gap between the two baselines , with cbow achieving gains of 14 . 6 % and 25 . 4 % over the performance of cmow . this suggests that the cbow model may be better adapted to the rapidly changing nature of the 21st century information distribution .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level domains , and on all subtasks except for sst2 . it improves the precision scores for subj and mpqa by 3 . 4 points in each direction ( micro - f1 ) and roc scores by 2 points in sick - r .
table 6 shows the performance of our method compared to state - of - the - art cbow models on the four downstream tasks . our cbow model improves upon the strong baselines by 3 . 8 points in the sts12 and sts13 tasks , while the cmow - r model improves by 2 . 2 points in sts16 .
the results are shown in table 5 . we observe that the compactness and length measures are the most important factors in the model ' s performance improvement over the strong baselines . subjnum and coordinv contribute significantly less than topconst , but contribute significantly more than cbow and cmow . cmow - r achieves state - of - the - art results , outperforming both cbow - c and topconst . it achieves the best score on concatenated keyphrases with a gap of 10 . 8 points from the last published results ( gillick et al . 2008 ) and 8 . 6 percent from the previous state of the art .
the best performances are obtained by the method known as cbow - r , which improves upon the strong baselines by 3 . 6 points in the sub - category of mpqa . it outperforms the strong baseline sick - e model by a noticeable margin .
the results are shown in table 2 . supervised learning models outperform all supervised and unsupervised methods except for the case of " name matching " . in all but one case , the system performs better than the supervised model when trained and tested on all loc and all misc datasets . in particular , the difference between the performance of " loc " and " misc " is much larger in the e + loc and e + misc datasets , which shows the extent to which the semantic information injected into the model by the additional cost term can be used to improve the model performance .
the results on the test set are shown in table 2 . in all but one case , the models show lower f1 scores than the best state - of - the - art models ( mil - nd , model 1 ) . in model 2 , the results are slightly higher than those of mil - nd ( model 2 ) . however , in model 1 , the difference between accuracies between supervised and unsupervised learning is much smaller . supervised learning reduces the error of the model in the f1 test set , but does not reduce the error in the name matching test set . to test the effect of the additional cost term on model performance , we compare the model with the model using only one type of learning algorithm , namely , the supervised learning method proposed by peyrard and gurevych ( see x4 ) . in both cases , the model performing poorly in the name matching task ( e . g . name matching , f1 - matching ) is better than in the other case when using only supervised learning . when using both supervised learning and automatic learning , the error reductions are much smaller ( p < 0 . 05 ) .
table 6 shows the ablation results on the nested models . our model obtains the best results , outperforming all the base lines with a gap of 10 . 86 % in terms of ref and 10 . 45 % on gen . by further adding entity nodes , the g2s model improves its performance on both nested and unsupervised models . it closely matches the performance of s2s with only 0 . 08 % absolute difference .
we compare our model against previous state - of - the - art models on the ldc2015e86 and ldc2017t10 datasets . the results , summarized in table 3 , are broken down in terms of performance on bleu , meteor and s2s . our model improves upon the previous state of the art on all three datasets by 3 - 4 points on average . g2s - ggnn , on the other hand , performs much worse than our model , achieving a lower performance on two of the four datasets ( ldc2017 and 2017t10 ) . as shown in the table , models trained on the golbeck et al . ( 2017 ) dataset perform similarly to those on the k2s dataset , but on the larger scale , with the exception of that on ldc 2015e86 . the performance gap between our model and the other two baselines is narrower than that on the smaller scale , indicating that the performance gain comes from a better model design .
table 3 shows the performance of our model on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model improves upon the strong lemma baseline by 3 . 60 points over the previous state - of - the - art .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state - of - the - art models with a gap of 10 . 6m get points from the last published results ( get + bilstm ) while surpassing our strong lemma by 3 points .
the results are shown in table 5 . we observe that the g2s model with the shortest sentence length outperforms all the alternatives with a margin of 3 . 51 % in terms of average sentence length compared to the baseline model .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) and the fraction of elements of the input graph that are completely missing ( miss , for the test set of ldc2017t10 . the token lemmas are used in the comparison . s2s outperforms g2s - gat , indicating that the model can rely less on syntactic gimmicks . though the model performs slightly better on the input dataset , it should be noted that it has been trained on a significantly larger corpus of reference sentences , making it more suitable for this task . gold refers to the reference sentences .
table 4 shows the sem and pos tagging accuracy using different target languages trained with different parallel corpus ( 200k sentences ) on a smaller parallel corpus . the results show that the pos features significantly improve the semantic performance of the model , and the sem feature also improves .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform the embedding method ( 95 . 06 % vs . 87 . 41 % ) , but do not exceed the upper bound of the baselines ( 81 . 00 % ) in terms of pos . unsupemb also outperforms word2tag in semantic accuracy , however it is inferior in syntactic analogy .
table 4 presents the system ' s performance on the four types of tagging accuracy . we observe that , let alone a reduction in performance , the system performs well on all aspects , with the exception of the accuracy in the pos tagging accuracy .
we can see from table 5 that pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model shows that the transfer learning method can significantly improve the generalization ability of the model , improving the three target languages ' semantic performance by about 10 % in the standard task formulation and to parity in the bi model .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary is minimal , however we see significant difference in the rate at which the attacker reaches the end goal . sentiment and gender features are the most difficult ones to detect , as it takes a considerable amount of effort to pick out . age and gender are the two less difficult groups to detect than race and gender , however the attacker is still much more accurate .
accuracies are shown in table 1 . overall , the models show considerable accuracy improvement over the best baseline ( pan16 ) , confirming the value of gender - neutral pronouns in the low - supervision settings . gender bias also seem to have little effect on the performance of the models , indicating that gender stereotypes do not impact the performance .
table 2 shows the results for the balanced and unbalanced data splits . the results show that the gender and racial disparities are the most severe in the task prediction tasks , while age and sentiment are the less severe . interestingly , the presence of the word " b * tch " in the sentence prediction results in less than 1 % of the data splits , indicating that gender bias is less prevalent in the early stages of prediction .
the performance of these models on the adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in the leakage metric , indicating that the presence of some highly productive features can help the model predict responses based only on superficial cues . gender and race features are particularly difficult to detect , as their accuracy is low compared to other protected attributes . finally , age and sentiment are the only two groups that are relatively stable , showing that the features discussed in the previous section hold high predictive value for the task being addressed .
the results are shown in table 6 . rnn encoders significantly outperform [ gillick et al . 2018 ] the weak embeddings for the three protected attributes . the difference in accuracy between the weak and strong encodings is less pronounced for rnn , but still suggests some vulnerability in the design .
we compare our model with previous state - of - the - art on the following three similarity test sets : the ptb base , wt2 base and finetune test sets , and the wt2 model . the results , summarized in table 2 , are broken down in terms of performance on the similarity test set and the dynamic test set , where our model obtains the best results . our model improves upon the previous state of the art on all three metrics by 3 - 4 points on average . note , however , that the difference between the base and the final score on wt2 is narrower than on ptb , indicating that finetune and tuning contribute differently to the model performance . the size and type of training data available in the data augmentation set are reported in yang et al . ( 2018 ) and ( 2018 ) , while the number of models in the lrn dataset is smaller than the lstm dataset ( 13m vs . 15m ) . as shown in the second part of the table , combining the training data from the two sets improves the results on both ptb and wt2 . the model performs better on the smaller scale compared to the comparison set on the larger scale , adding dynamic features improves performance on both datasets , but does not improve the small scale lrn model by much .
the results in table 3 show that the training time and the average time to solve each relation are the most important factors in the model performance . the model performs well in terms of both base acc and time metrics , with an absolute improvement of 2 . 5 % on the base acc metric compared to rocktäschel et al . ( 2016 ) . in addition , the model performs faster than the lstm model when trained and tested on the same dataset ( table 3 ) . table 3 compares the performance of our model with other approaches that allow boundary expansion . we see that the gru model is comparable , but does not outperform our model , in part due to the large size of the training data set ( 6 . 41m vs . 8 . 36m ) . sru also performs slightly worse than our model ( 5 . 28 vs . 5 . 59 % ) on the base acc metric , indicating that it is unable to learn the task to a high degree . when training and evaluating the model , we notice small differences in performance between various approaches , as shown in the second group of table 3 , we concatenate the training and test set performance of the different approaches . the smaller size of training data suggests that some approaches may not be suitable for production use .
table 3 shows the test set for amapolar err , yelppolar time and yahoo time , compared to zhang et al . ( 2015 ) . the results are presented in bold . as can be seen , in both datasets , the average time taken to compute an err and time to compute a yelp time is relatively consistent , while the difference between amafull and full time time is much smaller . table 3 compares the performance of these models on the four datasets with the previous state - of - the - art . we observe that the lstm model performs better than both the atr and gru model on all four datasets , indicating that the translation quality obtained by our model can be further improved with a reasonable selection of the lexical resource from which the model was derived . also , we notice that the sru model performs slightly worse than gru on the three datasets suggesting that it is unable to learn the task to a high degree .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the model makes use of a fast recurrent learner network architecture and multiheaded attention system , and learns sentences in seconds compared to the time taken to train and decode one sentence on the newstest2014 dataset . gnmt also outperforms the other methods with a large margin . although the number of parameters used to encode one sentence is small , the time to decode a sentence is considerable compared to other methods , as shown in table 3 . lrn also performs significantly worse than atr and sru , indicating that neural network approaches are more effective in generation of sentence decoders . though sru has the advantage of training in a smaller training set , it is still comparable with atr in terms of decoding time . in addition , the model performs much worse than gnmt and lrn when trained in tesla p100 .
we present the exact match / f1 - score on the squad dataset in table 4 , it can be seen that all the methods we considered had decent match rates , but the difference between elmo and rnet * was greater than expected . besides , the gru model had lower f1 score compared to lrn , indicating that more parameters were required to improve the model . table 4 also highlights the differences in parameter number between base and model architecture . although the lstm model had the most parameters , it was unable to improve significantly over the lrn model , indicating the importance of the number of parameters considered in parameter selection . sru also had lower match rates than gru , though it outperforms lrn in terms of match rate , we found that it was better than atr on two out of the four scenarios ( table 4 ) it is possible to improve upon this by increasing the parameter number of base .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the model achieved 90 . 94 % f1 on average compared to the previous best performance of 90 . 35 % by lample et al . ( 2016 ) . although the number of parameters in our model is small , it is comparable to other sophisticated neural models which achieve higher f1 scores on similar ner tasks . at the same time , our sru model ( sru ) achieves the highest score ( 88 . 56 % ) compared to lrn ( 87 . 35 % ) . gru also achieves the best score ( 89 . 61 % ) compared with atr ( 87k ) . table 7 shows that all the neural models that can be considered as alternatives to lstm perform similarly on the ner test set . however , lrn has the advantage of training on a larger corpus .
table 7 compares the performance of our model with the base and elrn models on the snli and ptb tasks . with the base setting , our model obtains 85 . 56 % accuracy on snli task and 169 . 81 % on the ptb task . by contrast , glrn ( 84 . 72 % ) gives a performance gain of 2 . 26 % on both tasks .
table 2 presents the system and sentence recognition results . oracle retrieval is more accurate than human on all systems with two exceptions . sentiment detection is more difficult than system detection , however it is comparable with human on two of the four systems . system and sentence detection are both more accurate on the oracle dataset , with the exception of # word , which is broken down in table 2 . retrieving the word clusters from a database is relatively straightforward , with only a minor drop in performance compared to human . word acquisition is more challenging than sentence acquisition , with a 17 . 08 % drop on oracle compared to system . mentions are more difficult to detect than those on sentence acquisition . we show the r - 2 and mtr scores of all systems , comparing them with each other .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates high accuracy . among all the systems , seq2seq ( 25 . 6 % ) and candela ( 30 . 2 % ) achieves higher performance on grammatical accuracy and appropriateness ( appr ) , while retrieval ( 50 . 0 % ) achieves lower performance on content richness ( k 2000 ) . although seqseq2seq has done great job on domain prediction ( retrieving 50 . 2 % of data from supporting documents in the past , its performance on syntactic and semantic information is still significantly worse than most of the automatic systems . h & w hua and wang ( 2018 ) shows that neural network approaches are comparable , but do not outperform the best human model , likely in part due to the smaller size of the data set .
the results are shown in table 6 . our model outperforms all the base the performance on the three domains that we base it on is significantly worse than the previous state - of - the - art . on the other hand , our model performs better than both the best previous approaches on three of the four domains : dsim , slqs , tf and docsub . the difference is most prevalent in the ted talks domain , where our model ( en ) and pt ( pt ) achieve better performance than those of the other two baselines .
the results are shown in table 3 . we observe that for ted talks , the model performs comparably to both the original embeddings and the dsim model , with the exception of the case where it performs slightly better . on the other hand , for europarl , we see that performance is considerably worse . the difference between the en and pt scores is most prevalent in relation to dslqs , when we compare these models on the corpus and tf datasets , we find that the former performs better than the latter on all but one of the four datasets ( except for the case of the slqs dataset , when it is tested on the tf dataset ) . table 3 shows that for the three datasets , the difference between en , pt and ted talks is less pronounced for the latter , but still suggests that there are some differences in performance between the two baselines .
the results are shown in table 5 . we observe that for ted talks , the model performs comparably to both the original embeddings and the dsim model , with the exception of the case where it performs slightly better . on the other hand , for the two datasets , the gap between the en and pt scores on the slqs and tf subsets is much smaller . when we compare these models on the corpus and docsub datasets , we find that the former performs better than the latter . this is reflected in the average score of p ( en ) and r ( pt ) on the three datasets , while on the latter , it is slightly higher than the former . table 5 shows that the semantic features extracted by dsim and tf are comparable , but do not exceed the performance of the former , likely in part due to the smaller size of the dataset .
the most representative metrics for each category are the depth metrics and the average depth . according to the table , europarl has the best average depth of 11 . 05 , while docsub has the worst averagedepth . the number of tokens per row is 1 , 588 , which means that more than half of the tokens in the dataset are actually missing .
we show the performance of our model on the five metrics in table 1 . the first set of metrics shows that our approach significantly improves the generalization ability of the feature set . europarl now has the best averagedepth and averagedepth metrics , while docsub has the worst performance . according to the table , depth is the most important metric for feature extraction . our model establishes a new state - of - the - art on all five metrics , lowering the performance gap with df and slqs by 10 % on average .
we present the performance ( ndcg % ) on the validation set of visdial v1 . 0 . the enhanced version of our model ( lf ) outperforms the baseline model by 3 . 42 points in terms of ncg % compared to the strong base case . moreover , it achieves a better result on question type , answer score sampling , and hidden dictionary learning , while matching the performance of the strong baselines . r0 , r1 , r2 , r3 denote regressive loss , weighted softmax loss , binary sigmoid loss , and generalized ranking loss , respectively . as shown in table 1 , the enhanced model performs better than the baseline on all three aspects .
the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set is shown in table 2 . note that only applying p2 indicates the most effective one ( i . e . , the hidden dictionary learning model ) , indicating that p2 is the most efficient way to learn the dictionary .
we observe that the hmd - f1 model significantly outperforms the wmd - bigram model and the hmd - recall model , indicating that precision on hard and soft alignments is relatively high . the results are shown in table 5 , the hmd models perform similarly on both datasets , with the exception of the case of " sci - en " .
the results are shown in table 5 . the first set of results show that the baselines set up by meteor + + and ruse ( * ) give decent results , but are significantly worse than the best baseline set by bertscore ( f1 ) . the second set shows that the method can significantly improve performance when using smd + w2v , sent - mover and word2v are combined , giving a significant improvement ( bertscore - f1 ) over the baseline score of 0 . 716 on the direct assessment metric and 0 . 866 on the syntactic verification metric .
the results are shown in table 5 . the first set of results show that the semantic threshold for bertscore - f1 is relatively high while the numerical threshold for sfhotel is low . when we add the w2v baseline , our model achieves the best results with a f1 score of 0 . 176 on the boundary test set , significantly outperforming the previous state - of - the - art on both metrics .
word - mover achieved the best results with a precision of 0 . 949 on the m1 metric and 0 . 866 on m2 metric , both when using the spice and the leic embeddings . when using the bertscore - recall baseline , the accuracy of the word - mover is improved by 0 . 723 points , but still performs substantially worse than the baseline on m1 and m2 . the semantic threshold on leic is set at 0 . 594 , which indicates that the model performs better when using only one type of clustering feature , namely , that the semantic representation generated by the word is localized on a single entity . we notice that the clustering quality of the smd model is relatively high compared to that of wmd - 1 , indicating that features derived from elmo are important for the model to perform well . sentiment classification is very similar , with the difference being less pronounced for smd .
we further compare these models with the baseline models on the hidden test set of simnet . adding the word " para " improves the results for all models except for those using 2d language . for example , m1 and m2 achieve better results with the addition of para - para than they do with language - neutral models . m3 and m4 achieve the best results with a gap of 3 . 8 points between the baseline and the best new model ( m6 ) with the help of lexical and syntactic improvements .
table 3 shows the transfer quality and semantic preservation results . our model establishes a new state - of - the - art on all three high - level metrics , and on all subtasks except transfer quality . semantic preservation and transfer quality are the most stable aspects , while fluency is the weakest . we observe that yelp significantly outperforms google translate , both in terms of transfer quality ( m0 , m2 , m7 ) and length ( m7 ) . in addition , the model performs slightly better on semantic preservation . to test the contribution of parameter sharing , we compare our model to other models that do not use word embeddings such as pascal - voc or word2vec topics .
table 5 presents the results of human evaluation for validation of the three metrics . we use 200 examples for each metric for validation . the results show that both the human evaluation method and the method by itself verifies the accuracy of the summaries ( acc ) and the human ratings of semantic preservation ( fluency ) . however , the difference between human evaluation and machine evaluation is less pronounced for acc , indicating that human evaluation methods may rely on superficial cues .
the results are shown in table 6 . the first set of results show that when only using the word " para " , the model performs better than both the models using syntactic and semantic word embeddings . the second set shows that combining the two types of syntactic features benefits the model , improving performance in all but one case .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results shown in table 6 show that our model achieves higher acc than previous work on similar sentiment transfer tasks . however , the difference between transfer and transfer using the standard style embeddings is less pronounced , with the transfer of 22 . 3 % of sentences achieving acc compared to prior work ( p < 0 . 001 ) . further , transfer with lm instead of delete / retrieve achieves the highest acc , but is worse than simple transfer . we show the results of unsupervised and supervised models using the same set of features , but using different classifiers for each transfer row . when using the multidecoder approach , the multi - decoder model achieves lower acc than the model using the slot - based learned classifier . the results of yun et al . ( 2018 ) show that combining the features of transfer and delexcalization , combined with the learned classifiers , gives a better result than transfer using only human references . finally , our model ( yuan et al . , 2018 ) achieves a higher acc level than the previous state of the art .
in table 2 , we report the percent of reparandum tokens that were correctly predicted as disfluencies and the average length of repetition tokens . reparandum length is the average of the number of tokens in a sentence , and is the length of the repetition sentence divided into terms of length 1 , 2 , 3 and 8 .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . it is clear from table 3 that the repair tokens contain a significant fraction of tokens , however , the majority of tokens belong to the repair category , making it difficult to solve these cases . similarly , the amount of tokens containing a function word is small but significant , constituting a large fraction of the total predicted disfluency .
in the en - de news commentary section , we compare the models trained on tweets from one domain with tweets from other domains , both for english ( the single model ) and spanish ( the double model ) and german ( the german model ) . the results are shown in table 2 . in both languages , the performance gap between the models based on the text alone and the innovations is much smaller . however , for english , the gap is much larger . the average number of tweets per second between the two models is significantly larger , with text alone contributing more than innovations . when using only raw data , the model performs slightly better than the single model , but still performs substantially worse than the late model . adding in the text and domain innovations improves performance for both english and german , but does not improve the dev performance .
we observe that our model exhibits high accuracy comparable to the state - of - art algorithms on the fnc - 1 test dataset . our model achieves 28 . 43 % accuracy on average compared to the accuracy achieved by cnn and rnn ( 24 . 43 % ) in terms of relation extraction . moreover , it achieves a lower percentage of disagreement compared to self - attention embeddings , indicating that the accuracy obtained during sentence prediction is more accurate .
table 2 compares the performance of the four methods that we base our model on . the attentive neuraldater model significantly outperforms all the previous methods , and achieves higher accuracy than any previous approach . by a large margin , it even outperforms the best - performing ac - gcn model .
table 3 compares the performance of the component models with and without word attention . with word attention , neuraldater ( which relies on graph attention ) shows marked performance improvement , but still performs substantially worse than ac - gcn ( 63 . 2 % vs . 63 . 8 % ) . with attention , however , neuraldater shows a slight improvement , improving its performance to 63 . 9 % compared to 62 . 6 % in the comparison set with respect to the baseline neural model .
the results are shown in table 1 . the most representative models are the dmcnn , jrnn and jmee models . in general terms , all models perform well , with the exception of cnn . embedding and t - test embeddings achieve better results overall , but do not exceed the upper boundary of what is considered " safe " . cnn and jrnn achieve higher accuracies than both argument and sentence prediction . triggering and argument prediction achieve higher performance than any other stage , but are still inferior to any other approach . on the other hand , when combining all three stages , the jmee model achieves the best results , clas s performing better than both the cnn model and the jrnn model .
table 1 presents the results on event identification and event classification . our method establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except for argument identification . the proposed method significantly improves the joint f1 score of argument identification and classification by 10 . 8 points in the standard task formulation . it achieves the best results with a f1 - score of 68 . 7 on the cross - event dataset , which shows the significant improvement from the state of the art on previous approaches .
the results are shown in table 5 . the first group shows that english - only and spanish - only models perform better than both the original and the shuffled - lm models , while the second group shows the performance of the fine - tuned model . when we add in the acc metric and the dev acc metric , the results of both languages are slightly better than the original ones . however , the accuracy gap between english and spanish is much narrower , showing that the difference between the two languages comes down to semantic choice . fine - tuning gives a performance gain of 2 . 36 points over the original model , but still not enough to break the trend . we notice that the slightly better results obtained by shuffling the lm comes from a better model architecture , indicating that more effort is required to learn the correct pronunciation of each word .
results on the dev set and the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model improves upon the strong baselines by 3 . 8 points in the standard task formulation and by 4 . 2 points in fine - tuned mode .
the results shown in table 5 show that fine - tuning the l2r model reduces the error of the dev model compared to monolingual model using only one type of gold sentence , namely , that the gold sentence contains two words and two spaces , and that the accuracy is greater in the dev set when the gold sentences are only coded in one language ( hence , there is no advantage to using two languages for the test set ) .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline model to the current state - of - the - art is statistically significant ( p < 0 . 05 ) and r = 0 . 61 , indicating significant improvement in the performance by using the best performing feature set .
we show the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset in table 5 . the improvement from the baseline model to the type combined model is statistically significant ( p < 0 . 05 ) and r = 0 . 35 , which shows that the ability to combine gaze features with word embeddings improves the interpretability of the model .
in table 1 , we apply the hpcd and glove - extended embeddings described in faruqui et al . ( 2015 ) to wordnet 3 . 1 . the results show that the lstm - pp model significantly outperforms the ontolstm model with a gap of 10 . 7 points from the last published results ( hpcd , 88 . 8 % ) on the ppa test set . by further adding syntactic skipwords , the model achieves a marginal improvement of 2 . 7 % on par with the original wordnet model ( 88 . 7 % ) but still performs substantially worse than the original ( 84 . 3 % ) .
results shown in table 2 show that the hpcd model outperforms the original lstm model with features coming from various pp attachment predictors and oracle attachments . the model further improves upon these results by adding ontolstm - pp as an attachment prediction feature and further improving the accuracy by 0 . 59 points .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that the model achieves a significant improvement in precision ( p < 0 . 05 ) compared to the original model ( p < 0 . 01 ) with the removal of context sensitivity .
in table 2 , we report the bleu % scores of adding subtitle data and domain tuning for image caption translation . the results show that the domain - tuned model performs better than the model using en - de and mscoco embeddings . the model using domain tuning achieves the best results with a gap of 3 . 7 % in blueu % from the baseline . adding subtitle data improves the results for both languages , but the improvement is narrower than for en - fr : the ensemble - of - 3 model achieves a marginal improvement of 2 . 2 % compared to the baseline model .
we observe that domain - tuned models perform better than those without . the hoco model achieved the best results with a margin of 3 . 8 points over the strong baselines on the en - de model , and a gap of 2 . 5 points with the strong lm model .
table 4 shows the bleu scores of the models using only one type of automatic image captions and marian amun . the results show that the model using multi30k embeddings achieves better results overall , with the exception of en - de , where it gets a slight improvement . the model using only autocap 1 and 5 achieves the best results . adding automatic captions further improves performance for both models , but the improvement is less pronounced for mscoco17 , indicating that more captions are required to improve the model .
the results in table 5 show that the enc - gate and dec - gate strategies give better results than en - de and mscoco17 , respectively , when using the multi30k + ms - coco + subs3mlm model . the difference is less pronounced when using detectron mask , but still suggests some advantage to using a decoder . when using transformer , the model achieves the best results with a bleu % of 62 . 08 % , slightly higher than the previous state - of - the - art . further , using the domain - aware decoderron mask improves performance by 4 . 53 % over using plain msoco .
we observe that the multi - lingual features model performs better than the text - only model in both languages . the results are slightly worse than those of the en - de model ( gillick et al . , 2016 ) when we switch from the word " visual features " to " language features " . the results of the " + ensemble - of - 3 " model compared to " text - only " show that the visual features alone do not harm the model , but do harm the performance negatively when combined with the other linguistic features . subsequently , we switch to using the " detailed word embeddings " model instead , which improves the results for both datasets .
the results are shown in table 5 . in general terms , the models perform comparably to each other on the test set , with the exception of en - fr - rnn , which is slightly worse than the others . transforming the word " transgender " into " rnn " improves performance for both models , but does not improve performance for " smt " . in addition , the model using mtld reduces repetition , as measured by the drop of precision in the ttr scores over the baseline .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the total number of sentences for each language pair is 1 , 467 , 489 , 499 , 487 and 7 , 723 .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the performance gap between en – fr and spanish is minimal , however it is significant compared to english , both for the original and the second variation .
automatic evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems generate strong baselines comparable to the best state - of - the - art models ( cf . table 5 ) . however , their performance is significantly worse than those of the best - performing models ( paired t - test ) when using the french word embeddings . the ter evaluation scores are slightly higher than the bleu evaluation scores , indicating that ter features are a better complement to the syntactic features provided by the french language . when using english , the evaluation results are slightly less clear , but still consistent with the strong french model , showing the advantage of using english - based partridges .
the results on flickr8k are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised using the learned rsaimage embeddings . the mean mfcc score of segmatch match is 0 . 2 , slightly higher than the mean of 0 . 0 for vgs . mean recall @ 10 is low but high compared to rsaimage , indicating that the model performs well in terms of recall .
the results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised using the learned rsaimage embeddings from ng et al . ( 2017 ) . the average recall @ 10 level is 0 . 9 , the mean mfcc level is 1 . 414 and the mean chance level 0 . 0 are the highest .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the dan model shows that it is easier to turn on a on a screenplay that is at the edges than the edges , making it more suitable for use in production . cnn also shows improvements with the addition of paragraph selection . rnn shows a decrease in performance with respect to screenplay turns since the original embedding model only works on sentences that are edges - aligned , so the difference between the two is less pronounced . we report further examples in the appendix .
part - of - speech changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . however , the percentage of instances in the correct sentence has not changed , indicating that the model has not learned to rely less on syntactic analogy . rnp shows a decrease of 3 . 5 % in the accuracy rate since the last experiment , though still performing substantially better than cnn . verbs and adjectives have a higher percentage of occurrences , indicating the importance of combining syntactic and semantic information . nouns and verbs show a decrease in accuracy of 2 . 5 % , but a gain of 4 . 7 % on adjectives . punctuation is the only area where the rnn model shows a drop of more than 2 % .
the numbers indicate the changes in percentage points with respect to the original sentence . flipped to positive sentiment flips the sentiment score significantly , and flipped to negative sentiment flips it significantly . the flipped sentiment scores show that the ability to flip negative labels induces the generation of stronger positive sentiment .
table 2 compares the results of different approaches for positive and negative sentiment . for positive sentiment , we see that our approach outperforms all the other approaches with a large margin . on the negative side , our approach achieves a positive f1 score of 98 % , but does not exceed the threshold of 98 % .
