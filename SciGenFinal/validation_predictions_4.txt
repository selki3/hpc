table 2 shows the performance of the treelstm model on our recursive framework , fold â€™ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the other hand , the iter and recur approaches perform better on training and inference .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . table 1 shows the performance of the treernn model with different tree balancedness in terms of the number of instances for each batch size . the best performance is obtained when the size of the batch is increased from 10 to 25 .
the max pooling strategy consistently performs better in all model variations . the results are shown in table 2 . conll08 and ud v1 . 3 outperform all the other models with different representation , with the exception of softplus , which achieves the best performance with a dropout probability of 0 . 15e + 01 points .
table 1 shows the effect of using the shortest dependency path on each relation type . the macro - averaged approach achieves the best f1 ( in 5 - fold ) with sdp , while the model - feature approach obtains the second - best f1 . we observe that the macro - adaptive approach has the greatest effect , as shown in table 1 .
the results are shown in table 3 . we observe that the best performing y - 3 model is the one with the highest percentage of f1 and f1 50 % success . it achieves a f1 score of 67 . 58 % on average , which is slightly higher than the previous best performing model ( 67 . 32 % ) and slightly better than the second best performing one ( 66 . 59 % ) . the best performing state - of - the - art model is also the one that achieves the best performance on the f1 test set . the performance of this model is comparable to that of the best - performing y - 2 model .
the results are shown in table 1 . the mst - parser achieves the best performance on all three test sets . it achieves 100 % accuracy on all test sets except for one , where it achieves 50 % accuracy . on the other hand , on the other two test sets , it achieves 100 % .
table 4 shows the performance of the lstm - parser and stagblcc on the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference in performance between the two systems is less pronounced on the paragraph level , indicating that the difference is due to the larger number of sentences .
the results are shown in table 1 . the results show that the original and clean versions of the system outperform the original on all metrics except for meteor and rouge - lstm . when the original is cleaned , the system performs better than the original . however , when the clean version is added , it performs worse . this is evident from the difference in performance between the two systems . in particular , the original outperforms the original when the system is added and cleaned .
table 1 shows the results for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . our cleaned version shows a significant drop in the number of distinct references , but a significant improvement in the ser .
the results are shown in table 1 . we observe that the original and tgen + embeddings outperform sc - lstm in all but one case . the difference is most pronounced in the training set , where the original embedding performs better than the original . however , the difference is not statistically significant in the test set . it can be seen that the difference between the two systems is due to the fact that the tgen embedding mechanism is more sensitive to the training data .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the results are shown in table 4 . adding , missing , and disfluency are the most common types of errors in tgen . disfluency is the smallest type of error we found , and is most common in the missing and wrong values . it is also the most frequent type of disfluence we found .
table 3 shows the results of our approach on the performance of graphlstm and snrg . our approach outperforms the previous state - of - the - art approaches by a significant margin . the results are shown in table 3 . we observe that our approach is comparable to previous work ( konstas et al . , 2017 ) .
table 2 shows the main results on amr17 . our model achieves 24 . 5 bleu points . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves only 21 . 5 points . table 2 also shows that our model achieves higher performance than the best performance of the other approaches .
table 1 presents the results of our experiments on english - german and czech - czech datasets . the results are presented in table 1 . we observe that the best performing models are the ones that use bow + gcn ( bastings et al . , 2017 ) and seq2seqb ( beck et al . 2018 ) . the best performing single - domain models are those that use the best performance of the best - performing multi - domain approaches . these models use a combination of birnn and gcn to achieve the best results .
table 5 shows the effect of the number of layers inside a block on the performance of our model . we observe that our model outperforms the previous state - of - the - art model by a significant margin . the effect is most pronounced in the second and third blocks , where our model achieves the best performance . in the first and second blocks , the effect is less pronounced , but still significant .
table 6 shows the results of our model on the baselines . the results are shown in table 6 . our model outperforms all baselines except for dcgcn4 ( 36 ) in terms of the number of connections with residual connections , which shows that the residual connections are the most important factor in the performance of the gcn .
table 4 shows the results of our approach on the performance of the dcgcn model . the results are shown in table 4 . our approach outperforms the previous state - of - the - art approach by a significant margin . we observe that our approach achieves the best performance on all metrics except for d and b .
table 8 : ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . the results show that removing the dense connections in the i - th and ii - th blocks significantly reduces the number of connections . we observe that the dcgcn4 model obtains the best ablation results . it achieves the highest ablation performance with a precision of 25 . 8 % compared to the previous state of the art .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the coverage mechanism used by the embeddings in the decoder achieves the best performance , with the exception of the dcgcn4 module , which achieves the worst performance .
table 7 shows the performance of the initialization strategies on probing tasks . glorot achieves the best results on all three tasks . it achieves the highest performance on the n ( 0 , 0 . 1 ) and wc tasks , while our paper achieves the second highest performance . our paper achieves a better performance than the previous work on all the three tasks except for the length task .
table 3 shows the performance of the h - cmow and h - cbow approaches with respect to concatenated embeddings . the results are shown in table 3 . the hcmow approach outperforms the previous state - of - the - art approach by a significant margin . it achieves the best performance with a precision of 87 . 2 % compared to the previous best performance of 86 . 9 % . the results show that the best performing approach is based on the concatenation of the best - performing method with the worst - performing one . it can be seen that the performance gap between the best and worst performing approaches is due to the large difference in the performance between the two baselines .
the results are shown in table 1 . we observe that the cmow / 784 method outperforms all the other approaches except cmp . it achieves the best results on all metrics except for subj and mrpc . the best performance is achieved by the hybrid method , which achieves the highest performance on subj , mrpc , trec and sick - e .
table 3 shows the results on unsupervised downstream tasks attained by our models . the results are shown in table 3 . hybrid achieves the best performance on all the downstream tasks , with the exception of sts15 , where it achieves the second - best performance . on the sts13 and sts16 tasks , our model achieves the highest performance with respect to hybrid .
table 8 shows the results for initialization and supervised downstream tasks . glorot achieves the best performance on all three tasks , with the exception of mpqa , where it achieves the second - best performance . our paper achieves the highest performance on the supervised downstream task .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the cmow - r method outperforms the cbow - c method on all the tasks except for sts13 and sts14 , where the cmow is superior .
table 3 shows the performance of the cmow - r method compared to the previous state - of - the - art cbow method . the results are shown in table 3 . the cmow method outperforms the previous best state of the art cbow by a significant margin . it achieves the best performance on all metrics except for length , which is achieved by a margin of 0 . 6 points . in addition , it achieves the highest performance on the subjnum metric , which shows that the best performing method is the one with the highest concatenation rate . on the other hand , it is the best on the topconst metric .
table 3 shows that the cmow - r method outperforms all the other methods except for mpqa and trec . it achieves the best performance on all three datasets except for sick - e , where it achieves the second best performance .
table 3 shows the results of our supervised and unsupervised learning approaches on the e + loc and misc datasets . the results are shown in table 3 . our supervised learning approach outperforms all the other approaches except for Ï„mil - nd , which shows that it is more suitable for the task at hand . in particular , our supervised learning system outperforms the other two approaches in all but one case . for example , it achieves a performance improvement over the previous state - of - the - art in all cases except for the name matching task .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . we observe that the supervised learning approach outperforms both the automatic and supervised learning approaches on all f1 and e + f1 scores , with the exception of the name matching set , where the automatic learning approach performs better . however , the results are slightly worse on the e + f1 test set . in the case of name matching , the automatic approach performs slightly better than the supervised one , with a f1 score of 42 . 42 Â± 0 . 59 vs . 42 . 38 Â± 1 . 15 and a Ï„mil - nd score of 38 . 57 Â± 0 . 68 vs . 37 . 57 . the difference is due to the large difference in f1 scores between the two approaches . it can be seen that the automatic model performs better in all cases except for name matching . on the other hand , the supervised model performs slightly worse than the automatic one on all but one of the test sets .
table 6 shows that the g2s - gat model outperforms all the other models in terms of the number of instances in which the model is included in the dataset . the model is significantly more likely to be trained on the gat dataset , as shown in table 6 . however , it is less likely to perform as well as the other two models on the s2s dataset , which is shown in fig . 6 .
table 3 shows the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s - gat model outperforms the previous state - of - the - art models in all but one case . it achieves the best performance on both datasets , with the exception of konstas et al . ( 2017 ) , which achieves the second best performance .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model achieves the best performance on the test set , with a bleu score of 31 . 23 % higher than the previous state of the art .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are summarized in table 4 . we observe that the bilstm model performs better than the get model on the development set , with a performance improvement of 3 . 6 % over the previous state - of - the - art model . moreover , the performance gain is larger than that of the geb model , which shows that the combination of the two is beneficial .
the results are shown in table 1 . the g2s - gin model outperforms the gat model on all metrics except for the length of the sentences . it achieves the best performance on all but one metric , with a gain of 0 . 51 % over the previous state - of - the - art model . on the other hand , it achieves the worst performance .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - ggnn outperforms the s2s model by a large margin , as shown in table 8 . the difference is most pronounced in terms of the number of tokens missing from the generated sentences .
table 4 shows the pos and sem tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . the pos tagging accuracy is significantly higher than the sem accuracy on the smaller corpus , indicating that the features are more useful for the task .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . our unsupervised embeddings outperform both the baselines by a significant margin . word2tag classifier achieves the best performance with a baseline of 91 . 11 % and a higher bound of 95 . 06 % .
table 3 shows the results of our model on the pos and sem datasets . the results are shown in table 3 . our model achieves the best performance on all three datasets , with the exception of the pos dataset , which achieves the highest performance on the sem dataset . we observe that our model outperforms the previous state - of - the - art model on all metrics except for pos .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the uni and bidirectional embeddings are more accurate than the res encoder , but are less accurate than bi and residual . in addition , the uni embedding improves the pos tagging accuracy by 0 . 9 points over the res embeddling , but does not improve it by as much over the bi embedding .
table 8 shows the attacker â€™ s performance on different datasets . results are on a training set 10 % held - out . the results are shown in table 8 . as expected , the attacker performs significantly worse than the corresponding adversary on all datasets except pan16 .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . we observe that gender and age are the most important predictors of sentiment accuracy , followed by race and age . gender and age predict sentiment accuracy significantly better than age and gender .
table 2 shows the results of our model on the unbalanced and balanced data splits . the results are shown in table 2 . we observe that gender and age are the most likely to leak , followed by gender and race . gender and race are also the most frequently reported to be the ones to have the most leakage .
table 3 shows the performance on different datasets with an adversarial training . the results are shown in table 3 . in pan16 , we observe that gender and age are the most difficult features to detect , with the gender - neutral features having the highest performance . gender and age also have the highest leakage , which indicates that these features are difficult to detect . on the pan16 dataset , we see that the gender and race features have the most significant impact on the performance , with a drop of 3 . 5 % in performance .
table 6 shows the performance of the protected attribute with different encoders . rnn and guarded embeddings outperform rnn in all cases except for leaky , which shows that the leaky embedding is more difficult to detect . the rnn encoder outperforms the guarded encoder in both cases .
table 3 shows the results of our work on the ptb and wt2 baselines . the results are presented in table 3 . we observe that the work performed by yang et al . ( 2018 ) is comparable to previous work ( yang et al . , 2018 ) in terms of performance on both base and dynamic ptb baselines , with the exception of lrn , which achieves the best performance on all baselines except for finetune . our work achieves the highest performance on the wt2 base and finetune baselines with a performance gap of 2 . 7 % over previous work . it is also comparable to the work of yang et . al . , ( 2018 ) . in addition , our work shows that the lrn baselines outperform the previous work with a margin of 3 . 3 % over the previous state - of - the - art work .
table 1 shows the results of our work on the lstm and gru models . the results are presented in table 1 . we observe that the work performed by the gru and sru models is significantly better than the work done by the previous work . however , the results are not statistically significant . our work is comparable to previous work by rocktÃ¤schel et al . ( 2016 ) in that it achieves the best performance on all metrics except for the base time . it is also comparable to the work of the previous study , which shows that gru achieves the highest performance .
the results are shown in table 1 . the results of zhang et al . ( 2015 ) are summarized in table 2 . our model outperforms the previous work by a large margin . it achieves the best results on all metrics except for yelp , where it achieves the second - best performance .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task . the gnmt model outperforms all the other approaches except for the olrn model , which is slightly better than the sru model in the case of the german translation task ( table 3 ) . however , it is slightly worse than the lrn model on the english translation task , as shown in table 3 .
table 4 shows the results of our approach on the squad dataset . the results are shown in table 4 . we observe that our approach outperforms the previous state - of - the - art lstm and lrn models in terms of exact match / f1 score . however , our approach is slightly worse than previous work ( wang et al . , 2017 ) . the difference is due to the large number of parameter number of base . our approach achieves the best results with the parameter number in the range of 2 . 14m to 2 . 67m . it achieves a f1 score of 76 . 41 / [ bold ] 79 . 83 and a gru score of 75 . 14 / [ f1 ] 83 . 83 , respectively .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and sru denotes the results reported by the model . lrn and gru have lower f1 scores than sru and atr on the ner test set , indicating that lrn is more suitable for the task . sru has higher f1 performance than atr .
table 7 shows the results of the test on snli and ptb tasks with and without base + ln setting . the results are shown in table 7 . lrn outperforms elrn on both the snli task and the ptb task by a large margin .
table 1 shows the performance of human and human - generated sentences on the oracle retrieval test set . human and human generated sentences are significantly more likely to be sent than those generated by oracle . sentences generated by human are much more frequently received by oracle than by human generated ones . in addition , human generated tweets are more frequently sent by oracle - generated tweets . the results are shown in table 1 .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with âˆ— ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the human evaluation is more accurate than the automatic system . the best performance is achieved by candela ( 2018 ) , which is the only automatic system to achieve the best performance on all three metrics . seq2seq ( 2018 ) achieves the second - best performance on the content richness scale , with a standard deviation of 2 . 2 points .
the results are shown in table 1 . we observe that the best performance is obtained by using the best - performing embeddings from the ted talks corpus ( ted talks ) and from the europarl corpus ( europarl ) . in particular , we observe that when we use the best features of both datasets , we get the best results . however , we also observe that these results are not statistically significant when using the worst - performing features of the two datasets . the best performing features are those of the docsub and docsub clusters .
the results are shown in table 1 . we observe that the best performance is obtained by using the best - performing embeddings from the previous work ( lang et al . , 2017 ) and the best performing ones from ted talks . europarl outperforms all the other baselines except for df and docsub , where it is slightly better than both of them .
the results are shown in table 1 . we observe that the best performance is obtained by using the best - performing embeddings from the ted talks corpus ( ted talks ) and from the europarl corpus ( europarl ) . we see that the most important features of the corpus are the best performing ones , and the worst performing ones are the ones with the least number of features . the best performing features are those with the largest number of instances . these are the most frequently used features in the corpus and the most difficult ones to learn . in particular , we see that these features are most useful in the context of domain specific languages ( dsim , tf , docsub , tftf , df , dfdf , dftf ) and tftf . however , we also see that they are less useful for domain specific tasks ( lang et al . , 2017 ) . these features are more useful in domain specific contexts ( e . g . , dsim and docsub ) than the others .
table 1 shows the results of our model on the number of roots and number of numberrels . the results are shown in table 1 . europarl outperforms all the other systems on all metrics except for the maxdepth and maxdepthcohesion metrics . on the other hand , it performs slightly better on the totalroots and totalterms metrics .
table 1 shows the results of our model on the number of roots and the average depth of each row . europarl outperforms all the other systems on all metrics except for numberrels , with the exception of docsub , where it is slightly better . the results are shown in table 1 .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the enhanced version of our approach outperforms the baseline model by a significant margin . we observe that the enhanced version is more sensitive to question type , answer score sampling , and hidden dictionary learning , as shown in table 1 . however , it is less sensitive to ranking loss , which is the most important factor in the performance of the enhanced approach . in addition , the enhanced model is more responsive to softmax loss than the baseline one . as a result , it achieves a better performance than both the baseline and the enhanced one .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the results are shown in the table . the best performing model is the hidden dictionary learner ( p2 ) with a performance of 73 . 63 % compared to the baseline .
comparison on hard and soft alignments is shown in table 5 . hmd - f1 and ruse outperform wmd - unigram and wmd - bigram on both soft and hard alignments . however , the performance gap between the two approaches is less pronounced on soft alignment , as shown in the table . the performance gap is smaller when using hmd - recall + bert , which improves upon the performance of ruse by 0 . 6 points . on the other hand , it is larger when using the hmd pre - calibration algorithm .
the results are shown in table 1 . we observe that bert scores are significantly higher than the average across all the baselines except for ruse ( * ) and meteor + + ( table 1 ) .
the results are shown in table 1 . we observe that bleu - 2 outperforms meteor and bertscore - f1 by a significant margin , with the exception of the fact that it has a lower f1 score . the results also show that the sfhotel model outperforms the bert score by a large margin .
the results are shown in table 1 . word - mover is the most important metric in our model . the results show that word - mover is significantly more important than word recall on all the baselines except for leic , where it is significantly less important than leic . we observe that the best performance is obtained with the combination of wmd - 1 + elmo + p and bertscore - recall .
the results are shown in table 1 . para + para + lang improves the performance of our model by a significant margin over the previous state - of - the - art approach . we observe that the best performing model is the one with the best performance on the shen - 1 test set . the best performance is obtained when the model is trained with the para - para feature . in particular , we observe that our model outperforms all the previous models with the exception of m0 , where it is slightly worse .
table 3 shows the results of our model on the yelp dataset . the results are shown in table 3 . we observe that our model outperforms all the other models on all metrics except for transfer quality and semantic preservation . our model achieves the best transfer quality with a drop of 0 . 9 points from the previous state - of - the - art . it also achieves the highest performance on the semantic preservation metric . in particular , it achieves a transfer quality of 75 . 7 % and a semantic preservation of 73 . 4 % . the best performance is achieved on the transfer quality metric , which shows that the best performing model is the one with the highest transfer quality .
table 5 shows the results of human sentence - level validation of the metrics for each dataset . the results are shown in table 5 . we observe that the human ratings of semantic preservation and fluency are significantly higher than those of the machine , indicating that human judgments are more likely to match the human judgments .
the results are shown in table 1 . para + para improves the performance of our model over the previous state - of - the - art model by a significant margin . we observe that the best performing model is the one with the best performance on the shen - 1 test set . the best performing variant is the m0 model with the highest accuracy and the highest gm .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform the best previous work by a large margin . however , our best model achieves higher acc than the best prior work , indicating that the classifiers in use are better than those in use in the unsupervised setting . in addition , we observe that our best models achieve higher acc âˆ— than prior work at similar levels of acc .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . the most common disfluency types are nested disfluencies and rephrase tokens , which are the ones that are most likely to contain repetition tokens . these are the types that are the most difficult to predict as disfluent , as they have the highest number of repetition tokens and the longest length of disfluences . however , the number of tokens that are correctly predicted to be disfluential is slightly higher than those that are not , indicating that the disfluence is more likely to be caused by repetition .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . for disfluency that contain only a function word in the content - function domain , it is more likely to be disfluent than those containing a repair word , indicating that the content word is the most important part of the disfluence . the content - content and function - function domains are the most common types of disfluences .
the results are shown in table 1 . we observe that the best performing model is the one with the best performance on the test set with the highest number of test cases . the best performance is obtained when the model is combined with innovations , and the worst performance is achieved when it is only combined with the single word embeddings .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . in table 2 , we show that our embeddings are comparable to the state of the art in terms of accuracy and discuss rate . the difference is most pronounced when we use a self - attention neural network ( rnn ) instead of a cnn - based embedding . our embedding achieves the best performance on the micro f1 test set , with an accuracy of 83 . 43 % and a discuss rate of 71 . 54 % .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . for example , the ac - gcn model outperforms the previous state - of - the - art neuraldater by a significant margin . the neuraldaters also significantly outperform the oe - gcns on both datasets .
table 3 shows the performance of neuraldater with and without graph attention on the word attention task . this results show the effectiveness of both word attention and graph attention for this task . the results are shown in table 3 . with and without attention , the neuraldater component models are comparable in accuracy .
the results are shown in table 1 . we observe that the jmee model outperforms the dmcnn model on all stages except for the 1 / 1 and 1 / n stage , where it performs better . however , the difference is less pronounced for the first stage , which is the one where the argument is generated . in this case , we observe that jmee performs better than the previous state - of - the - art embeddings .
table 1 shows the results of our approach on the cross - event dataset . the results are shown in table 1 . our approach outperforms the previous state - of - the - art approach by a significant margin . we observe that our approach achieves the best performance on the cross - event dataset , as shown in the table .
the results are shown in table 3 . we observe that the best performing variant is the one with the best performance on dev perp and test wer . the best performance is obtained by using the best - performing variant with the highest number of test acc . however , this is not the case for english - only and spanish - only variants , which show higher performance on test acc and dev wer than the other two languages .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned approach outperforms the cs - only approach by a significant margin . the results are shown in table 4 . fine - tuned training achieves the best results on both the dev and test sets , with the exception of the full test set , where it achieves the highest performance .
table 5 shows the performance of the fine - tuned approach on the test set and the dev set . the results are shown in table 5 . the fine - tuned approach achieves the best performance on both test and dev sets , with the exception of test cs , where it achieves the highest performance . the fine - tuned approach outperforms the monolingual approach on both sets .
table 7 shows the results of using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the results are shown in table 7 . the type combined approach shows a significant improvement in precision and recall compared to the baseline approach .
table 5 shows the results of using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . the type combined approach shows a significant improvement in precision and recall over the baseline approach , with a f1 score of 3 . 35 points higher than the baseline .
table 1 shows the results on belinkov2014exploring â€™ s ppa test set . glove - extended embeddings outperform lstm - pp and syntactic - sg on all test sets except for one , which is the one on which the original hpcd implementation is based . the results are shown in table 1 . syntactic sg embedding achieves the best performance on the test set , outperforming both the original and the original syntactic skipgram implementation .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd ( full ) and ontolstm - pp ( partial ) outperform all the other approaches except for the lstm ( full ) . the best performance is obtained by using onto - lstms as a dependency parser . in particular , it achieves a ppa acc . score of 98 . 97 % , which is higher than the previous best performance of 94 . 59 . it can be observed that the best performing approach is the one using lstms .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we observe that the ppa acc . improves with the removal of the context sensitivity .
table 2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . adding subtitle data improves the performance of the multi30k embeddings , but it does not improve the bleu % score . moreover , the domain tuning results are slightly worse than the sub - subsfull approach . however , it is still better than the domain - tuned approach .
table 3 shows that domain - tuned h + ms - coco outperforms all the other approaches except for en - de and mscoco17 . domain - tuning improves the performance of en - fr and flickr16 , but not flickr17 , and the results are not statistically significant . in general , domain - tuning improves performance for all the models except flickr15 .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results are shown in table 4 . adding automatic captions improves the performance of marian amun â€™ s multi30k and en - de embeddings , but does not improve performance for mscoco17 and flickr16 . as a result , the performance is comparable to that of en - fr , but is slightly worse than that of marian amun . multi30k is the best performing embedding , but the performance drops slightly when adding the best 5 captions .
table 5 shows the results for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , en - de and mscoco17 are shown in table 5 . the enc - gate + dec - gate approach achieves the highest bleu % score ( 68 . 38 % ) and the best overall performance ( 67 . 40 % ) on all three datasets . the best performance is achieved by using the img [ italic ] embeddings with the dec - gate layer , which achieves the best performance on all datasets .
the results are shown in table 1 . the results show that the multi - lingual detectron performs better than the text - only and visual features detectrons . however , the results are slightly worse than the results obtained by using the ms - coco classification scheme . in particular , the performance of the multilingual detectron is significantly worse than that of the visual features - based detectron . this is evident from the difference in performance between en - de and en - fr classification schemes .
the results are shown in table 1 . the results show that en - fr - ht and en - es - ht outperform all the other approaches except for the rnn - rnn - ff embeddings . however , the difference between the two approaches is not statistically significant . it can be seen that the difference in performance is due to the large number of words in the corpus and the small number of translations . in addition , the differences in performance are more pronounced in the mtld domain , which indicates that there is a need to further improve the performance of the embedding algorithm . finally , we see that the performance gap is less pronounced in mtld .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the results are shown in table 1 . we found that the most common language pair is en â€“ fr , followed by en â€“ es and en - es - fr , which have the most parallel sentences .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . the results show that our model performs well on both languages .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . our system outperforms the previous state - of - the - art rev system by a significant margin . it achieves a bleu score of 43 . 8 % and a ter score of 44 . 7 % , both of which are significantly higher than the previous best state of the art system .
table 2 shows the results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one trained on the same dataset . the results are shown in table 2 . rsaimage achieves the highest recall @ 10 and the highest mfcc . vgs achieves the second highest recall .
table 1 shows the results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled u is the auditory supervised one . the results are shown in table 1 . rsaimage achieves the highest recall @ 10 % with a mean mfcc of 1 . 414 and a median rank of 0 . 5 .
table 1 shows the examples of the different classifiers on sst - 2 . we report further examples in the appendix . for example , the cnn classifier turns in a screenplay that is at the edges , and the rnn classifiers turns in one that is on the edges . the cnn classifiers turn on a on ( in in the the the edges ) and a off ( in the edges ) . the dan classifiers , on the other hand , turns on the on ( on the edges ) , and turns in the off ( on edges ) .
table 2 shows the results of fine - tuning in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the number of occurrences of each word in the final sentence . these results indicate that the rnn has significantly improved the performance on the part - of - speech ( pos ) test set . rnp has a higher percentage of occurrences than cnn and dan .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 . the positive sentiment score increases by 9 . 9 points and the negative sentiment score by 6 . 3 points .
table 1 presents the results of our experiment on pubmed and sst - 2 . the results are shown in table 1 . we observe that pubmed outperforms both corr and sift by a large margin , with corr achieving an acc of 98 % and a negation of 99 % .
