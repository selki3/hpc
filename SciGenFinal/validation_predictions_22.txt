table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the table shows , both approaches benefit from the scalability of gpu exploitation , and the improvement over iterative learning on the training dataset comes from a larger number of instances , making it more suitable for production use . the difference in performance between the two approaches is less pronounced for the movie review dataset , but still significant , with a batch size of 25 . 2 instances and a throughput of 269 . 1 instances , which shows the advantage of scalability . recur and fold both give similar improvements on training and inference , but fold has higher performance overall , since it requires fewer instances to train , and therefore requires more iterations to learn the task . on the other hand , the improvement on inference is larger than that on training , since the size of the iterations is smaller . this validates our selection of the recursive approach .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . balanced data also exhibits lower performance than linear dataflow , because the size of the instances in the balanced dataset is smaller than the size in the linear one , making it more suitable for wasserstein and schmidhuber ( 2018 ) . with a batch size of 25 , the performance reach the best , but again , there is a small amount of performance that can be achieved with w . r . tallelization .
the max pooling strategy consistently performs better in all models with different representation as shown in table 2 . it achieves the best performance with a f1 score of 1 . 83 on the conll08 test set ( t - test , p < 0 . 05 ) and on the ud v1 . 3 test set , which shows that the use of the max - pooling strategy induces the model to perform better in the task at hand . the dropout prediction function also gets better performance when using softplus instead of sigmoid , indicating that softplus is a better complement to softplus . as hard coreference problems are rare in ud , we do not have significant performance improvement with softplus , however , we notice that using the sb representation achieves the most consistent results , we observe that combining the feature - values with the dropout probability decreases the error of the model over the best baseline , i . e . , by a factor of 2 . 3 . this validates our hypothesis that a better feature - mapping approach is beneficial for scalability in the production setting . we note , however , that this analysis fails to account for the frequency with which the features can be extracted from the training data .
table 1 shows the effect of using the shortest dependency path on each relation type . we find that using sdp significantly improves the f1 score for all relation types , and for macro - averaged models as well . the biggest gains are seen for topic and model - feature , which show that sdp helps the model to predict entity entities better . further , for part_whole , the effect is even larger , showing that part - ofspeech tags have a significant impact on the model performance . token distance and topic are the only relation types that are impacted negatively by sdp , so it is clear that the use of sdp does not improve the prediction performance for these relation types .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of embeddings perform comparably when trained and tested on the same dataset , with the exception of the case where y - 3 performs much worse than r - f1 and r - y .
the results of paragraph prediction on the simulated test set are presented in table 1 . we show that our system achieves outstanding results , outperforming all the state - of - the - art parsers on every metric by a significant margin . on the test set , mst - parser achieves 100 % on average , which means that it achieves better than all the other systems apart from our model on all metrics except for paragraph prediction accuracy .
the performance of our system compared to the lstm - parser on the test set is shown in table 4 . the difference in c - f1 score between the two systems is much lower than that reported in table 2 , our system obtains the highest performance , showing that our proposed parser is more effective at parsing the word embeddings .
table 1 shows the results for tgen + and sc - lstm compared to the original model on the training data . the results show that tgen has seen a noticeable drop in performance since the introduction of object detectors , the difference is most prevalent in bleu and nist , as these models use object detectors pre - trained on pascal - voc , they are less likely to perform well when the original object is cleaned . on the other hand , when combining all the data from the two sets , the results are slightly better than the original on all metrics but still significantly worse than tgen − . our model performs better than both the original and the cleaned model on two out of the four scenarios when using the best performing feature set .
table 1 shows the comparison of the original e2e data and the cleaned version , as measured by the number of distinct mrs and total number of textual references in our slot matching script , see section 3 . the difference in quality between the two sets is less pronounced for the training data , but still significant : in the training set , there are 1 , 358 mrs ( 17 . 5 % ) and 4 , 299 references ( 11 . 69 % ) compared to the original train and test sets , respectively , which shows that our approach has comparable performance on the training and test set . ser is also significantly lower than the original set , indicating that the quality of our approach is less effective in deterministic ways . since we do not have access to the source code for this data , we used slot matching instead . our approach relies on word embeddings specialized via pascal - voc , which can be seen as a complement to the slot matching part .
table 1 shows the results for tgen and sc - lstm on the hidden test set of bleu , nist , and ser . the results show that tgen + improves upon the original model substantially , and achieves state - of - the - art results on all metrics except meteor , rouge - l and cider . as can be seen , the difference between the original and the improved results on these test sets is much narrower than that on the original test set , when using only original utterances , the effect of the syntactic distance boosts performance for both systems , adding the correct utterances after training , however , does not improve results significantly compared to adding the wrong ones . this is mostly due to the high overlap between the training and the test set size of the two sets , across all metrics , the average number of correct answers for each add / remove case is significantly less than the original , therefore , we maintain performance at the level of the state - ofthe - art , with a marginal drop of 2 . 8 % compared to the original .
the results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . the number of errors we found in our system is relatively low , however we found significant ( p = 0 . 0088 ) absolute numbers of errors caused by missing , missed , and wrong values ( see table 4 ) . the majority of these errors ( 62 . 4 % ) are caused by adding instances that were already in the training set ( 22 . 6 % ) but we found that there are also instances that have been added that are slightly disfluencies ( 14 . 6 % ) . the difference between the true and actual errors is less pronounced for the cleaned instances , however still significant . disfluencies are mostly caused by the presence of grammatical errors in the named entity representation ( e . g . , the number of times the correct value of zsgn is used in the correct derivation function is much higher for this example than for the original zgn .
table 3 compares the performance of our approach with previous stateof - the - art models on the hidden test set of gcnseq . our joint model achieves the best results , outperforming all the base lines with a gap of 10 . 2 % on average compared to the previous state of the art .
table 2 shows the performance of our model on amr17 . our ensemble model achieves 24 . 5 bleu points , which is significantly better than the previous state - of - the - art on seq2seqb and ggnn2seq . the difference is less pronounced on gcnseq , however , reaching an absolute improvement of 2 . 4 bleu points over the previous best results . table 2 also shows that our model achieves competitive performance with other ensemble models , since it has the advantage of training on a larger corpus .
table 3 presents the results for english - german , czech and french , compared to single - and multi - word embeddings . we show that the models using bow + gcn outperform all the base models that do not use bow clustering . the results are broken down in terms of performance on the standard tasks , with the exception of german , where the single - labeled model performs better than all the other models apart from birnn . table 3 compares the performance of the different classifiers on the two standard tasks . as the results show , the use of bow - based clustering improves the generalization ability of the model in both languages . the average number of entries per label for each language is higher than the previous state - of - the - art on both datasets , with respect to the czech republic , the new classifier ggnn2seq ( beck et al . , 2018 ) achieves the best performance with 43 . 8 % on average compared to the previous model . on the german and french datasets , the difference is less pronounced , but still significant , with single - classifiers performing better than the other two .
table 5 shows the effect of the number of layers inside our dkrms on the performance improvement over state - of - the - art models . the first layer brings the performance of the model closer to the state - ofthe - art when using only one layer , the second layer increases performance by a noticeable margin , as the results of applying the second layer show , when using more layers , the model performs better overall , i . e . , it obtains a performance gain of 1 . 8 points over the state of the art on par with previous models .
table 6 compares the results of rc and rc + la with baselines for the gcns with residual connections . the results , summarized in table 6 , show that both cues yield strong baselines comparable to the strongest ontonotes - trained systems ( cf . table 4 ) . the difference between rc and la is less pronounced for dcgcn , but still suggests some reliance on residual connections as these gcns have been designed to perform well in the past ( e . g . , gcn1 ( 9 ) and 2 ( 18 ) . adding rc information improves the results for both gcns , it improves the bias metric by 4 . 8 % in the standard task formulation and to parity in the gold - two - mention case . moreover , it improves the generalization ability of the model by 2 . 2 % overall compared to the baselines .
the performance of our model compared to previous models on the hidden test set is presented in table 4 . we observe that , let alone a drop in performance , our dcgcn model achieves state - of - the - art results , outperforming all the other models that do not use word embeddings with cbns pre - trained on the training data .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly decreases the number of connections , and consequently , the performance of our model decreases as well .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results of " - global node & linear combination " and " - direction aggregation " remove some of the aliasing effects , but not all of them , as shown in table 9 . the results show that combining the global and local nodes induces a significant drop in performance , which is expected in a production setting . also , under these two approaches , the model performance drops significantly when using domain - adaptive decoding and domain - aware attention .
table 7 shows the performance of our initialization strategies for different probing tasks . glorot obtains the best results , outperforming all the base natives with a gap of 10 . 8 points from our paper . subjnum and topconst obtain the highest scores , both on the depth and tense measures , and on coordinv metric , respectively , which shows the extent to which the initialization strategies can improve the generalization ability of the wasserstein distance measures . the difference between the average and average score of these two approaches is less pronounced for sub - jnum , but still suggests some performance improvement . topconst and coordinv achieve the highest score on the tense metric , a result not found to be significant even under the difficult requirement of a low false positive rate . finally , our paper achieves the best generalization score , a performance gap of 2 . 6 points from the gloot baseline .
we observe that the h - cbow variant outperforms both the original cbow and the hcmow variants on every metric by a noticeable margin . the difference is most prevalent in the deep layers , where the cbow variant obtains a performance gain of 2 . 8 points over the cmow variant . this confirms the value of the compactness of our object - based learning schemes .
the results of cbow and cmow are shown in table 1 . hybrid mode outperforms all the base the results show that cbow obtains significant gains over the monolingual method on all metrics , and even outperforms the cmp . baseline on some of the worst performing sub - committees . on the sst datasets , it achieves gains of 2 . 6 % and 3 . 4 % over cmow , respectively , on average compared to the baseline .
table 3 shows the improvements on unsupervised downstream tasks that our models achieve when trained and tested on the hidden test set of sts12 and sts13 . the results show that cbow and cmow achieve considerable gains over the strong monolingual cbow baseline on these tasks , outperforming cmp . hybrid also achieves gains over cbow , but are less significant than cmow , as shown in table 3 , the effect of cbow on the downstream tasks is less pronounced than that on the original test set , however , it is still significant to see that our model has outperformed cbow in all but one of the 13 cases ( and slightly outperforms cmow in the other 11 cases ) by a margin of more than 2 . 5 % .
table 8 presents the performance of our system on these three supervised downstream tasks . glorot initialization achieves the best results , outperforming all the base lines with a gap of 10 . 6 points from the last published results ( glorot et al . ( 2014 ) by a margin of 3 . 4 points on mpqa . the difference is less pronounced under sst5 , but still significant with an absolute improvement of 2 . 2 points over the previous state of the art on all three datasets . our system achieves state - of - the - art performance across all three sub - topics : on sst1 , sst2 , and sts - b , it obtains a significant improvement over the strong glorot baseline by 4 . 8 points on average .
table 6 shows the performance of our method compared to the state of the art on the four downstream tasks . the cbow - r method shows marked improvements over the state - of - the - art on all the four tasks . it achieves gains over both supervised and unsupervised methods , and outperforms cmow in all but one of the comparisons . on the sts13 and sts16 tasks , it achieves gains of 2 . 2 and 1 . 9 points over the best supervised method .
table 3 presents the numerical results on the hidden test set of somo and wc . the proposed method outperforms both the original cbow and cmow embeddings on every metric by a noticeable margin . topconst and topconst achieve gains over the strong baselines on both hidden test sets , while topconst achieves the best performance on somo , it should be noted that it is trained on a significantly larger corpus . subjnum and length are the most difficult metrics to solve , the difference between the performance of these methods is less pronounced for cbow - r , but still significant , cmow - c shows a performance drop of more than 2 . 5 points when compared to the original cmow .
we observe that the method developed by cbow achieves state - of - the - art results on all test sets , outperforming all the base the results reconfirm that the cbow - r model , when trained and tested on the simulated test data , is well - equipped to perform on the demanding subset of the training data . the results of sick - e , sst2 and sst5 are particularly striking , with cbow achieving gains of 3 . 6 and 6 . 2 points over the previous state of the art on each test set .
the results of all models are shown in table 1 . in all but one case , the system performs better than the best supervised model . specifically , all models show improvements on the loc and misc datasets , with the exception of τmil - nd . supervised learning underperforms all the other models except for the case when it comes to name matching . the difference is most prevalent in e + loc , in this case , automatic learning systems perform better than all the supervised and unsupervised alternatives except the case of mil - nd , which shows the diminishing returns from cross - domain supervision .
results on the test set are shown in table 2 , in all but one case , the models perform comparably to the best state - of - the - art models ( tables 1 and 2 ) . the difference is most prevalent in the f1 score , name matching and supervised learning have the highest confidence intervals , however , when using τmil - nd instead of supervised learning , model 2 shows a slight improvement over the model 1 in all but name matching and f1 scores . supervised learning also improves the generalization ability of the model , it achieves the best results in terms of e - muli and r - measure , as the results of both settings show , once the training set is optimised , the model can further improve its performance with a noticeable drop in performance between t - test and final test set .
table 6 shows the ablation results on the hidden test set of g2s - gat compared to the models trained on s2s . the results are broken down in terms of performance on extractive and abstractive keyphrases , with the former performing better than the latter on both ref and con . as the results show , combining the features of both abstractive and interpretive keys brings the model closer in performance to the state - of - the - art . on the other hand , combining all the information from the two sets results in significantly worse performance than the former state of the art . retrieving the full set of features from the supplementary material yields significantly better results than concatenating them into a single sentence , the difference is most prevalent in ref , as can be seen in table 6 , when we concatenate the features extracted in the extractive part of the model , the performance drops significantly compared to that in the abstractive part . this indicates that the model is more specialized , and requires a lot more data to reproduce the results .
table 3 presents the results on the hidden test set of bleu and meteor , compared to previous stateof - the - art models on the ldc2015e86 and ldc2017t10 datasets . the results of g2s - gat outperforms all the other models on both sets , with the exception of konstas et al . ( 2017 ) , whose model obtains a performance drop of 2 . 42 points from the previous state of the art on both datasets . on the other hand , our model achieves a performance gain of 1 . 53 points over the previous best state - ofthe - art on both benchmarks , outperforming all the previous models by a noticeable margin .
table 3 shows the performance of our model with additional gigaword data on the test set of ldc2015e86 . our g2s - ggnn model improves over the previous state - of - the - art on this test set by 1 . 5 bleu points .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state of the art on all metrics with a gap of 2 . 6m bleu points from the last published results ( get ) to the current state - of - the - art on meteor . the difference is most prevalent in terms of size , with the bilstm model performing better than the get model on both datasets .
the results of the second variation of our model are shown in table 1 . we observe that g2s - ggnn shows marked improvements over the baseline model across all metrics , with the exception of sentence length . the difference is most prevalent in the graph diameter and length metrics , from 0 - 7 δ , when we switch from golbeck - keller distance measures to s2s distance measures , the difference between the two models is less pronounced , but still significant , reaching 1 . 51 % and 0 . 8 % on average compared to the baseline . from 20 - 50 δ to 240 δ , the improvement is much larger , reaching 2 . 2 % and 1 . 7 % respectively , on average .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when s2s is applied to the test set of ldc2017t10 . the token lemmas are used in the comparison . the g2s model significantly outperforms the sgs model in terms ofiss , as shown in table 8 , the smaller size of the missing tokens indicates that the g1s model is more effective in generation of new sentences . s2 has the better generalization performance , since it requires fewer tokens to generate a sentence , and therefore requires fewer iterations to reproduce the output . also , the difference iniss between gold and gold is much smaller than that between sgs and gat , we observe that the size and type of tokens in the gold - backed tokens is less significant , therefore , we use only plain tokens for comparison .
table 4 shows the pos and sem accuracy for the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos features significantly improve over the original nmt embeddings , showing that the semantic features extracted from the 3rd nmt layer are more useful for target - specific tasks . sem also improves significantly compared to the pos feature , showing the translation quality of the new features . the difference is less pronounced for ru and zh , but still suggests that the pos features have a significant impact on the model performance .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform unsupemb and word2tag encoders in both metrics , confirming the value of using unsupervised classifiers without supervision . the upper bound on pos also indicates that the encoder decoding has a significant upper bound , meaning that it can be trained to predict the most frequent tags and the average number of tags in a sentence with high accuracy . it is clear from table 2 that word2tags has superior performance on both metrics .
table 4 presents the performance of our system on the four types of tagging accuracy measures . our system outperforms all the stateof - the - art systems on all metrics with a gap of 10 . 8 % on average compared to the previous state of the art . on the pos and sem datasets , our system achieves the best performance with an absolute improvement of 2 . 1 % on pos tagging accuracy and 1 . 9 % on sem tagging performance .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as the results show , the bi - layers approach achieves the best performance , with an absolute improvement of 1 . 9 points over the plain uni encoder . the difference between bi - and residual is less pronounced for the two layers , but still significant enough to warrant a statistically significant increase in our model ' s performance .
table 8 shows the attacker ’ s performance on these test datasets . results are on a training set 10 % held - out . as shown in the table , the difference between the attacker score and the corresponding adversary is minimal , however we see significant difference in performance due to different classifiers being trained on different training sets , gender and race are particularly difficult for the attacker to pick out , as shown in table 8 . on the other hand , age and gender are the only classifiers that the attacker does not seem to have difficulty picking out . overall , the classifiers are comparable in difficulty , with gender and race being the only ones that show significant performance drop .
table 1 shows the performance of our system for training directly on a single task . the results show that our proposed system significantly outperforms all stateof - the - art methods across all three metrics . gender - neutral pronouns significantly improve the performance for all groups , and for all gender - specific pronouns , with the exception of age . overall , the system shows marked performance improvement across all gender and classifiers .
table 2 shows the results for balanced and unbalanced data splits , gender and racial disparities are the most prevalent in both datasets , followed by age and gender . gender - based disparities are less prevalent in the balanced dataset , but still represent a significant racial disparity in performance , as shown in table 2 . overall , all the racial and gender - based classifiers show that the presence of these features in the training data is beneficial , improving upon the performance of gender - neutral classifiers by 4 . 5 points in the task prediction accuracy and to parity in the gender - parity task prediction . the accuracy drop from the balanced to the unbalanced set indicates that these features are important for future models to consider in the development of their features .
the performance of our system on these test datasets is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in leakage due to the high number of tokens in the training set , which indicates that the presence of gender bias is a significant part of the problem . gender - specific tokens have the highest performance , so we observe that gender - specific tweets are particularly difficult for the system to solve . overall , the performance is comparable across all the datasets , with the exception of pan16 , which shows lower performance due to larger variation in training set size and training time .
the results of rnn and guarded encoders are shown in table 6 . the difference in the performance between the protected and unencoders is minimal , however we see significant difference in interpretability due to different encoder types having different interpretability thresholds for each protected attribute . rnn is significantly better at reading out - of - vocabulary ( o2 ) tokens than guarded , as expected , the rnns that are guarded are much more interpretable .
table 1 compares the performance of our approach with previous stateof - the - art models on the hidden test set of ptb and wt2 , both with and without finetune . we benchmark against the following models : the base models of yang et al . ( 2018 ) and lstm ( lstm + dynamic ) . the results of both models are shown in table 1 . the difference in performance between the base and the final model is minimal , but significant , with the difference being more than 2 . 5 % on average compared to the previous state of the art . lrn also achieves competitive or better results than other approaches , with a total of 11m training examples , the size of the training set is comparable to that of the best previous work ( yang et al . , 2018 ) , but larger than the previous best results by a margin of 2 . 3 % on the wt2 base . finetune - trained models perform better than unbalanced ones , so we observe that the difference between the two is less pronounced for lrn , but still significant . we conjecture that this is due to the larger training set size and the high overlap between mirrored alternatives , retrieving the hidden messages from the past is difficult , since the size and type of training data is different for each model , and sometimes very noisy . our approach obtains competitive or superior results than previous work on two of the four models by a large margin . on the other hand , it is less effective on the third model , because training on this larger dataset requires significantly more data than available in the previous work .
table 3 presents the results of our final model on the hidden test set of lstm in the distractor and fullwiki setting , respectively . the results show that our model significantly outperforms the previous stateof - the - art on all metrics with a gap of 10 . 3 % on average compared to rocktäschel et al . ( 2016 ) . the difference is most prevalent in terms of acc time , with an absolute improvement of 2 . 5 % on the fullwiki setting . table 3 compares the performance of our model with previous models on different training sets with different feature sets . retrieving the raw data from one of the baselines after replacing all the parameters with the ones from the other two gives a significant performance gain ( 1 . 5 % ) on both datasets . this confirms the value of redundancy in the low - resource settings . our model obtains the best results with a full complement of features on both training and evaluation set , outperforming all the other baselines except atr by a noticeable margin .
table 3 presents the results of our second study on yelppolar and amafull time on the hidden test set in the setting of google translate . the results reconfirm the value of parameter sharing and parameter sharing in the low - supervision settings . our proposed system outperforms both published and unpublished work on every metric by a significant margin . for example , amapolar err and yelp time are both significantly higher than the previous state - of - the - art on both datasets ( paired t - test , p < 0 . 05 ) on both systems , with an absolute improvement of 2 . 36 points over the results in zhang et al . ( 2015 ) . table 3 compares the performance of our proposed system with previous state of the - art models on different test sets . we benchmark against the following baselines : full - time , part - time and work - time . our model obtains the best results on all metrics , outperforming all the other models apart from the case of the atr which is significantly lower than the other two .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . the model makes use of the best performing feature set available from the newstest2014 dataset , namely , the case - inensitive tokenization of the sentence with the best performance on gnmt and atr . gnmt also significantly outperforms the other methods which use object - specific tokenization . retrieving all the tokens from the training data after each training step takes a negligible amount of time compared to the time to decode one sentence . lrn also performs well compared to other methods with a training time of less than a second . sru and olrn obtain significantly better case - sensitive tokenization scores than gnmt , as shown in table 3 , combining the features of object - and sentence - specific tokens improves the model ' s performance over other methods . at the same time , the model exhibits a significant drop in performance when using atr and sru compared to gnmt . gru also achieves a significant improvement over the atr model by more than 2 . 5 % on the translation task , though the difference is less pronounced for lrn , it is clear that lrn is better at decoding sentences with fewer tokens , with respect to training time , lrn requires significantly less time to train and decode compared to atr ,
table 4 shows the exact match / f1 - score of our model on the squad dataset . the model significantly outperforms all the base lines with an f1 score of 1 . 14 / [ bold ] and rnet * = 71 . 83 , which means that our model obtains a significant improvement in performance over the baseline lstm model by more than 2 . 5 points . table 4 also shows that the number of parameter numbers in our base is significantly higher than the previous state - of - the - art models , as expected , when we add elmo as a parameter , our model exhibits severe overfitting since it has only one parameter number of elmo , which results in a significant drop in performance . retrieving all the parameters from the elmo dataset , however , helps the model to regain its competitive edge . at the same time , it improves its overall performance by 4 . 67 points , which shows the diminishing returns from mixing the input and output labeled with elmo . finally , we show the model ' s performance with respect to parameter sharing . we use the scalability metric rnet to compare our model with previous models . in wang et al . ( 2017 ) , we report the results of re - scoring our model after replacing the elmo with the base of our lrn model . lrn has a total of 2 . 67m parameters , which is more than twice the size of the original elmo - based model .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the number of parameters in question is the most important factor in the model performance , as it affects the translation quality . as the number of parameter numbers increases , the model performs worse than all the other methods that do not use sru , lrn and atr . gru also has a significant effect on the performance , however it is less significant than lrn , as shown in table 6 , it is comparable to lrn in that it obtains the best performance with respect to translation quality as well . at the same time , gru and lrn both receive significantly higher f1 scores than atr , which shows the diminishing returns from using sru and other specialized neural networks . table 6 also shows the effect of the increased parameter number on the model ' s performance . with 245 . 94 k parameters , the lstm model achieves the best result .
table 7 shows the test performance on snli and ptb tasks with base + ln setting compared to the original setting . with the base setting , our model obtains a significant improvement in performance over the strong lemma - based glrn model . the difference is most prevalent in snli task , where the lrn model achieves 7 . 56 % higher performance on average compared to elrn . on the ptb task , the difference is less pronounced , but still significant , at 2 . 3 % . the performance gap between the two sets is modest but significant .
table 2 presents the system and sentence quality on the test set for english . our system outperforms all the other systems with two tasks . the difference is most prevalent in b - 2 and b - 4 , where oracle retrieval generally leads to better results . sentence quality is relatively consistent across all systems , with the exception of human , which is significantly worse than both systems . system and word quality are relatively consistent with each other with respect to sentence quality , retrieving the same word or phrase twice as often as human under - performs both systems , with respect to r - 2 scores , all systems show significant performance drop when using oracle , since the number of tokens in query is much less than in human , so the difference between the two is less pronounced for oracle . on the other hand , when using mtr and sentence type , the difference to human is less significant , showing that human is better at selecting the relevant words and its output is more interpretable .
table 4 presents the results of human evaluation . our system obtains the best result among all the automatic systems , outperforming all the other systems with a gap of 1 . 8 points from the last published results . the highest standard deviation among all is 1 . 2 points , which indicates that our system is well - equipped to perform in the low - resource settings . among all the systems , seq2seq is the only one that obtains significantly higher performance on grammatical accuracy ( grammaticality ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the second highest performance on all metrics is obtained by candela ( 30 . 2 % ) , followed by h & w hua and wang ( 2018 ) . table 4 shows that candela is comparable in syntactic and semantic analogy with human evaluation , but does not receive the highest score on content richness , likely in part due to the high cost of data acquisition . retrieval , on the other hand , is comparable , obtains a lower standard deviation , and achieves the highest result , showing that it is more suitable for production use .
the performance of our system on the test set is presented in table 1 . we observe that compared to ted talks , our approach outperforms all the base the difference is most prevalent on the text - similarity test set , in particular , we see that our system performs on par with or better than the best previous approaches on all three datasets on all but one of the test sets , table 1 shows that our approach yields superior results on all the three datasets when trained and tested on the single - domain dataset , on the other hand , our model performs slightly worse than the previous state - of - the - art on two of the four datasets , when trained on the ted talks dataset , the gap between the en and r scores on both datasets grows significantly , as this indicates , the model performance obtained on the europarl dataset may be impacted by the training size and the number of training instances , however , on the smaller - scale datasets , our system outperforms the other two baselines on average , our model outperforms both ted talks and the svm - trained ted talks on all metrics except for the case where it obtains the best performance on the small - scale dataset .
the performance of our system on the test set is presented in table 1 . we observe that compared to the previous state of the art on all three datasets , our approach outperforms all the stateof - the - art methods on all but one of the four datasets . on the exception of ted talks , our system obtains the best performance on all the three datasets with a gap of 2 . 5 points from the last published results . the difference is most prevalent on the smaller scale , table 1 shows that our system performs on par with the best - performing ted talks dataset , outperforming all the other methods apart from the case of svm with which we have qualitatively evaluated it .
the performance of our system on the test set is presented in table 1 . we observe that compared to ted talks , our approach outperforms all the base the difference is most prevalent on the " ted talks " subset , where our system performs in the low - supervision range . on this subset , our system obtains the best performance among all the mod table 1 shows that our proposed system outperforms the ted talks baseline on all metrics by a significant margin .
the system performs well on all metrics with a gap of 10 . 46 % on average compared to the previous state of the art . europarl achieves the best results with a maxdepth of 21 . 82 % , which means that it is more than 5 . 5 % better than the average of previous state - of - the - art systems across all metrics . the difference is less pronounced for docsub , however , which shows the extent to which docsub embeddings can be improved with a reasonable selection of the lexical features from which its roots can be derived . from table 1 , we can further calculate that total terms and roots are the most important components of a model ' s performance , followed by the number of tokens and the average number of roots per row , from this group of metrics , the most interesting ones are depthcohesion and maxdepth , which measures the degree to which a model can maintain its semantic integrity . according to our system , both of these measures result in significantly better performance than average , since the difference is narrower than that between maxdepth and average .
the system performs well on all metrics with a gap of 10 . 29 % on average compared to the previous state of the art . europarl achieves the best results with a weighted average depth of 9 . 43 % , which means that it has the best overall performance . the difference is most prevalent in relation to docsub and slqs , docsub has the highest correlation with depth , so we observe that the difference between the average and maxdepth scores is much larger in docsub . from this group of metrics , we can further calculate that the number of roots and the average number of tokens per row of docsub are the most important factors in the clustering performance of the system , since those are the ones that are derived from the lexical relations of the original embeddings . at the same time , our system exhibits the smallest difference in performance between maxdepth and average depth , which shows the diminishing returns from mixing lexical and semantic information . according to our system , the maxdepth metric alone results in a significant performance drop , since it requires significantly less data and time to train , compared to mindepth .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model , named lf , obtains significantly better performance than the baseline model in all aspects ( except for question type and answer score sampling . it achieves over a 4 . 5 % improvement over the performance of the plain l2r model on average compared to the baseline , and more than a 4 % improvement compared to p1 . 7 % on average . in addition , the model exhibits a significant drop in performance when using weighted softmax loss instead of binary sigmoid loss , and generalized ranking loss , respectively . this indicates that the use of the best performing feature set induces the model to perform better in the production setting . as evident from the results of the second study , using the enhanced loss reduces the performance gap between the baseline and the enhanced version , i . e . that the model can learn to reason over more questions with a better answer score .
the performance ( ndcg % ) of these experiments on the visdial v1 . 0 validation set is shown in table 2 , it can be observed that applying p2 improves the model ' s performance over the baseline substantially , and that it is the most effective one . as hard coreference problems are rare in a production setting , we observed no advantage to applying p1 instead of p2 . the rva scores of the models improved significantly when we applied p2 as well , showing that the use of hidden dictionary learning induces the model to perform better than the baseline under all three scenarios .
table 5 shows that the bert models perform comparably to wmd - unigram and wmd - prec when trained and tested on the hard and soft alignments . hmd - f1 and hmd - recall achieve gains over both the plain - english and french word embeddings , but do not exceed the performance of ruse , which shows the performance reach the limits of bert ' s performance . the results of re - training with bert after replacing these features show that the models perform comparable to the pre - trained hmd models on the soft alignment test set , the difference is less pronounced for the french - trained models , but still significant , with ruse outperforming all the other models except wmd .
the results of the second variation of our method are shown in table 1 . the results are broken down in terms of baselines and set - metrics . the first set shows that meteor + + significantly outperforms all the baselines except bertscore - f1 , which is significantly better than all the other baselines but still performs substantially worse than ruse ( * ) and svm . the second set shows svm outperforms the baseline on all metrics except direct assessment . svm achieves the best results on two of the four sets when we included the sent - mover the difference is most prevalent in the case of ru - en , the largest of the three sets , where svm performs the worst . on the other hand , if we include the w2v - based evaluation method , svm obtains the highest score on all the sets , so we do not have significant difference in performance between the two sets .
the experimental results on the hidden test set of hotpotqa are shown in table 1 . the system performs well on all metrics with a gap of 10 . 5 % on average compared to bleu - 1 and 2 . 0 % on sfhotel . on the other hand , when using meteor as the baseline , the system performs much worse than bertscore - f1 , because the semantic signal is significantly less concentrated in the smd embeddings , reaching an absolute zero on all but one metric . when using w2v as a baseline for the training data , the results are slightly better than the baseline but still significantly worse than any other baseline . we observe that combining the semantic and syntactic information from the two sets of data improves the results for all metrics except for the inferences regarding salience . sent - mover is indeed effective for improving the salience of the final score , it achieves a noticeable improvement over the baseline with a f1 score of 0 . 176 compared to the previous state - of - the - art on both sets . it is clear from table 1 that the semantic information injected into the system by the wasserstein - keller distance boosts the predictive performance of the system , as measured by the difference in f1 scores between baseline and smd . further improving performance by high margins
word - mover on the other hand , when using the bertscore - recall baseline , shows significant performance improvement over the baseline on all metrics except leic ( * ) and spice . the difference is most prevalent on m1 and m2 metric , sentances are significantly better than those on spice , indicating that the dependency distance based learning method is effective in the low - supervision settings . when using the word - layer embeddings of word2vec and elmo , the performance gap between the wmd - 1 and w2v measures less than the performance of any other classifier . sentance is relatively consistent across all the metrics with the exception of leic , which shows the performance reach the upper limits of the performance range when using only elmo and bert scores . we observe that the feature - rich clustering approach further boosts the performance for both groups , word2v and smd achieve state - of - the - art results , with the help of these features , we can further calculate that the average number of frames per second of the average sentence is improved when the word is moved using the best clustering scheme , the performance gap is modest but significant with the upper boundary of the gap being less than that with the baseline .
we observe that the effect on generalization is also not notable , i . e . that the models using para - para outperform those using shen - 1 and 2d on all metrics except sim , with an absolute improvement of 1 . 81 points over the baseline m1 score in all but one case .
table 3 presents the results on transfer quality and semantic preservation . our proposed system outperforms all stateof - the - art methods across all metrics on both datasets with a large margin . the results show that yelp significantly improves upon the state of the art on all metrics with a gap of 2 . 8 points in semantic and transfer quality compared to previous models . semantic preservation results are notably worse than those of google translate , in particular , the difference is much larger with respect to transfer quality . on the other hand , the improvement on semantic preservation is much smaller . our system achieves a transfer quality improvement of 1 . 6 points over the state - ofthe - art model on yelp , which shows the extent to which the semantic features captured by the model can be improved with a reasonable selection of the lexical features from the training data .
table 5 shows the results of human evaluation for each metric for validation on the yelp dataset . the summaries generated by our system match the human evaluation results , but are slightly inferior in accuracy compared to spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation . as expected , the difference between human and machine evaluation is less pronounced for pp metrics , but still significant enough to warrant a human evaluation . acc is the average number of tokens per sentence that match human evaluation , and it measures the degree to which a sentence contains grammatical errors . it is clear from table 5 that human evaluation methods yield significantly better results than machine evaluation .
the results of the models with different classifiers are shown in table 6 . the results are broken down in terms of performance on sim and goalnet , with the results of " + cyc " classifiers achieving the best results . adding the classifiers with para - lex and 2d as en ( 2018 ) coder improves the results for all but m1 . the difference is most prevalent in the case of shen - 1 , where the classifier with the most out - of - vocabulary features ( cyc + para ) obtains the best performance . in general terms , the results are similar for m1 and m2 , with m1 achieving the highest acc and gm scores of all classifiers when using both the language - para and the syntactic antonyms , moreover , the improvement is much larger when using the 2d classifier , which further boosts performance with the help of lexical redundancy .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results shown in table 6 show that our model achieves significantly higher acc than previous approaches that use different classifiers and language embeddings . note that the difference in acc between transfer and untransferred sentences is less pronounced for our model , but still suggests some reliance on human judgement . we notice that the classifiers in use achieve lower acc than in previous work , because the training set size is much smaller . however , transfer with human references yields the best results , and automatic classifiers like lm and founta achieve the highest acc , so we do not include these results in the table as they are worse than the results from simple - transfer .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent when we included repetition tokens and nested disfluencies . reparandum length is the average of the number of tokens in a sentence , so the difference between the length of repetition and disfluency is less pronounced for nested tokens . however , for rephrase tokens , the difference is much larger , reaching 1 . 5 times as much as in the case of repetition tokens . overall , the performance of our system is comparable to state of the art [ kutuzov et al . , 2018 ] , with the exception of the rephrase token , which is slightly worse than repetition .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . table 3 shows that for all three categories , our system predicts the majority of tokens to belong to each category well , with a relative f1 of 0 . 83 ( 50 % ) and 0 . 80 ( 32 % ) on par with the best previous model ( luo et al . , 2018 ) . the difference between the f1 and r1 scores for content - and function - function - disfluencies is less pronounced for the repair category , but still significant . reparandum length is the most important part of the picture , as it contains the most tokens , and it is the category that is most likely to contain a repair word . function - function also has a significant effect on prediction performance , however it is less significant .
the results are shown in table 2 . we observe that text - adaptive models perform well both when trained and tested on the single - domain dataset , with the exception of the case when the text classification task is performed in the unsupervised setting . the difference between the performance of text and raw variants is most prevalent in the dev mean , as the results show , when text is combined with innovations , the performance reach the best , and the average number of iterations taken to achieve the best result is close to those of the single model , however , for brevity we also observe that in the more realistic second case , when we only consider the test set with text and innovations , our model performs much worse than either the single or the combination model . this suggests that incorporating all the information available in the data during training may help the model to improve its performance in the future .
the performance of our model compared to the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the state of the art on all metrics except for the accuracy in relation to discuss . the difference is most prevalent in micro - f1 , where our model obtains an absolute improvement of 3 . 43 % over the previous state of - the - art embeddings .
table 2 shows the performance of all the methods that we consider for the document dating problems . our unified model significantly outperforms all the previous methods on both apw and nyt datasets , showing that the attentive neuraldater is better than all the other methods combined .
table 3 compares the performance of our method with and without word attention for this task . our approach shows that both word and graph attention have comparable performance , however the accuracy is higher for neuraldater . with respect to s - gcn , the method shows that it is more effective than the approach by using word attention alone . the accuracy of our approach also shows that graph attention alone does not harm the model performance , and in fact improves it considerably compared to word attention .
the performance of all models on this data is presented in table 1 . embedding + t achieves the best results , outperforming both cnn and dmcnn , with a gap of 10 . 6 points from the last published results ( hochreiter and schmidhuber 2008 ) . triggering and argument are the most difficult aspects of the task for both models to solve , as the performance gap between them is much larger than that on cnn . the difference is less pronounced for jrnn , but still significant , with an absolute improvement of 2 . 2 points over the previous state of the art on event coreference . from this group of models , the most interesting ones are the ones that rely on syntactic or semantic information embeddings . the jrnn model achieves state - of - the - art results on both event and argument coreference ; it significantly outperforms the previous approaches on both on - demand and offline test sets . on the other hand , when trained and tested on a single entity , the gap between the performance of the two approaches is much narrower , with jrnn achieving 6 . 6 vs . 7 . 3 points on average . semantic keyphrases alone achieve competitive or better results than any other approach , as shown next .
table 1 presents the system ' s performance on event prediction . the proposed method significantly improves upon state - of - the - art methods across all metrics , with the exception of identification . it significantly boosts the performance for cross - event tasks , since it requires significantly less data and time to train , compared to traditional methods . specifically , it achieves a joint proficiency ( p < 0 . 05 ) on all event prediction tasks with a gap of 10 . 5 points from the last published results ( kutuzov et al . , 2018 ) on argument prediction using the best performing feature set . on the other hand , it severely underperforms traditional methods in terms of both event prediction and role prediction , with an absolute improvement of 2 . 9 points on argument identification and 1 . 1 points on trigger prediction .
consistent with the observations by vaswani et al . ( 2018 ) , all models show lower precision on the dev perp and test set compared to the plain english - only model , the results of fine - tuning the model after applying our shuffling scheme seem to have far - reaching effects on the performance of all models , as can be seen in table 1 , fine - tune the model with different language features to improve the performance for each sub - category . the best results are obtained using the spanish - only - lm model , which achieves an absolute improvement of 2 . 36 points over the plain - english model . further , the difference is less pronounced for french - only and german - only , both when using the shuffled - lm and the original french - language embeddings , but still significant enough to warrant a mention . finally , we notice that the finetuned model performs better than the original ones when using only the concatenated part of the word " lexicorn " . when using both english - and spanish - language features , the performance gap between the two is much narrower , with the cs - only + vocab - lm showing a performance drop of 1 . 45 points compared to that of cs - last .
results on the dev set and the test set are shown in table 4 , fine - tuning the model with only subsets of the code - switched data improves the results for both sets . the improvement over cs - only training is substantial , with gains on both test and dev sets reaching 4 . 2 % and 2 . 5 % overall improvement over the strong baselines ( differences are statistically significant with t - test , p < 0 . 05 ) on both sets , which shows the advantage of finetuning the model during training .
the results of fine - tuning the dev set and the test set are shown in table 5 , where the results of language adaptation using the best performing dev model ( cs - only - lm ) are shown . the difference in accuracy between the two sets is minimal , however we see significant difference in performance between the test and dev set due to different types of gold sentences in the gold sentence set , which indicates the difficulty of adapting the lexical features of the dev model to the task at hand . fine - tuned - discing improves the results for both sets , since it reduces the repetition rate and the error of the final product , i . e . , the accuracy decreases as a result of more training examples having the same gold sentence in the set , compared to monolingual ones .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline to the current state - of - the - art is statistically significant ( p < 0 . 01 ) and r > r = 60 . 61 , which shows that type combined gaze features have a significant impact on the interpretability of our model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvement from baseline to type combined gaze features is statistically significant ( p < 0 . 05 ) and r > r = 94 . 03 , which indicates that the type - gathering feature has a significant impact on the model ' s performance . it also improves the f1 score by 1 . 3 points over the baseline , which again shows that the ability to combine gaze features induces the model to perform better .
results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . the hpcd system using glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 , and it uses the type - based skipgram embeddeddings derived from the original paper ( farrequi et al . , 2015 ) . the lstm - pp , on the other hand , uses syntactic skipgrams and word - error type - weights based on the type and type of tokens . the difference between the accuracy of the two systems is minimal , however we see significant performance drop when using the augmented - lexical - sg feature instead of the original type of wordnet , because the latter requires much more data and time to encode . further , the difference between automatic and manual extension is not statistically significant , it is clear from table 1 that both systems perform well with the use of syntactic and semantic typeweights .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results shown in table 2 show that the hpcd model significantly outperforms the original lstm - pp model with a gap of 10 . 59 % in accuracy compared to the performance of rbg with no features . also , the hpd model achieves a significant improvement over ontolstm on the full uas dataset , achieving 94 . 60 % ppa acc . and 98 . 97 % acc . on the oracle pp dataset , which shows the scalability of using object - aware dependency parsers .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that the model performs significantly worse when the context sensitivity is removed . in fact , the ppa acc . score of the model decreases by 9 . 5 % when we remove the sense and attention tags .
in table 2 , we report the bleu % scores of adding subtitle data and domain tuning for image caption translation using the three models described in section 2 . the results show that the domain - tuned model achieves gains over the en - de model on all three models . the difference is most prevalent in the multi30k dataset , where the ensemble - of - 3 approach achieves a gain of 2 . 8 % over " subsfull " . however , for en - fr and mscoco17 , the gains are less pronounced , with an overall drop of 1 . 6 % and 2 . 0 % respectively compared to the models using sub - sparse subtitle data .
the results of domain - tuned hoco are shown in table 3 . as can be seen , the models using domain - adaptive features perform better than those using monolingual features alone . the difference is most prevalent in en - de , where the effect is most pronounced for mscoco17 . adding all the labels from the domain - aware glossary improves the results for en - fr and flickr17 , but the difference is less pronounced for flickr16 . the difference between the hocos scores of the models with and without domain - specific features is minimal , but significant enough to warrant a significant improvement in our model .
table 4 shows the bleu scores of our model taking only the best 5 captions and adding automatic image captions after concatenating them with marian amun . the results show that , in all but one case , adding automatic captions improves the model ' s performance . the difference between en - de and en - fr and mscoco17 is less pronounced , but still significant , with an overall improvement of 2 . 2 bleus compared to the model using monolingual captions alone . adding all the captions , however , does not improve performance , and in fact decreases it by more than 2 . 5 points .
table 5 compares the bleu % scores of our method with the previous approaches using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and zsgnet . the results show that our encoder achieves the best results , outperforming all the other methods combined with a gap of 10 . 40 % in terms of visual information integration . further , the difference is less pronounced with respect to en - de , the difference between the w and w - score of encoder and decoder is less striking , however , it is still significant , encoding using the dec - gate method achieves a noticeable improvement over en - fr , as can be seen in table 5 , when using only plain text encoder , the model using sub - mlm performs better than the encoder using the best performing masking scheme .
we observe that the ensemble - of - 3 approach by subs3m achieves the best results , outperforming all the approaches that do not use multi - lingual features . the results are particularly striking when we consider the effects of the visual features on the detectron performance , visual features alone result in a significant drop in performance compared to the text - only approach , subsequently , all models using ms - coco outperform their monolingual counterparts on both datasets when we switch from the word " visual features " to " speech features " . the results of the second variation of our model , sub - 3m , show that incorporating the features of the ensembles of 3 improves the results for all but the case of gn2048 , the difference is most prevalent in en - de , as the results of this variation are less clear , but we observe that it is comparable on both flickr and mscoco17 , overall , the results displayed in table 3 show that the features obtained using the ensemble approach are beneficial , improving the generalization ability of the model by 4 . 36 points in the standard task formulation .
table 1 shows the performance of our system on the test set in terms of ttr and mtld . the results are presented in bold , en - fr - rnn - back and en - es - trans - ff achieve significantly better results than those of the monolingual systems , the difference is most prevalent in the translation of hotpotqa , as the comparison of the two approaches shows , once the word embeddings have been applied to the hidden regions of the speech , the models perform comparably to the original ones . in particular , the difference is much larger in mtld , due to the high overlap between the original and the targetted utterances , the effect of the back - transformation is sometimes not noticeable , however , this difference is less pronounced for hotpotqa than in traditional models , in our case , it is mostly due to the larger size of the difference between the mirrored objects in question , when using translate2vec and translate1vec , we do not have significant difference in performance between the two models on this test set , again , this is primarily due to large differences in the number of frames in question between the training and the test sets , with respect to the feature - rich mtld dataset , enfr - smt - back achieves the best performance , it achieves close to parity with the model by itself , but is inferior on both mtld and chime - 4 ,
for brevity we only report the number of parallel sentences in the train , test and development splits for the language pairs we used . the total number of sentences in our splits is 1 , 467 , 489 , which means that there are 723 , 487 words in total , which is more than double the number in the original embeddings .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the difference in performance between the two sets is minimal , however we see significant variation in the src scores due to different vocabulary types being used in the training set , which indicates that the training vocabulary is quite useful for the model development .
automatic evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems achieve better automatic evaluation results than the systems using ter and bleu , respectively , with an absolute improvement of 2 . 8 and 4 . 7 points over the baseline rev system . while the improvement on ter is modest , it is significant and shows that the model can be further improved with a reasonable selection of the lexical resource from which the transformation functions are derived .
table 2 shows the performance of our model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the one supervised using rsaimage . the difference in recall between the two approaches is minimal , however we see significant difference in the mean mfcc score due to different size of the training set and type of recall drops during the validation set development . rsaimage has the better overall performance , showing that the model is more suitable for production use . segmatch also has a lower recall percentage compared to rsaimage , showing the difficulty of finetuning a training set to a smaller size .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual supervised model . the difference in recall between the two approaches is minimal , however we see significant difference in performance between rsaimage and vgs due to the different embeddings used in these models . in rsaimage , the average recall number of frames per second is significantly higher than that of audio2vec - u , which shows the performance reach the limits of the learned reward function . on the other hand , the difference between chance and recall is much smaller in vgs , meaning that there is less chance to exceed the recall threshold in rsaimage . segmatch also achieves a significant improvement over chance , showing the viability of finetuned reward learning in the low - resource settings . we observed that the shared vocabulary approach effectively complements the task by increasing the recall and the mean mfcc score of the model , since it can be trained and tested on the generated rsaimages as well .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the dan classifier turns in a sentence that expresses the sentiment that the user wants to hate hate hate it . as the name implies , the rnn turns on a sentence with the aim to express negative sentiment , and thereby expresses the user ’ s desire to hate the object . cnn also shows the effect of the additional classifiers on sentence generation . it turns out that dan significantly improves the interpretability by increasing the number of correct answers when a given sentence is turned in , and the average number of incorrect answers is reduced as well . it is clear from table 1 that the use of these classifiers has a significant impact on the model performance , as shown in the second example , when a dan - trained rnn is applied to a screenplay , it turns out to be much more difficult to generate negative answers than the original .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same since fine - tuning the original sentence in sst - 2 . overall , rnp has increased by 3 . 5 % and outperforms cnn by a significant margin . the difference between the average number of occurrences for each part - of - speech sentence is less pronounced for rnp , but still significant enough to warrant a statistically significant change in our model .
the results in table 3 show that the sentiment score increases as a result of the flipped labels being flipped from positive to negative labels . the flipped labels cause the rnns to generate significantly more positive and negative sentiment , and the cnn scores to increase as well . it is clear from table 3 that once the negative labels are flipped out of the negative vocabulary , the sentiment scores on the cnns increases significantly .
table 1 presents the results on the validation set for sst - 2 and pubmed . the results show that our approach outperforms all the stateof - the - art methods on both datasets with a large margin . in fact , it achieves a positive f1 score of 98 % , outperforming all the other approaches with a gap of 10 . 5 % on average compared to the previous best results .
