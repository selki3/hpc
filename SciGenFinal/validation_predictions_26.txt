table 2 shows the throughput for processing the treelstm model on our recursive framework , fold â€™ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results show , both approaches benefit from the scalability of using a small number of instances to train a model , and from the highthroughput of training instances to inference .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
the results in table 2 show that the max pooling strategy consistently performs better in all models with different representation . specifically , the model with the best performance is conll08 ( hochreiter and schmidhuber 2008 ) , while ud v1 . 3 achieves the best results with a f1 score of 75 . 57 . as hard coreference problems are rare in ud , we do not consider them in this report .
table 1 shows the results for the best f1 models on each relation type without and with sdp . the results show that using the shortest dependency path to derive the shortest relation path has the greatest effect .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of models perform comparably to each other when trained and tested on the same dataset . however , in the more realistic second case , when trained on a larger corpus , the results are markedly worse . for example , the performance of y - 3 : y < italic : r < / italic > on the 50 % held - out validation data is significantly worse than that on the f1 data . this suggests that the model could rely on superficial cues .
the results for the second variation of our model , mst - parser , are shown in table 4 . the results show that , compared to our original model , the parser performs better on all three categories . on the essay level , it achieves 50 % and 50 % accuracy on average , respectively , better than the previous best model .
table 4 shows the overall performance of our lstm - parser system compared to the best previous state - of - the - art stagblcc model on the essay vs . paragraph level . the difference in c - f1 score between the two systems is less pronounced for the essay than for the paragraph level , however it is still significant .
table 1 shows the results for the original and the cleanups of models trained and tested on the same dataset . the results show that , compared to the original model , the cleansed tgen + model performs better on both datasets . the difference is less pronounced for models trained on the original dataset but still suggests some performance variation .
table 1 shows the data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the difference in overall performance between the original and the cleaned version is minimal , however we see significant difference in ser score due to different training and test sets having different training contexts .
table 3 presents the results of models trained and tested on the same dataset . the results are presented in bold . the results show that the original tgen model is more stable and therefore requires less data to train , while the difference between the original and the improved tgen + model is less pronounced . relis significantly outperforms sc - lstm in terms of accuracy on both training and test set .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the errors we found were mostly caused by errors caused by missing training data ( significant errors in the syntax highlighting , mostly in the wrong values ) , and slight disfluencies caused by adding correct values .
table 3 presents the results on both internal and external test sets . our model ( all ) achieves the best results , outperforming both the previous state - of - the - art on both datasets in terms of all metrics .
table 2 presents the main results on amr17 . our model ( dcgcn ) achieves 24 . 5 bleu points on average compared to the ensemble model of seq2seqb ( hochreiter et al . , 2018 ) and 27 . 5 on gcnseq ( damonte and cohen , 2019 ) . the difference in performance between ensemble and single model is less pronounced for dcgcn , but still suggests some scalability in the low - resource settings . as table 2 shows , both the size and type of parameters contribute similarly to the task , with the latter having a bigger impact . the model size that interacts with the decoder is the most important factor in the model performance . as the size of the ensemble decreases , the model performs less well .
table 1 shows the results for english - german and czech , compared to english - czech . the results show that , let alone a reduction in performance , the models perform better on both languages when trained and tested on the same dataset . the largest gains are seen in the german and czech datasets , both for the single model and the ggnn2seq variant .
table 5 shows the effect of the number of layers inside our dc model on the performance . the first group shows that , let alone a reduction in performance , more layers inside the model are crucial for the success of our model . the second group show that more layers , in turn , are beneficial for future work .
table 6 shows that the gcns with residual connections to the baselines are comparable to those without . adding rc and la information improves the results for both gcns . the difference between the bias metric and the rc + la metric is less pronounced for the dcgcn2 model , but still suggests some reliance on residual connections . when using only rc information , the difference between bias and la is less striking .
the performance of these models on the simulated test set is presented in table 4 . the results show that , let alone a reduction in performance , the models are well - equipped to perform this task . the average number of iterations per generation of the dcgcn model is close to those of the original ( dcgcn1 , 2 , 3 ) while the number of instances per model is shorter than the former state of the art .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that , let alone removing the dense connections in the i - th block , dcgcn4 has 25 . 2 % higher density than the previous state of the art model .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that , under the " - global node " and " - linear combination " models , the models perform significantly worse than those using - global node . also , we see that the results of " - direction aggregation " and - graph attention " show a considerable performance drop .
table 7 presents the performance of our initialization strategies for various probing tasks . our framework outperforms glorot and subjnum in all aspects ( depth , tense , coordinv , length , topconst ) except for the score for length . gloot ' s initialization strategy improves the generalization ability of our framework , and overcomes the performance limitations of gloot - based topconst .
table 3 presents the results on concatenated test sets . our h - cbow model outperforms the previous state - of - the - art on all metrics except tn and coordinv . it achieves the best results with a gap of 3 . 8 points from the previous best results .
table 3 presents the results on the hidden test set of subj , cr and mpqa . the results show that , let alone a drop in performance , the cbow model significantly outperforms the cmow / 784 model and the cmp . model , indicating that the training data injected into the model by the additional cost term are beneficial .
table 3 shows the performance of our models on the four downstream tasks as well as the results of unsupervised supervision . the results show that , compared to hybrid , our model perform significantly better on all the three downstream tasks .
table 8 presents the performance of our system ' s initialization strategies on supervised downstream tasks . glorot ( 86 . 6 % ) and mpqa ( 83 . 4 % ) achieve extremely high scores for both subj and cr ( 87 . 2 % ) and trec ( 83 ) . in comparison , n ( 0 , 0 . 1 ) and sick - r ( 71 . 2 % ) , a result not found to be significant even at the 90 % level ) even though it achieves a lower overall score .
table 6 shows the performance of our method compared to the state - of - the - art cbow model on the four downstream tasks . the results show that , compared to cmow - r , our approach is more accurate and therefore requires less training data and time to set up and maintain .
table 3 presents the results of models trained on the hidden test set of somo . the results show that , let alone a reduction in performance , the obtained precision is significant , and that the obtained objects are of high quality . topconst is indeed comparable to the original somo model , and indeed outperforms it in terms of all metrics .
the results of the best models are shown in table 5 . the results show that , let alone a reduction in performance , the cbow model consistently outperforms all the mod table 5 shows that the cbows - r model can learn to reason over more than simple sentences with simple examples .
table 1 shows the test bias scores of all models trained on the same loc dataset . in general terms , the results are presented in bold . our model ( tmtmil - nd ) obtains the best results . it outperforms all the other models apart from the name matching case . moreover , it outperforms the best previous models in terms of all metrics on both loc and misc datasets .
results on the test set under two settings are shown in table 2 . in both settings , the models show marked improvements in the f1 scores over the best previous model ( hochreiter and schmidhuber , 1997 ) . however , in the more realistic Ï„mil - nd model ( tables 2 and 3 ) , the results are slightly worse . name matching and supervised learning are only weak improvements above the previous best results . supervised learning , in turn , leads to a drop in the performance of both models . to test the effect of the additional cost term on model performance , we compare the results of both methods in the unsupervised and supervised settings . in the supervised setting , we observe that the model performing best in terms of f1 score is the result of a better model design .
table 6 presents the results of models trained on the stacked learner set . the results show that , let alone a reduction in performance , the g2s models are well - equipped to handle the task at hand . they yield strong baselines comparable to the best previous models ( cf . table 4 ) .
table 3 presents the results on the hidden test set of the bleu + meteor model in the distractor and fullwiki setting , compared to previous work on the topic . the proposed g2s - gat improves upon the previous state - of - the - art model on both datasets with good recall scores . on the ldc2015e86 and ldc2017e86 datasets , the model achieves a final score of 30 . 57 / 71 . 03 and 29 . 28 / 59 . 55 respectively , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous best state of the art .
table 3 shows the performance of our g2s - ggnn model on the ldc2015e86 test set when models are trained with additional gigaword data . the results show that compared to using glove pre - trained word embeddings , the model performs better on the test set .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that our model significantly outperforms the previous state - of - the - art models in terms of both generalization and precision .
the results of the second variation of our model are shown in table 4 . compared to the original model , g2s - ggnn shows a performance gain of 1 . 51 % and 0 . 95 % over the best baseline model , respectively .
table 8 shows the results for the second variation of our gold model compared to the original s2s model in terms of the fraction of elements in the output that are not present in the input ( added " vs . miss " in table 8 ) . as the name implies , the gold model only works on reference sentences , and is strictly limited to those that are in the lexical reference sentences . the token lemmas are used in the comparison . the results show that gold significantly outperforms the original model and the g2s - gat model ,
table 4 shows the pos and sem tagging accuracy for different target languages using the 4th nmt encoding layer of the dewey decoder . the results show that , compared to using glove pre - trained word embeddings , the pos feature is more useful for the task at hand , and consequently requires less training data and time to train . sem also shows a drop in accuracy compared to glove , indicating that the semantic features used in the pre - training layers have a high interpretability quality .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . our word2tag embeddings outperform the best - performing unsupemb encoder and word2tag encoder . furthermore , the upper bound on pos accuracy is significantly higher than the baselines , confirming the effectiveness of our model . word2tag also outperforms mft in terms of most tags .
table 4 presents the system ' s performance on the pos and sem tagging accuracy . we show the results for english and german captions . for english captions , the results show 8 . 28 % improvement on average compared to the previous state of the art . sem is 4 . 1 % better than fme model on average and 6 . 6 % better compared to golbeck et al . ( 2017 ) .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results show that the res and uni layers are comparable in terms of both precision and recall , while the bi layer is significantly better than the uni layer .
table 8 shows the attacker â€™ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the gender and race features are the most difficult to predict , however , the age and gender features are relatively stable . the classifier also shows the difference in the attacker score between the training set and the corresponding adversary .
as shown in table 1 , the training set size and accuracy are the most important factors in predicting gender stereotypes . the gender - neutral classifier outperforms pan16 in both tasks . overall , the classifier shows 87 % accuracy and 86 % recall .
table 2 shows the results for tasks with balanced & unbalanced data splits . the results show that the racial disparities in the task prediction accuracy are less pronounced for pan16 than those for winocoref , i . e . there is less variation in the average time taken to solve a task .
the performance of our model on these test datasets is shown in table 3 . the results show that the gender - based disparities in task performance are less pronounced than those in pan16 . however , the gender disparities still result in a significant drop in the attacker score . overall , the model shows that gender disparities are less prevalent than age and race in the task performance .
the results of the experiments shown in table 6 show that the guards on the leaky embeddings are comparable to those on the protected ones . however , the rnn encoders are more accurate , achieving an accuracy of 67 . 5 % compared to 62 . 8 % for the leaky encoder .
table 1 shows the results on the training set for the best performing model . our model outperforms the previous state - of - the - art on all metrics on both the ptb and wt2 datasets . the difference is most prevalent in terms of finetune tuning , which improves the model ' s performance on both datasets by 3 . 36 points in the final score . on the other hand , the difference between the best and worst performing model is less pronounced for wt2 . this suggests that finetune and tuning contribute differently to the task at hand .
table 3 presents the results on the training set of rocktÃ¤schel et al . ( 2016 ) . the results reconfirm that the training time and the average time taken to compute an sentence are the most important factors in the model ' s performance . the model performs well over both the base acc and time scales , with an absolute improvement of 2 . 5 % on the acc metric over the previous state of the art .
table 3 presents the results of experiments on the yelp and ama datasets . the results reconfirm that the translation quality of our model can be further improved with a reasonable selection of the lexical features from the previous literature . yelppolar err and time is consistently better than the best previous state - of - the - art model on both datasets , while atr is only slightly better than lstm . table 3 compares our model with previous work on yelp , amapolar and yahoo time . in particular , we highlight the results from zhang et al . ( 2015 ) and zhang et al . ( 2014 ) on the extractive and abstractive keyphrases , respectively , and summarize their results in table 3 . the proposed algorithm outperforms both published and unpublished work on every metric by a noticeable margin .
table 3 shows the case - insensitive tokenized bleu score of our model ( gnmt ) on the wmt14 english - german translation task . the model makes use of the best performing feature set available from the newstest2014 dataset , namely , lexical redundancy removal . gnmt also outperforms the best previous model , olrn , in terms of both training and decoder time . although the model has seen more training instances , it is still better than the other models at decoding one sentence per training batch . at the same time , the model is more accurate than the previous best model on the german translation task , giving a 0 . 9 / 3 . 26 bleu improvement over the previous state - of - the - art . table 3 also highlights the scalability of using multi - params learning pedagogical attention . as the results show , combining all features boosts the model ' s performance .
table 4 shows the exact match / f1 - score on the squad dataset of wang et al . ( 2017 ) . it can be seen that the model performs well with the assistance of elmo ( elmo et al . , 2017 ) in terms of match rate and f1 score . the model also benefits from the incorporation of the parameter number of base . as the results show , the number of parameters in the base is the most important factor in model performance . after incorporating elmo , the model achieves the best results . table 4 also highlights the extent to which the model can rely on superficial cues . in particular , we highlight the model performance with the " + elmo " parameter .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . as the results show , the model exhibits considerable performance improvement over the previous state - of - the - art models across all parameter sets . the model achieves the best result with a 90 . 56 % f1 overall score on the ner test set .
table 7 shows the test performance on snli and ptb tasks with and without base - based ln settings . with the base + ln setting , our model obtains 85 % accuracy and 61 % perplexity on the snli task compared to the previous best performing model ( glrn , glrn ) .
table 2 presents the results on paragraph prediction using the best feature set . our system outperforms all state - of - the - art systems in terms of all metrics on both datasets with two tasks . retrieving sentences with the best performance boosts r - 2 and r - 4 scores by 1 . 08 and 0 . 59 points over previous work .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with âˆ— ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 % and seq2seq achieves the highest overall quality . retrieval also receives high marks for grammatical accuracy and appropriateness ( appr ) , and content richness ( cont . ) , both on a scale of 1 to 5 ( best ) . although candela obtains the most accurate predictions ( 30 . 2 % ) on the content richness scale , it is only 2 . 2 % better than human on the grammatical aspect . in comparison , human is 7 . 5 % better at selecting the correct grammatical relations and 2 . 6 % more accurate than seqseq . the highest appr score is 2 . 3 % for content richness , which shows the extent to which the syntactic patterns captured by the model can be improved with a reasonable selection of the lexical resource from which the sentence was derived .
table 3 shows the test bias scores for english , spanish , french , dutch , russian and turkish for all models tested on the corpus and ted talks datasets . for english , the results are slightly better than those for spanish , but still significantly worse than the results for french . in particular , the difference between the en and pt scores is much larger for the ted talks dataset than for the other two .
table 3 shows the test bias scores for english , spanish , french , dutch , russian and turkish for all models tested on the corpus and ted talks datasets . for english , the results are slightly better than those for spanish , but still significantly worse than the others . in particular , the difference between the en and pt scores is much larger for the ted talks dataset than for the other two .
table 3 shows the test bias scores for english , spanish , french , dutch , russian and turkish for all models tested on the corpus and ted talks datasets . for english , the results are slightly better than those for spanish , but worse than that for french . in particular , the difference between the en and pt scores is much larger for the ted talks dataset than for the other two .
from left to right , we show the performance of the greek word embeddings for english , french , dutch , turkish , russian and turkish . for english , the results are presented in table 2 . overall , the number of terms in the dataset is 1 , 588 , the average depth is 11 . 05 and the average number of roots is 3 . 46 .
from left to right each row displays the averagedepth , maxdepth , averagedepth and depth cohesion of the greek word embeddings . for europarl , the average depth is 9 . 43 % , while the maxdepth is 2 . 29 % . according to the table , both are significantly better than the average of 1 . 7 % for df .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model ( lf ) outperforms the baseline model ( p1 ) in terms of both qt and rn ( table 1 ) . as the results show , the advantage of using softmax loss over regressive loss is also reflected in the higher rn and average f1 scores . however , the difference between the baseline and the enhanced version is less pronounced , i . e . the lf model achieves an f1 score of 6 . 08 vs . 6 . 63 for the baseline .
the performance ( ndcg % ) of the ablative studies on the different models on visdial v1 . 0 validation set is shown in table 2 . the results show that , when only applying p2 to the baseline model , the hidden dictionary learning is the most effective and hence requires the most accurate model to perform best .
table 5 shows that hmd - f1 and hmd - recall achieve remarkably similar performance on the hard and soft alignments . the results show that , let alone a drop in performance , the models using bert and wmd - bigram are comparable on both sets .
the results of the best models are shown in table 1 . the average score of bertscore - f1 and ruse + w2v on the direct assessment tasks is reported in bold . compared to meteor + + and svm , our model achieves higher performance on both the direct and indirect assessment tasks .
the experimental results on the training set of hotpotqa are shown in table 2 . the proposed hgn outperforms both published and unpublished work on every metric by a noticeable margin . for example , hgn achieves an f1 score of 0 . 55 on the " influence " and " nat " metrics , respectively , with an absolute improvement of 2 . 3 % and 0 . 7 % over the previous best state - of - the - art .
word - mover on the other hand , shows much better results on the training set when trained and tested on the original embeddings . the results are presented in table 9 . sentiment is consistently better than word - mover on all the training sets except for those using spice as the dependency distance . furthermore , the results are much better on the leic + recall dataset , showing that the semantic features extracted by the spice layer are more useful for the task at hand .
the results of the best models are shown in table 6 . the results show that the combination feature - rich training set benefits the models with the least and the best performing ones have the most consistent performance .
table 3 presents the results on the transfer quality and semantic preservation tests . the results show that yelp significantly outperforms the best previous models across all three domains . semantic preservation and transfer quality are particularly high in the black - box settings , while fluency is close to the state - of - the - art in all but one of the comparisons . to test the contribution of parameter sharing , we compare our proposed model to previous models trained on the same dataset . we observe that , let alone a drop in performance , the model performing best on the yelp dataset has the best overall results .
table 5 presents the results of human sentence - level validation on yelp reviews . the summaries generated by the models match the human evaluation criteria , but do not match the criteria for acc . as table 5 shows , the difference in accuracy between human and machine evaluation is less pronounced for yelp , but still suggests some overlap in the quality of the summaries .
the results of the best models are shown in table 7 . the results show that the combination feature - rich training set benefits the models with the least and the most from the syntactic and semantic diversity . the largest gains are obtained by the shen - 1 model , which significantly boosts generalization performance .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results shown in table 6 show that our best model ( fu - 1 ) achieves higher acc than previous work on similar sentiment transfer tasks . however , the results also show that it is better to transfer sentences untransferred than to transfer them with human references . since the training set size is small , we use only the best results for transfer .
in table 2 , we report the percent of reparandum tokens that were correctly predicted as disfluent as well as the average length of the repetition tokens in the original sentence . the difference in performance between nested and non - nested disfluencies is less pronounced for nested tokens , but still suggests some degree of dependency on repetition .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparlabelled and repaired tokens ( hence , there is no difference in the prediction quality between the two categories . the repair tokens have the highest percentage of tokens in the content word , constituting 50 . 3 % of the total prediction tokens . the rest of the tokens belong to each category .
the results of the best models are shown in table 2 . the text improvements over the best single model are 15 . 08 % on average compared to the average of both the best and the best - performing debiased models .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the previous approaches . it closely matches the performance of the best - performing cnn - based embeddings and self - attention sentences . moreover , it achieves 8 . 43 % higher accuracy on average compared to the average of the self - taught word2vec model . this indicates that our model can learn to reason over more than simple sentences .
table 2 shows the performance of all the methods that aim to improve the interpretability of supporting documents . our unified model significantly outperforms the previous stateof - the - art models on both datasets .
table 3 shows the performance of the component models for this task with and without word attention . neuraldater ( which relies on graph attention ) shows marked performance improvement . ac - gcn even gets better performance than oe - gcn ( 63 . 9 % vs . 63 . 2 % ) on the word attention task . with and without attention , the svm of neuraldater also improves .
the results of experiment 1 are shown in table 2 . the first group shows that , let alone a reduction in performance , the argument argument is crucial for the success of all models . the argument is the most important part of the model , and is what leads to argument identification . it is also what sets the model apart from the others . the second group shows the performance of different classifiers . trigger - based dmcnn and jrnn achieve state - of - the - art results , while cnn and its variants get close to the best results . finally , jmee achieves the highest score , surpassing all the other classifiers except for embedding + t .
table 1 shows the results for event identification and event classification . our proposed approach dkrn outperforms all state - of - the - art methods and features on both datasets in terms of both event and argument identification . specifically , it achieves the best results with a f1 - score of 68 . 7 on the event identification test set and rn ( p < 0 . 01 ) . the best results are obtained by using the cross - event method , which shows significant performance improvement over traditional methods .
the results for english - only and spanish - only models are shown in table 4 . the results show that , let alone a reduction in performance , the models using only one type of vocabulary perform better than those using all the other languages combined . perhaps the most striking thing about the accuracy drop is that it is almost entirely due to small size of vocabulary set : the average number of tokens in each vocabulary is the smallest in both languages , which indicates that the training set is very small . fine - tuned models perform similarly to the original ones .
results on the dev set and the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model improves upon the naive approach by 3 . 8 points in the standard task formulation and by 2 . 2 points in fine - tuned .
the performance of our model compared to fine - tuned - lm on the test set is shown in table 5 . in dfgn , fine - tuned discs give a 0 . 8 / 3 . 53 % boost in performance compared to fine - tuned monolingual text - switched word embeddings . additionally , the accuracy is higher on the dev set , both for the gold sentences in the set and for the standard english ones .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements from the baseline model to the type combined model are statistically significant ( p < 0 . 01 ) and r = 0 . 61 , both for the baseline and the current test set , which shows the significant improvement from a baseline baseline to a state - of - art model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvements from baseline to type combined gaze features are statistically significant ( p < 0 . 01 ) and r = 0 . 03 , both for the type combined model ( g & l ) and the baseline model ( a )
results on the belinkov2014exploring test set are shown in table 1 . the results show that ontolstm - pp , glove - extended and wordnet are comparable in difficulty to the original embeddings ( hpcd , syntactic - sg and verbnet ) . wordnet , on the other hand , achieves a performance improvement of 1 . 8 % over the original hpcd model ( farrequi et al . , 2015 ) and 3 . 7 % over ontolp - pp ( roche and schÃ¼tze , 2015 ) .
table 2 shows the results for the second variation of our dependency parser with features coming from various pp attachment predictors and oracle attachments . the results show that the hpcd model can further improve upon the performance of the original rbg model with the addition of ontolstm - pp and oracle pp features .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that the full model performs better than the attention - free model when the context sensitivity is removed .
in table 2 we report the bleu % scores of the models using subtitle data and domain tuning for image caption translation . the results show that the domain - tuned model performs better than the model using en - de and mscoco pre - trained word embeddings . the model using the best subtitle data is the multi30k model from ( marian amun et al . , 2017 ) on flickr16 .
the results of domain - tuned models are shown in table 3 . the h + ms - coco model outperforms the models using the best label labelling . the results show that , when labels are added to the en - de model , the model performs better both in terms of generalization and on - wiki captioning .
table 4 shows the bleu scores of the models using only one type of automatic image captions and marian amun ( cf . table 4 ) . the results show that , in all but one case , adding automatic captions improves the model ' s performance . the largest gains are on the multi30k dataset , which shows that incorporating all 5 captions bridges the gap between the performance of en - de and en - fr ( and mscoco17 ) .
table 5 shows the bleu % scores of our model using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and zsgmead . the results show that our encoder and decoder achieve better results than the en - de and mscoco models , both when using the correct mask layer and the correct decoder .
the results of " visual features " and " multi - lingual " models are shown in table 4 . the results show that , let alone a reduction in performance , the " + ensemble - of - 3 " model significantly outperforms the " text - only " model , indicating that the visual features in question are a significant part of the model ' s appeal . however , the results are less clear regarding the effect of the additional visual features on the overall performance . perhaps the most striking thing about the results is that the ' visual features ' alone do not improve the results for any model other than the text - only one .
table 3 shows the results for english translations on the hidden test set of hotpotqa . in general terms , the results show that en - fr - ht and en - es - ht models perform better than the models based on the previous state - of - the - art on all metrics except ttr .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of sentences per language pair is 1 . 467 , 489 , 499 , 487 and 723 , 723 , respectively , compared to 1 . 472 , 203 , 472 and 459 , 633 in en â€“ es .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results show that the models can learn to reason over more than simple verbs with a minimum of 80 % accuracy .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - trans - rev and en - es - rnn - rev models achieve better results than the models using the ter and bleu metrics . in particular , the improvements are much larger under rev than those under transformer ( table 5 ) .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the model supervised using rsaimage . in both cases , the average recall @ 10 and mean mfcc score are significantly higher than rsaimage , indicating that the model is more suitable for production use .
the experimental results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the second row labeled rsaimage is the model supervised using the best performing rsaimage embeddings . the average recall @ 10 and mean mfcc score of the models is 0 . 9 / 0 . 0 and 1 . 4 / 1 . 0 respectively , significantly higher than the previous best results . audio2vec - u also outperforms the other approaches with a large margin . as can be seen in the results table , the relative lower recall drops significantly for both audio2vec models .
table 1 shows the comparison of the different classifiers for use on sst - 2 . originally , all the examples shown in table 1 were for reference only . since the dan model only works on one type of screenplay , it has the advantage of being able to turn on a single sentence at once . however , this advantage is not shared by all models , as it can be seen in only one example . cnn also shows that it is easier to use than rnn and dan .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same since fine - tuning the original sentence in sst - 2 . as the results show , the number of occurrences in question has increased , but the overall number of words has decreased .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the sentiment score increases as a result of the flipped labels being flipped from negative to positive .
table 2 presents the results of the best and worst performances . results show that pubmed outperforms both sift and corr ( 98 % vs . 98 % ) , and sst - 2 by a noticeable margin .
