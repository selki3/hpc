table 2 shows the training and inference performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative approach shows the best performance on inference and training with a minimum of 10 instances per iteration .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 compares the performance of the balanced and the linear datasets with different tree balancedness using the same batch size and the same set of training instances . as the table shows , the performance increases as the size of the batch increases , but only when the parallelization is high enough .
the max pooling strategy consistently performs better in all model variations . table 2 shows the performance of our hyperparametrization approach compared to the default values with different representation configurations . the best performance is obtained with conll08 , with a f1 score of 1 . 57 / 9 . 05 and a dropout probability of 0 . 63 / 0 . 57 compared to ud v1 . 3 , sb and sigmoid , respectively . we observe that softplus also performs better than softplus in all models with different representations .
table 1 shows the effect of using the shortest dependency path on each relation type . our macro - averaged model obtains the best f1 score in 5 - fold test set without sdp and with sdp .
the performance of these models on the f1 and r - f1 metrics is reported in table 3 . the results are presented in bold . our model achieves the best performance on all metrics with an absolute improvement over the previous state - of - the - art on three of the four metrics .
the results are shown in table 1 . our model outperforms all the state - of - the - art models except mst - parser in terms of paragraph accuracy . we observe that our model achieves 100 % accuracy on average on all three categories . on the essay level , it achieves 50 % and 50 % respectively on average .
table 4 shows that the average c - f1 score for the two systems is 60 . 40 ± 13 . 57 % ( paragraph and essay ) and 56 . 24 ± 2 . 87 % ( paragraph ) . the mean performances are lower than the majority performances over the runs given in table 2 , but higher than the performance of the majority .
the results are shown in table 1 . the results show that when tgen is cleaned , it performs better than original tgen and sc - lstm on every metric except for meteor .
table 1 compares the original e2e data with our cleaned version . the difference in number of distinct mrs and total number of textual references between the original and the cleaned version is small ( 0 . 5pt / 2pt ) but significant ( 17 . 69 % vs . 11 . 42 % ) . the difference between slot matching and slot matching performance is less pronounced for the trained version , but still suggests that slot matching is beneficial .
the results are shown in table 3 . the original model outperforms the original model on every metric except meteor and rouge - lstm by a significant margin . as the table shows , the difference between original and original model is less pronounced when trained with tgen + and tgen − on the original test set . on the test set with the same training set , original model performs better than original model with a gap of 2 . 83 points in bleu score . when trained with the original and the original tgen model , the gap between the two is much smaller .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as can be seen , the number of errors we found in the training set is relatively low ( around 0 . 5 % ) , however , the size and type of errors are much larger than in the original set , indicating that the training data is properly balanced .
table 3 presents the performance of our model with respect to entity clustering . our joint model outperforms all the state - of - the - art models on all metrics except for tree2str , which achieves the best performance with an all score of 25 . 9 % on the ensemble test set . the only exception is graphlstm ( song et al . , 2018 ) , which achieves a lower all score than seq2seqk and snrg .
table 2 shows the performance of our ensemble model compared to the previous state - of - the - art models on amr17 . our model achieves 24 . 5 bleu points on average compared to 19 . 5 by the previous best ensemble model . table 2 also shows that our model achieves a significant performance improvement over previous models in terms of multi - parameter ensemble performance .
table 3 presents the performance of our model with respect to english - german , czech and slovak language embeddings . our model outperforms all the previous models except birnn and bow + gcn on all three languages . we observe that the performance gap between the single and multi - class models is small , with the exception of seq2seqb .
table 5 shows that the number of layers inside each dc layer has a significant effect on the performance of our model . our model obtains the best performance with n = 3 . 7 layers , and m = 22 . 3 . we observe that the size of the layers inside the layers has the greatest effect .
table 6 shows the performance of rc and rc + la models with residual connections compared to baselines . rc models outperform rc models without residual connections , but do not exceed the baselines in terms of overall performance . moreover , rc models perform better than rc models with multi - region connections .
the results are shown in table 4 . our model outperforms all the state - of - the - art models on every metric by a significant margin . we observe that the average number of iterations per generation is close to that of the best previous models , with the exception of dcgcn ( 2 ) , which is closer to the state of the art .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block reduces the overall number of connections , but does not improve the model performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that when domain - aware domain attention is used , the model performs better than when the model is combined with linear combination . the results also show that the coverage mechanism used by the decoder is beneficial , improving upon the performance of the original embeddings .
table 7 shows the performance of our initialization strategies on probing tasks . our glorot model outperforms all the other models except our paper in all metrics except for the tense metric .
table 3 shows the performance of our h - cmow model compared to the previous state - of - the - art h - cbow model . our model obtains the best performance with a gap of 3 . 6 points from the previous best state - ofthe - art .
table 3 shows the performance of our method compared to the state - of - the - art in terms of subj and mpqa sub - journals . our model outperforms all the other methods except for sick - e , which shows a slight performance drop . we observe that our cbow model performs better than cmow and trec on all subjournals except sst5 and sts - b .
table 3 shows the performance of our models on unsupervised downstream tasks as well as the relative change in cbow and cmow scores with respect to hybrid . the results show that our proposed method outperforms the state - of - the - art on most of the downstream tasks . however , on sts13 and sts16 , our model performs slightly worse than hybrid .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our system outperforms glorot and trec by a large margin . it achieves state - of - the - art performance on all three metrics . on sst5 and sts - b datasets , it achieves 87 . 6 % and 86 . 4 % overall improvement over the best previous work on these metrics .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our method outperforms the state - of - the - art cbow and cmow - r models on all the downstream tasks except for sts13 .
table 3 shows the performance of our method compared to the state - of - the - art cmow and cbow models on the hidden test set of somo and wc . our method outperforms the best previous approaches on every metric by a significant margin . the difference is most striking in the subtasks of concatenation and length , where our method obtains the best performance .
table 3 shows the performance of our method compared to the state - of - the - art in all three sub - topics . our model outperforms all the other methods except sick - e by a large margin . the difference is most prevalent in sub - category sub - categories , where our model cbow achieves the best performance .
table 3 shows the performance of our system in terms of supervised and unsupervised learning on the loc and misc datasets . our model outperforms all the state - of - the - art systems on all metrics except for name matching . the performance improvement over the best previous models is most striking when we consider τmil - nd , which achieves a final score of 57 . 57 % on loc and 43 . 59 % on misc dataset .
results on the test set under two settings are shown in table 2 . our model achieves the best performance with an f1 score of 43 . 38 ± 1 . 03 in name matching and 42 . 42 ± 0 . 59 in f1 on the supervised learning set . these results show that the supervised learning approach can further improve the generalization ability of the model without sacrificing performance in the task formulation .
table 6 shows the performance of our model compared to the state - of - the - art g2s models . our model obtains the best results with an absolute improvement of 3 . 86 points over the previous state - ofthe - art model .
table 3 presents the performance of our model compared to previous state - of - the - art models on the ldc datasets . our g2s model outperforms all the previous models except for konstas et al . ( 2017 ) . it achieves the best results on three of the four datasets ( ldc2015e86 , ldc2017t10 , and ldc2018t10 ) . on the other hand , it performs slightly worse on the other two datasets .
table 3 shows the performance of our model with additional gigaword data trained on the ldc2015e86 test set . our g2s - ggnn model outperforms the previous state - of - the - art models by a large margin .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model outperforms all the state - of - the - art models on every metric except meteor by a large margin .
we observe that g2s - ggnn model has the best performance on sentence length and average number of characters per sentence , with an absolute improvement of 3 . 51 % over the best baseline model ( gat - gat ) .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence as well as in the missing sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gat outperforms s2s in terms of both fraction of elements missing and miss as measured by the metric of missing sentence fraction .
table 4 shows the sem and pos tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . the results show that the pos features extracted from the 4th nmt encoding layer are comparable to the best state - of - the - art features in terms of semantic performance . however , the difference between pos and sem is less pronounced for the smaller corpus , indicating that these features are more useful for target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . our embeddings outperform the unsupervised embedding method by a significant margin . our encoder encoder decoder even achieves the upper bound on pos tagging accuracy . word2tag also achieves the best performance on sem tagging .
table 4 presents the system ' s performance on the pos and sem metrics . our model outperforms all the state - of - the - art systems on all metrics except for pos tagging accuracy . it achieves the best results on three of the four metrics , with a gap of 3 . 8 points from the previous state of the art on average .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the res and bi layers have the best performance , with res having the highest absolute precision and bi having the lowest absolute precision .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . for pan16 , we observe that the attacker performs significantly worse than the adversary on all three datasets .
table 1 shows the performance of our model with respect to training directly towards a single task . our model outperforms all the state - of - the - art models on every metric by a significant margin .
table 2 shows the performance of our model with balanced and unbalanced data splits . the results are shown in bold . our model outperforms all the baselines except pan16 in terms of task accuracy and gender - based features .
the performance on different datasets with an adversarial training set is shown in table 3 . as can be seen , the difference between the attacker score and the corresponding adversary ' s accuracy is small but significant , indicating that the presence of a pre - trained classifier can improve the performance for the task prediction .
table 6 shows the ablation accuracies of the protected attribute with different encoders . guarded embeddings perform better than leaky ones . the rnn encoder performs better than the leaky encoder when trained with only one protected attribute .
table 3 shows the performance of our model compared to state - of - the - art models in terms of finetune fine - tuning and multi - params . our final model outperforms all the previous models except the lstm by a large margin . the size and type of parameter space are the most important factors in our model performance , with an absolute improvement of 3 . 36 points over the previous state of the art on all metrics .
table 3 shows the performance of our model compared to previous work on the lstm and gru models . our model achieves state - of - the - art results on par with previous work by rocktäschel et al . ( 2016 ) and sru ( 5 . 28 % ) on all metrics except for the time taken to compute the base and last parameter space .
table 3 presents the performance of our model compared to previous work on yelp and amapolar time . our model obtains the best results on all metrics with an absolute improvement over the previous state - of - the - art on three out of the four metrics .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task on tesla p100 . our model obtains the best performance with an absolute improvement of 2 . 67 points over the previous state - of - the - art model on this task .
table 4 shows the exact match / f1 score of our model on the squad dataset . our model outperforms all the state - of - the - art models except for atr and gru in terms of parameter number and f1 score .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model achieves the best result with an overall score of 90 . 56 % on the three parameter - based ner tasks .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the lrn models outperform elrn and glrn on both tasks with base and ln setting .
table 3 shows the system and word embeddings performance on the word analogy task . the results are presented in terms of r - 2 scores and mtr scores . our system outperforms all the other systems except human when trained with oracle retrieval . the difference between human and system is most prevalent on sentence level , with oracle outperforming both systems on sentence and sentence level . sentence level is relatively consistent across all systems , with the exception of human , where it is much lower .
table 4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 2 points , which indicates that our system is comparable to the best human evaluation performed by h & w hua and wang ( 2018 ) in terms of overall quality . the second best result is obtained by candela ( 30 . 2 % ) on the content richness scale , which shows that it is comparable in syntactic and semantic quality to human evaluation . the third best result by seq2seq is achieved by retrieval ( 25 . 6 % ) , which shows a slight improvement over the performance of seq1seq .
the results are shown in table 1 . table 1 shows that our model outperforms the best previous models on every metric by a significant margin . we observe that the performance gap between en and r = p < 0 . 01 when using europarl and ted talks is small , but is much larger when using sub - tables and slqs .
the results are shown in table 1 . table 1 shows that our model outperforms the best previous models on every metric by a significant margin . the performance gap between en and r = p < 0 . 01 and r > 0 . 03 indicates that our proposed model performs better on datasets with fewer training examples . on the ted talks dataset , the gap is narrower but still significant .
table 3 shows the performance of our model compared to the best previous models on the test set of ted talks and europarl . our model outperforms all the other models on every metric by a significant margin . the results are shown in bold .
the results are shown in table 1 . we observe that the maxdepth and averagedepth metrics are relatively consistent across all metrics , with the exception of the depth cohesion metric , which is closer to zero for europarl . the maxdepth metric is closer than the averagedepth metric for all metrics except for docsub , which shows that it is more difficult to solve problems in shallow regions .
the results are shown in table 1 . we observe that the maxdepth and averagedepth metrics are relatively consistent across all metrics , with the exception of europarl , where the averagedepth metric is closer to zero . also , the number of roots per row is lower than in previous work , but higher than in slqs .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model outperforms the baseline model in terms of both qt and d scores . the difference between the performance of the enhanced and baseline model is less pronounced , but still indicates that our proposed approach can improve the performance in the low - supervision settings .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
table 5 shows the performance on hard and soft alignments . the hmd - f1 model outperforms all the other approaches except wmd - unigram and wmd - bigram with a gap of 0 . 7 % on hard alignments compared to ruse . hmd pre - training with bert and hmd + recall perform similarly to wmd + f1 , but with a larger recall drop . finally , ruse performs slightly worse than wmd + bigram and hmd - recall ,
the results are shown in table 1 . the average score of bert score and the average weighted average of ruse scores are reported in bold . we observe that bert scores are relatively consistent across all three sets , with the exception of ru - en , which is significantly worse than de - en and zh - en .
the results are shown in table 1 . sent - mover and w2v models outperform bertscore and meteor on all metrics except for f1 score . we observe that the quality of sfhotel is relatively high across all metrics , with the exception of bleu score , which is slightly lower than the other baselines . further , we observe that bert score is slightly higher than the bert scores on some of the other metrics .
the performance on the word - mover metric is reported in table 3 . word - mover performance is reported using the best multi - factor classification system , namely , word2vec and word3vec classification systems . the results are shown in bold . we observe that when word2vec is used in combination with bertscore - recall , sentence prediction performance is significantly better than when using the original wmd - 1 embeddings .
the results are shown in table 7 . we observe that the model with the best performance is m6 + shen - 1 + para , while m7 + para + lang gives the best result .
table 3 shows the transfer quality and semantic preservation metrics for yelp and semantic preservation . the results are shown in bold . our model obtains the best results with a transfer quality improvement of 3 . 7 points over the best state - of - the - art model on average across all metrics . we observe that semantic preservation and transfer quality are the most stable aspects of the model , with a gap of 2 . 6 points between the best and worst performances on average .
table 5 shows the results of human sentence - level validation for each metric for validation of acc and gm . the results show that the human ratings of semantic preservation and fluency are comparable to those of the machine ( p < 0 . 01 ) . however , the difference between human and machine ratings of linguistic recall is much larger , showing that human ratings are more sensitive to syntactic cues .
the results are shown in table 7 . para - para model outperforms all the other models with different classifiers except for shen - 1 , where it achieves the best performance . with the exception of m6 , all other models perform better than m7 when using the combination of para - and 2d classifiers .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher acc than prior work at similar levels of acc with the same number of tokens , but are worse than the best simple - transfer models ( left table ) . the difference between transfer and untransferred sentences is less pronounced when we use multi - decoder instead of tuple - based classifiers , but still shows significant performance drop when using only one classifier . we observe that our best model , yang2018 ( yang2018 ) , achieves the highest acc with a minimum of 22 . 6 bleus , which implies that the classifiers in use are better than the original ones .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent as well as the overall number of disfluencies for each type when we included repetition tokens . reparandum length is the average of the number of tokens in a sentence divided into repetition and disfluency tokens , and the overall percentage of tokens divided into these two types is reported in table 2 . as the table shows , nested disfuncions are less common than rephrase tokens , but still represent a significant drop in performance when we include them .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . for disfluency that consist entirely of function - function words , the prediction rate is only slightly higher than for those that consist only of a repair word . the average number of tokens in each category is less than the average number in the rest .
the results are shown in table 3 . we observe that the model performs best when trained with only one type of transformation , namely , when the text transformation is combined with the innovations transformation . moreover , when we add the text and innovations transformation , the model achieves the best results .
the performance comparison between our model and the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with an accuracy improvement of 3 . 43 % over the state of the art rnn - based embeddings . moreover , our model achieves a 3 . 53 % overall improvement over the self - attention approach .
table 2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . it achieves the best performance with an absolute improvement of 10 . 2 % over the previous state - of - the - art model .
table 3 shows the performance of our neural dater with and without word attention . our approach shows that word attention alone does not improve the performance for this task , however , graph attention does improve it by 3 . 6 points .
the results are shown in table 1 . we observe that the model with the best performance is the one with the highest t - score , followed by jrnn and jmee . further improving performance over the previous state - of - the - art neural models , we observe that argument argument is the most difficult stage to solve , while embedding model performs best on all other stages . the performance gap between the best and worst models is small , with the exception of dmcnn .
table 3 presents the results on event identification and event classification . our model outperforms all the state - of - the - art methods on both event and argument identification . the results are presented in terms of f1 and f1 points , with a gap of 3 . 7 points from the previous state of the art on argument identification and 4 . 9 points from cross - event classification . we observe that our model performs well on both triggers and event features .
the results are shown in table 3 . we observe that the fine - tuned model outperforms all the other approaches except for spanish - only , which shows that it is more difficult to fine - tune the final model than the original one . further , the performance drop is less pronounced for english - only than for other languages , indicating that the training data are more concentrated within a single language .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms fine - tuned cs - only model on both sets , showing that the training set size and the number of training instances are the most important factors in the performance improvement .
table 5 shows the performance of fine - tuned and fine - tuned models on the dev and test sets , respectively , compared to monolingual models using the standard rl - based learning schemes . the fine - tuned model achieves 75 . 40 % accuracy on the test set compared to 62 . 20 % accuracy in the dev set and 75 . 60 % on the standard set .
table 7 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision and recall compared to the baseline model using only the baseline gaze features .
table 5 shows precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as inputs . the improvement over baseline is statistically significant ( p < 0 . 01 ) with a f1 score of 0 . 03 ( t - test , p < 0 . 001 ) .
the results on belinkov2014exploring ’ s test set are shown in table 1 . syntactic - sg embeddings outperform glove - extended and ontolstm - pp on all metrics except for the test acc . metric . the difference between the type and type of tokens is small but significant , with an absolute improvement of 3 . 7 points over the previous state - of - the - art hpcd model . further improving performance by 4 . 8 points on the test set with the same type of token over the original model .
table 2 shows the performance of our dependency parser with features derived from various pp attachment predictors and oracle attachments . our hpcd model obtains the best performance with an accuracy of 98 . 97 % on the full uas test set .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a ppa acc . score of 89 . 7 % on the full test set .
the results in table 2 show that domain tuning improves the bleu % scores for both en - de and multi30k models , and improves the general performance for all models except for marian amun ( see table 2 ) . the domain tuning also improves the sub - domain performance for en - fr and mscoco17 models , but the improvement is less pronounced for the former .
we observe that domain - tuned models perform better than those without . the h + ms - coco model outperforms the lm + ms model on all datasets except for en - de and flickr16 , where it performs better than mscoco17 .
table 4 shows the bleu scores for en - de , en - fr and mscoco17 models as well as the multi30k model for marian amun . the results show that automatic captions improve the general performance for all models except for those using multi - task learning on flickr16 and flickr17 .
the results in table 5 show that enc - gate and dec - gate strategies are superior to en - de and mscoco17 on flickr16 and flickr17 , respectively , in terms of bleu % scores . however , when using multi30k + ms - coco + subs3mlm with detectron mask surface , the results are slightly worse than those using transformer , indicating that more information is required to integrate visual information into the model .
we observe that the ensemble - of - 3 approach outperforms the monolingual approach by a large margin . the results are shown in table 3 . multi - lingual approaches outperform the text - only approach when we consider only visual features . the results of en - de and en - frandonation are reported in table 1 . as the results show , when we only consider visual features in the ensemble of 3 , the performance of the subs3m and subs6m models is significantly worse than when we include all the other cues .
the results are shown in table 3 . the results show that en - fr - ht and en - es - ht embeddings are comparable in terms of performance with respect to ttr and mtld . however , the performance gap between the two sets is much larger when en - rnn - ht is used , as shown in fig . 2 . as expected , the transition speedups are small but consistent with the improvement in mtld performance over the baseline .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – es model splits into 7 language pairs for each training and development split .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in bold . our model outperforms the best previous models on average in both languages .
table 5 shows the automatic evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems perform similarly to each other in terms of bleu and ter scores . however , the performance gap between the two sets is much smaller when using the transformer transformation scheme .
table 2 shows the performance of our model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsa is the trained model from chrupala2018 . our model obtains the highest recall @ 10 . 0 and the median rank is 0 . 0 .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . it achieves the best performance with a mean recall of 27 . 4 % and a median rank of 0 . 9 % higher than the best performing rsaimage model .
table 1 shows the example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that has edges at the edges and a on ( in in the the the edges ’ s so clever “ want to hate it ” sentence ) . the rnn classifiers turn in screenplay that have edges at edges and edges . cnn turns on a on sentence that is in the shape of a question mark . as can be seen in the table , the difference between the average number of turns in the screenplay and the original is less pronounced for dan than for cnn .
table 2 shows that the number of occurrences in sst - 2 has increased , decreased or stayed the same through fine - tuning . the last row indicates the overlap with the original sentence in terms of part - of - speech pos . the numbers indicate the changes in percentage points with respect to the word embeddings in the sentence as well as the percentage of occurrences for each word in the correct sentence . we observe that the rnn has learned to fine - tune significantly more terms for nouns , verbs and adjectives . however , the rnp has not learned much about the structure of the sentence .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results in table 3 show that the sentiment score increases as a function of the number of tokens flipped from negative to positive .
table 1 presents the results for pubmed and sst - 2 . our approach outperforms all the other approaches except for corr et al . ( 2018 ) by a large margin . the results show that our approach is comparable to the best previous approaches .
