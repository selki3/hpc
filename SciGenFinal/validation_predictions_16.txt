table 2 shows the throughput for training and inference of our treelstm model on our recursive framework , fold â€™ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 also shows the improvements in throughput over iterative and recursive approaches . since the training data is small , the average number of instances per training iteration is less than the number of iterations per inference iteration , making it more suitable for production use .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . table 2 presents the results for ud v1 . 3 , conll08 , sb , and softplus models with different representation . with different representation , the hyperparametrization performance of these models can be further evaluated with the f1 scores reported in table 2 . the softplus model obtains the best performance with a f1 score of 1 . 83 .
table 1 shows the effect of using the shortest dependency path on each relation type . as the results show , macro - averaged models generally perform better than the best f1 models without sdp ,
the results are shown in table 3 . the performance of the best performing models is reported in terms of r - f1 and f1 scores . for example , the y - 3 model ( yuan et al . , 2017 ) obtains the best results with a f1 score of 51 . 59 / 71 . 86 and a r / f1 of 53 . 71 / 59 . 86 .
the results of paragraph prediction accuracy are presented in table 1 . we show that mst - parser achieves 100 % accuracy on average on all three categories with a gap of 2 . 59 points from the previous best state - of - the - art system .
for the two indicated systems , we report the mean and average c - f1 scores of the best performances on the test set , respectively . note that the mean performances are lower than the majority performances over the runs given in table 2 . the average score for paragraph level is slightly higher than the average for essay level .
table 3 presents the results of models trained and tested on the original and cleaned datasets . the results are presented in bold . as can be seen , the original model is significantly worse than the cleanups performed by all the other systems except sc - lstm . table 3 shows that , when the training data is optimised , the error reduction is much greater .
table 1 compares the original e2e data with the cleaned version . the difference in mr statistics can be seen in table 1 , with the original having 17 . 42 % more distinct mrs and a higher ser percentage , while the original has 10 . 69 % more textual references .
table 3 presents the results of models trained and tested on the same training data . the results are presented in bold . as the table shows , original and original models perform comparably to each other , with the exception of sc - lstm , which obtains the best results with a gap of 2 . 83 points from the original .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the errors we found were mostly caused by errors caused by missing training data , however we did find a slight percentage of errors due to slight disfluencies in the training data .
the performance of our model compared to previous approaches is presented in table 1 . graphlstm ( song et al . , 2018 ) obtains the highest all score , surpassing all state - of - the - art models except for tree2str , which is only slightly better than parallelism . further , the gap between the best and worst performing models is narrower with respect to entity representation , table 1 shows that the hierarchical structure of the entity representation is the most important factor in model performance , followed by the number of entities used for entity representation . all models perform similarly on the single - domain and entity representation datasets , as shown in the table , the hierarchical clustering of entity representations is crucial for model performance improvement .
table 2 presents the performance of our model on amr17 . table 2 shows that our ensemble model achieves 24 . 5 bleu points . by comparison , the previous state - of - the - art models seq2seqb ( beck et al . , 2018 ) and ggnn2seq achieve 21 . 6 and 27 . 5 points , respectively . the difference in performance between ensemble and single - model models is most prevalent in the low - supervision setting , as shown in table 2 , the difference between s and e indicates that the number of parameters in our model is small , but large enough to allow the model to achieve high precision .
table 3 presents the results of models trained and tested on english - german , czech , french , spanish , dutch , russian and turkish . the results are presented in table 3 . as the table indicates , all models perform comparably to the best single models except for birnn + gcn , which achieves state - of - the - art results in all languages except english .
table 5 shows the effect of the number of layers inside a dc network for english , spanish , french , dutch , russian and turkish captions . the first group shows that for english captions , bdi has the least effect , while for spanish captions and russian captions it has the most significant effect .
comparisons with baselines are shown in table 6 . rc denotes residual connections , and refers to gcns with residual connections . as a baseline , we compare our model with the best performing baselines ( dcgcn1 , 2 , 3 ) and the average number of connections per gcn , adding rc reduces the gap between the bias metric ( b 16 . 8 vs . b 18 . 9 ) and achieves the best performance overall .
the results are shown in table 4 . the first group of results show that , for example , the hierarchical clustering approach based on dcgcn ( 1 ) achieves state - of - the - art results with a f1 of 1 . 4 and bias score of 0 . 9 / 10 . 4 . the second group shows that the model is more than 4 . 5x better than the previous state of the art model on most metrics .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . as the table indicates , removing the dense connections in the i - th block degrades the model , and consequently decreases the performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that , under the best performing model , the domain - aware approach applies the best coverage mechanism . after applying the coverage mechanism , the model exhibits the best performance . as shown in the table , the best results are obtained under " - global node " and " - linear combination " .
table 7 presents the scores for initialization strategies on probing tasks . glorot obtains the highest score with 35 . 8 % on average compared to our 35 . 2 % overall score .
the results are shown in table 3 . we observe that the h - cbow method outperforms the previous state - of - the - art approaches on every metric by a significant margin . the difference is most prevalent in the last group of data , subjnum , with a gap of 3 . 8 points from the last published results . while the gap is slim , it is significant enough to warrant further study .
table 3 presents the results of models trained and tested on the same dataset . the results are presented in bold . hybrid models outperform both monolingual and multi - lingual methods . we observe that , for example , cmow / 784 achieves 90 . 6 % improvement on sick - r score compared to the previous best state - of - the - art model , while cmp . 7 achieves a marginal gain of 0 . 2 % over the previous state of the art model . although the improvement is slim , it is encouraging to see that it is possible to improve the sub - score for some sub - committees as well .
table 3 shows the relative improvements on unsupervised downstream tasks attained by our models . the cbow method shows a considerable performance gain over the hybrid approach . it achieves gains of 26 . 5 % and 25 . 8 % over the cbow baseline on sts12 and sts14 , respectively , while hybrid achieves a gain of 42 . 2 % and 21 . 0 % . as the results show , the performance gain comes from a better model design with respect to the four sub - sts13 tasks , the difference between cbow and cmow is less pronounced , but still significant .
table 8 presents the performance of our system for initialization and supervised downstream tasks . glorot outperforms all the state - of - the - art systems on all three metrics with a gap of 3 . 6 points from our previous work .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our method outperforms the state - of - the - art cbow - r model in all but one of the cases .
table 3 presents the results of our final model on the hidden test set of somo . the results are presented in bold . our proposed method outperforms the previous state - of - the - art methods on every metric by a significant margin .
the results are shown in table 1 . the first group of results show that the cbow - r method outperforms all the other methods except sick - e , indicating that it has the advantage of training on a larger corpus of data .
the results are shown in table 1 . name matching and supervision are the most effective for error reduction , followed by hierarchical clustering . in general terms , all loc and misc - based systems perform similarly , with the exception of Ï„mil - nd , which obtains a marginal improvement of 0 . 36 points over the previous state - of - the - art model . supervised learning is signifi cantly better than all the alternatives except name matching , in particular , it obtains an absolute improvement of 3 . 48 points in loc and 7 . 03 points in misc .
results on the test set under two settings . in table 2 , we report the f1 scores of all models trained and tested on the same test set . name matching and supervised learning achieve the best results , with a f1 score of 15 . 03 Â± 0 . 59 and 15 . 38 Â± 1 . 03 respectively . further , Ï„mil - nd ( model 2 ) shows a significant improvement over the previous state - of - the - art model , improving from a 35 . 42 Â± 0 . 59 eq . score to a 43 . 63 Â± 087 f1 result in less than a day . supervised learning also achieves the best result , improving to a 42 . 63 / 71 . 59 f1 outcome on a test set with a 17 . 42 / 29 . 03 comparison .
table 6 presents the results of model training on the hidden test set of g2s - gat . the results are summarized in table 6 . our model obtains the best results with an absolute improvement of 3 . 86 points over the previous state - of - the - art model .
table 3 presents the results of experiments on the ldc2015e86 and ldc2017t10 datasets . our model outperforms all the previous state - of - the - art models except for konstas et al . ( 2015 ) . g2s achieves the best results with a bleu score of 29 . 28 / 71 . 53 and meteor score of 30 . 16 / 71 , respectively , while s2s is only marginally better at 28 . 42 / 59 . 59 and 21 . 63 / 59 , respectively . the results of previous studies are summarized in table 3 . we report summaries of the best performances on three of the four datasets . the results are broken down by year of publication , with the average of the three summaries being reported in parentheses .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model outperforms all the previous models with a gap of 10 . 60 bleu points from the last published results .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model obtains the best results with a bleu score of 62 . 42 % and meteor score of 59 . 59 % .
the results are shown in table 1 . we observe that g2s - gin outperforms all stateof - the - art models in terms of average sentence length , average graph diameter and average number of frames per sentence , with an absolute improvement of 1 . 51 % and 0 . 59 % over the previous state of the art model , respectively .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) and the fraction of miss , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s outperforms the other models in terms of both productivity andiss percentage . g2s - gat shows lower productivity , but higher miss percentage , indicating that the model is better able to learn the task to extract the most relevant information from the input and generate the output .
table 4 shows the pos and sem accuracy for the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos features significantly improve the pos tagging accuracy over the original embeddings .
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform unsupemb embedding models in terms of pos , sem and most frequent tags .
table 4 presents the results of the best performing models for english and spanish captions . our model outperforms all the other models except for the case of the pos tagging accuracy . it achieves the best results with an accuracy of 91 . 8 % on average and 91 . 9 % on sem tagging accuracy .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as shown in table 5 , the residual encoder performs best , while the uni encoder obtains the best overall performance .
table 8 shows the attacker â€™ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the gender - based insults are the most difficult for the attacker to predict , followed by age , race , and sentiment . finally , the presence of a gender - neutral pronoun indicates that the attacker has learned to pick out the most offensive parts of the speech and target these insults .
table 1 : accuracies when training directly towards a single task . as shown in the table , the pan16 model significantly outperforms the state - of - the - art f1 model in terms of accuracy on all three aspects .
table 2 presents the results of unbalanced and balanced data splits . as expected , gender - based and racial disparities are most prevalent in pan16 data , followed by age disparities . gender - based disparities are less prevalent in the unbalanced data split , but still represent a significant racial disparity . the presence of racial disparities in terms of task accuracy and task accuracy is the most prevalent under pan16 . sentiment and age disparities are the least prevalent , however , indicating that there is still a need to learn more about these disparities .
as shown in table 3 , the adversarial training set establishes a new state - of - the - art model for the task prediction on the pan16 dataset . it achieves the best performance with an f1 score of 7 . 9 % on the dial task and leakage of 9 . 2 % on pan16 .
the results in table 6 show that the rnn encoders significantly outperform the weak embeddings . the difference in performance is most prevalent when the protected attribute is named " leaky " . the rnn embedding model significantly outperforms the weak encoder ,
table 3 presents the results of our final model on the hidden test set of wt2 + finetune . the results are presented in table 3 . our model outperforms the previous state - of - the - art models on every metric by a significant margin . specifically , our model obtains the best results on all metrics with a gap of 3 . 36 points from the last published results ( ptb + finetune ) , while surpassing our previous best by 3 . 59 points .
table 3 presents the results of experiments by rocktÃ¤schel et al . ( 2016 ) on the training set of lstm and gru . the results are summarized in table 3 . as the table indicates , the training time increases as the number of parameters increases , the average number of iterations increases , and the average time taken to compute each iteration decreases . table 3 compares the performance of these models with other stateof - the - art models trained on the same training set . specifically , we compare against the following baselines : + ln acc time , + bert time , * ( cid : 27 ) n + k âˆˆ 0 . 01 ) this model obtains the best performance with an absolute improvement of 3 . 36 % on the base acc metric .
the results of zhang et al . ( 2015 ) are summarized in table 1 . table 1 presents the results of the best performing models on the four domains for english , spanish , french , russian and spanish . our model obtains the best results with an absolute improvement of 3 . 36 % on average compared to the previous state of the art .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the summaries in table 3 summarize the results of the training and the decoder . our model obtains the best performance with a case - in - error score of 34 . 33 % on the training set and a corresponding 45 . 67 % case - out - of - vocabulary score on the decoding set .
table 4 shows the exact match / f1 - score of our model on squad dataset . as the table 4 shows , all the parameter numbers we consider have a significant impact on the model performance . the most interesting ones are the parameter number of base . after adding elmo as a dependency term , our model obtains the best results . finally , the sru also achieves the highest f1 score with 7 . 41 / [ bold ] 79 . 83 and slightly outperforms the previous best state - of - the - art model , atr , with 6 . 45 / 7 . 41 and 7 . 08 / 6 . 59 points respectively . table 4 summarizes the results of our final model . it follows wang et al . ( 2017 ) model closely in terms of match rate and f1 scores . however , the difference in parameter number between atr and lrn is not statistically significant ,
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model obtains the best result with a 90 . 56 f1 . it closely matches the performance of the best previous model , lample et al . ( 2016 ) .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model improves its performance on both snli and ptb tasks . it achieves the best results with a gap of 9 . 56 % in accuracy over the previous stateof - the - art model .
table 3 presents the results for english , spanish , french , dutch , russian , turkish and turkish . retrieving the max - bound word embeddings for each system is presented in table 3 . we show r - 2 and mtr scores for both systems . for english , we show the results of b - 2 , b - 4 and b - 5 while excluding system retrieval . the results are presented in tables 3 and 4 .
table 4 presents the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with âˆ— ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is more than likely to exceed human evaluation standards in terms of grammatical quality . table 4 summarizes the results of human evaluation . our system obtains the highest average score on all three metrics , surpassing the previous best automatic systems by a significant margin .
the results are shown in table 1 . table 1 shows that for all 3 domains , our approach outperforms the previous state - of - the - art approaches on average except for the case of ted talks . for example , europarl outperforms df and docsub by a significant margin . on the other hand , for ted talks , we see a drop of more than 2 points . the performance gap between en and pt is narrower than that of df and slqs , indicating that our approach has superior generalization ability . finally , for the three domains , we show that our proposed hclust model outperforms all the other baselines except for corpus .
the results are shown in table 1 . table 1 shows that for all 3 domains , our approach outperforms the previous state - of - the - art approaches on average . on the three domains , the average en and pt scores are significantly higher than the previous best state - ofthe - art systems on all but one of the test sets . for example , on the " ted talks " dataset , europarl achieves an en / pt score of 0 . 57 / 0 . 59 and 0 . 44 / 071 , respectively , with an absolute improvement of 2 . 36 / 037 and 6 . 45 / 4 . 36 points over the previous system . the " docsub " dataset outperforms all the other two baselines except for " slqs " . on the other hand , it gets a marginal improvement of 1 . 59 / 2 . 01 and 3
the results are shown in table 1 . table 1 shows that for all 3 domains , our approach outperforms the previous state - of - the - art approaches in terms of en , eps , and pt scores . according to the table , the best performances are obtained by our approach is obtained on the " ted talks " dataset , while the " europarl " and " docsub " datasets are slightly better than the others . finally , the " hclust " dataset outperforms all the other baselines except " slqs " .
for europarl , the average depth is 11 . 05 , slightly higher than the previous state - of - the - art average , while the maxdepth is 3 . 46 . the difference is mostly due to the smaller size of the training data set , which means that more training instances are required to obtain the correct maxdepth . as shown in table 1 , maxdepth and averagedepth are the most important factors in evaluating the quality of a dataset , followed by number of roots , average length of derivational trees , and clustering quality .
for europarl , the average depth is 9 . 43 , slightly higher than the maxdepth , while the average number of roots is 2 . 29 , slightly lower than the europarl average . the maxdepth metric is used to calculate the number of tokens in a paragraph , the type of entity , and the clustering quality of the total terms . according to table 3 , the max and average depth metrics are the most important factors in evaluating the quality of a paragraph . as the table shows , both maxdepth and averagedepth measures are relatively high for a single entity , but the difference is less pronounced for a larger number of entities .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our lf model outperforms the baseline model in terms of both qt and r0 scores . as the improvement in qt indicates , the enhanced version can further improve the generalization ability of our model . moreover , the difference in ndcg % between baseline and enhanced model is less pronounced , though still significant .
the performance ( ndcg % ) of the ablative studies on the visdial v1 . 0 validation set is shown in table 2 . as table 2 shows , applying p2 indicates the most effective one ( i . e . , hidden dictionary learning ) , while only applying p1 indicates the less effective one . with respect to rva metrics , the model achieves the best performance with 73 . 63 % on the baseline validation set .
table 5 compares our hmd - f1 model with previous approaches on hard and soft alignments . the results are presented in table 5 . first , we report the ruse scores for cs - en , de - en and lv - en . compared to the previous state - of - the - art hmd models , our hmd model performs significantly better on both soft and hard alignments , with a gap of 0 . 823 points from the previous best state - ofthe - art model .
the results are shown in table 1 . the average score of the baselines and the average number of frames for each setting is reported in bold . for example , meteor + + has the best average score , while bertscore - f1 is close to the mid - point . sent - mover has the worst performance , with an average of 0 . 815 and a gap of 3 . 7 points from the previous best state - of - the - art system . when we add in the w2v metric , our average baseline improves further , to 0 . 866 .
the results are shown in table 1 . the first group of results show that the bleu - based system outperforms all the baselines except meteor , the second group shows that bertscore achieves the best results with an f1 score of 0 . 176 . next , we observe that the sfhotel model significantly outperforms other baselines in terms of all metrics with two out of the four scenarios . when using the w2v - based baselines , the sent - mover achieves the highest score , reaching 0 . 181 .
the results are shown in table 1 . word - mover accuracy is relatively consistent across all baselines with the exception of leic ( * ) where it is significantly better than the average . the classification performance of the word - mover is also comparable across all the baselines except for spice , which is only slightly better than meteor . when using bertscore - recall as recall baseline , the average bert score is slightly higher than the other baselines , but still significantly lower than other methods . sentance accuracy is also significantly less consistent than other approaches , with an average of 0 . 723 on m1 and 0 . 949 on m2 .
the results are shown in table 1 . para - para - based models outperform all the alternatives except shen - 1 , indicating that the syntactic patterns chosen by the model can further improve the generalization ability of the model . as shown in the table , the effect is most prevalent in the lower - supervision settings , where m0 and m0 are both consistently better than m0 [ italic ] with a gap of 0 . 81 points .
table 3 presents the results of model transfer quality and semantic preservation on the yelp dataset . the results are presented in terms of transfer quality , semantic preservation and precision . table 3 shows that yelp significantly outperforms other models in all three respects . semantic preservation is particularly strong , with Î´pp > 0 . 5 indicating that the model has the best overall performance . in particular , the semantic preservation results show that yelp has the highest f1 score among all the models .
table 5 shows the results of human sentence - level validation for each metric . the summaries generated by our system match the human ratings of semantic preservation and fluency , but do not match the accuracy of acc .
we show the results of models trained only on shen - 1 and max - score . the results are presented in table 1 . as the table indicates , when training with only shen , the model performs better than it does with any other combination of language features . para - para models significantly outperform all the alternatives except for the case of sim we observe that , for example , m0 [ italic ] + cyc = 0 . 817 , m1 [ italian ] + para = 1 . 012 , m3 [ italy ] + 1 . 012 and m7 [ ger ] > 0 . 805 .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher acc than prior work at similar levels of acc with the same number of tokens , and their transfer accuracy is 31 . 4 % higher than the best previous work . note that the definition of acc varies by row because of different classifiers in use . the multi - decoder model achieves the highest acc , while the delete / retrieve model obtains the lowest . finally , the classifier with the best performance is the yang2018 model , achieving 22 . 3 % acc .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of repetition tokens in a sentence , the average number of iterations in the sentence , and the overall number of disfluencies . as the table 2 shows , repetition tokens have a high impact on prediction performance , leading to significantly less accuracy .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , as shown in table 3 , is the fraction of tokens in each category that belong to each category .
the results are shown in table 1 . as the results show , the text - based approach outperforms the single - domain approach when trained and tested on the same dataset , and when the model is tested on both the dev and test sets with the same number of iterations . moreover , the performance gap between the two approaches is slim , with the former achieving 86 . 48 % and the latter 86 . 59 % . as shown in the second group of table 1 , when training and testing on both sets , the approach is more stable and hence requires less data , leading to less effort to reproduce the results .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embeddings achieve extremely high accuracy ( 83 . 43 % ) and average f1 scores ( 10 . 54 % ) in terms of sentence prediction accuracy . rnn - based sentence prediction also achieves high accuracy , but is less effective than self - attention sentence prediction . our model obtains the best performance with an f1 of 10 . 54 and a gap of 3 . 59 points from the average of the previous state of the art .
table 2 shows the performance of all the methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . specifically , the ac - gcn model improves by 9 . 2 % in accuracy over the previous stateof - the - art approach . the attentive neuraldater improves by 4 . 6 % over the best previous model . maxent - joint also improves significantly , by 6 % .
table 3 compares the performance of our method with and without word attention . our neural model obtains 61 . 6 % accuracy with and 65 . 2 % accuracy without attention . with and without attention , our method obtains 63 . 9 % and 63 . 8 % accuracy , respectively .
the performance of all models on the test set is reported in table 1 . embedding + t achieves the best results with a 1 / 1 improvement and a 3 / n improvement over the previous state - of - the - art dmcnn model . further improving performance by high margins , the jrnn model achieves 75 . 2 % improvement on average compared to the previous best state of the art model , and even surpasses the cnn model by 3 / 1 points . as shown in the second group of table 1 , argument argument is the most difficult type of argument to solve , followed by derivation , while derivation and argument propagation are the better performing ones .
table 1 presents the results for event identification and event classification . the results are presented in table 1 . first , we report the results of event identification , classification and event role . all methods show significant improvements in performance . specifically , all methods show marked improvements in both event and argument identification . as expected , cross - event event identification is the most important for event prediction , followed by role prediction .
the results are shown in table 1 . as a baseline , we compare spanish - only and english - only models , both for dev perp and test wer ( for both languages , see x4 ) . in both cases , the average number of errors per case is significantly less than that of the other models , indicating that the training data used to train the model is more accurate . however , fine - tuning the model can help improve the results for both languages . spanish only has the advantage of training on a larger corpus , since it has more data , and hence requires less data to train . the best results are obtained by using the concatenated summary of the max - acc and average - score of the test cases , followed by the precision of the acc metric . finally , we also compare our model with the best performing variant of shuffled - lm .
results on the dev set and on the test set using only subsets of the code - switched data . fine - tuned models perform well on both sets , improving upon the strong baselines by 3 . 8 points in the standard task formulation . cs - only model outperforms fine - tuned models with a gap of 3 . 2 points .
as shown in table 5 , fine - tuning reduces error on the dev set and on the test set , and upsampling improves the results on the monolingual test set . the results are statistically significant with respect to gold sentences , indicating that fine - tuned dictionaries are more useful for production .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision from baseline to current state - of - the - art model is statistically significant ( p < 0 . 01 ) with a f1 score of 7 . 61 ,
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our approach shows significant improvements over the baseline model with a f1 score of 1 . 35 , which indicates statistically significant improvement in the performance .
results on the test set of belinkov2014exploring â€™ s ppa test set . syntactic - sg embeddings outperform lstm - pp , wordnet , and ontolstm , indicating that the type - based approach can further improve the generalization ability of the model . further improving performance by 1 . 7 points over the previous state - of - the - art hpcd model ,
results in table 2 show that the hpcd - based system outperforms the original ontolstm - pp model with a gap of 9 . 59 points in the uas acc . from table 2 , the best performance is obtained by using the hierarchical dependency parser with features derived from various pp attachment predictors and oracle attachments .
table 3 shows the effect of removing the sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a ppa acc . of 89 . 5 % on the validation set .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are presented in table 2 . substitle data alone improves the bleu % score by 1 point , but only marginally over the strong baselines of en - de and mscoco17 . domain tuning improves the multi - domain performance by 3 points .
the results of domain - tuned models are shown in table 3 . the h + h model outperforms the plain plain models with a gap of 10 . 8 points from the last published results ( fancellu et al . , 2016 ) on all metrics except for en - de , where it achieves 60 . 3 % h + moco .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding automatic captions improves the results for all models except for those using marian amun . the multi - task approach improves performance for en - de and mscoco17 models , but does not improve performance for the en - fr and flickr17 models .
table 5 compares the bleu % scores of our method with other approaches for visual information integration . our encoder outperforms all the alternatives except for flickr16 , mscoco17 and multi30k + ms - coco + subs3mlm . encoding visual information improves the w - score of the model , but does not improve the generalization ability .
we observe that the multi - lingual approach outperforms the plain text - only approach , confirming the importance of the visual features diversity . however , the results are slightly less pronounced for models using ms - coco features , the results of en - de and mscoco17 models show that incorporating all visual features improves the results for both datasets , as expected , the performance of models using only text - based features is slightly better than those using multilingual features . subsequently , we observe that combining all features improves results for all but one of the models except for the case of gn2048 .
the results are shown in table 1 . first , en - fr - ht achieves the best performance with a ttr of 2 . 0172 and mtld score of 1 . 5986 . en - es - ht , on the other hand , achieves a marginal improvement of 0 . 2793 points over the average ttr score . as the results show , the semantic information injected into the model by the additional cost term boosts performance for both types of translation .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en â€“ fr model splits 1 , 467 , 489 sentences into 7 , 723 sentences and 1 , 472 , 203 sentences into 5 , 734 sentences .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the average number of words per training sentence is 113 , 132 , 131 , 104 , and 168 , 195 .
automatic evaluation scores ( bleu and ter ) for the rev systems . table 5 shows that the en - fr - rnn - rev and en - es - trans - rev systems perform comparably to the best state - of - the - art systems . however , the ter scores are slightly higher than those of en - servers , indicating that the training data is more interpretable .
table 2 shows the performance of our model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . the mean mfcc score of our vgs model is 0 . 2 , slightly higher than the previous best performing rsaimage model .
results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . the second row labeled rsaimage is the audiovisual supervised model . the acoustic embeddings based on audio2vec outperform all the alternatives except segmatch in terms of recall @ 10 . as shown in table 1 , the average recall percentage of the sampled pairs is slightly higher than the mean mfcc , indicating that the model is more likely to match the target image . however , the difference is small , less than 0 . 0 % overall , which indicates that the performance gain comes from a better model design .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . originally , all the examples shown in table 1 were for cnn , but we have since added additional examples for dan and rnn as well as cnn for use in the experiments described in section 1 . we report further examples in the appendix .
table 2 shows the pos percentage changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . as the table indicates , the number of occurrences has increased , decreased or stayed the same as a result of the improvements in the part - of - speech metric . also , the average number of instances for each sentence has increased as well .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . as shown in table 3 , the flipped labels cause the sentiment score to increase as well .
table 1 presents the results of pubmed and sst - 2 . results show that pubmed outperforms both published and unpublished work on every metric by a significant margin . for example , pubmed achieves an acc / f1 score of 98 . 98 % and 98 . 99 % on the positive and negative measures , respectively , with an absolute improvement of 2 . 36 % and 1 . 38 % over the previous state of the art .
