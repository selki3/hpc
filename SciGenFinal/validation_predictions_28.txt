table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results show , both approaches benefit from the gpu exploitation , which further boosts performance . table 2 also highlights the advantage of using a small number of instances to train a model , as compared to a large number of training instances .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
table 2 shows the results for each model with different representation . the max pooling strategy consistently performs better in all models , and the dropout probability is lower in the sb model than in the conll08 model . the softplus representation also achieves better performance than the softplus variant , as hard coreference problems are rare in ud v1 . 3 , we do not have significant performance improvement over softplus in this variation . we observe that the activation func . is slightly better than softplus even under the difficult requirement of a low coverage . finally , the f1 score improves with the growth of the feature set as shown in fig . 3 .
table 1 shows the effect of using the shortest dependency path on each relation type . as the results show , the macro - averaged model significantly improves its f1 score over the best f1 model without sdp .
table 3 presents the results on the validation set . results show that the three types of tokens are comparable in terms of performance on both f1 and r - f1 scores . however , the performance gap between the two sets is much larger on the y - 3 dataset , which indicates that the quality of the transfer learning method is significantly better in the low - supervision settings .
the results are shown in table 1 . we observe that mst - parser achieves the best results with an accuracy of 50 % , on average , in paragraph prediction .
we note particularly the large difference in c - f1 score between the two indicated systems , which indicates that the lstm - parser parser is more effective at parsing word embeddings with fewer errors . the results are shown in table 4 .
table 3 shows the results for the original and the cleanup experiments . as can be seen , the results show , the cleaner tgen model performs better than the original model when the error reduction is considered over multiple training instances . however , the difference between the two is less pronounced for the cleaned model , indicating that there is less variation in performance between training instances that are cleansed and original .
table 1 shows the comparison of the original e2e data and the cleaned version . as table 1 indicates , the difference in quality between the two sets is minimal , however we see significant difference in ser as measured by a drop of more than 2 % in the slot matching percentage .
table 3 presents the results of experiments with different combinations of tgen and sc - lstm models trained on the same dataset . the results are presented in bold . tgen + significantly outperforms the original model , and achieves the best results with a gap of 2 . 7 points in bleu score from the original . with respect to meteor , rouge - l and ser , the results are slightly better than the original , but still significantly worse than tgen − . adding the effect of the missing data on the training set is minimal , however it results in a significant improvement on the final score . this corroborates our intuition that adding the data from the two sets of data actually improves the performance for the model , as the improvement is larger on the original dataset .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that the majority of errors in our tgen instances ( 71 % ) are caused by errors caused by the incorrect addition of correct values or the presence of wrong values in the training data ( 23 % ) , as shown in table 4 .
table 3 presents the results on the hidden test set of gcnseq in the distractor and fullwiki setting . as a baseline , we also include the results of graphlstm ( song et al . , 2018 ) and table 3 ) as a supplementary material . the results , summarized in table 3 , show that the hierarchical clustering approach based on the dcgcn - ensemble model achieves the best performance with an all score of 25 . 2 . however , the gap between the two highest performing models is much narrower with a gap of 2 . 6 points .
table 2 : main results on amr17 . our ensemble model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art on both datasets . as can be seen in the results , the number of parameters in our model is considerably smaller than the other two baselines , but still achieves a considerable improvement over both the baselines .
table 3 presents the results for english - german and czech . the results show that , compared to the single model , the ggnn2seq model by beck et al . , ( 2018 ) achieves better results on both languages with a gap of 10 . 5 % in the average number of iterations . the gap is narrower with the japanese model ( 7 . 6 % vs . 8 . 2 % ) but the gap is much larger with the english model ( 9 . 9 % ) and the czech model ( 6 . 6 % ) . table 3 compares the performance of these models with the previous state - of - the - art on both datasets . as can be seen , the smaller performance gap between the single and multi - model models indicates , the performance performance is impacted by the type of training data used .
table 5 shows that the number of layers inside the dkr - based network is the most important factor in model performance . the model performs significantly better when there are fewer layers in the network , meaning that more information is available to the model at once , thereby reducing prediction noise .
table 6 shows that the rc - based model outperforms the baselines with residual connections in terms of gcn performance . the difference is most prevalent in relation to residual connections , adding rc improves the results for both gcns , however , the biggest performance increase is seen on the dcgcn2 model , which shows the advantage of incorporating the residual connections . when we add rc and la features to the baseline model , the improvement on gcn2 becomes much larger .
the performance of these models is presented in table 4 . we observe that the coreference signal is localized on specific regions of the graph and that these regions are in the deep layers of the network ( e . g . , the right - of - center regions ) .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . as the results show , removing the dense connections in the i - th block reduces the number of connections , but does not reduce the overall performance .
table 9 shows the results of an ablation study for modules used in the graph encoder and the lstm decoder . the results of " - global node " and " - linear combination " models result in significantly worse performance than the models using " - direction aggregation " and " coverage mechanism " models . the results also show that the effect of the domain - aware attention on the decoder is less pronounced than those of the global nodes .
table 7 , shows the performance of our initialization strategies . our framework establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except for the one where we observe glorot perform best .
table 3 presents the results of models trained on the hidden test set of somo . the results are presented in bold . our h - cmow model obtains the best results with a gap of 10 . 2 points from the previous best performance .
table 5 shows the results of the best performing models . our model ( the cbow / 784 model ) obtains the best results with a reduction in error of 0 . 6 % in subj and mrpc score .
table 3 shows the improvements on unsupervised downstream tasks that our models achieve as a result of the switch from cbow to hybrid mode . the results show that , compared to the monolingual cbow baseline , the hybrid model is more useful and therefore requires less supervision . moreover , the performance of the cmp . ow model has also improved significantly compared to hybrid , showing the advantage of finetuning word embeddings during training .
table 8 shows the performance of our system with respect to these three types of initialization strategies . the results show that glorot outperforms the best previous approaches across all three domains , and its performance on the supervised downstream tasks is close to the best .
table 6 shows the performance of our method compared to the best previous approaches across the four downstream tasks . the cbow - r improves significantly over the cmow model in all but one of the four scenarios . it achieves the best performance with a gap of 10 . 5 % on the sts12 and sts16 tasks .
table 3 presents the results of models trained on the hidden test set of somo . the results are presented in bold . our proposed method obtains the best results with a gap of 10 . 5 points from the previous state of the art on all metrics .
the results are shown in table 5 . the first group shows that the cbow - r model achieves outstanding results , outperforming all the base lines with a gap of 10 . 2 points in the sick - e score . the second group of results show that the transfer learning method is more effective than the monolingual approach .
table 3 shows the results for both systems . in general terms , we see that the results displayed in table 3 show the performance of the supervised and unsupervised learning approaches are comparable , with the exception of the case of the former . however , when we add in the effect of the additional cost term on the loc and the misc dataset , our model exhibits a significant performance drop . moreover , the performance gap between the two sets is much narrower with the former performing better on both loc and misc datasets . supervised learning methods appear to be more effective than the former state - of - the - art model ,
the results on the test set are shown in table 2 . in both settings , the effectiveness of our approach is proved by an increase in the f1 score over the best previous model ( hochreiter and schmidhuber , 1997 ) . however , the difference between the two sets is less pronounced in the more realistic second setting , where our model ( mil - nd ) obtains a slight improvement . name matching is indeed harder than in the first setting , but still results in an improvement over the upsampling baseline . we observe that the transfer learning method , when combined with the best performing domain - adaptive feature set , yields a significantly better result . this corroborates our intuition that supervised learning and reward learning are different .
table 6 shows that the model with the best performance is the g2s - gin model . the model achieves extremely high precision in terms of both ref and gen with an absolute improvement of 3 . 86 points in the last analysis . however , the gap between the two closely matches the performance of the s2s model with only 0 . 08 points .
table 3 presents the results of experiments on the four datasets from the different labs . the results are presented in bold . the g2s model by guo et al . ( 2018 ) achieves the best results with a f1 score of 29 . 28 on the ldc2015e86 and 29 . 53 on the lc - vec dataset . by further adding meteor and bleu features , the model achieves a marginal improvement of 0 . 3pp over the previous state of the art on both datasets , though still performing substantially worse than the s2s baseline .
table 3 shows the results for models trained with additional gigaword data on the ldc2015e86 test set . the g2s - ggnn model achieves a new best performance of 32 . 23 % on the test set , significantly improving over the previous state - of - the - art .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model obtains substantial gains in performance over the baseline bilstm model , and its performance is slightly better than the get model with a gap of 2 . 6m points .
the results are shown in table 5 . we observe that the g2s model with the shortest sentence length obtains the best results . while the average number of tokens per sentence is slightly longer than the ggnn model , the difference between the two is much smaller . the smaller difference between s2s and ggnn indicates that the model is more suitable for production use .
table 8 shows the results for the test set of ldc2017t10 . the model with the best performance is the g2s - gat model , closely followed by the s2s model . the smaller fraction of elements in the output that are not present in the input graph that are missing in the generated sentence is the largest ( miss ) , which shows the extent to which the model can rely on syntactic cues from the reference sentences . as shown in the comparison , the smaller miss fraction indicates that the model has better interpretability in the low - resource settings .
table 4 shows the pos and sem accuracy for different target languages as well as the founta et al . ( 2018 ) evaluation results . the results show that , let alone a reduction in performance , the improvement in pos accuracy is significant , indicating that the model can rely on such superficial cues as word embeddings to predict the answer based only on the nearest neighbours of the target without understanding the task .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms the original embeddings and performs best in terms of pos . the upper bound encoder also achieves the best performance on the sem dataset . it shows significant performance improvement over the baselines for both subsets .
table 4 presents the results of the best performing models . we show the performance on the sem and tagging accuracy . the results show that , let alone a reduction in performance , the improvement in accuracy is significant across all metrics , with the exception of the pos tagging accuracy .
table 5 shows the pos and sem tagging accuracy with features from different layers of our four - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as the results show , the bi and uni layers both give considerably better performance than the res - based layer , which shows the advantage of the bi - based encoding scheme .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the presence of gender and race features seem to have little effect on the performance , however , the age and gender features have a significant effect . the attacker achieves high performance in the diversity task , showing that he is well - equipped to pick out the most interesting features . overall , the performance is very similar across the three datasets , with the exception of the race feature .
as shown in table 1 , the training set makes substantial gains in accuracy over the upsampling baseline . the gender - based disparities in performance are less pronounced , but still represent a significant racial disparity . overall , the mention task is very difficult to solve , as it requires significantly more data and time to train .
table 2 shows the results of unbalanced and balanced data splits . as expected , the results show , there is a significant imbalance in the accuracy between the balanced and unbalanced data splits , which indicates that the presence of some highly productive classifiers can cause the task to get misclassified . gender - aware features result in significantly better performance than any classifier other than gender - aware ones . the presence of the word " gender " in the label " attention " helps the classifier to more precisely detect instances of gender bias . overall , the performance is very similar across all classifiers .
the performance of these models on different datasets with an adversarial training set is shown in table 3 . the difference in the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in terms of leakage . as the results show , the presence of gender and race features in the protected attributes helps the model to distinguish between the true response and the wrong response . additionally , the gender - based part - ofspeech features contribute significantly to the task performance , as it increases the recall rate for all mentions with a gender - neutral pronoun . finally , the accuracy drop is much larger for mentions containing race and age .
the results are shown in table 6 . the rnn encoders perform similarly to the guards , with the exception of the case of leaky . as hard coreference problems are rare in the unsupervised setting , we do not have significant performance improvement .
table 3 presents the results on the hidden test set of ptb and wt2 in the distractor and fullwiki setting , compared to yang et al . ( 2018 ) . the results , summarized in table 3 , are broken down in terms of performance on the training set with the best performing model being the lstm model . the difference is most prevalent in the finetune setting , where the model performs best . this confirms the value of finetune feature - rich training data . we also observe that the largest part of the difference in performance between the two sets is due to the large variation in training set size between distractor and fullwiki setting . the largest of these is the gru model , which clas s performing at the level of the state - ofthe - art .
table 3 presents the results of the second study of the set of experiments in table 3 . we show that the combination time and baseline acc time are the most important factors in model performance , followed by the number of parameters and the average number of seconds taken to compute each parameter . the results reconfirm that the importance of time and accuracy are both factors in the equation of model performance . the model performs significantly better when the time is considered in combination with the base acc time , indicating that the model is well - equipped to perform this task in a production setting . table 3 quantitatively compares the performance of these models with the previous state of the art in terms of both acc and time . we observe that the gru model obtains the best results with a gap of 2 . 5 % in acc time from the last published results ( hochreiter et al . , 2016 ) while the lstm model gets the worst performance . sru also exhibits significant performance drop .
table 3 presents the results of experiments in the setting of yelppolar and amafull time . we benchmark against the best performing models from the previous literature on both sets . the results are presented in bold . according to the table , this model obtains significantly better results than the previous state - of - the - art models on both yelp and the amapolar dataset . the gap between the two sets is much smaller with this model achieving 4 . 28 % higher err and 3 . 48 % higher time weighted average . table 3 quantitatively compares the performance of these models with the best previous work on both datasets . we observe that this model significantly boosts the precision of the yelp feature set , and boosts the generalization ability of the model to perform in the high - frequency settings .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task . as shown in the table , the gnmt model obtains the best performance with an absolute improvement of 2 . 67 points in bleu compared to the previous best performing model , olrn . the difference is less pronounced with respect to gold - two - params model , but still suggests some performance advantage . at the same time , the performance of the atr model is still significantly worse than the other two baselines . although the number of parameters used to encode one sentence is less than the lrn ' s , the time to decode is considerably longer . sru also exhibits performance drop as shown in table 3 . though the model has seen more training time , it is still inferior to atr and sru in completing the task .
table 4 shows the exact match / f1 - score on the squad dataset of wang et al . ( 2017 ) . it can be seen that the incorporation of the elmo factor improves the model ' s performance , and upsampling has a generally positive effect ( p < 0 . 05 ) . however , the greatest performance increase is seen on the gru model , which obtains the best performance . at the same time , the lstm model shows a slight drop in performance compared to the previous state - of - the - art model . table 7 summarizes the results . the largest performance drop is on the recurlink dataset , where the rnet * parameter number alone results in a drop of 2 . 67 points in f1 score .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the model obtains a substantial improvement in performance over the previous state of the art model . as the results show , the number of parameters in our lstm model is considerably less than the other state - of - the - art models , but still represents a significant performance improvement . at the same time , the sru model shows a slight drop in performance compared to the other two baselines . table 6 also highlights the scalability of parameter sharing . we managed to reduce the repetition rate with a reduction of 9 % over previous work .
table 7 shows the performance of our model with the base and the multi - factor ln setting . with the base setting , our model obtains 85 . 72 % accuracy on the snli task and elrn model achieves 83 . 49 % accuracy . the difference is less pronounced with the multfactor setting , but still indicates significant performance improvement .
table 2 shows the results for english . our system outperforms all the other systems with two tasks . the difference is most prevalent in terms of # word , with oracle retrieval showing a significant performance drop of 2 . 8 % in b - 2 and 3 . 6 % in mtr . table 2 presents the results of multi - task learning on the 20 % held - out validation data . we show that the redundancy removal step is crucial for the success of our system , as it eliminates repetition and allows further improvements in sentence quality .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that the seq2seq system is significantly better at selecting the correct grammatical patterns and its output is more interpretable . the second best result is by a large margin : candela ( 30 . 2 % ) obtains the highest percentage of evaluations , followed by h & w hua and wang ( 2018 ) ( 38 . 8 % ) and retrieval ( 50 . 2 % ) . table 4 shows that the ability to select compact regions induces the generation of better captions . although the average number of tokens per sentence is slightly more than seqseq , the quality of the output is high , about 5 % higher than any automatic system .
table 6 shows the results for english , spanish , french , dutch , russian and turkish . the results are broken down in terms of performance on extractive and abstractive keyphrases . for english , we see that our proposed system outperforms all the base the competition on every metric by a noticeable margin . however , on the other hand , our proposed hclust model performs slightly better than the other baselines on three out of the four datasets suggesting that there is a need to refine the feature extraction procedure for further performance improvement .
table 3 shows the results for english , spanish , french , dutch , russian and turkish . the results are presented in bold . our model outperforms all the base the results show that the text - similarity based baselines , such as wikipedia , docsub and symmetricwiki , are well - equipped to perform this task with a minimum of 80 % accuracy .
table 6 presents the results of models trained on wikipedia and ted talks . the results are broken down in terms of performance on similarity benchmarks . according to the table , the best performances are obtained by our model is obtained on the ted talks dataset . on the other hand , our europarl model achieves the highest performance with a gap of 2 . 5 points from the last published results .
the performance of our approach is presented in table 1 . we show the five most important performance metrics . first , we show the averagedepth and totalterms . according to the table , europarl has the best performance with a gap of 11 . 05 % in terms of total terms . additionally , our hclust model significantly boosts the number of correct answers for question marks , with an average of 1 . 59 % increase over the previous state of the art . our approach relies on the concept of depth cohesion , which allows more correct answers to be identified when a query contains multiple entities .
the performance of our approach is presented in table 4 . we show the five most important performance metrics . first , we show the averagedepth and totalterms . according to the table , europarl has the best performance with a gap of 9 . 43 % on average compared to the other baselines . the difference is less pronounced for docsub , but still suggests some performance variation . our approach relies on word embeddings with a high degree of semantic overlap , as measured by the number of roots in the dataset , which results in consistently better performance .
we present the performance of our approach with respect to the ndcg % on the validation set of visdial v1 . 0 . as table 1 shows , the enhanced version of our loss function outperforms the baseline model by a noticeable margin . moreover , the model exhibits a significant drop in performance when using the weighted softmax loss instead of the binary sigmoid loss , indicating the advantage of using a more balanced loss function .
the performance ( ndcg % ) of the ablative studies on different models that we apply to the visdial v1 . 0 validation set is shown in table 2 . the results indicate that the use of p2 improves the performance for all models except for the one using the history shortcut . the model with the best performance is the one that receives the most effective p2 treatment .
table 5 shows that the hmd - f1 model significantly outperforms the wmd - bigram and hmd - recall models in the hard and soft alignments , both on the standard and unigram datasets .
the results are shown in table 1 . the first group shows that the baselines significantly outperform the method using the best performing feature set , ruse ( * ) on the direct assessment metric . on the other hand , the average score of bertscore - f1 is slightly higher than the ruse baseline , indicating that bert score is already well - equipped to perform this task " out - of - the - box " . the second group of results show that the transfer learning method from meteor + + to smd + w2v significantly boosts the general performance of the model , both on the direct and indirect assessment metrics .
the results are shown in table 5 . the first set shows that the performance of the baselines on the similarity test set is relatively consistent , with the exception of bertscore - f1 . although the improvement is slim , it is encouraging to continue researching into principled ways of improving the human judgement in these difficult tasks . we noticed that the sfhotel scores are relatively consistent with the baseline scores , the second set of results show that the transfer learning method is comparable to state of the art work from ( fancellu et al . , 2016 ) on both sets . sent - mover achieves the best results with an f1 score of 0 . 176 on the comparison set with the smd baseline . w2v induces the bleu - 2 to compute significantly more responses than the baseline .
we further analyze our results with respect to the word - mover task . the results are shown in table 5 . word - mover with the best performance is achieved with the help of the multi - factor classification system ( wmd - 1 + w2v ) . the difference is most prevalent in the m1 and m2 metrics , with respect to accuracy , the best results are obtained on the leic ( by a noticeable margin ) and spice ( by an even larger margin ) . when we add elmo and bert scores to the baseline classifier , we get 0 . 8 % improvement on average which shows that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement in the prediction quality . additionally , the bertscore - recall classifier achieves the highest performance with an absolute improvement of 0 . 9 % on the m2 metric ,
the results are shown in table 6 . the results of the best performing model is the one using the best adaptive decoding scheme , namely , the cyc + para model by vaswani et al . ( 2018 ) . the results show that the advantage of incorporating the word " para " improves the performance for all models except for those using shen - 1 . moreover , the results are slightly worse for models using 2d as en ( 2018 ) coder instead of 3d .
table 3 shows the results for english and spanish . the results show that yelp significantly outperforms the best previous approaches across all three domains . semantic preservation results are notably better than those of google translate ( table 3 ) , confirming the importance of word embeddings adaptation . finally , the drop in transfer quality between yelp and semantic is less pronounced , but still suggests some reliance on superficial cues . we observe that the transfer quality drop between the two domains is minimal , however it is significant enough to result in a drop in performance .
table 5 shows the results of human evaluation . our approach verifies the accuracy of the summaries with a minimum of 94 % , matching the performance of the best human evaluators . the difference between human and machine evaluation is minimal , however we see significant difference in the percentage of summaries that match human evaluation criteria .
the results are shown in table 7 . the results of the best performing model is the one using the best multi - para model , namely , shen - 1 . the model achieves the best results with a reduction in error from 0 . 7 to 0 . 8 on the sim and gm benchmarks .
table 6 shows the results for yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results show that the transfer method with the best performance achieves higher acc than prior work with similar levels of transfer , but is worse than simple transfer . we note that the difference between transfer and simple - transfer is less pronounced for the unsupervised model , but still suggests some reliance on human references . as shown in the table , transfer with lm and multi - decoder achieves the highest acc , and the best results are achieved with the transfer of the best five tokens . note that the definition of acc varies by row because of different classifiers in use . the transfer results shown in table 6 show that with the right classifier in use , the transfer process is more accurate , but the results are still significantly worse than the simple transfer approach .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluencies and the average number of repetition tokens . as the results show , the repetition tokens are the most difficult part to solve , as they have the highest correlation with repetition .
we observe that for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , the model predicts the word " disfluency " as well as the function - function with a high probability . this indicates that the part of the speech pattern that contains the content word is highly productive for the task at hand .
table 3 presents the results of experiments with different combinations of text and innovations for english . we observe that the text innovations alone result in significantly better results than the single innovations model . however , the gap between the best and worst performances is much narrower with the exception of the late case , when text + innovations results in significantly worse performance . it is clear from table 3 that the innovations alone do improve the results for english - speaking users .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model shows marked performance improvement . it closely matches the performance of the best performing rnn - based embeddings and self - attention algorithms . moreover , it achieves a slight improvement in accuracy over the conversational baseline as well . we conjecture that this is due to the larger variation in the training set size and the higher accuracy rate at which the models are tested .
table 2 shows the performance of the unified model , which significantly outperforms all previous methods . the accuracy is higher than the previous state - of - the - art models on both datasets , indicating that the model performs better in the production setting .
table 3 shows the performance of our neuraldater model compared to ac - gcn with and without graph attention . the difference in accuracy between the two approaches is minimal , however we see significant difference in performance due to the higher correlation between word and graph attention . with and without attention , neuraldater achieves a significant performance improvement which shows the effectiveness of both word attention and graph attention for this task .
the results are shown in table 1 . the first group shows that after applying our domain - adaptive delexcalization and domain - aware belief modeling , the task completion ability of the dmcnn model becomes better while the performance of the jrnn model gets better . further , the argument argument is the most difficult part of the model to solve , as it requires much more data and time to train . we find that the best performing feature - rich model is the jmee model , which achieves 75 . 2 % on average .
table 1 presents the results of experiments in the setting of event detection . our approach establishes a new state - of - the - art in the low - supervision setting , with f1 achieving 6 . 3 % improvement on the match rate with bootstrapping and permutation tests . the results reconfirm that the semantic information injected into the model by the additional cost term is significant , and that the model can rely on such features to improve the prediction quality over traditional methods .
consistent with the results of vaswani et al . ( 2018 ) , we observe that the best performance is obtained by the spanish - only model , which achieves the best results with a gap of 10 . 5 % on the test set . however , fine - tuning gives a performance gain of 2 % overall . this confirms the effectiveness of the finetuned approach . we notice that the slightly increased effort required to fine - tune the model results in a drop of performance over the best baseline model .
results on the dev set and the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model improves upon the strong lemma baseline by 3 . 8 points in the standard task formulation . however , the results do not improve significantly over the upsampling baseline , which shows the advantage of finetuned models .
fine - tuned - disc improves the performance on the test set , and the results are shown in table 5 . the improvement is much larger than that on the dev set , which shows the advantage of finetuned word embeddings . additionally , the performance increase is much smaller for monolingual ( mono ) compared to code - switched ( cs ) and dev mono , showing that the finetuning ability to pick out the most interesting parts of the gold sentence leads to a better model performance .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline setup to the type combined setup is statistically significant ( p = 0 . 0088 , double - tailed ttest ) with respect to the accuracy in table 7 , which shows that the ability to combine gaze features induces the model to make better predictions .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset . the improvement from the baseline model to the type combined model is statistically significant ( t - test , p < 0 . 05 ) and r = 94 . 35 , f1 > 0 . 88 , both with respect to the baseline and the type - combined model .
table 1 : results on belinkov2014exploring ’ s ppa test set . the results show that the lstm - pp approach significantly outperforms the ontolstm approach , and the glove - extended approach achieves the best results with an accuracy of 89 . 8 % . further , the hpcd approach by using syntactic - sg improves the vqa task by 3 . 7 % in the standard task formulation and to parity in the gold - two - mention case .
table 2 shows the results for rbg with different pp attachment predictors and oracle attachments . the results show that the ability to extract relevant pp information from the input documents improves the uas performance over the plain hpcd model by 1 . 59 points . further , the accuracy increases with the growth of the lstm - pp feature set .
table 3 shows the effect of removing the sense priors and context sensitivity from the model . the results show that the ppa acc . decreases significantly as a result of the reduced effect of the context sensitivity .
table 2 shows the bleu % scores of the models using subtitle data and domain tuning for image caption translation . the results are slightly better than the results with marian amun ( marian amun et al . , 2017 ) in the multi30k dataset , but still significantly worse than the en - de model . subsfull subtitle data also improves the results for both models , as the results show , the better performing model with subtitle data is more likely to receive domain tuning as well . finally , the smaller performance gap between the two models with the domain tuning decreases as a result of the larger variation of the subtitle dataset .
table 3 shows that the domain - tuned h + ms - coco model achieves the best results with a f1 - score of 66 . 7 on the en - de and mscoco datasets . the difference is less pronounced with the subs1m model , but still suggests some reliance on domain - specific features .
table 4 shows the bleu scores of the models using only one type of automatic image captions ( the best one or all 5 ) and marian amun . the results show that the combination feature improves the general performance of all models , with the exception of those using multi30k embeddings . however , the biggest performance increase is seen in the en - de model , which shows the advantage of finetuning the word " camera " during training .
table 5 shows the results for multi30k + ms - coco + subs3mlm , detectron mask surface , and transformer with transformer as en ( 2018 ) coder . the results show that the enc - gate and dec - gate strategies achieve higher bleu % scores than the en - de and mscoco strategies , indicating that the decoding information is more useful in the low - supervision settings . however , the ability to detect variations in the mask is only slightly better with the dec - gated approach , showing the advantage of finetuning word embeddings during training . finally , the presence of the word " error " in the captions helps the model to improve its performance .
we observe that the cues for multi - lingual features are markedly different from those for text - only features , the results of the " + ensemble - of - 3 " model are slightly better than the results of " text - only " model , but still significantly worse than the " visual features " model . further , the results are slightly worse for " ms - coco " compared to " n " - grammatical feature - free model , as seen in the second group of results , the combination feature - based model with the best performance achieves the best results with respect to the three languages .
the results are shown in table 6 . as en - fr - trans - back refers to the second layer of the model , the performance on mtld is significantly worse than that on yule ' s i . the difference is most prevalent in the transformer transformer layer , which shows that the model performs much better in the back - of - speech task . on the yule ’ s i dataset , enfr - smt - back achieves the best performance with 6 . 48 % overall improvement over the model using the plain rinse - and - repeat pattern .
table 1 shows the number of parallel sentences in the training , test and development splits for the language pairs we used . the total number of words in the splits is 1 , 467 , 489 , 499 , 487 and 7 , 723 , which represents a significant imbalance .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table indicates , there is a significant difference in the performance between the two sets , which indicates that the training set is more suitable for production use .
as shown in table 5 , the automatic evaluation scores ( bleu and ter ) indicate , the en - fr - trans - rev system is comparable to the best state - of - the - art transformer ( g & l ) model from ( fancellu et al . , 2016 ) in terms of performance on both rev and transformer metric . however , the improvement is much larger on rev metric , which indicates that the translation quality of the model is significantly better .
table 2 shows the performance of our model compared to [ 7 , 17 ] the best performing rsaimage model . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the unsupervised model from flickr8k . our model obtains a significant improvement in recall @ 10 and average mfcc score , which shows that our model can significantly improve the interpretability without a drop in performance .
the results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . it achieves the best performance with a recall @ 10 of 0 . 9 and a chance @ 0 . 0 . the average number of recalls per chance is also close to the highest , at 0 . 3 . audio2vec - u is only slightly better than the vgs model in terms of recall , however it obtains the best chance to match the human judgement . segmatch also shows a significant performance drop . we conjecture that this is due to the larger variation in recall between human judgement and machine learning models .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as can be seen , all the examples show that the ability to pick out the most interesting parts of a paragraph leads to a better output . the dan classifier is particularly useful for this task as it turns in a screenplay that is easily pickable at the edges , and has the best performing feature set . cnn also shows a marked improvement in performance . we report further examples in the appendix . it is clear from table 1 that the use of dan improves the interpretability of the output .
table 2 shows the effect of fine - tuning on the number of occurrences in sst - 2 . the numbers indicate that the quality of the output has increased , decreased or stayed the same as the original sentence has increased . however , the percentage of instances in question that contains " good " words has decreased as a result of the increased number of instances of " evil " words . this indicates that the scope for improvement of the dan model is limited .
the results in table 3 show that the sentiment score decreases as a result of the flipped labels being flipped from positive to negative sentiment . the flipped labels cause the rnn to generate significantly more positive sentiment .
table 2 presents the results of the second study . as can be seen , the results confirm what we observed in the first study : the ability to pick out the good and the bad moments from the same dataset is relatively high , with an f1 of 1 . 0 . while the difference between pubmed and sift is slim , it is significant enough to result in a positive result .
