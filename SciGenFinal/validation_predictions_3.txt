table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 summarizes the results of the training and inference steps in table 2 . as expected , the recursive framework performs better on inference and training than the iterative method .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization .
table 2 shows the results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 and ud v1 . 3 outperform all the other models in terms of f1 and f1 scores .
table 1 : effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp is shown in table 1 . macro - averaged models have the highest f1 , while macro - factor - based models have lower f1 .
table 3 shows the performance of the models in terms of f1 , r - f1 , and f1 50 % on average . the results are shown in table 3 . as expected , the best performing models are the ones with the highest f1 scores . the best performing model is the one with the best f1 score , which is the model with the lowest f1 percentage .
table 3 shows the average accuracy of mst - parser and mate . table 3 compares the performance of the two approaches on the same test set . the average accuracy on both sets of tests is significantly higher for mate than for the other two approaches . mate outperforms both the other approaches in terms of average accuracy .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . stagblcc outperforms lstm - parser in terms of the percentage of sentences that contain the word " paragraph " and " essay " .
table 3 shows the results of the training and test set . the results are shown in table 3 . as expected , the clean - up time for tgen + and tgen − is significantly higher than the original cleanup time , indicating that the training set is more suitable for improving the performance of the system compared to the original .
table 1 compares the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . our cleaned version has the best performance compared to the original data .
table 3 shows the results of the training and test set for each model . the results are shown in table 3 . as expected , the results show that the original tgen + and tgen − model outperforms the original sc - lstm model in terms of performance on all tests .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the number of errors per instance is shown in table 4 . table 4 also shows the percentage of errors in the training data that are added , missed and wrong values .
table 3 shows the performance of the models in table 3 . the results are shown in table 2 . the best performing models are those that use graphlstm ( song et al . , 2018 ) and seq2seqk ( konstas et al , 2017 ) . these models outperform all the other models except tree2str and pbmt .
table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . table 2 : main results on amr17 . our model achieves a bleu score of 24 . 5 points , which is significantly higher than the previous state - of - the - art seq2seqb ( beck et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) models , which achieves 24 . 3 points and 21 . 5 bleus , respectively , compared to the previous best performance .
table 3 presents the results of our model on the english - german and english - czech datasets . the results are shown in table 3 . table 3 shows the performance of the models on the german and czech datasets compared to the previous state - of - the - art models . as expected , the best performing models are the ones that use bow + gcn ( bastings et al . , 2017 ) and seq2seqb ( beck et al , 2018 ) .
table 5 shows the effect of the number of layers inside the dc on the performance of the model . the number of layer in the dc is shown in table 5 . as expected , the effect is small , but it is still significant . table 5 summarizes the results of our experiments . we found that the number and size of layers in the layers of the dc are the most important factors in the performance improvement . in particular , we found that adding more layers leads to better performance .
table 6 shows the performance of the gcns with residual connections compared to the baselines . the results are shown in table 6 . rc denotes the number of connections between the gcn and the residual connections , and rc + la denotes the percentage of connections that are residual . the average number of residual connections between gcns and residual connections is 16 . 9 , compared to 18 . 1 and 18 . 7 , respectively .
table 3 presents the results of our study on the performance of the dcgcn model compared to the previous state - of - the - art models . the results are shown in table 3 . our model outperforms all the other models in terms of performance on all metrics except # p .
table 8 : ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . the results of the ablation study show that removing the dense connections in the i - th block leads to a significant drop in the density of the connections . as shown in the table , removing the sparse connections leads to an increase in the number of connections in each of the four dense blocks .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the coverage mechanism used by the decoder achieves the best coverage on average , and the direction aggregation mechanism achieves the highest coverage .
table 7 : scores for initialization strategies on probing tasks . glorot and our paper are shown in table 7 . the results are summarized in table 6 . as expected , the glorfot initialization strategy outperforms all the other initialization strategies in terms of accuracy and tense .
table 3 shows the results of the best performing models in terms of h - cmow and h - cbow . the results are shown in table 3 . the best performing model is the one with the highest accuracy , which shows that the best - performing model has the best correlation with the shortest length . the other two models are the ones with the lowest accuracy .
table 3 shows the results of the best - performing method and the best performing method in terms of sst2 , sst5 , and sts - b . the results are shown in table 3 . cmow / 784 outperforms all the other methods except for sick - e , which shows that it is better to use a hybrid approach .
table 3 : scores on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change with respect to hybrid . our models outperform the previous state - of - the - art approaches on all the downstream tasks except sts13 and sts14 .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot and sick - r are shown in table 8 . the results of our paper show that the initialization strategies are comparable to those of our previous work , and are comparable in terms of the number of subjets and sub - jets used .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . cmow - r and cbow - c have the highest scores for each training objective on the downstream tasks compared to the supervised tasks .
table 3 shows the performance of the cmow - r and cbow - c models compared to the previous state - of - the - art cmow models . the results are shown in table 3 . the cmow model outperforms all the other models except the cbow model in terms of accuracy .
table 3 shows the performance of the cmow - r and cbow - c models compared to the previous state - of - the - art cmow models . the results are shown in table 3 . the cmow model outperforms all the other models in terms of performance .
table 3 shows the results of our supervised learning approach compared to the previous state - of - the - art approach . the results are shown in table 3 . our approach outperforms all previous approaches in terms of performance on all metrics except for loc and e + loc .
table 2 shows the f1 scores on the test set under two settings . the results are shown in table 2 . in both settings , the model with the highest f1 score , τmil - nd ( model 2 ) outperforms the other two models in terms of both f1 and e + r . as expected , the difference in f1 between the two models is small , but it is still significant .
table 6 shows the results of our model in terms of ref and ref ⇒ ref . the results are shown in table 6 . our model outperforms the previous state - of - the - art models in all cases except for g2s - gat , where it performs worse than the other two models .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . the results are shown in table 3 . we note that the g2s - gat model outperforms all the other models in terms of bleu and meteor scores . it also outperforms the previous state - of - the - art models on all the three datasets .
table 3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . g2s - ggnn outperforms all the other models in terms of bleu score . it also outperforms the previous state - of - the - art models when trained with gigaword data as well .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we found that the bilstm model significantly outperforms the previous state - of - the - art models in terms of the size of the model .
table 3 presents the results of our model in table 3 . the results are shown in table 1 . the model outperforms the previous state - of - the - art models in terms of the number of sentences and the average length of the sentences . g2s - gin outperforms all the other models except for gat and ggnn , where it is slightly better .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are shown in table 8 . the model with the best performance is the one with the highest percentage of elements missing from the input .
table 4 shows the pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . the average pos accuracy is 88 . 6 % , and the average sem accuracy is 85 . 2 % .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag classifier achieves the highest pos accuracy with a baseline of 91 . 11 % and the highest sem accuracy of 82 . 55 % compared to word1tag , respectively .
table 4 presents the results of our study on the performance of our model in terms of pos and sem tagging accuracy . the results are shown in table 4 . our model achieves the best performance on all three metrics , with the exception of the pos tagging accuracy , which is slightly lower than the previous model .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . as expected , the best performing layers are those with bidirectional and residual nmt encoding .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the results are shown in table 8 . the attacker score is shown as the difference between the attacker score and the corresponding adversary ' s accuracy .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . as expected , the pan16 model outperforms all the other models in terms of accuracy and sentiment accuracy .
table 2 shows the results of our model on the unbalanced and balanced data splits . the results are shown in table 2 . we show the results for pan16 and pan16 - based models .
table 3 shows the performance on different datasets with an adversarial training set . the results are shown in table 3 . as expected , gender and race are the most important factors in the performance of the adversarial model . gender and race have the highest percentage of mentions in pan16 , while age and gender have the lowest percentage .
table 6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 . rnn and guarded encode the leaky and leaky embeddings , respectively .
table 3 presents the results of our work on the wt2 base and finetune model . the results are shown in table 3 . our work shows that the lrn model outperforms the previous state - of - the - art lstm and gru models in terms of the number of params in the ptb base and finetune baselines . we also show the results on the weighted average number of parameters in the model .
table 2 presents the results of our work on the lstm and gru models in table 2 . the results are shown in table 1 . table 2 shows the performance of the model with respect to the number of parameters and the time taken to compute each parameter . as expected , the results are similar to those of the previous work , with the exception of the work on sru .
table 3 presents the results of our work on the amapolar err and yelppolar time models . the results are shown in table 3 . the results of zhang et al . ( 2015 ) are summarized in table 1 . we show the results for all three models in table 2 . our model outperforms the previous state - of - the - art models in terms of err , time and accuracy .
table 3 shows the bleu score on wmt14 english - german translation task on tesla p100 . the gnmt model outperforms all the other models in terms of the case - insensitive tokenization score . the lrn model achieves the best performance on the translation task , and the sru model performs the best on the newstest2014 dataset .
table 4 shows the results of our model on the squad dataset . the results are shown in table 4 . our model outperforms the previous state - of - the - art lstm and lrn models in terms of accuracy and f1 - score . we note that our model achieves the best f1 score on all the three datasets .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru denote the reported results . lrn is the parameter number of the parameter in ner tasks . sru and gru are the parameter numbers in gru task , while lrn are the parameters in lrn task . atr is the number of parameters in the parameter - based task , which is reported in table 6 .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . lrn and glrn outperform elrn on both tasks .
table 2 presents the results of our study on oracle retrieval and mtr in table 1 . the results are shown in table 2 . our model outperforms the previous state - of - the - art on all metrics except for human and human - sentence . we also show the results for oracle retrieeval in table 3 .
table 4 shows the results of the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . table 4 also shows the percentage of evaluations that are ranked in the top 1 / 2 for the overall quality of the system . the results are shown in table 4 .
table 3 shows the results of our model in terms of p < table 3 . the results are shown in table 4 . our model outperforms the previous state - of - the - art europarl model by a significant margin . our model achieves the highest p < 0 . 05 on all metrics except for eqs , where it achieves the best performance .
table 3 shows the results of our model in terms of p and pt . the results are shown in table 3 . our model outperforms the previous state - of - the - art europarl model by a significant margin . our model achieves a p < 0 . 01 and a pt of 0 . 03 , respectively .
table 3 shows the results of our model in terms of p < table 3 . the results are shown in table 1 . our model outperforms the previous state - of - the - art europarl model by a significant margin . our model achieves the highest p < 0 . 05 and the best performance on all metrics . we also show the results in table 2 .
table 3 presents the results of the europarl test set . the results are shown in table 3 . we show the average depth and the average number of roots per row in each category . the average depth of each row is also shown in the table . table 3 also shows the number of rows per row of each category in terms of numberrels .
table 3 presents the results of the europarl test set . the results are shown in table 3 . we show the average depth and the average number of roots per row in each category . the average depth is 9 . 43 and the max depth is 19 . 29 , respectively . table 3 shows the performance of each category in terms of the number of rows per row .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . as expected , the enhanced version of the lf model outperforms the baseline model in terms of ncdcg % and qt . the enhanced version also outperforms both the baseline and the baseline baseline on the weighted softmax loss and binary sigmoid loss models .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . in table 2 , we show the performance of the best performing models on the validation set with p2 applied .
table 5 : comparison on hard and soft alignments . the results are shown in table 5 . hmd - f1 + bert is the best performing method on hard alignments , while wmd - unigram and wmd - bigram are the worst performing ones . ruse and ruse - recall are the only two other methods that show significant performance improvement on both alignments compared to the previous state - of - the - art approach .
table 3 summarizes the results of our approach . the results are shown in table 3 . we show the average average of the baseline scores for all the three sets of metrics . metrics are presented in table 1 . we also show the weighted average of bertscore - f1 , ruse ( * ) and meteor + + .
table 3 presents the results of the bertscore - f1 test set . the results are shown in table 3 . bleu - 2 and meteor are the baselines used in the test set , while bert score is used in table 4 . the results of bert scores are presented in table 5 . table 5 shows the results for all the three sets of baselines .
table 3 presents the results of our test set on leic ( * ) and meteor . the results are shown in table 3 . we note that the accuracy of our model is significantly higher than those of the previous state - of - the - art models . in particular , our model achieves the best performance on all metrics except for the word - mover metric .
table 3 shows the results of our model on the table . the results are shown in table 3 . our model outperforms the previous state - of - the - art models on all metrics except for acc and gm . our model achieves the best performance on all the metrics except acc .
table 3 presents the results of our model evaluation . the results are shown in table 3 . table 3 shows the transfer quality and transfer quality scores of all the models in the dataset compared to the previous state - of - the - art models in terms of transfer quality , transfer quality score , and semantic preservation . we also show the results for the models with the best transfer quality .
table 5 presents the results of human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . table 5 shows the percentage of machine and human judgments that match on each metric , as well as human ratings of semantic preservation and fluency .
table 3 shows the performance of our model on all metrics . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in terms of accuracy and gm . as expected , our model achieves the best performance on all the metrics .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleus than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . the results are shown in table 6 . our best model , fu - 1 , achieves the highest acc ∗ and the best classifier , yang2018unsupervised , is the best at the same level of acc as the previous work .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . the average number of disfluencies in each category is shown in table 2 . the average length of the disfluency tokens is also shown in the table 2 . as expected , the number of tokens that are disfluential is significantly higher in nested disfluences than in the case of repetition .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) . the number of tokens that belong to each category is shown in table 3 . percentages in parentheses show the fraction of tokens in each category that are predicted to be disfluential for each disfluency .
table 3 shows the results of our model in terms of test mean and test best . our model outperforms the previous state - of - the - art model by a large margin . the results are shown in table 3 . as expected , our model achieves the best performance on all metrics except for the test best score .
table 2 shows the performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms all the state of - the - art embeddings except self - attention and rnn - based sentence embedding .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , the best performing method is the neuraldater model , which outperforms the previous models on both datasets .
table 3 shows the accuracy ( % ) comparisons of component models with and without attention . neuraldater outperforms oe and oe - gcn in terms of both word attention and graph attention for this task .
table 3 shows the performance of our model on the 1 / 1 and 1 / n test sets . the results are shown in table 3 . we show that our model outperforms the previous state - of - the - art dmcnn and jrnn models on all test sets except for the first stage , where it outperforms both of them .
table 1 presents the results of our study on cross - event inference in table 1 . the results are shown in table 2 . table 1 shows the average f1 and f1 scores of the method and the role of each argument in each event . we also show the classification and role of the argument in the event of a cross - event event .
table 3 shows the performance of the best - performing models on average . the results are shown in table 3 . the best performing models are those with the best performance on average on all metrics except test acc and test wer .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . the fine - tuned approach outperforms the cs - only approach in terms of both train dev and train test .
table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . the results are shown in table 5 . the fine - tuned - disc approach achieves the best performance on both the dev and test sets .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision compared to baseline .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) compared to the baseline dataset . type combined features significantly improve recall and precision compared to baseline . the type combined features also significantly improve f1 score .
table 1 : results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . syntactic - sg embeddings obtained by running autoextend rothe and schütze ( 2015 ) on glove are shown in table 1 . the results on the second set of test sets are presented in table 2 . as expected , the results are similar to those on the first set .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd ( full ) and ontolstm - pp ( partial ) outperforms all the other approaches except onto onto lstm ( full ) . the difference in ppa acc . score between the two approaches is small , but it is significant .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . as expected , the effect of removing context sensitivity and context priors on the ppa acc . score is significant .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . as expected , domain tuning improves the bleu % score for both en - de and en - fr , and improves the performance of the multi30k model . in addition , the domain tuning helps to improve the performance for the multi - domain model .
table 3 shows the results of the domain - tuned approach compared to domain - based approaches . the results are shown in table 3 . domain - based approach outperforms all other approaches in terms of h + ms - coco and lm + ms . the results for en - de and mscoco17 are summarized in table 4 .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . all results with marian amun . the results are shown in table 4 . as expected , the best captions are those with only the best 5 captions , while the best ones with all 5 are the ones with the best 1 - 5 captions and the best 3 - 5 .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the best performing strategies are en - de and dec - gate , while en - fr and mscoco17 outperform enc - gate in terms of bleu % and img scores .
table 3 shows the results of our model with respect to the three models . the results are shown in table 3 . our model outperforms all the other models except for en - de and en - fr , which show that it is more likely to detect the presence of visual features in the images .
table 3 shows the results of our approach on the yule ’ s i test set . the results are shown in table 3 . the results show that en - fr - smt - ff and en - regex - trans - ff have the best performance on the test set compared to the previous state - of - the - art approach . as expected , the difference in performance between the two approaches is small , but still significant . in particular , the performance gap between en - es - ht and the other approaches is much larger than that between the previous approach .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of sentences in each split is shown in table 1 . we used en – fr and en – es as our language pairs .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our models are trained on both english and french data .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . the rev system has the highest bleu score and the second highest ter score , both of which indicate that the system is performing well on automatic evaluation .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage embeddings from the same source . the results are shown in table 2 . rsaimage has the highest recall @ 10 % and the highest average rank .
table 1 shows the results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage - based model from rsaimage . the results are shown in table 1 . as expected , the vgs model has the highest recall @ 10 .
table 1 shows the examples of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the classifiers are shown in table 1 .
table 2 shows the part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the percentage of words that have been added or subtracted in the final sentence . a score of 0 indicates that the sentence has not changed in terms of number of words , and a score of 1 indicates that it has changed in percentage .
table 3 shows the changes in sentiment score in sst - 2 . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
table 2 presents the results of our study on the sst - 2 test set . the results are shown in table 2 . we show the results for pubmed , corr , pubmed and pubmed - pubmed - sift .
