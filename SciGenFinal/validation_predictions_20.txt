table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , both approaches yield comparable or better performance in terms of training and inference . table 2 also shows that the recursive approach yields better performance when training with a larger number of instances , as it requires fewer iterations to make the model work .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
table 2 shows the hyperparametric optimization results for each model with different representation . the max pooling strategy consistently performs better in all models , indicating that the selection of the best filtering size and the dropout probability are the most important factors in model performance . also , when using softplus instead of sigmoid , the model performs better than the original conll08 model ( by a factor of 1 . 83 points ) . as shown in the table 2 , selecting the correct hyperparam parameters decreases the recall and increases the learning rate . finally , selecting only one type of feature map reduces the noise in the model , and allows further optimization of the parameters .
table 1 shows the effect of using the shortest dependency path on each relation type . we show that macro - averaged models produce significantly better f1 scores than the best models without sdp , indicating that the use of sdp degrades the model performance in relation extraction .
consistent with the observations by vaswani et al . ( 2017 ) , we report the results of r - f1 and f1 on the hidden test set of hotpotqa in table 3 . the results show that for both sets , our model outperforms the previous state - of - the - art model on almost every metric by a significant margin . on the other hand , for y - 3 , we see that it gets only 50 % of the f1 and 50 % on average compared to the previous best state of the art model .
the results are shown in table 1 . we show that our model outperforms all state - of - the - art parsers except mst - parser , which gets close to 100 % accuracy on average .
we show the mean c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . as table 4 shows , the difference in performance between the two indicates that the parser is more sensitive to the word " essay " and " paragraph " than the other two .
the results are shown in table 1 . as can be seen , the original and the cleaned models perform comparably to each other when the training data is added and removed . the difference is most prevalent for the original model , however , when we add in the miss and wrong aspects as well as meteor and rouge - l , indicating that these models are already well - equipped to perform this task . table 1 shows that once the errors are removed , the models perform similarly to the original ones .
table 1 shows the comparison of the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the difference in mr statistics between the original and the cleaned version is less pronounced , but still significant ( 0 . 5pt / 2pt ) compared to the original , which shows that our model can handle redundancy removal exceptionally well .
table 3 shows the results for tgen and sc - lstm on the hidden test set of hotpotqa . the results show that , compared to the original tgen model , the model is more stable and therefore requires less data to train , indicating that it has better generalization ability . as can be seen , the difference between the original and the original is less pronounced for both systems , when trained and tested on the original dataset , the error reductions are much smaller than those for the original . finally , we see that when redundancy removal is applied to the training data , the results are only slightly worse than the original for both models .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as can be seen , the majority of errors in our system ( 62 . 4 % ) are caused by errors caused by missing training data ( 23 . 6 % ) or by incorrect values ( 14 . 6 % ) .
the performance of our model compared to previous approaches on the hidden test set is presented in table 4 . graphlstm ( song et al . , 2018 ) achieves the best performance with a 28 . 9 % overall improvement over the previous state - of - the - art model , and a 3 . 6 % increase over the gap with the best performing ensemble model , snrg ( song and konstas , 2017 ) .
table 2 shows the performance of our model on amr17 . our model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art models across all parameters .
table 3 presents the results for english - german and czech , compared to english - czech . the results show that , for both languages , the model performs better than the single model when trained and tested on the hidden test set of bow + gcn . as the results show , the advantage of redundancy removal comes from the small size of the training data set : in english , we get 43 . 8 % improvement on average compared to ( bastings et al . , 2017 ) in terms of average number of tokens per model , while in czech we get 36 . 6 % improvement compared to birnn .
table 5 shows that the number of layers inside the network has the most significant effect , i . e . , it decreases the performance in low - supervision settings . we find that for example , the reduction of layer 1 affects our model performance by 3 . 8 points , or about 1 . 8 percent , which is significant for a single layer .
table 6 shows that the rcn models outperform the baselines with residual connections in terms of bias metric , confirming the importance of rc in model design . the difference between rc and rc - la is less pronounced for generalization , however , it is significant enough to show that it does not harm the performance for specific gcns . we notice that the gcn models using residual connections have higher correlation with baselines than those without .
the performance of our model compared to previous models on the unsupervised test set is presented in table 4 . we show that our model obtains competitive or better results than the previous state - of - the - art models on all metrics except for d - score .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . as the table indicates , removing the dense connections in the i - th block degrades the model , and consequently leads to a drop in the overall performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results show that both the language modeling objective and the coverage mechanism contribute similarly to the task , with the former having a bigger impact . as shown in the table , the models using the coverage mechanism have higher precision , indicating that they are more sensitive to coverage issues .
table 7 shows the performance of our initialization strategies on probing tasks . our paper establishes a new state - of - the - art on all three metrics , and outperforms glorot and topconst by a large margin .
the results are shown in table 1 . we observe that the h - cmow model outperforms the simple cbow / 400 model on every metric by a significant margin . it achieves state - of - the - art results on three of four metrics , and on two of the four metrics it beats the original cbow model by more than 2 points .
the results are shown in table 1 . we see that our method outperforms the previous state - of - the - art methods on all metrics by a noticeable margin . on the sst datasets , it achieves gains of 3 . 6 % and 6 . 4 % over the upsampling baseline on average .
table 3 shows the model ' s performance on the four downstream tasks as well as the overall improvement on the hidden test set of sts12 and sts15 . as can be seen , the cbow model has achieved considerable gains over the strong baselines ( i . e . , a gain of 26 . 2 % on average compared to hybrid ) in unsupervised mode . further , the difference between cbow and cmow is much smaller than that between cmp . 7 % and 62 . 0 % . hybrid also shows a considerable increase in performance relative to cbow , showing that the model can learn the task to a high degree .
table 8 shows the evaluation results for initialization strategies on supervised downstream tasks . our paper establishes a new state - of - the - art on all three sub - topics , outperforming glorot ( 86 . 6 % vs . 74 . 2 % ) , trec ( 76 . 4 % vs . , 62 . 9 % ) and sick - r ( 71 . 6 % ) by a significant margin .
table 6 shows the performance of each method for different training objectives on the unsupervised downstream tasks . the cbow - r method shows marked improvements over the strong baselines across the board , showing that it is better at selecting the correct target and its output is more interpretable . cmow also shows a significant improvement over cbow when trained only on sts12 and sts14 , indicating that it has better generalization ability .
the results are shown in table 1 . we show that our method outperforms the previous state - of - the - art methods on every metric by a significant margin . for example , it achieves 3 . 6 % higher precision on subjnum , 6 . 5 % higher on coordinv and 7 . 9 % higher score on topconst .
the results are shown in table 1 . we show that our method outperforms the previous state - of - the - art methods on all metrics except for sick - e . it improves upon the strong baselines by 3 . 6 points in the sub - category of mpqa and mrpc , and upsampling its performance on sst2 and sst5 .
table 1 shows the test bias scores for all loc and misc datasets , and the test misc dataset for all nested systems . in general terms , the results show that our model outperforms the previous state - of - the - art models across all metrics , indicating that it learns the task well .
table 2 shows the performance of our system in the test set under two settings . the first set shows that our model ( mil - 1 ) obtains the best performance with an f1 score of 43 . 57 % , while the second set shows a slight improvement of 0 . 38 % . the difference between the f1 scores of the two sets is less pronounced under the third set , indicating that the model using supervised learning has better generalization ability . as shown in the table , when only the name matching is used , the model gets the better performance .
table 6 presents the results of model training on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . the results show that , compared to g2s - gin , the model has better generalization ability and hence requires less data to train , as shown in table 6 .
table 3 presents the results of experiments on the hidden test set of hotpotqa in the distractor and fullwiki setting , respectively . our model outperforms the previous state - of - the - art models on every metric by a noticeable margin . for example , g2s - gat shows a performance gain of 3 . 42 ± 0 . 28 points over the model by konstas et al . ( 2017 ) and 3 . 45 ± 0 . 59 points over g2ss - gin . similarly , it achieves a 3 . 57 / 0 . 53 increase over the performance of s2s , which shows the diminishing returns from naive modeling . we notice that the performance gap between our model and the other models is narrower than that between the published and unpublished results .
table 3 shows the model performance on the ldc2015e86 test set when trained with additional gigaword data . g2s - ggnn shows significant gains over the strong lemma baseline on this test set ,
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . we show that our model significantly outperforms the previous state - of - the - art models across all metrics except meteor .
the results are shown in table 1 . we observe that g2s - ggnn shows considerable performance improvement over the baseline model across all metrics , showing that it can considerably reduce the training time and error when trained and tested on the large scale web content .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added " and miss " , for the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , g2s - gat shows a significant improvement over s2s in terms of both accuracy andiss percentage , confirming the viability of parameter sharing
table 4 shows the pos and sem accuracy using different target languages trained with different nmt encoding layers , trained with a smaller parallel corpus ( 200k sentences ) . as can be seen , the pos features significantly improve the generalization ability of the model , and sem also shows a drop in performance compared to the original embeddings . the difference is most prevalent in the semantic regions , which show that the semantic features extracted by the 4th nmt layer are particularly useful for target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag encoder decoder has the best performance , confirming the value of unsupervised word embeddings . it also shows that it has the upper bound on the most frequent tag , indicating that it is well - equipped to handle the high frequency of tags .
table 4 presents the system ' s performance on each error generation algorithm for pos and tagging accuracy . we show the results for english , spanish , french , dutch , russian , turkish and turkish for each language . for english , we show 35 % higher accuracy on average compared to the previous state of the art model .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as can be seen , the first layer shows much higher accuracy than the second and third , indicating that the bias in our model is less pronounced for languages with lower precision .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the difference between the attacker score and the corresponding adversary ' s accuracy is significant , indicating that the attacker has learned a lot about human judgement . the classifiers trained on this training set are particularly sensitive to gender bias , as shown in table 8 . also , they notice a significant drop in the performance for mentions of race and age compared to pan16 .
accuracies when training directly towards a single task are shown in table 1 . the results show that pan16 significantly outperforms pan16 in all aspects ( except for gender bias , gender bias and age bias ) , confirming the importance of gender bias in the generation of neural models .
table 2 shows the results for balanced and unbalanced data splits . as the results show , there is a significant imbalance in the distribution of the data that gets leaked , indicating that some of the classifiers are more sensitive to gender bias . also , the racial disparities shown in the table show that the presence of gender bias contributes less to task prediction accuracy .
the performance of our model on these datasets is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy indicates that our model can distinguish between the true response and the leakage of information during the training phase . as shown in the table , gender and race features have the highest impact on the performance , which indicates that they are particularly difficult for the model to distinguish .
we show the ablation results for different encoders for the protected attribute in table 6 . the results show that the rnn model can easily distinguish between the presence of the protected and non - protected states , and the accuracy of the encoded attribute .
the results are shown in table 1 . we show that our model outperforms the previous state - of - the - art models on every metric by a significant margin . for example , it achieves a final score of 85 . 15 % on the " ptb " and " wt2 " metrics , which shows that it has mastered the task of parameter sharing and tuning well . on the " finetune " metric , we get a performance improvement of 2 . 59 % on average .
the results are shown in table 1 . we show that our model obtains the best results with a gap of 3 . 5 % in acc time from the last published results ( rocktäschel et al . , 2016 ) . the difference between the base and current state - of - the - art models is less pronounced with respect to time , however we see significant difference in performance between the two sets due to larger variation in the number of parameters used for training and validation set .
table 3 presents the results for the second variation of our model in the yelppolar and amapolar datasets . the results show that our model outperforms both published and unpublished work on every metric by a significant margin . for example , it achieves an err of 4 . 57 / 1 . 59 and a time - error of 0 . 59 / 0 . 63 on the yelptime metric , respectively , which shows that it has significantly better generalization ability . also , we notice a drop in performance between the two sets when we switch from baselines to full - time models .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task . as shown in the table , gnmt outperforms all the base systems except olrn , which shows the advantage of scalability in low - supervision settings . the difference between gnmt and other methods is less pronounced for german , but still indicates that gnmt has superior generalization ability . although lrn has seen more training time in the past , it is still inferior to other methods in terms of decoding one sentence , as shown in table 3 . it is clear from table 3 that there is a significant advantage to using a more specialized neural network for this task , as it takes significantly less time to train and decode compared to other approaches . table 3 also shows that the size and type of training data used to train the model is the most important factor in its performance . apart of the training data size and the number of parameters used to encode the sentence , the model also requires a significant amount of time to decode .
table 4 shows the exact match / f1 - score of our model on the squad dataset . it can be seen that the parameter number of base . 5 gives a good indication of the model ' s performance , indicating that it has good generalization ability . as shown in the table , other sophisticated neural models also have comparable performance with our model in terms of match rate and f1 score .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the model obtains the best performance with a f1 - score of 90 . 94 % on the three parameter number comparison . although the number of parameters in our model is small , it represents a significant improvement over the previous state - of - the - art model . we show that our lstm model can easily distinguish between the true response and negative responses .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 26 % accuracy on both snli and ptb tasks , compared to the previous state - of - the - art model ' s 62 . 57 % accuracy .
table 2 presents the results for system and sentence prediction using the best performing feature set . our system outperforms all the base table 2 shows that it is more than 4 . 5x better at selecting the correct word pairs and its average number of tokens , while only slightly outperforming human on sentence prediction . retrieving the word pairs is easier than setting up the sentence prediction ; however , it requires significantly more data and time to set up and maintain the system , which results in significant performance drop .
table 4 shows the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is indeed comparable to human judgement on these three aspects . the second best result is achieved by candela ( h & w hua and wang ( 2018 ) , which reaches a final ranking of 10 . 2 points higher than human on all three metrics .
the results are shown in table 1 . we show that for all 3 domains , our model outperforms the previous state - of - the - art models on every metric by a significant margin . for example , on the " food and drink " dataset , we see that our model ( hclust ) outperforms all the other models except " food & drink " by 2 - 2 . 5 points on average . on the " ted talks " dataset we see a gap of 1 - 2 points on the food & drink dataset , which shows the competitiveness of our model over the other two .
the results are shown in table 1 . we show that for all 3 domains , our model outperforms the previous state - of - the - art models on every metric by a significant margin . for example , on the " food and drink " dataset , we see that our model ( hclust ) outperforms all the other models apart from " food & drink " by a large margin . on the " ted talks " dataset we see a gap of more than 2 points , which indicates that the training set size and type of data used for this model is relatively small .
the results are shown in table 1 . we show that for all 3 domains , our model outperforms the previous state - of - the - art models on every metric by a significant margin . for example , on the " ted talks " dataset , we see that our model ( at p < 0 . 001 ) outperforms all the other models except " dsim " and " slqs " . on the " food & drink " dataset we see a gap of more than 2 points with the other two models on average .
the performance of our model on the three metrics is presented in table 1 . we show that it significantly outperforms the previous state - of - the - art models on all metrics except for total terms . the difference is most prevalent in relation to depth , according to the table , depth is the most important metric in our model , followed by total terms , while averagedepth is the second most important .
the system performs well on both datasets with different threshold measures set at different depths . for example , europarl achieves 9 . 43 % averagedepth and 2 . 29 % maximumdepth , which shows that it is well - equipped to handle large datasets . on the other hand , it suffers from low correlation with other metrics like total terms and roots .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the enhanced version of our model ( lf ) outperforms the baseline model ( p1 ) by 3 . 42 % in terms of r0 , r1 , r2 , r3 and svm r3 scores , indicating that it has better generalization ability . also , it achieves a higher r0 / r1 and r3 / d score , showing that it is more sensitive to the loss function of the question type and the answer score sampling , i . e . , that it can learn the answer to more questions without sacrificing too many correct answers .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . note that only applying p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . the model with the best performance is the one that applies p2 and gets the best rva score .
table 5 shows that the hmd - f1 model outperforms the other approaches on both soft and hard alignments . the difference in performance between hard and soft alignments is most prevalent in the case of " de - en " , " lua - en " and " lv - en " . when we add in the effect of ruse , the difference is less pronounced , but still significant .
the results are shown in table 1 . the first group shows the results for direct and indirect assessment . our method outperforms all the baselines except ruse ( * ) by a large margin . the difference is most prevalent in the case of ru - en , where our method obtains the best average score .
the experimental results on the hidden test set are shown in table 1 . the first group shows that all metrics we consider have low correlation with human judgement . among all the metrics , the meteor score is relatively high while bertscore - f1 is relatively low . when we add in the w2v metric , the difference between the two sets becomes less pronounced , indicating that the human judgement is less important . we further compare our model with other methods for selecting the best features and training the model . we see that among the many methods , the most interesting ones are smd and its variants , especially the sent - mover variant , which significantly outperform the other methods . also , we notice a drop in performance between baseline and smd + w2v model when using the additional parameter of qualitative noise for training .
the semantic threshold for each metric is set at 0 . 9 , which indicates that the model can rely on superficial cues alone to predict the answer . we further analyze our model with respect to semantic threshold using the following three metrics : leic ( % ) , spice ( % ) , bertscore - recall and word2v ( which relies on word2vec embeddings ) , and word3vec modeling . the results are shown in table 1 . the first group shows that the semantic threshold used for classification is relatively high , indicating that the training data used for this model can be used to train the model without understanding the task . word2vec modeling has the best performance , showing that it can learn the task to a high degree .
the results are shown in table 6 . the first group shows that when we only consider error reduction with shen - 1 and parameter sharing , our model obtains competitive or better results than the best previous state - of - the - art model .
table 3 presents the results on transfer quality and semantic preservation . our model obtains the best results among all three metrics . it significantly outperforms the other two baselines in terms of transfer quality , and its average number of correct answers is close to those of the original ( i . e . , m0 = 0 . 01 , m2 > 0 . 05 ) . semantic preservation is relatively consistent with the strong baselines , while the drop in transfer quality is much smaller .
table 5 shows the human sentence - level validation results for each dataset for validating acc and pp ; see text for validation of gm . it can be seen that both the human and machine methods use the same set of metrics to validate their judgments , and that their summaries match the human ratings of semantic preservation and fluency . however , the difference between the accuracy of spearman ’ s and human ratings is much larger for both metrics , indicating that there is a need to design more complicated methods for future research into the topic .
the results are shown in table 6 . the first group shows that when we only consider training models with one type of classifier , fine - tuning the model leads to better generalization . the model m0 achieves the best generalization performance : it achieves m0 [ ital ] + para 0 . 783 on par with the best - performing model , shen - 1 . 2 % overall improvement over the strong lemma - trained baseline .
table 6 shows the results for sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table and yang2018unsupervised ) achieve higher acc than prior work at similar levels of accuracy , indicating that the classifiers in use are more useful for this task . however , the results show that it is harder to transfer large numbers of sentences than to transfer small ones . we notice that the difference in acc between transfer and simple - transfer is less pronounced for unsupervised and supervised models , but still indicates that there are some classifiers that are better at this task than others .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of tokens in a sentence , and consists of repetition tokens , rephrase tokens and initialization tokens . the length of the repetition tokens is the most important factor in predicting disfluencies . as shown in the table , for all but one of the 8 cases , repetition tokens have a significant effect on prediction performance .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparlabelled and unreparulated forms , or in neither . it can be seen that both the length and type of tokens in a disfluency are important for prediction , as the difference in the prediction frequency between the two is minimal , however it is significant for repair . as shown in the table , the repair tokens have the highest prediction frequency , indicating that they are of high importance for prediction .
the results are shown in table 2 . we show that for both datasets , the text - based approach is superior to the single - input approach when we only consider the test set with one error , as the results show , once the error reduction is factored in , the model performs better than the single model under all three scenarios except for the exceptional case where it is trained on the data with two errors .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the previous state of - art methods . it closely matches the accuracy of the best performing algorithm , rnn - based sentence embeddings , in terms of both accuracy on generalization and on the relation extraction tasks .
table 2 shows the performance of all the methods that we trained on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater shows marked performance improvement over the previous methods ,
table 3 compares the performance of our method with and without word attention for this task . our neuraldater model shows marked accuracy improvement over the state - of - the - art oe - gcn model , confirming the effectiveness of both word attention and graph attention . the difference in accuracy between attention and non - attention is less pronounced for neuraldater , but still shows significant performance improvement .
the performance of all models is shown in table 1 . embedding + t achieves the best results , surpassing all the base models except for cnn , which shows significant performance improvement over argument - based neural network . further , the gap between the best and worst performances is narrower when we only consider 1 / 1 and 1 / n , indicating that training on single - headed attention is beneficial .
table 1 presents the results for argument and event identification . the first group shows that argument identification and classification are the most important aspects for prediction success , followed by trigger and event classification . as shown in the table , both methods yield high accuracies for both groups , with the exception of cross - event , which shows significant performance drop . further , the difference between event and entity identification is less pronounced for argument , showing that both methods rely on syntactic and semantic information .
the results are shown in table 4 . as can be seen , all models show lower precision on the test set when trained and tested only on english , spanish , french , dutch , russian and turkish , respectively , compared to the original models . the smaller difference in accuracy between the original and the updated models indicates that the original model is more sensitive to grammatical gender bias . fine - tuning also reduces precision , as shown in fig . 4 .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuned neural models perform well over both subsets , showing that the training set can be further trained with a greater diversity of training instances , as shown in fig . 4 .
the results in table 5 show that fine - tuning has a significant ( 7 % ) impact on the gold sentence prediction accuracy , both on the dev set and on the test set , compared to monolingual ( mono ) . as hard coreference problems are rare in gold sentences , we do not see significant difference in accuracy between the two sets ,
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as compared to the baseline model ( table 5 ) . type - aggregation significantly improves recall and precision ( p < 0 . 01 ) and r = 0 . 03 , indicating that the model can rely less on superficial cues and more on interpretable cues .
results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . we use the type and semantic embeddings obtained by running autoextend rothe and schütze ( 2015 ) on glove . the resulting hpcd ( full ) and ontolstm - pp ( partial ) outperform the original model , indicating that the semantic information injected into the lstm by the additional cost term has a significant impact . further , the difference between the accuracy of the original and the extended model is less pronounced , showing that semantic information can be extracted from the same word without losing its semantic functions .
table 2 shows the results for rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the system achieves high accuracies for both uas and ppa accuracy ( 98 . 60 % vs . 86 . 59 % ) , indicating that the dependency features extracted by rbg are useful for future research in this direction .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the model achieves a ppa acc . ( accuracy ) of 89 . 7 % and 87 . 5 % respectively , which shows that context sensitivity has a significant effect on model performance .
we show the bleu % and roc % scores for adding subtitle data and domain tuning for image caption translation in table 2 . the results show that , when domain - tuned , the model performs better than en - de and mscoco17 , indicating that the domain - aware subtitle data do improve the translation performance . further , the improvement is much larger when the model is added with domain tuning as well as subtitle data .
we show that domain - tuned h + msoco outperforms the plain model , confirming the importance of the domain - adaptive nature of parameter sharing . the results show that , when domain - aware , the model performs better than the model without , indicating that the model is more sensitive to the local context .
table 4 shows the bleu scores of all models that add automatic image captions ( only the best one or all 5 ) . the table shows that the model using marian amun ' s captions achieves the best results . the model using only the best five captions obtains 62 . 2 % overall improvement on the combined task .
table 5 shows the bleu % and w - score of our method for visual information integration using multi30k + ms - coco + subs3mlm , detectron mask surface , and detectronmask , it can be seen that our method significantly outperforms the previous state - of - the - art methods in terms of both metrics , indicating that it has superior generalization ability .
we show the results for en - de and mscoco17 when we switch from the simple model to the more complicated multi - lingual model . the results are shown in table 1 . the first group shows that , when we only consider visual features , our model obtains competitive or better results than the previous state - of - the - art model on both datasets .
the results are shown in table 1 . the first group shows that en - fr - ht and en - es - ht achieve better results on the hidden test set compared to the models using pure mtld , indicating that the semantic information injected into the model by the additional cost term is a significant factor in boosting performance . next , we show the results of re - training the model after replacing these 10 files with the ones from the original set . the results show that , once again , the use of mtld information helps the model to improve its performance .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . it is clear from table 1 that our model can easily handle multiple languages at once , with a minimum of 50 , 000 sentences per language split .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results show that the models can learn to reason over more than simple verbs , and that the vocabulary is diverse enough to maintain high recall .
table 5 shows the automatic evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems achieve decent performance , but are lower than the ter and bleu systems , indicating that the translation quality of the original neural network is less than the performance of the rephrase .
table 2 shows the performance of our model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . the model obtains the highest recall @ 10 and median rank , indicating that it has the best generalization ability . segmatch also shows lower recall , but higher mean mfcc .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . the acoustic embeddings outperform the others in terms of recall @ 10 and average rank , indicating that the model can learn to reason over a large corpus . the difference is less pronounced for rsaimage , which shows that it is easier for the model to learn and reason over large datasets . as shown in the second row , acoustic2vec - u also outperforms other approaches which allow more variation in program implementation . it shows that the challenge of embedding multi - headed attention is in how to extract the semantic information from the image and synthesize it .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as can be seen , all the classifiers turn in a screenplay that is easily distinguishable from the original ( hence , there is no need to worry about loss of accuracy ) . however , for dan , it is harder to distinguish between the edges and the shapes , as shown in table 1 . the difference between the turns in a smooth and rough screenplay is less pronounced for rnn , as it turns on a on ( around 3 % ) , but still significant enough to show that it is possible to distinguish the differences . we report further examples in the appendix .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same through fine - tuning of sst - 2 . as shown in the table , the number of occurrences has increased as a result of the decrease in the overlap with the original sentence , indicating that the size of the vocabulary has not changed as much as we expected . also , the average number of instances per sentence has increased , as shown in table 2 . however , the difference between the average of the two sets of tokens indicates that there is still a significant overlap between the correct and incorrect answer to questions , i . e . that the correct answer is still within the correct sentence .
table 3 shows that sentiment scores in sst - 2 have increased as a result of the flipped labels being flipped from positive to negative . the numbers indicate the changes in percentage points with respect to the original sentence . flipped labels cause the rnn to generate significantly more positive sentiment .
table 1 presents the results of the second study . our model outperforms all the baselines except pubmed , confirming the effectiveness of our approach . it shows that the ability to distinguish between good and bad states is relatively high , i . e . , > 98 % .
