table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results of training and inference are shown in table 2 , the recursive approach shows the best performance on both inference and training .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . table 1 compares the performance of the balanced and linear datasets with the best performing batch size for the treernn model with different tree balancedness . with a batch size of 25 , the balanced dataset shows the best performance .
the max pooling strategy consistently performs better in all model variations . for example , the conll08 model with the best f1 score ( 71 . 57 % ) outperforms ud v1 . 3 with a dropout probability of 0 . 66e + 00 points in the dropout prediction function . with the softplus representation , the model performs slightly better than the sb model , but still performs better than softplus on the feature maps .
table 1 : effect of using the shortest dependency path on each relation type . the results are shown in table 1 . our approach outperforms both the approach without sdp and the approach with sdp by a significant margin .
the results are shown in table 3 . the performance of our model on the f1 and r - f1 metrics is shown in bold . our model achieves the best results on both metrics with a f1 score of 100 % and f1 percentage of 50 % . on the other hand , our model performs slightly worse than the best previous state - of - the - art on f1 , with a r - score of 53 . 57 % compared to 50 . 45 % .
the results are shown in table 3 . we can see that mst - parser outperforms all the other approaches except for our approach in terms of accuracy . our approach obtains a significant improvement in accuracy over the previous state - of - the - art on all metrics .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between paragraph and essay performance is less pronounced for the lstm - parser system .
the results are shown in table 3 . the results of original and clean - up are presented in bold . we observe that the original model performs significantly worse than tgen − and tgen + on the test set with respect to both accuracy and precision . however , the results are slightly better on the training set when the model is combined with sc - lstm .
table 1 compares the original e2e data with our cleaned version . the results are presented in table 1 . the difference in the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . as expected , the difference in ser between the original and the cleaned version is less pronounced for the test set .
the results are shown in table 3 . we observe that the original tgen + model outperforms tgen − and sc - lstm on every metric except for bleu and rouge - l by a significant margin . the difference between original and original is most striking when we compare against the performance of the original model with respect to the training set and the test set .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) , and the number of instances for which we found a missing value or a wrong number of correct values . the results are shown in table 4 .
the results are shown in table 3 . our proposed method outperforms all the state - of - the - art approaches except for tree2str and pbmt , which are comparable to our approach in terms of performance .
table 2 : main results on amr17 . our model achieves 24 . 5 bleu points compared to seq2seqb ( 28 . 4 % ) and 21 . 1 % improvement over the performance of the previous state - of - the - art ensemble models . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves a marginal improvement of 3 . 5 points over the previous best performance . table 2 shows the results of our model compared to previous state of the art ensembled models .
table 3 presents the results for english - german , german - czech and czech . the results are presented in table 3 . our proposed method outperforms the best previous approaches on all three languages . we observe that our proposed method improves upon the performance of previous approaches with a noticeable margin .
table 5 : the effect of the number of layers inside the network on the performance of our model is shown in table 5 . our model outperforms the previous state - of - the - art model by a significant margin . the effect is most pronounced for blocks containing 6 layers or more , with the effect of layers containing 3 or more being negligible .
comparisons with baselines are shown in table 6 . rc denotes residual connections with residual connections , while rc + la ( 6 ) denotes connections without residual connections . the results of rc + rc are presented in bold . we observe that the results obtained with rc + rc significantly outperform the baselines for all gcns except for the case of dcgcn1 ( 22 . 9 % ) , which shows that the residual connections do not contribute significantly to the performance of these gcns .
the results are shown in table 4 . we observe that our proposed model outperforms the previous state - of - the - art models in terms of both d and b - score by a significant margin . our proposed dcgcn ( 1 ) improves upon the performance of the best previous models by 3 . 4 points on average .
the results of the ablation study are shown in table 8 . the results show that removing the dense connections in the i - th block significantly reduces the density of connections on the dev set of amr15 .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results are presented in table 9 . our proposed approach outperforms the best previous approaches by a significant margin . our approach obtains the best results with respect to coverage and direction aggregation .
table 7 : scores for initialization strategies on probing tasks . glorot and somo outperform our paper on all three of these tasks , with the exception of somo , which gets a slight improvement over our proposed framework on two of the three tasks ( table 7 ) .
the results are shown in table 3 . we observe that the h - cmow model outperforms the approach of cbow and h - cbow with a margin of 3 . 6 points over the best previous state - of - the - art results . further , the accuracy of h - bcow improves with time as well . it achieves an accuracy of 82 . 9 % on average compared to the previous best state of the art model of 77 . 6 % .
the results are shown in table 3 . we observe that the cmow / 784 model outperforms all the other approaches except for sick - e , with a margin of 3 . 6 % over the best baseline on sst2 and 7 . 4 % over sst5 . hybrid models outperform all the alternatives except cbow , as shown in the table .
table 3 : scores on unsupervised downstream tasks attained by our models . the results are presented in table 3 . our proposed method outperforms both the cbow and cmow baselines with respect to all the downstream tasks except for sts13 and sts16 . hybrid outperforms cbow on all the tasks except sts15 , where our model achieves 62 . 6 % higher score . we observe that the performance of our proposed method is comparable to that of cbow , but the difference between the two is less pronounced on sts14 , where the model achieves 63 . 5 % higher performance .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot and sick - r outperform all the other approaches except mpqa and trec by a significant margin . we observe that the effectiveness of our initialization strategies is primarily due to the large improvement in the precision scores for subjoint sub - joint tasks .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our proposed cbow - r method outperforms the cmow - c on all the tasks except for sts13 and sts14 , where it performs slightly better .
the results are shown in table 3 . we observe that our method outperforms the previous state - of - the - art approaches on all metrics except for bshift and subjnum , where our approach obtains the best results . on the other hand , our approach outperforms both the previous best state of the art method and our approach on all three metrics except bshift , which shows that our approach can improve upon the performance of previous approaches .
the results are shown in table 3 . we observe that the cmow - r model outperforms all the other approaches except for sick - e , with the exception of mpqa , where the cbow - c model performs better than the other methods . the performance gap between sst2 and sst5 is almost entirely due to the small size of the training data set , but it is still significant enough to warrant further study .
the results are shown in table 3 . the results for all loc and misc based systems are presented in bold . we observe that our system outperforms the previous state - of - the - art models on all metrics except for name matching , which shows that our supervised learning approach can improve upon the performance of the best previous models with a noticeable margin .
results on the test set under two settings are shown in table 2 . name matching and supervised learning achieve the best f1 scores , while τmil - nd ( model 2 ) achieves the best overall f1 score . we observe that the model with the best performance is the one with the highest precision on the name matching test set . the results on the f1 test set are presented in bold . in table 2 , we observe that our approach outperforms both the previous state - of - the - art approach and the best supervised learning approach .
the results are shown in table 6 . our model outperforms all the alternatives except for g2s - gat , which shows that the gat model has superior generalization ability . we observe that the model with the best generalization performance is gat - ggnn , which improves upon the performance of s2s by 3 . 86 points in the accuracy metric .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s model outperforms previous state - of - the - art models on both datasets by a significant margin . the results are presented in table 3 . we observe that our model performs significantly better than previous state of the art models on all but one of the three datasets , with the exception of the case of konstas et al . ( 2017 ) . on the other hand , we observe that the performance gap between our model and the previous best state - ofthe - art model is much narrower on the two datasets on which we base our model , with a gap of only 0 . 03 points .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . g2s - ggnn outperforms the best previous models on both external and internal test sets . the results are shown in table 3 .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results are presented in bold . our model outperforms the previous state - of - the - art models on all metrics except for meteor score .
the results are shown in table 3 . we observe that the g2s - gin model outperforms all the alternatives on all metrics except for sentence length , with the exception of length of the last sentence , which shows that it is able to handle longer sentences better than the alternatives .
table 8 shows the fraction of elements in the output that are missing in the generated sentence for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gin outperforms s2s and gat in terms of the fraction of elements missing from the output ( miss ) . as shown in table 8 , the model with the best performance is gold . we observe that gold significantly improves upon the performance of the other approaches with a significant margin .
table 4 shows the pos and sem accuracy for different target languages trained on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is significantly higher than the sem accuracy , indicating that the features extracted from the 4th nmt encoding layer have a significant impact on the model ' s model performance .
table 2 compares the pos and sem tagging accuracy with baselines and an upper bound . the results are presented in table 2 . our proposed embeddings significantly outperform the baselines for both tags , with the exception of pos , which is slightly worse than the word2tag results .
table 4 compares the performance of our system with the previous state - of - the - art systems on the pos and sem metrics . our system outperforms all the previous systems on all metrics except for pos tagging accuracy . the results are shown in table 4 . table 4 shows that our system is comparable to previous state of the art systems in terms of both accuracy and precision .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are presented in table 5 . our proposed system outperforms the previous state - of - the - art approaches on all but one of the four target languages , with the exception of english .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . for pan16 , the attacker performs slightly better than the adversary on all three datasets . for example , the difference is less than 0 . 1 % on pan16 .
table 1 : accuracies when training directly towards a single task . the results are presented in table 1 . our approach outperforms previous approaches by a significant margin . for example , the pan16 model outperforms pan16 with an accuracy of 83 . 2 % on the dial task and 77 . 9 % on sentiment task . the gender and age - based features also contribute significantly to accuracy .
the results are shown in table 2 . the results of the balanced and unbalanced data splits are presented in bold . for pan16 , we see that there is no significant difference in the performance between the baselines for tasks with and without balanced data splits . for tasks with unbalanced and balanced task splits , the results are slightly worse .
the performance of these models on different datasets with an adversarial training set is shown in table 3 . the results are presented in terms of the difference between the attacker score and the corresponding adversary ’ s accuracy . as the table 3 shows , the presence of gender and race features contribute significantly to the performance of our model , but only marginally to the overall performance .
the results are shown in table 6 . rnn and guarded encoders perform similarly on the leaky and guarded embeddings , with the difference being less pronounced for the former . with the guarded encoder , rnn performs slightly worse than guarded , but still performs better than the original encoder .
the results are presented in table 3 . table 3 shows the performance of our model compared to previous state - of - the - art models on the ptb and wt2 baselines . our model outperforms all the previous state of the art models except for the lstm , which achieves a final score of 63 . 36 % on the wt2 base and 62 . 86 % on wt2 + dynamic ( yang et al . , 2018 ) . we note that our model performs slightly better than the best previous work on both base and dynamic ptb baselines , as shown in yang et al . ( 2018 ) .
table 3 presents the results of our model with respect to the training data . the results are presented in table 3 . our model outperforms the previous state - of - the - art lstm model by a significant margin . we observe that our approach achieves the best results with a base time of 0 . 86 seconds and a maximum bert time of 3 . 03 seconds , which is comparable to the best performance by rocktäschel et al . ( 2016 ) .
the results of zhang et al . ( 2015 ) are shown in table 3 . table 3 presents the results of our approach compared to previous work on yelppolar , amapolar and yahoo time . our approach outperforms the previous state - of - the - art models on all metrics except for yelp time . the results are presented in bold . we observe that our proposed method outperforms all the previous approaches except for the case of yelp time , where it performs slightly better than the best previous state of the art .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on tesla p100 dataset . our model significantly outperforms the previous state - of - the - art approaches on both case - insensitive tokenized and un - tokenized tokens . our approach obtains the best performance with an absolute improvement of 2 . 67 points over the previous best performance by olrn and sru on the german translation task .
table 4 : exact match / f1 - score on squad dataset . the results of our approach are presented in table 4 . our approach outperforms all the state - of - the - art approaches except for the performance of rnet * ( 71 . 41 % f1 score vs . 71 . 67 % on average ) . we observe that our approach is comparable to previous work ( wang et al . , 2017 ) in terms of match rate and f1 scores . however , our approach obtains a slight improvement in accuracy compared to the results of wang et al . ( 2017 ) with the addition of elmo parameter .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model significantly outperforms the previous state - of - the - art models on both english and german ner tasks . our model obtains the best performance with an f1 of 90 . 56 % on the english ner task compared to the previous best performing model of 89 . 61 % .
table 7 shows the test results on snli and ptb task with base + ln setting and test perplexity on base setting . the results are shown in table 7 . lrn outperforms all the other approaches except elrn ( 83 . 56 % vs . 83 . 49 % ) on the snli task with the base setting and the perplexity task with a base setting of 61 . 81 % . with the incorrect base setting , the model performs significantly worse on the ptb test .
the results are shown in table 3 . word embeddings outperform human on both systems . the results for human and oracle are presented in tables 3 and 4 . as expected , the results for both systems are significantly worse for human than for oracle . for example , for human , the average number of words per sentence is lower than that for oracle , while for oracle it is higher .
table 4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points ( candela , 2018 ) and the second highest is 2 . 6 points ( h & w hua and wang ( 2018 ) . the results are presented in table 4 . table 4 shows that the best performing automatic system is seq2seq ( 25 . 6 % ) on the content richness and grammatical accuracy ( 18 . 8 % ) , with the second - best performing system ( 28 . 8 % ) on syntactic accuracy ( 24 . 6 % ) . the best performing human evaluation is obtained by h & w - hua ( 2018 ) with 38 . 8 % of evaluations being ranked in top 1 or 2 for overall quality .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the previous approaches except for the case of ted talks , which shows that our approach is superior in terms of generalization . we observe that the performance gap between our approach and previous approaches is less pronounced with respect to the ted talks dataset , but still significant .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the baselines except for df and tf by a significant margin . we observe that our approach significantly outperforms the previous state - of - the - art approach on all metrics except for dslqs and tf .
the results are shown in table 3 . table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the previous approaches except for the case of ted talks , which shows that our approach is superior in terms of generalization . we observe that the performance gap between our approach and previous approaches is less pronounced with respect to the ted talks dataset , but still significant .
the results are shown in table 3 . we observe that the average depth of our system is 11 . 05 , which is slightly better than the previous state - of - the - art on both datasets . however , the difference between maxdepth and averagedepth is less pronounced on the slqs dataset , which shows that our approach relies on superficial cues .
the results are presented in table 3 . we observe that our approach outperforms the previous state - of - the - art approach on all metrics except for one metric , namely , the average depth . europarl achieves the best average depth with a precision of 9 . 43 % , which is slightly higher than the previous best performance of 8 . 29 % . the difference between maxdepth and averagedepth is due to the high correlation between the number of roots and the depth of the lexical features of the word embeddings .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . our enhanced model outperforms the baseline model in terms of both qt and ncdcg % by a significant margin . we note that the enhanced version of our lf model improves the model ' s performance on the question type , answer score sampling , and hidden dictionary learning , but does not improve on the ranking loss .
the results of the ablative studies are shown in table 2 . adding p2 improves the performance of all the models except for the hidden dictionary learning model , which is the most effective one ( i . e . , p1 + p2 = 71 . 63 % ) . however , for the coatt model , the difference between baseline and p2 is only 0 . 88 points ( tables 2 and 3 ) .
table 5 compares the performance of our hmd - f1 + bert model with previous approaches on hard and soft alignments . the results are presented in table 5 . the hmd - recall + bert model outperforms all the other approaches except wmd - unigram , which achieves 0 . 823 / 0 . 817 on hard alignments and 0 . 864 / 0 . 971 on soft aligned ones . we observe that the performance gap between the hmd pre - trained model and the hmd trained model is less pronounced with respect to recall , however , the results are still significant . finally , we see that the accuracy gap between our model and previous approaches is much narrower than that with wmd - bigram .
the results are shown in table 3 . the average score for each setting is reported in terms of bertscore - f1 and ruse ( * ) as well as the weighted average of meteor + + and w2v scores . for example , the results for de - en , ru - en and zh - en are reported as 0 . 716 / 0 . 719 and 0 . 686 / 0 . 866 on average , respectively , while the results of fi - en ( 0 . 838 / 1 . 012 ) are 0 . 864 / 1 , 0 . 871 / 2 . 005 on average . we observe that the baselines used to train our model are comparable to those used in the previous work .
the results are shown in table 3 . the results for bleu - 1 and bleud - 2 are presented in bold . our proposed method outperforms the previous state - of - the - art baselines on all metrics except for bertscore - f1 by a significant margin . we observe that our proposed method improves upon the performance of the previous baselines with respect to all the metrics except the sent - mover metric .
the results are shown in table 3 . we observe that word - mover performance is comparable across all the metrics with respect to leic ( * ) and spice , with the exception of word prediction accuracy , which is slightly lower than that of meteor ( 0 . 939 on m1 and 0 . 949 on m2 ) . word prediction accuracy is slightly higher than the performance of bertscore - recall , indicating that the accuracy obtained by word prediction alone is not high enough to achieve the best results .
the results are shown in table 7 . the results of the best performing models are presented in bold . our model outperforms all the baseline models except for the case of shen - 1 , where it performs slightly better than the previous best performing model .
the results are shown in table 3 . table 3 shows the results for the transfer quality , semantic preservation and fluency tests . our model outperforms the previous state - of - the - art on all metrics except transfer quality and transfer quality tie by a significant margin . we observe that our proposed transfer quality improvement over the state of the art is primarily due to a drop in transfer quality for the semantic preservation test set . on the other hand , the improvement on transfer quality on the fluency test set is less pronounced , but still significant .
table 5 shows the results of human sentence - level validation for yelp and yelp lit . for each dataset , we use spearman ’ s [ italic ] ρ b / w ρ values of semantic preservation and fluency to validate the accuracy of our summaries . the results are presented in table 5 . the accuracy of the summaries obtained by our system is 94 % , with a match rate of 84 . 67 % on acc .
the results are shown in table 6 . the results of the best performing models are presented in bold . our model outperforms all the baselines except for the case of shen - 1 , where it performs slightly better than the best previous state of the art .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results in table 6 show that our best model achieves the highest accuracy with respect to the accuracy of acc ∗ . our best models ( right table ) outperform the best previous work by a significant margin . we note that the accuracy achieved by our model is higher than that by previous work , indicating that our classifiers are better at predicting sentiment .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent for each type of nested disfluency . reparandum length is the average of the length of the shortest and longest tokens , and the average number of repetition tokens , which are used to represent the number of tokens in a sentence . as shown in the table , rephrase tokens are particularly difficult to predict as disfluencies , as their length is much shorter than repetition tokens . however , when we include repetition tokens in the training data , we get a significant improvement in performance .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the repardiation of a disfluency with a function word ( function - function ) or in neither . the average length of tokens for each category is shown in table 3 . percentages in parentheses show the fraction of tokens belong to each category . for example , the content - content tokens have the highest percentage of tokens , but the function - function tokens are the smallest .
the results are shown in table 3 . we observe that the best model is the one with the best dev mean and best test result . as the results show , when the model is trained on raw data , it is able to converge significantly faster than the best single - input model . however , when we add in innovations , the results are slightly worse .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance with an accuracy of 83 . 43 % on the micro f1 test set compared to the previous state of - the - art embeddings .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , the accuracy of the unified model is significantly higher than the performance of any previous approach .
table 3 compares the performance of our method with and without word attention for this task . our approach outperforms both the approaches by a significant margin . the accuracy ( % ) of our approach is 62 . 6 % , which shows the effectiveness of both word attention and graph attention for the task .
the results are shown in table 3 . the performance of all models on the 1 / 1 and 1 / n test sets is reported in bold . we observe that all models perform similarly on both test sets , with the exception of the jrnn model , which performs slightly worse than the previous state - of - the - art on both sets . however , the difference between the performance of the two approaches is less pronounced on the test set for the first stage , where the dmcnn model performs slightly better than the other approaches .
table 3 presents the results for cross - event event detection . our proposed method outperforms the state - of - the - art on both event detection and event prediction . the results are presented in table 3 . as the table shows , our proposed method significantly improves upon the state of the art on event detection with respect to both event identification and classification .
the results are shown in table 3 . the results for english , spanish , french , german , dutch , russian and turkish are presented in bold . as expected , the results for all models are significantly worse than those for english - only - lm . however , fine - tuned models perform slightly better than all the other approaches except for the case of spanish - only , as shown in the table 3 . for german - only models , we see that fine - tuned - lm improves performance by 3 . 42 points on average compared to the original model .
results on the dev set and on the test set using only subsets of the code - switched data . the results are shown in table 4 . fine - tuned models outperform cs - only models with a significant margin . the fine - tuned model outperforms fine - tuned models with 75 % and 75 % train dev on both test sets .
as shown in table 5 , fine - tuned - disc improves the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set , with fine - tuned - disc improving the performance for both sets of gold sentences . the results are slightly worse for monolingual gold sentences than for code - switched gold sentences , however , the improvement is much larger on test set .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the results are shown in table 7 . the type combined approach shows significant improvements in precision and recall compared to the baseline approach .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as well as the results of type combined gaze features . our approach shows significant improvements in precision and recall compared to the baseline model .
results on belinkov2014exploring ’ s test set are shown in table 1 . syntactic - sg outperforms both lstm - pp and glove - extended on the ppa test set . the results of hpcd ( from the original paper ) are presented in table 2 . however , the results on the test set of the second paper are slightly worse than those of the first paper . we note that the difference between the performance of the two systems is due to the large difference in the type of tokens generated by the word embeddings . further , the performance gap between the two approaches is less pronounced with respect to syntactic tokens .
results are shown in table 2 . the results of our hpcd - based system outperform the best state - of - the - art lstm - pp model by a significant margin . further improving upon the performance of the hpcds with the use of ontolstm improves the accuracy of the uas by 2 . 5 points .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are presented in table 3 . as the results show , the ppa acc . accuracy improves with the removal of these two features .
the results are shown in table 2 . adding subtitle data and domain tuning for image caption translation improves the bleu % scores for both en - de and multi30k models . the results of en - fr and mscoco17 are slightly better than the results with marian amun ( marian amun et al . , 2017 ) . adding domain tuning improves the results for both models , but the improvement is less pronounced for en - fran , where the ensemble - of - 3 approach improves only by 0 . 5 bleus . further , domain tuning reduces the precision of the final image caption by 2 . 5 % . as shown in fig . 3 , when domain tuning is added to the ensemble of 3 , it improves the performance of both models by 3 . 5 points .
the results of domain - tuned h + ms - coco for en - fr and en - de are shown in table 3 . the results are presented in bold . domain - tuning improves the performance for both sets of models , but only slightly over the baseline baseline . as expected , the results for mscoco17 are slightly worse than the baseline , but still superior to the baseline on en - fr and flickr16 .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results for en - de and mscoco17 are shown in bold . adding automatic captions improves the performance for both sets of models . for en - fr , the results are slightly better than those for mscoco17 , but still significantly worse than the results of en - dde . as shown in table 4 , using only the best 5 captions of the original image improves performance for all models except for marian amun ' s ( marian amun et al . , 2017 ) .
the results in table 5 show that enc - gate and dec - gate significantly improve the bleu % scores for integrating visual information ( from 44 . 45 % to 62 . 38 % for en - de and 62 . 86 % for mscoco17 ) . however , the performance of en - fr + dec - gate is slightly worse than that of enc - de + enc - gate , as shown in fig . 3 . further improving performance by 2 . 38 points over en - deg by using the multi30k + ms - coco + subs3mlm layer improves performance by 3 . 45 points .
the results for en - fr and en - de are shown in table 3 . we observe that the performance of en - fran models outperforms those of mscoco17 by a significant margin . however , the results are only slightly better than the results obtained by en - disambiguation . the performance of subs3m [ italic ] and subs6m is slightly worse than that of the ensemble of 3 models .
the results are shown in table 3 . the performance of en - fr - ht and en - es - ht compared to the previous state - of - the - art models is presented in bold . as expected , the performance of the former is slightly better than the latter . however , the difference between the two approaches is less pronounced when we consider the number of frames in question , which indicates that the effectiveness of the transition is less than that of the original model . finally , we note that the difference in performance between the original and the back - tracked model may be due to the small size of the training corpus .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model outperforms the previous state - of - the - art model by a significant margin .
table 2 presents the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms previous models on both languages with a significant margin .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . our system outperforms the previous state - of - the - art systems in terms of bleu , ter and rnn - rev by a significant margin . the performance gap between en - fr - rnn - rev and en - es - smt - rev is narrower , but still significant .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled segmatch is the model from our second submission . we observe that the vgs model significantly outperforms the rsaimage model in terms of recall and recall @ 10 % .
results on synthetically spoken coco are shown in table 1 . our approach outperforms all the alternatives except for audio2vec - u , which obtains the best recall @ 10 % . we observe that our approach improves upon the performance of the previous approaches in terms of recall and chance by a significant margin .
we report further examples in the appendix . as shown in table 1 , the rnn turns in a screenplay that is at the edges and the edges are the most difficult for the cnn model to turn in . the dan model , on the other hand , turns the screenplay into a screenplay with only edges edges edges shapes , which is much easier for cnn to handle . finally , we report the example sentences of the different classifiers compared to the original on sst - 2 .
table 2 shows the results of fine - tuning for sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the number of occurrences for each part - of - speech ( pos ) and the percentage of words that have increased , decreased or stayed the same as a result of fine tuning . as the table 2 shows , the accuracy of the rnn improves with the addition of a few words . however , the rnp still performs significantly worse than cnn and dan .
the results are shown in table 3 . and indicate that the score increases in positive and negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
the results are shown in table 1 . the results of the best and worst performing approaches are presented in bold . our approach outperforms all the approaches except for corr ( p < 0 . 001 ) and pubmed ( p = 0 . 00089 ) .
