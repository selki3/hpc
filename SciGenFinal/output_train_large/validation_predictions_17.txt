2 shows the performance of our iterative approach on the large movie review dataset . the system performs the best on inference with efficient parallel execution of the tree nodes , and on training with a large dataset .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , due to the small size of the batch size .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . we can also see that the max pooling strategy consistently performs better in all model variations . the number of hyper parameters and the number of feature maps with the maximum number of parameters are the most important factors in the model performance . as shown in fig . 2 , the maximum pooling strategies consistently outperform all the other approaches .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that the macro - averaged model achieves the best f1 ( in 5 - fold ) without sdp dependency path .
results in table 3 show that the y - 3 model outperforms y - 2 in terms of f1 and r - f1 scores .
3 shows the paragraph level on the essay and essay level . the results are presented in table 3 . in both cases , the results are significantly better than those of [ empty ] and [ mst - parser ] on the f1 level . as expected , the accuracy of the word embeddings is significantly higher than those on the other two scenarios .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 1 . 97 and 56 . 24 ± 2 . 87 respectively compared to the majority performances of the other systems .
1 shows the results of the original and the second set of subtasks . our system performs better than the original on all but one of the subtasks we tested . the results are shown in table 1 . original and second set have completely different performance on all subtasks except for the one that is completely original . the bleu scores are significantly worse than those on the original set . these results are mostly due to the small size of the training set and the high number of errors .
statistics for the original e2e data and our cleaned version are shown in table 1 . the original and the cleaned versions are comparable in terms of number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . however , the cleaned version is slightly worse than the original .
3 shows the original and original test scores . original scores are shown in table 3 . the original scores are presented in bold , while the original ones are in italics . the difference in accuracy between original and original scores is minimal , however the difference is significant due to the fact that the tgen + model performs on par with the original and tgen − model .
results of manual error analysis of tgen on a sample of 100 instances from the original test set ( table 4 ) . we found a total of 22 errors ( 17 . 6 % ) in the original training set ( 14 . 6 % ) . these errors are caused by slight disfluencies in the training data .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous state - of - the - art models . for the external dataset , we see that all models perform better than all the other models except for seq2seqk ( konstas et al . , 2017 ) and pbmt ( pourdamghani and cohen , 2018 ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points and achieves 57 . 5 ( cao et al . , 2018 ) . note that our model size is smaller than the ensemble seq2seqb model .
3 shows the results for english - german and english - czech . the results are shown in table 3 . all the models trained on the data are in english . the best performing model is bow + gcn ( bastings et al . , 2017 ) . we observe that the single model outperforms the other models in english - language and german - language , and that the difference in performance between the two is due to the size of the data pool , not the type of features .
5 shows the effect of the number of layers inside dc on the performance of the layers in table 5 . we observe that when we add layers of layers to the dc stack , they tend to have less effect on the overall performance .
6 shows the performance of baselines with residual connections . with residual connections , we observe that the gcn with the highest gcn performance is comparable to the baseline dcgcn2 ( 27 ) . with concatenated connections , our model achieves a comparable performance to both baselines .
model f1 shows that dcgcn has the best performance on all three of the test sets . the results are shown in table 1 .
8 shows the ablation study results for the dev set of amr15 . we observe that the dense blocks have the greatest effect on the performance of the model .
show the ablation study results for the different types of encoder modules . encoder modules used in the lstm decoder outperform all the other models except for the one used in table 9 . the hierarchical clustering of the nodes in the graph encoder leads to a remarkably uniform coverage mechanism .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach obtains the best performance with a minimum of 0 . 1 f1 .
1 and 2 show the performance of our method on the subtense and threshold metrics . our method outperforms all the other methods except for the one that we use . it obtains the best performance on the threshold metrics , and achieves the highest score on both metric metrics . it is clear from the table that our method obtains a superior performance on all metrics .
1 shows the performance of our model compared to other methods . our model outperforms all the other methods except subj and mpqa except for sick - e . we observe that the cbow / 784 model performs better on all metrics except mrpc . it obtains the best performance on all three metrics , except for the mrpc metric .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp on all downstream tasks , except for sts13 , which shows the relative change with respect to hybrid .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the best performing initialization strategies are sst2 , sst5 and sts - b .
6 shows the performance for different training objectives on the unsupervised downstream tasks . we observe that cbow - r outperforms cmow - c on all three tasks except the sts13 and sts14 tasks .
1 shows the performance of our method . our method outperforms all the other methods except for cbow - r , which shows the diminishing returns from mixing the subtasks .
3 shows the performance of subj and sick - r on the mrpc test set . our model outperforms all the other baselines except for sst2 , sst5 and sts - b . we observe that all the baselines trained on the subj test set are superior in terms of mrpc score . however , the differences are less pronounced on sst - b and sst10 .
3 shows the e + and per scores of all systems trained on the same dataset . our system obtains the best results with a minimum of org , e + per and misc scores . we observe that the system performs well in all three scenarios , with the exception of the one in the [ italic ] dataset where the org scores are significantly higher than those in the [ mil - nd ] dataset . this is reflected in the fact that all the system trained on a single dataset ( except for mil - nd ) obtains an org score higher than the other two baselines .
2 shows the results on the test set under two settings . our system achieves 95 % confidence intervals of f1 scores , which shows the effectiveness of supervised learning . we observe that our system outperforms all the models except mil - nd in terms of e + p score .
6 shows the results of ref and ref compared to ref on all models except g2s - gat . ref significantly outperforms ref in all but one of the cases where ref is used .
results are shown in table 1 . the models trained on the ldc2015e86 dataset outperform all the other models except for the ones trained on ldc2017t10 . we observe that the model performs better on all metrics except the meteor metric .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models by a significant margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the bilstm model performs better than the other two models on the development set .
results are shown in table 1 . we observe that the model significantly outperforms the g2s - gin baseline in terms of sentence length and sentence length . in particular , the model has the advantage of having shorter sentences and therefore has lower error rates .
shown in table 8 , the fraction of elements missing in the output that are present in the reference sentences ( g2s - gin ) , for the test set of ldc2017t10 . it is clear from table 8 that these models do not need to rely on token lemmas to derive their output .
4 shows the performance of our approach with respect to target languages . we use the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our approach obtains the best performance with 96 % accuracy .
2 : pos and sem tagging accuracy with baselines and an upper bound . unsupemb and word2tag are the most frequently used tags in the semantic domain , and both the upper bound and the f1 score are significantly higher .
results are shown in table 1 . our system outperforms all the other methods except for the one that significantly improves accuracy . we observe that the accuracy obtained by our system is significantly better than those by the others .
5 shows the accuracy with features from different layers of the uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . note that our uni model obtains exactly the same accuracy with the same number of features as our res model .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 % higher on the training set compared to the baseline .
1 shows the accuracies when training directly towards a single task . for pan16 training , we trained directly towards the single task ( the word " sentiment " ) . the results are shown in table 1 .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced data splits . we observe that the word " task " has the most positive effect on the task , but it has the least positive effect . the word " gender " also has a positive effect , however , it does not have a negative effect .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary score is measured in δ ( difference between accuracy and leakage ) . sentiment and gender are the most important factors in predicting the performance of a given conversation , and the percentage of responses predicted to be incorrect .
6 shows the performance of different encoders when embeddings are trained on the same protected attribute . embedding is easier for both models to do , but it is harder for rnn to do it .
3 shows the performance of our lstm model compared to the previous stateof - the - art models . the results are summarized in table 3 . our model outperforms all the baselines except the wt2 baseline , and achieves a 3 . 2 bleu score . the results of our model outperform all the other models except for the one that uses finetune features . we observe that our model performs on par with the original wt2 model on a large scale .
3 shows the performance of our model compared to previous models . the results are shown in table 3 . our model obtains the best performance with a minimum of time to train . we observe that when training with lstm , the training time taken to train with it is significantly shorter than the time taken with the other models . this suggests that our model is more suitable for the task at hand .
results of experiment 1 are shown in table 1 . our model outperforms all the other models in terms of err and f1 scores . we observe that the amapolar time model significantly improves upon the original lstm model by 4 points .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models except sru and lrn in terms of decoding one sentence . we observe that the sru model performs better than the other two models in both languages .
4 shows the performance of our model on squad dataset . our model obtains the best match / f1 score with the parameter number of base and lrn . the results published by wang et al . ( 2017 ) show that the sru model significantly outperforms the lstm model in terms of f1 score .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result ( lample et al . , 2016 ) . however , it does not report the result ( f1 score ) .
performance on snli task with base + ln setting and test perplexity on ptb task with base setting setting . the results are shown in table 7 .
system retrieval and word embeddings are presented in table 1 . word embedders outperform human on all metrics except system evaluation ( mtr , rtr , mtr ) . word - based attention mechanisms ( emtr ) are particularly effective for system evaluation . sent attention mechanisms are very specific to system evaluation , word attention mechanisms such as mtr ( oral recurdings ) and word retriveval ( oversunotes ) are used to train the system ' s attention mechanisms . the word attention mechanisms [ mtr ] and word attention mechanisms ( oursunotes ] are very specialized in system evaluation and are able to handle multiple tasks at once . when word attention is applied to the system , the attention mechanisms of the word attention are markedly different from human attention .
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . our system outperforms all the automatic systems except seq2seq ( k 1000 ) .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other baselines except for the ones trained on docsub . we observe that all the baselines trained on corpus are comparable in terms of performance , with the exception of docsub , where the difference between the performance on corpus and docsub is minimal .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other baselines except for the ones trained on docsub . we observe that all the baselines that rely on word - of - speech embeddings are significantly better than the others . for example , docsub is slightly worse than docsub , but it is comparable to docsub in terms of performance and p < chen et al . , 2016 .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other baselines except for the ones trained on docsub . we observe that all the baselines trained on corpus are comparable in terms of performance , with the exception of docsub , where our model performs slightly worse than the other two .
embeddings are shown in table 1 . our system achieves the best performance with a minimum of 3 . 5 % truedepth compared to the maxdepth of europarl . we observe that our system obtains the best results with a maximum of 1 . 5depth .
embeddings are shown in table 1 . our system achieves the best performance with the maxdepth and maxdepth metrics . our model outperforms all the other baselines except for the one that embeds our maxdepth metric : europarl , maxdepth and totalroots achieve the highest score with a 1 . 1 / 1 . 1 score .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf outperforms the enhanced lfn model in terms of r0 , r2 , r3 and gsm learning .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . our model obtains the best performance ( i . e . , hidden dictionary learning ) on the three validation sets .
5 shows the performance of our models on hard and soft alignments . the results are summarized in table 5 . we observe that the wmd - prec model outperforms the hmd - f1 model on hard alignments , but it does not match the strong lemma baseline .
3 shows the performance of our approach on the direct assessment task . our approach outperforms all the baselines except for the ones that do not have bertscore - f1 scores .
3 presents the bleu - 2 and sfhotel scores on the test set . the baselines are set at 0 . 25 and 0 . 111 respectively , which significantly outperform the baselines on all metrics except bertscore - f1 and meteor .
3 presents the metric and baseline scores of all models trained on the m2 dataset . the baselines are summarized in table 3 . the summaries are presented in bold , with the exception of those trained on w2v . leic - recall ( p < 0 . 001 ) and spice ( p > 0 . 005 ) achieving the best score on both metrics . however , the summaries obtained by spice are significantly less accurate than those obtained by word - mover , indicating that the reliance on elmo and bert scores may have a negative effect on performance .
results are shown in table 3 . we observe that the m0 model performs better than the m1 model on all metrics except for the one that relies on word embeddings . as expected , the performance of m0 models is significantly worse than those of m1 and m2 .
results are shown in table 3 . semantic preservation and transfer quality are the most important aspects of the semantic preservation and semantic preservation tasks . the clustering performance of all the models is significantly better than those of the other baselines , indicating that semantic preservation has a high correlation with semantic preservation . syntactic preservation is the most difficult part of semantic preservation , but it is easier to achieve with a single set of features than with multiple sets of features .
5 shows the results of human validation . the results are shown in table 5 . we observe that the accuracy obtained by using these metrics is significantly higher than the human ratings of fluency , indicating that the system is able to match sentence quality .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in terms of word embeddings . in fact , it even outperforms both the m2 and m3 model by a margin of 2 - 3 points .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu ( acc ∗ ) than those using simple - transfer and unsupervised embeddings . however , their performance is slightly worse than those of previous work using the same classifiers . we also observe that the use of multiple classifiers in the same sentence leads to a slightly worse acc ∗ score than using only one classifier .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be disfluent . the percentage of tokens that are correctly predicted to have a number of repetition tokens is 8 - 8 .
3 shows the percentage of tokens correctly predicted to contain a content word in both the reparandum and the repair ( table 3 ) . the fraction of tokens that contain the content word is shown in parentheses , indicating that the disfluencies in the reparandum are caused by the loss of the function .
results are shown in table 1 . we observe that the best models using text + innovations outperform the single model in terms of dev and rewards . in addition , we observe that single model has the advantage of having more innovations in the development of the model .
performance on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with the state - of - art algorithms on the test dataset . we observe that our model has the highest accuracy and the highest f1 score .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of our method in terms of word attention and graph attention . we report the accuracy ( % ) of our model with and without attention . the results show the effectiveness of our approach .
1 and 2 shows the performance of the models trained on the same training set . we observe that the jvmee model outperforms all the other models except for the one that relies on the trigger feature .
3 shows the identification and classification results for all stages of the event . our method outperforms all the methods except the one that we use for the event classification . we observe that the method performs well in both scenarios , with the exception of the one in the event of a non - triviality case . in both cases , the system is able to distinguish between the events without a significant difference in the identification rate . the method has the advantage of having a relatively high number of trained participants .
results are shown in table 1 . all but fine - tuned - lm models outperform all the other models except for the ones that do not use the word " attention " .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev with only subsets of the code - switched data . this shows that fine - tuning can improve the train dev performance by a significant margin .
5 shows the performance of our system on the dev set and the test set , compared to monolingual and code - switched systems . the results are shown in table 5 . we observe that our system performs slightly better than the mono model on both sets .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 scores for the three eye - tracking datasets tested .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . note the significant drop in precision ( from 94 . 38 to 94 . 35 ) in the f1 score for type combined gaze features compared to the baseline .
results on belinkov2014exploring ’ s ppa test set . the hpcd system uses syntactic - sg embeddings as the base layer for wordnet and wordnet 3 . 1 , and it uses semantic skipgram as the embedding layer . the results on the original paper are shown in table 1 . it relies on syntactic sg embedding to maintain the semantic embedding layers of wordnet , and glove - retro is used to train wordnet with syntactic features .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 shows the performance of our model with domain - tuned captions . our model outperforms all the en - de models except for the ones without domain tuning . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) shows that our model performs better than the other models .
3 shows the performance of subs1m models in en - de and en de . the models trained on the flickr16 and mscoco17 datasets are significantly better than those trained on subs1ms . subdomain - tuned models outperform all the other models except for the ones trained on flickr17 .
4 shows bleu scores in terms of the automatic captions added to the flickr16 and mscoco17 datasets . the results are shown in table 4 . we noticed that the multi30k model outperforms all the other models except for the one that has the best number of captions . this is due to the fact that the model requires fewer image captions to perform .
5 compares the performance of our approaches with previous approaches . we observe that enc - gate and dec - gate have the most positive effect ( bleu % scores ) on the visual information ( table 5 ) . in particular , we observe that the img w embeddings significantly outperform the en - de approach ,
1 shows the performance of subs3m on the en - de and flickr16 datasets compared to subs6m on both datasets . we observe that the embeddings alone do not improve performance , but do improve over the single - lingual model . sub - coco and mscoco17 datasets have the advantage of having multiple layers of 3m in the same dataset , which helps to reduce noise .
3 shows the performance of our system compared to other en - fr - based systems . we observe that our approach outperforms the best state - of - the - art systems on all metrics except tftr and ltr .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 3 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) are significantly better than ter , indicating that the system is well - equipped to handle multiple scenarios .
results on flickr8k are shown in table 2 . the vgs model achieves the best performance with a minimum of 0 . 2 recall .
results on synthetically spoken coco are shown in table 1 . the model trained on the embeddings of chrupala2017representations is comparable to the one trained on audio2vec - u . however , the difference is less pronounced .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig < c > turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . < r > rnn ( rnn ) turns in the same screenplay as it does in the original . rnn also shows the benefits of edges edges edges . it is so clever to hate hate it and the other classifiers as well . we report further examples in table 1 .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . the most striking ones are the ones that have increased , decreased or stayed the same in the number of occurrences compared to the original sentence . these results indicate that fine - tuning has not impacted the quality of the sentence .
shown in table 3 , the sentiment changes in sst - 2 when the negative labels are flipped to positive . this shows that the sentiment increases with the change in sentiment with respect to the original sentence .
results are presented in table 1 . we observe that the competitive nature of word embeddings ( pmi ) is relatively high ( 98 % vs 98 % ) . however , it is difficult to distinguish between positive and negative states ( 98 % , vs 98 % ) due to the large size of the corpus ( 98 % ) .
