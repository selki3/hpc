3 shows the performance of our recursive framework on training , with the large movie review dataset showing the best performance on training .
3 shows the performance improvement of the balanced dataset when the batch size increases from 1 to 25 .
4 - 5 shows the performance of the max pooling strategy for each model with different representation . we show the results for all model variations .
3 shows the effect of using the shortest dependency path on each relation type . we show the results of our method on the best f1 ( in 5 - fold ) without sdp .
results are shown in table 1 . y - 3 : y , y , y and y are significantly better than y - 2 .
results are shown in table 1 . we show that the results of the mst - parser are comparable to those of our test . we observe that our test results are significantly better than those of the test results .
4 shows the performance of the two indicated systems over the runs . the performance of both systems is significantly lower than that of the other two systems . our results are shown in table 4 .
results are presented in table 1 . we show the results of the original and the original . the results are shown in table 2 . we observe that the original is more accurate than the original , and that the error is more pronounced than the error .
results are shown in table 1 . we compare the original e2e data with our cleaned version . we show that the original and the cleaned versions are comparable in performance .
results are presented in table 1 . we show the results of the original and tgen models . we show that the original model outperforms the tgen model by a significant margin .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . the results are shown in table 4 . we find that tgen has a large number of errors , and that the errors are small .
results are presented in table 1 . all models outperform all models . we observe that all models perform better than all models except for the ones that do not perform well . our model outperforms all models by 0 . 2 % and 0 . 1 % respectively .
results on amr17 are presented in table 2 . we observe that the model size is significantly smaller than that of ggnn2seqb ( 2016 ) . we also observe that our model size has a significant impact on the performance of our model . our model size also has a large impact on performance .
results are presented in table 1 . we show that our model outperforms our previous model by a significant margin . the results are shown in table 2 . our models outperform our previous models by a large margin .
table 5 shows the effect of the number of layers inside dc on the performance of the layers . we observe that dc has a significant effect on the quality of layers in dc .
results are shown in table 6 . we observe that gcns with residual connections are more likely to have residual connections than gcns without residual connections . however , we observe that residual connections have a significant effect on gcn performance .
results are shown in table 2 . we observe that the dcgcn model outperforms our model by a significant margin . we also observe that our model performs better than our model in terms of performance .
table 8 shows the results of the ablation study on amr15 . we show that the ablation study shows that the density of the dense blocks in the i - th block is significantly lower than the dev set . we also observe that the embedding of dense blocks is significantly higher than the other dense blocks . our results show that embedding dense blocks improves the performance .
3 shows the results of the ablation study for the graph encoder and the lstm decoder . we show that the ablation study outperforms the previous model by a significant margin .
results are shown in table 7 . we show that our initialization strategies outperform our probing strategies on probing tasks .
results are presented in table 1 . we show the results of our method in table 2 . the results are shown in table 3 .
results are presented in table 2 . we show that our method outperforms the other methods in terms of performance . our model outperforms all other methods except for our model , which outperforms both our model and our model . the results of our model outperform the other two methods .
results on unsupervised downstream tasks are shown in table 3 . we show the relative change with respect to hybrid and cbow . our model outperforms the hybrid model .
3 shows the performance of our initialization strategies on supervised downstream tasks . we show that the initialization strategies outperform the original ones . our model outperforms the original one .
results are presented in table 6 . we show that cmow - r outperforms cbow - c on the unsupervised downstream tasks . we also observe that cbow performs better than cbow on the supervised downstream tasks , as shown in figure 6 .
results are presented in table 2 . we show the results of the cbow - r method . the results of our method are shown in table 1 .
results are presented in table 1 . we show the results of the cmow - c and cbow - r models . the results are shown in table 2 . our model outperforms all the other models except for our model .
results are presented in table 1 . we show the results of our system in table 2 . the results are shown in table 3 . our system outperforms all the other systems in the table . we also show that our system performs better than all the systems except mil - nd , which outperforms our system .
results on the test set under two settings are shown in table 2 . we show the performance of our system on the two settings . our system performs better than the system on both settings .
results are shown in table 6 . our model outperforms our model by a significant margin . the results show that our model performs better than our model . we also observe that our models outperform our model in terms of performance .
results are presented in table 1 . we show that the model outperforms the model in terms of performance . the model outperformed the model by a significant margin . we also observe that the models outperform the model on both the model and the model .
results on ldc2015e86 test set are shown in table 3 . our model outperforms all the other models in the test set . we also observe that the model performs better than the previous model .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the ablation results are comparable to that of the previous ablation studies .
results are shown in table 1 . our model outperforms the previous model by a significant margin . we observe that the model performs better than the other models by a large margin . our model performs worse than the previous models .
results are shown in table 8 . our model outperforms the reference sentences by a significant margin . we compare the model with the reference sentence . we also compare the output with reference sentences .
3 shows the accuracy of the nmt encoding layer on a smaller parallel corpus ( 200k sentences ) .
results are shown in table 2 . we show the results of our classifier , word2tag , with the highest accuracy with baselines .
results show that pos tagging accuracy outperforms the other pos - tagging accuracy models . the results are shown in table 1 .
results are presented in table 5 . we show that the accuracy of the uni and res encoders improves over non - english targets .
table 8 shows the performance of the attacker on different datasets . we show the difference between the attacker score and the corresponding adversary ’ s accuracy .
table 1 shows the results of training directly towards a single task . we show the performance of the task when trained directly towards it . the results are shown in table 1 .
3 shows the results of the protected attribute leakage experiments . the results are shown in table 2 . we observe that the results are statistically significant . the results show that the data splits are significantly smaller than expected .
3 shows the performance of the adversarial training on different datasets with different datasets . we show the difference between the performance on each dataset with a trained training and a trained adversary .
3 shows the performance of the protected attribute with different encoders . 4 shows the results of the embedding guarded and embedded attribute .
results are presented in table 2 . this model outperforms the previous model by a significant margin . we show that this model outperform the previous models by a large margin .
results are presented in table 5 . our model outperforms the previous model by a significant margin . we show that our model outperform the previous models by a large margin .
results are presented in table 2 . we show the results of our model in table 1 . we show that our model outperforms our model by a significant margin . the results of the model outperform our model .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms the previous model by 0 . 99 % and 0 . 01 % respectively . we also observe that our model performs better than our previous model .
results published by wang et al . ( 2017 ) show that the model performs better than the model on squad dataset . our model outperforms the model in terms of the parameter number . we observe that the models outperform the model by a significant margin .
results on conll - 2003 english ner task are shown in table 6 . we observe that the parameter number of lstm is significantly higher than the f1 score of lrn . our model outperforms lrn by a factor of 0 . 01 .
results are shown in table 7 . snli task with base + ln setting outperforms ptb task with base setting . our model performs better than ptb with base setting .
results are shown in table 2 . we show that our system retrieval model outperforms our system retrieval models . we observe that the word word is more effective than the word . the word word improves the performance of our system . it improves the overall performance of the system , as well as the overall word performance .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . our results are shown in table 4 . we show that human evaluation outperforms human evaluation . the best performance for human evaluation is shown in the table .
results are shown in table 1 . we show that the results of the ted talks dataset outperform those of the other two datasets . our results show that ted talks outperforms the two datasets in terms of performance .
results are shown in table 1 . we show that the results of the ted talks dataset are comparable to the results from the corpus dataset . the results of our test dataset are similar to that of corpus . our results show that ted talks outperforms both corpus and corpus .
results are shown in table 1 . we observe that the results of the ted talks dataset outperform those of the other two datasets . our results show that ted talks outperforms the two datasets in terms of performance .
results are shown in table 1 . the results are presented in table 2 . table 2 shows the results of our model . we show that our model outperforms our model by a significant margin . the results of the model outperform the performance of the other models .
results are shown in table 1 . our results show that our results are significantly better than our previous results . we show that the results of our model outperform our previous ones . the results are consistent with our model .
results are presented in table 1 . the results of the experiments are shown in the table 1 . our model performs better than the original visdial v1 . 0 .
3 shows the performance of the ablative studies on the visdial v1 . 0 validation set . the results are shown in table 2 . we observe that p2 outperforms p1 and p2 . the performance of p2 is comparable to p1 .
table 5 shows the performance of hmd - prec and wmd - prec on hard alignments and soft alignments . we see that the performance is comparable to that of wmd .
results are shown in table 1 . we show the results of the bertscore - f1 test . the results of our test are presented in table 2 . our model outperforms all the other test - based models . we observe that the results are better than those of the other models .
results are presented in table 1 . we show the performance of the bleu - 1 and sfhotel - 2 baselines . we observe that the baselines are significantly better than the baseline . our model outperforms the baseline by a significant margin .
results are presented in table 2 . we show the results of the bertscore - recall test . the results are shown in table 1 . our model outperforms our model by 0 . 939 points .
results are presented in table 1 . we show that the m0 model outperforms the m1 model by a significant margin . we also show that m0 outperforms m1 , m2 , and m3 by a large margin .
results are presented in table 1 . we show the results of our model with the best performance . our model outperforms all the other models . the results of the model outperform all other models except for the one with the worst performance .
3 shows the results of human sentence - level validation . the results are shown in table 5 . our results show that human sentence level validation outperforms machine and human sentences . we also observe that human sentences perform better than machine sentences .
results are presented in table 1 . we show that the m1 model outperforms the m2 model by a significant margin . we also show that m1 outperforms m2 , m2 outperforms both m2 and m2 by a large margin .
results on yelp sentiment transfer are shown in table 6 . our best models achieve the highest bleu than our best models . we also show that our best model outperforms our best ones by a significant margin . we show that the best models outperform the best ones .
3 shows the results of nested disfluencies . the results are shown in table 2 . reparandum tokens that were correctly predicted as disfluent . we observe that the repetition tokens are more accurate than the disfluency tokens .
3 shows the relative frequency of disfluent rephrases correctly predicted for disfluencies that contain a content word in both the reparandum and the repair ( content - function ) .
results are presented in table 1 . we show that our model outperforms the previous model by a significant margin . the results are shown in table 2 . we observe that the models outperform the previous models by a large margin .
3 shows the performance of word2vec embedding on the fnc - 1 test dataset . we show that our model performs better than the state - of - art models . our model outperforms the state of theart models on both fnc1 test datasets .
3 shows the performance of the unified model on the apw and nyt datasets . the unified model outperforms all previous models .
3 shows the effectiveness of word attention and graph attention for word attention compared to word attention . we show that word attention improves word attention performance compared to graph attention .
results are presented in table 1 . we show that the model performs better than the previous model . we also show that our model outperforms all the other models .
results are presented in table 1 . we show that the method outperforms the previous method by a significant margin . in table 1 , we show that we outperform the previous methods by a large margin .
results are shown in table 1 . all models have the same results , except for the spanish - only - lm .
results on the test set are shown in table 4 . we observe that the train dev set is significantly smaller than the train test set . we also observe that train dev is significantly larger than train dev .
results are shown in table 5 . the performance on the dev set and the test set is similar to that of the monolingual set . we observe that the performance of both sets is significantly better than that of mono .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( table 7 ) . precision and recall ( p ) are statistically significant improvements over the previous two datasets .
results are shown in table 5 . precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) .
results on belinkov2014exploring ’ s ppa test set are presented in table 1 . we show the results of the hpcd ( full ) and glove - retro ( full ) . we also show the performance of the lstm - pp test set on wordnet .
results from rbg are presented in table 2 . we show that rbg performs better than oracle pp and oracle pp . our model outperforms oracle pp by a significant margin .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) .
results are presented in table 2 . we see that the domain tuning results are significantly better than the domain - tuned domain tuning .
results are shown in table 1 . we observe that subs1m outperforms subdomain - tuned models in terms of performance . our results are consistent with the performance of subdomain tuned models . subdomain tuning outperforms the subdomain tuning models .
3 shows the bleu scores for the automatic image captions . the results are shown in table 4 .
results are presented in table 5 . we compare the performance of the two strategies for integrating visual information ( e . g . , enc - gate and dec - gate ) . we show that the two approaches outperform each other in terms of performance . we also show that enc - gates outperforms dec - gate . our model outperforms the other strategies by a significant margin .
results are shown in table 2 . we observe that subs3m is more likely to be text - only than non - text - only . we also observe that it is less likely to outperform non - verbal features . however , we observe that the non - visual features outperform the verbal features , which we observe .
results are presented in table 1 . we show that the en - fr - rnn - ff model outperforms the enfr - smt - ff dataset . the en - ffr - ff system outperforms both en - es - ht and en - fl - ff , respectively .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs .
3 shows the results of training vocabularies for the english , french and spanish data .
3 shows the results of automatic evaluation scores for rev systems . the automatic evaluation scores are shown in table 5 . we show the results for the rev system . we observe that the automatic evaluation score is significantly better than the automatic one .
results on flickr8k are presented in table 2 . we show that the vgs model outperforms the segmatch model by a significant margin .
results are presented in table 1 . we show that the performance of the audio2vec - u model is significantly better than that of the other two models . our model outperforms all the other models in terms of performance .
3 shows the results of the different classifiers compared to the original on sst - 2 . we report the results in table 1 . in the original , we see that the edges of the screenplay are so clever that it ’ s so clever you want to hate it .
3 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 .
3 shows the results of the sst - 2 experiment . the results are shown in table 3 . we show that the results are statistically significant . the results indicate that the score is significantly higher than the original sentence .
results are presented in table 1 . we observe that the performance of the sift - 2 model is significantly better than the sst - 2 models . our results are consistent with the results of our previous work . the results of this work are similar to those of the previous work , however , the results are significantly worse than the previous ones .
