2 shows the performance of our recursive approach on the large movie review dataset compared to our iterative approach . the results are shown in table 2 . as expected , the recursive approach performs the best on training , while the iteration approach shows better performance on inference . further , the use of gpu exploitation improves the performance on both training and inference .
results in table 1 show that the balanced dataset exhibits the highest throughput , but it does not improve as well as the linear dataset .
2 presents the results for each model with different representation . we show that the max pooling strategy consistently performs better than the other two approaches . the hgn outperforms both the hgn and sigmoid models in all three scenarios . hgn achieves the best performance with the maximum number of parameters and the number of feature maps .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp and macro - adapted models . the results are shown in table 1 . we observe that when using sdp as dependency path , the f1 is significantly lower than when using only sdp . however , this is only because sdp is used for dependency path generation .
results are presented in table 3 . we observe that y - 3 significantly outperforms y - 2 in terms of f1 and f1 score . the results are shown in bold .
3 presents the results of our method on the paragraph level and the essay level . the results are presented in table 3 . the results show that mst - parser outperforms all the other methods in terms of both word level and sentence level . as expected , the results are significantly better than those obtained using the f1 feature alone .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser , it is 60 . 62 ± 3 . 54 and 69 . 24 ± 2 . 87 respectively , compared to the majority performances of the other systems .
3 shows the performance of the original and the second set of test sets . the results are presented in table 3 . the original and second set are shown in bold . the two sets of test set have completely different performance on each of the three sets . original is better than the other two . the bleu and nist test sets are both significantly worse than the original set . the second set , sc - lstm , is much worse . it is clear from the results that the two sets are not related .
1 compares our original and our cleaned e2e datasets with the original ones . the results are shown in table 1 . the original and the cleaned versions have the highest number of distinct mrs as measured by our slot matching script , see section 3 . the cleaned version has the highest percentage of mrs and the highest ser .
results are shown in table 1 . original and original test sets are presented in bold . the best performing system is sc - lstm , which is based on the original tgen + dataset . it is clear from table 1 that the original features are superior to the ones derived from the original . this is reflected in the fact that both the original and the new features are completely different from the ones used in the original dataset .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found that adding errors caused a significant drop in the absolute number of errors . we also found that removing errors caused slight disfluencies in the training set .
3 shows the performance of our approach on the external and external datasets compared to the previous state - of - the - art models . the results are presented in table 3 . all models perform better than all the other models except for the one that relies on a single layer . table 3 shows that all the models that rely on the single layer are comparable in performance to the other two .
2 presents the results on amr17 . our model achieves a bleu score of 25 . 5 and achieves a score of 27 . 5 . the results are shown in table 2 .
3 shows the results for english - german and english - czech . the results are presented in table 3 . the results show that the single - language model outperforms the two - language models in english - language and german - language . as expected , the results are in the low - single - language setting , which means that the difference in performance between the two languages is minimal . we also observe that the language - specific bow + gcn model is more representative of english - speaking languages , with the exception of german , where it appears to be more representative . it is clear from the results that the multi - language approach is beneficial for both languages .
5 shows the effect of the number of layers inside dc on the performance of the system . table 5 shows that when we add layers of layers , we get a reduction of 1 layer from 3 to 3 layers . we observe that this effect is less pronounced for the " italic " and " m " layers .
6 shows the performance of gcns with residual connections . rc + la ( 2 ) and gcn + rc ( 4 ) show that the residual connections are beneficial , improving gcn performance by 2 . 5 points .
3 shows the performance of our dcgcn model compared to the previous state - of - the - art models . the results are presented in table 3 . we observe that dcgcns achieves the best performance on all three metrics , with the exception of the bias metric .
8 shows the ablation study results for amr15 . the results show that removing the dense connections in the i - th block leads to a better performance . table 8 shows that the reduction in the number of dense connections leads to better performance for the model .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . encoder modules used in the production encoder outperform the single - class model in terms of coverage . the two - class dgcn4 encoder has the advantage of having a single class of nodes in the encoder .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach outperforms the state - of - the - art method on all three tasks . it improves the performance by 3 . 5 points over the gold - based method . it also outperforms glorot and subjnum by 3 points .
3 presents the results of our method . the results are presented in table 3 . our method outperforms all the other methods except for the one that we use . it obtains the best performance on all three metrics . it achieves the best results on all metrics except for subjnum and coordinv , which show the performance of the h - cmow model on the subtense and subtense metrics . its performance is comparable to that of the other two methods .
3 presents the results of our method on the subj and sick - r datasets . the results are presented in table 3 . the subj model outperforms all the other models except for the one that cmp relies on . subj outperforms both the mrpc and mpqa datasets in terms of mrpc performance . it is clear that subj relies on selective attention and selective attention . this suggests that selective attention is required to select the best selective attention targets . this underscores the importance of selective attention , as the selective attention to selective attention leads to a better understanding of the target target . subj achieves the best results on both datasets .
3 shows the relative change from cmp to cmp on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both cmp and cmp in all downstream tasks , with the exception of sts14 , where it performs better .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms the state - of - the - art approaches on all three tasks . it also outperforms all the baselines except sst2 , sst5 and mpqa . it is clear from table 8 that the combination of initialization strategies improves the performance for both baselines .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . the cbow - r model outperforms all the other models except for sts13 , which shows that the training objectives are more beneficial for the downstream task .
3 presents the results of our method . the results are presented in table 3 . the topconst method outperforms all the baselines except for cbow - r , which obtains the best results . it also outperforms the other baselines , such as topconst and subjnum . it is clear from table 3 that the best performing baselines are the bshift and subjnum methods . these results show that bshift outperforms both the previous baselines .
subj and sick - r outperform all the other methods except subj . subj has the best performance on both mrpc and mpqa datasets , and is comparable to both sst2 and sst5 in terms of mrpc performance . it is clear that subj is better than sst1 on all three metrics , but it is superior on the mrpc dataset . this confirms the effectiveness of subj , which is based on the best state - of - the - art mrpc implementation .
3 shows the e + and per scores for all systems tested . our system outperforms all the baselines except for the one in [ italic ] where it obtains the best e + org score and the other baselines that do not . the results are presented in table 3 . we observe that the combination of domain - aware learning ( mil - nd , mil - nd ) and other supervised learning systems significantly improves the e − org scores and the per scores of all baselines tested . the results show that the combined org and misc scores are superior when combined with the number of baselines in the system .
2 shows the results on the test set under two settings . our system outperforms all the models except mil - nd , which is better at e + p and f1 score . the results are shown in table 2 . we observe that the system performs well in all settings , with 95 % confidence intervals of f1 scores . it also outperforms the previous state - of - the - art model in all three settings .
6 shows the results for all models except for those that do not use ref . the results are presented in table 6 . ref and ref significantly outperform ref in all cases except for g2s - gat , which does not have ref or ref - based features .
results are shown in table 1 . the results are presented in the tables 1 and 2 . table 1 shows that all models trained on the ldc2015e86 dataset outperform all the other models except g2s - gat , which is comparable in terms of performance .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous state - of - the - art model on all three metrics except for the one that is pre - trained with the additional data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that bilstm significantly improves the performance of the model on the development set by 2 . 5x compared to the baseline .
results are shown in table 4 . the results are presented in bold . we observe that the g2s - gin model outperforms all the other models in terms of terms of sentence length and sentence length . in particular , we observe that when the sentence length is increased , the average number of sentences is lower , indicating that the model is better at predicting sentences .
shown in table 8 , the fraction of elements missing in the output that are present in the input ( g2s - gin ) is lower than that in the standard ldc2017t10 . this is reflected in the fraction fraction metric , which shows that the use of token lemmas has a positive effect on the output .
4 shows the performance of our approach with respect to target languages . our approach obtains the best performance on a smaller parallel corpus ( 200k sentences ) . our approach outperforms the previous state - of - the - art approach using features extracted from the 4th nmt encoding layer .
2 shows the accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag has the highest pos score with 87 . 06 % and 91 . 55 % accuracies , respectively , compared to the previous state of the art , word3tag has 87 . 53 % and 90 . 55 % .
results are presented in table 4 . we observe that the accuracy obtained by our method outperforms all the other methods except for those that rely on the word embeddings . table 4 shows that our method significantly outperforms the competition in terms of accuracy and precision .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . our uni model outperforms both res and bi in terms of accuracy .
8 shows the performance of an attacker on different datasets . the performance of the trained adversary is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is statistically significant .
1 shows the performance of our system when training directly towards a single task . the results are shown in table 1 . sentiment and gender are the most important factors in predicting whether a given task will receive a reward or not . the performance of pan16 is significantly better than that of the other two systems .
2 shows the effect of the word " race " on the balanced and unbalanced task averages . it is clear from table 2 that word " gender " and " aggregation " have a negative effect on the task performance , leading to a drop in multi - task performance . the classifier pan16 is able to detect instances of gender - based classifiers as well as classifiers .
3 shows the performance on different datasets with an adversarial training . the performance on pan16 is the difference between the attacker score and the corresponding adversary ’ s accuracy . in pan16 , the performance is significantly worse than that of the trained classifier .
6 shows the performance of the embeddings for different encoders . embedding rnn with a different encoder helps the model distinguish between the two instances .
3 presents the results of our second study . the results are presented in table 3 . the results show that lstm outperforms all models except for the one that relies on finetune features . it is clear from the results that this approach is superior to the other approaches that rely on feature - rich neural network architecture . we observe that the performance gain from using finetuned neural network architectures is modest but significant . however , it does improve upon the performance of the other models by a significant margin .
results are shown in table 5 . the results show that our approach outperforms previous approaches in terms of both training time and training time . it is clear from table 5 that the lstm model is better at training time than the other approaches . as expected , the performance of this model is significantly better than the previous ones . when training time is used , the time taken to train time is significantly less than that of the previous model . we also observe that the performance gain from training time on the same dataset is small but significant .
results are shown in table 4 . we observe that our approach improves upon the state - of - the - art amapolar time model by a significant margin . the results show that it obtains a better performance on both datasets when compared to the original lstm model . however , it does not improve on the amafull time model , which shows that it is better at predicting the future events .
3 shows the bleu score on wmt14 english - german translation task compared to the previous state - of - the - art system on tesla p100 . it also shows the performance of the system when trained with only tokenized bleus . as expected , the sru model outperforms all the other models except for gru , which is more stable . the results are shown in table 3 . sru and olrn show significant performance improvement compared to previous state of the art systems .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our approach significantly improves match / f1 score over the baselines of lstm and sru . as expected , our model performs better than all the models except lrn and atr . we observe that the sru model outperforms all the other models except for the ones that do not use it .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number and sru shows the reported result . it is clear from table 6 that the use of # params leads to better performance . the performance of lrn is comparable with that of gru . however , the performance of sru is slightly worse than that of lslstm . sru also exhibits lower performance .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
3 shows the results of word - based system evaluation . word - based systems outperform system retrieval and mtr , word based system evaluation ( mtr ) outperforms both system and word based systems in terms of performance . word based system evaluations ( r - 2 ) yield significantly better results than word based methods . the word based approach ( rtr ) achieves the best results with a b - 2 score and a r - 4 score . using wordbased systems , wordbased system evaluations achieve the best performance with a minimum of performance loss . as expected , word based methodologies are beneficial for both systems , with the exception of wordbased methods , which do not perform well in combination with other methods .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the results are highlighted in bold , with statistical significance marked with ∗ ( p < 0 . 0005 ) . as expected , the performance of all the systems shown in table 4 is significantly better than that of seq2seq .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . our model outperforms all the other models except for the two that we tested , namely , europarl and ted talks . the performance of our model is significantly better than those of the other two models . we observe that our model significantly outperforms the competition on all three test sets , with the exception of docsub , where it performs slightly worse .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . our model outperforms all the other models except for the one that we tested , namely , europarl , df , docsub and docsub . we observe that the results are slightly better than those of the other two models .
3 shows the performance of all the models tested on the test set . the results are presented in table 3 . our model outperforms all the other models except for the one that we tested , namely , europarl . it also outperforms both the df and docsub datasets by a significant margin . it is clear from the results that the two models are better than the others on both datasets .
3 presents the results of our joint study . our joint study shows that our joint approach achieves the best results on all metrics . our approach outperforms all the baselines except for the two that we tested , namely , docsub and totalroots . the results are presented in table 3 . we observe that our approach obtains the best performance on all three metrics , with the exception of docsub , which achieves the worst performance .
3 presents the results of our approach . our approach outperforms all the baselines except for the one that is used in corpus ( which is europarl ) . our approach obtains the best results with a minimum of 3 . 5roots and achieves the best result with a maxdepth of 9 . 43 . the results are presented in table 3 . we observe that our approach achieves the highest performance with a maximum depth of 10 . 43 and a maximumdepth of 1 . 71 .
1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version , and r1 , r2 and r3 denote hidden dictionary learning , respectively . it is clear from table 1 that the enhanced lfn outperforms the original lfn model in terms of r1 and r2 metrics .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the best performing model is lf , which is comparable to coatt and coatt in terms of rva performance .
5 presents the results on hard and soft alignments . the results are presented in table 5 . the hmd - prec model outperforms all the other models except for the one that relies on bert . it is clear from table 5 that bert has a significant impact on the performance of both models .
3 presents the results of the direct assessment and bertscore - f1 tests . the results are presented in table 3 . the performance of the baselines is summarized in terms of ruse ( * ) and ruse ( * ) compared to meteor ( * ) on three of the four test sets . for the two test sets , the results are summarized in table 1 . the summaries are presented as follows : the summaries of all the test sets are summarized as follows ( table 1 ) . for the one of the two baselines , we show the results obtained using ruse - mover ( which relies on word embeddings ) as the baseline . these results show that the effectiveness of the ruse model can be further improved with the use of sentence embedding .
3 presents the bagel and sfhotel scores on the test set . the results are presented in table 3 . the bertscore - f1 scores are significantly better than those of bleu - 1 and smd , indicating that the baselines are properly trained on both sets of data .
3 presents the metric and baseline scores of the models trained on the baselines . the results are presented in table 3 . the metric scores are summarized in bold . leic and spice show that the combination of elmo and bertscore - recall significantly improve the performance for all models . spice and leic show that both baselines significantly outperform the baseline on both metrics . word - mover achieves the best performance on both sets .
results are shown in table 4 . we observe that the m0 model outperforms the m1 model by a significant margin . the results show that it is better to rely on plain word embeddings than on syntactic or semantic cues . as expected , the performance of m0 models underlined by the presence of syntactic cues is relatively low .
results are presented in table 4 . we observe that the semantic preservation approach significantly improves the transfer quality over the syntactic preservation approach . the results show that semantic preservation is beneficial for both semantic and semantic preservation . semantic preservation outperforms semantic preservation on both datasets , with the exception of the case of semantic preservation , which is more difficult to solve . syntactic preservation improves over semantic preservation by 3 . 5 points . finally , semantic preservation improves with the addition of syntactic features .
5 shows the results of human and machine validation . the results are shown in table 5 . the results show that both sim and human ratings of semantic preservation are good ( p < 0 . 001 ) and high ( p > 0 . 01 ) on average , indicating that the system is able to match sentence quality with human ratings .
results are shown in table 4 . we observe that the m0 model outperforms the m1 model by a significant margin . the results show that it is better to rely on plain word embeddings than on syntactic or semantic cues . as expected , the performance of m0 models underlined by the presence of syntactic cues such as shen - 1 and cn + 2d is low .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ score . our worst model , yang2018unsupervised , achieves the lowest acc score , but outperforms all the other models except for fu - 1 . multi - decoder achieves the best acc score and is comparable to the best state - of - the - art model . we also observe that the use of classifiers in the sentiment transfer setup ( e . g . delete / retrieve ) is less effective than using classifiers . the performance of the best model is also comparable to that of previous work , but the difference is less pronounced . finally , we observe that using a single classifier in sentiment transfer can improve the acc score by a significant margin .
2 shows the number of instances that were correctly predicted as disfluencies . the number of tokens that were predicted to be disfluent is reported in table 2 . reparandum length is shown in the table 2 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in table 3 . it is clear from the table that the disfluencies predicted as the content word have a significant impact on the prediction accuracy .
results are shown in table 4 . the results are presented in bold . text + innovations significantly improve the model ' s performance over both single and multi - factor models . in addition , text + innovations improves the model ’ s performance by 0 . 2 point over the previous state of the art model . table 4 shows the results of our model with respect to both features . we empirically show that the use of both text and innovations significantly boosts the model performance .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art word2vec embeddings . our model achieves the best performance with a low f1 score . it also outperforms the rnn - based model in microf1 score by a significant margin .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous methods except burstysimdater .
3 shows the performance of our method with and without attention . the results show that it is possible to improve upon the performance by using word attention and graph attention for this task .
3 shows the performance of all models trained on the same domain . our model outperforms all the other models except for the one that trained on jvmee . the results are presented in table 3 . the results show that the combination of pre - trained and unsupervised models significantly improves the performance for all models .
3 shows the performance of our method on the single event . our method outperforms all the other methods in terms of both event identification and event classification . we show that the method significantly improves the predictive performance for both event and event identification . in event identification , the method obtains a significant advantage over the traditional method due to its high precision .
results are shown in table 4 . all models except for those that use spectrained - lm outperform all the other models in terms of performance . the results show that all models are comparable in performance when trained on the same single - domain dataset .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . we observe that fine - tuning achieves the best results with a 50 % train dev and 75 % train test score .
5 shows the performance on the dev set compared to the monolingual set . the results are shown in table 5 . we observe that fine - tuned - disc outperforms the gold sentence - switched model on both the dev and test set .
7 shows the performance of type - aggregated gaze features trained on the three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show statistically significant improvement over the baseline baseline . the results are shown in table 7 .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the baseline , as shown in table 5 .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach relies on syntactic - sg embeddings , and it uses syntactic modeling to embed synset embedding . glove - retro is based on the original wordnet and wordnet schemas , and is used in wordnet 3 . 1 . syntactic embedding gives the advantage of syntactic semantic modeling , and the syntactic features are derived from wordnet 2 . 1 ( which has been tested in the original paper ) . syntactic syntactic sg embedding improves the performance of wordnet , but it does not improve wordnet performance . it improves the ppa performance by 2 . 5 points over the glovec embedding baseline .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear that the removal of context sensitivity significantly decreases the ppa accuracy .
2 shows the performance of domain - tuned and multi30k models compared to en - de . the results are presented in table 2 . we observe that domain tuning improves the image caption translation performance , but does not improve the performance for multi - de models .
results are shown in table 4 . we observe that the subs1m model outperforms all the other models except for those using domain - tuned h + ms - coco . the results are presented in the en - de setting , which means that the model is more suitable for the edit distance . subdomain - tuning improves the performance for all models except those using mscoco17 . as expected , the results are slightly worse for all the models using the same set of features .
4 shows bleu scores in terms of multi30k captions compared to the best ones . the results are shown in table 4 . it is clear that the automatic captions are beneficial for the model , but not beneficial for it .
5 shows the performance of our approach with respect to enc - gate and dec - gate . the results are summarized in table 5 . we observe that the approach achieves the best results with a low bleu % score , which indicates that it is possible to improve upon the performance by adding additional features .
3 shows the performance of subs3m and subs4m on the en - de dataset . the results are presented in table 4 . sub3m outperforms all the other models except for the one that relies on word embeddings . as expected , the performance on the single - language dataset is significantly better than the other two models . in addition , the combination of text - only and multi - lingual features improves the performance for both models .
3 shows the performance of our system compared to the previous state - of - the - art en - fr model . the results are presented in table 3 . we observe that our system outperforms all the other methods except for the one that relies on word embeddings . table 3 shows that our approach obtains better results than the other approaches .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . it is clear from table 1 that these language pairs perform well in the development splits .
2 shows the vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms all the other test sets except en – es .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is better than the previous state - of - the - art rev system .
results on flickr8k are shown in table 2 . the vgs model is the visually supervised model from chrupala2017representations . it achieves the best performance with a 3 . 5 % recall rate .
results on synthetically spoken coco are shown in table 1 . we observe that the visual supervised model improves the performance by 3 . 5 points over the adversarial model .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , orig < cid > turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . dan ( which has edges ) and rnn ( that has curves ) are examples of how to use these classifiers . for rnn , we report further examples in table 1 . for dan , we show that the edges of the screenplay are so clever that it is easier to hate hate it than it is for rnn . rnn is similar in that it has curves and edges .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . the results show that the number of occurrences have increased , decreased or stayed the same through fine - tuning . as expected , the results are very similar to those of the original sentence .
3 shows the effect of the flipped sentiment labels on sentiment . the results are shown in table 3 . it can be seen that the negative sentiment labels have a significant effect on sentiment , as shown in the second row .
results are presented in table 2 . the results are summarized in table 1 . it is clear that the competitive nature of our approach is that it is beneficial to investigate and to compare with other approaches . however , it is difficult to distinguish between positive and negative aspects of the approach . in addition , the results are not statistically significant , indicating that the approach is beneficial for both research and evaluation . as expected , there is no significant difference in performance between the two approaches . this is mostly due to the small size of the proposed corpus ( table 1 ) and the large number of participants ( table 2 ) .
