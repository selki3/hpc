2 shows the performance of our recursive framework on the large movie review dataset compared to our iterative approach . throughput is the best on the training dataset , with efficient parallel execution of tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , when the batch size increases from 1 to 25 , it exhibits the smallest improvement .
2 presents the results for each model with different representation . we show that the max pooling strategy consistently performs better in all model variations with different number of parameters . we also show the performance of sigmoid and softplus models with different input parameters . the maximum pooling strategies consistently outperform all the other approaches .
1 shows the effect of using the shortest dependency path on each relation type . our model outperforms the macro - averaged model in terms of f1 ( in 5 - fold ) without sdp .
results in table 3 show that the y - 3 model outperforms the previous state - of - the - art models in terms of f1 and f1 scores .
results are shown in table 1 . the results of our method are summarized in terms of paragraph level and f1 scores . our method outperforms all the other methods except mst - parser on paragraph level .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level is lower than the majority performances for both systems ; however , the improvement is significant .
results are presented in table 1 . the original and the original systems are shown in bold . all the errors in the original are corrected on the test set . the errors in both sets are caused by incorrect labeling . the error detection system is consistently worse than the original on all test sets .
shown in table 1 , the original and the cleaned versions of the e2e data are comparable in terms of number of distinct mrs and ser as measured by our slot matching script , see section 3 .
results are shown in table 1 . original and original systems outperform the best state - of - the - art systems on all metrics except bleu . in addition , both the original and the correct state of the art systems perform better than the original on all three metrics .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that the original training data had a significant number of errors , which we found to be caused by slight disfluencies in the training data . these errors were mostly caused by incorrect values .
results are shown in table 1 . all models outperform all the other models in terms of bias metric ( table 1 ) . however , for all models , all models perform better than all the models except for tree2str .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points . the results are shown in table 2 . we observe that our model size is comparable to that of seq2seqb ( beck et al . , 2018 ) .
results are shown in table 1 . the results are presented in english - czech , german , french , russian , turkish , russian and turkish . in english - german , the results are reported in tables 1 and 2 . we observe that the best performing model is the cnn + gcn model , which outperforms all the other models in terms of english - language performance .
5 shows the effect of the number of layers inside dc on the overall performance of the layers . table 5 shows that when we add layers of layers , we get a reduction of 3 layers .
6 shows the performance of baselines with residual connections . rc + la ( 2 ) and gcn + rc ( 6 ) indicate that gcns have residual connections to residual connections , which indicates that residual connections are important for gcn performance . however , gcn + rc ( 7 ) shows lower performance than other baselines .
results are shown in table 3 . we observe that the dcgcn model outperforms all the other models in terms of bias metric . the results are striking even when we add in the number of training examples .
8 shows the results of ablation study on the dev set of amr15 . the results are shown in table 8 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block , whereas - { 4 } dense block denotes removing them in the ii - th .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . encoder modules have the best correlation with the graph attention dataset , with a gap of 3 . 5 points in correlation to the dgcn4 dataset . in addition , the multi - decoder design has the worst correlation with graph attention .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our method outperforms the best state - of - the - art methods on all three tasks .
results are presented in table 3 . we observe that the best performing method is h - cbow / 400 , which is comparable to the best state - of - the - art cbow .
3 presents the results of our method on subj and mpqa datasets . our model outperforms all the other methods except for subj , which is superior in both categories .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp in all but one of these tasks .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the other approaches except for sst2 and sst5 .
6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . the cbow - r model outperforms all the other methods except for sts13 , which is more stable .
results in table 1 show that our method outperforms all the other methods in terms of depth and subtraction . however , it does not achieve the best performance on all metrics .
3 presents the results of our method on subj and mpqa datasets . our model outperforms all the other methods except sst2 and sst5 in terms of mrpc performance .
3 presents the results of our system in [ italic ] e + org and per . the results are summarized in table 3 . our system outperforms all the other systems in both e + and per in both systems , the org scores are significantly better than those of the previous state - of - the - art systems . we also observe that our system performs better than all the systems in all three systems .
results on the test set under two settings are shown in table 2 . our system outperforms all the previous models in terms of e + p , f1 and e + f1 scores .
6 : entailment ( ent ) and ref ( ref ) in the model outperform ref in all but one of the three cases .
results are shown in table 1 . the results are presented in tables 1 and 2 . we observe that the model outperforms the ldc2015e86 and ldc2017t10 in terms of bleu scores .
3 shows the results on ldc2015e86 test set when additional gigaword data is trained . our model outperforms the previous state - of - the - art models on both sets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the bilstm significantly improves the performance of the model when compared to the original model .
results are shown in table 3 . the results are presented in bold , indicating that the model is more accurate than the g2s - gin model in terms of sentence length . as expected , the model obtains better results when the sentence length is longer than the average length .
shown in table 8 , the fraction of elements in the input that are missing in the output that are present in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . the gold tokens are used in the comparison to the reference sentences .
4 shows the accuracy of our approach with respect to target languages extracted from the 4th nmt encoding layer , trained with different target languages on a smaller corpus .
2 : pos and sem tagging accuracy with baselines and an upper bound . accuracies are reported in table 2 . the best results are obtained using unsupervised word embeddings . word2tag has a lower upper bound than word1 .
results are presented in table 4 . our results show that our method outperforms all the other methods in terms of accuracy , with the exception of the one that significantly improves accuracy .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . our uni model outperforms the res model in terms of accuracy , with a slight improvement over res model .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored a weighted average of 9 . 2 % on a training set 10 % held - out .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards the task . the results are shown in table 1 . the gender of the participants is the most important factor in the performance .
2 shows the performance of our classifier in the context of balanced and unbalanced data splits . the classifier is trained on gender - neutral tweets , and is able to detect instances of gender - aligned tweets as well as instances of unbalanced tweets .
performance on different datasets with an adversarial training is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ ( differences in training performance ) . in pan16 , the difference between training accuracy and training accuracy is measured in terms of training performance .
6 shows the performance of the embedded and guarded encoders compared to the original embeddings . the results are shown in table 6 .
results are shown in table 1 . our model outperforms the previous state - of - the - art lstm on all metrics except for finetune . the results show that our model performs better than the other models on both datasets . however , the results are still slightly worse than those of the previous work on the wt2 dataset . we observe that this model is more suitable for the task of finetuning the models .
results are shown in table 5 . our model outperforms all the previous models in terms of training time . we observe that when training time is increased , the training time decreases significantly .
results are shown in table 4 . the results of our model outperform the best state - of - the - art models on all metrics , except for the amapolar time metric . we observe that our model performs better on both datasets , with a slight improvement on the yahoo time metric compared to the original model .
3 shows the bleu score on the wmt14 english - german translation task . our model outperforms the previous state - of - the - art model in both decoding and decoding tasks .
4 shows the performance of our model on squad dataset . our model outperforms all the models except lrn , which is better at match / f1 score . we observe that the sru model performs better than all the other models except for the lstm model .
6 shows the f1 score on conll - 2003 english ner task . the lstm model outperforms all the other models in terms of parameter number . in fact , it is the only model that achieves better f1 scores than lrn .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting .
results are shown in table 1 . word embeddings are used in all systems except for system retrieval . sent attention is used in both systems , word attention is beneficial for both systems as well as for the system . the word attention improves the results for all systems , however the word attention is less effective for the systems .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best results among all systems are highlighted in bold , with statistical significance marked with ∗ .
results are shown in table 1 . the results of our experiments are presented in table 2 . we observe that our approach outperforms all the other approaches except for the one on the df dataset .
results are shown in table 1 . the results of our experiments are presented in table 2 . we observe that our approach outperforms all the other approaches except for the one on the df test set .
results are shown in table 3 . the results of our experiments are summarized in table 1 . we observe that our approach outperforms all the other approaches except for df , which is more suitable for df .
results are shown in table 1 . our results show that our system has the best performance on all metrics when combined with the maxdepth of our model , which is comparable to the best on all datasets except df . we also observe that our model has the worst performance on both metric metrics .
results are shown in table 1 . our results show that our system has the best performance on all metrics when combined with the maxdepth of our model , which is comparable to the best on all datasets except df .
1 shows the performance ( ndcg % ) of our enhanced model on the validation set of visdial v1 . 0 . the enhanced model outperforms the original visdial model in terms of r0 , r2 and r3 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . using p1 + p2 shows the most effective ablative study .
5 presents the results on hard and soft alignments . the results are summarized in table 5 . the hmd - f1 model outperforms all the other models in terms of bert and lvs - en .
3 presents the results of the direct assessment and de - en evaluations . the results are summarized in table 3 . the best results are obtained by baselines and bertscore - f1 scores . as expected , the results are significantly better than those obtained by the baselines . however , the performance improvement is less pronounced on lemmatized test sets .
3 presents the bagel and sfhotel scores on the validation set . the results are summarized in table 3 . the bertscore - f1 scores are significantly better than the bleu - 2 scores on both sets .
results are presented in table 3 . we observe that the leic score - recall metric is significantly better than the meteor score - mover metric , leic scores are comparable to those of spice , but are slightly worse than spice scores . the results of leic metrics are reported in table 4 . our model outperforms all the other baselines in terms of performance .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model on all metrics except for the performance on the sim test set . as expected , the performance of m0 models is lower than those of m1 , m2 and m3 .
results are presented in table 3 . we observe that the semantic preservation and transfer quality features are the most important aspects of semantic preservation . the semantic preservation features are particularly important for semantic preservation , and the transfer quality quality is the most crucial one . semantic preservation and semantic preservation are the two most important features of semantic preserving , respectively . in both cases , semantic preservation outperforms semantic preservation in both cases .
5 presents the results of human sentence - level validation . the results are shown in table 5 . the results show that the human ratings of semantic preservation are better than the machine ratings , indicating that semantic preservation is beneficial . however , the results are less clear than those of the machine .
results are shown in table 3 . we observe that the m0 + para + lang model outperforms all the other models in terms of performance on all metrics except for the performance on the sim metric .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ scores . the results on simple - transfer are slightly worse than those on unsupervised sentiment transfer , where the classifiers in use are much worse .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be disfluent . the percentage of repetition tokens that were correctly predicted as disfluency is slightly higher than the percentage predicted as nested .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted to contain the content word is shown in table 3 . the percentage of tokens that contain the word is reported to be less than the fraction predicted as containing the word .
results are shown in table 3 . we observe that the best results are achieved when text + innovations are used in the same system as in the single model . text + innovations also improve the test mean by 0 . 2 points over the best state of the art .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . the results are summarized in table 2 . our model outperforms all the other methods in terms of accuracy .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater .
3 shows the performance of our method on the word attention task with and without attention . the results are shown in table 3 . we observe that the accuracy of our approach is comparable to that of the other two methods .
results are shown in table 1 . the best performing models are jnn , dmcnn and jrnn . the worst performing ones are trigger and jnn .
3 presents the results of our method in table 3 . our method outperforms all the other methods in terms of identification and classification . the results are presented in tables 1 and 2 . in table 3 , the method is used to classify all the events in the event .
results are shown in table 3 . all the models trained on english - only - lm outperform all the other models except for the ones trained on spanish - only . the results are summarized in table 1 . all but one of the models tested on spanish only - lm are better than all the others .
4 shows the results on the training set and on the test set using discriminative training with only subsets of the code - switched data .
5 shows the performance on the dev set and the test set , compared to the gold sentence in the monolingual set .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . note that the precision ( p ≤ 0 . 05 ) is significantly better than the recall ( p > 0 . 01 ) .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the previous state of the art model .
results on belinkov2014exploring ’ s ppa test set . the hpcd system uses syntactic - sg embeddings obtained by the original paper , and is based on the semantic skipgram embedding obtained by finoqui et al . ( 2015 ) and schütze ( 2015 ) . the results are shown in table 1 . syntactic sg embedding refers to the syntactic embedding of wordnet vectors , and glove - retro refers to semantic embedding . the results show that syntactic sg has a generally positive effect on ppa performance .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 .
2 shows the results of domain tuning for image caption translation . the results are summarized in table 2 . subdomain tuning improves the multi30k performance by 3 . 5 points over the domain - tuned model , while domain tuning improves by 2 . 3 points .
results are shown in table 4 . we observe that the subs1m model outperforms all the other models when domain - tuned , which indicates that domain tuning is beneficial for improving performance .
4 shows bleu scores in terms of multi30k captions compared to en - de captions . the results are summarized in table 4 . the best results are obtained by adding automatic image captions ( dual attn . ) in the final set of 5 .
5 compares the performance of our approaches with those using en - de and dec - gate . the results are summarized in table 5 . we observe that the enc - gate approach outperforms the other approaches in terms of bleu % scores .
3 shows the performance of subs3m on the en - de dataset compared to subs6m in the multi - lingual setting . the results are summarized in table 4 . we observe that the multilingual model outperforms subs3 and subs3ml in terms of performance .
results are shown in table 3 . the best results are obtained using the en - fr - ht and mtld variants . as expected , the results are slightly worse than those obtained by the original embeddings .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the results of training vocabularies for the english , french and spanish data used for our models .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is better than the previous state - of - the - art rev system .
results on flickr8k are shown in table 2 . the vgs model is the visually supervised model from chrupala2017representations . the mean mfcc rank is 0 . 2 , which indicates that the model is more suitable for training .
results on synthetically spoken coco are shown in table 1 . our model outperforms all the previous models in terms of recall @ 10 ( % ) with a slight improvement .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that is very clever at the edges ; it ’ s so clever you want to hate it . similarly , for rnn , the edges edges edges are very clever .
2 presents the results of fine - tuning on sst - 2 . the results indicate that the number of occurrences have increased , decreased or stayed the same for the three types of words .
3 shows the effect of the flipped sentiment on sentiment . the results indicate that the sentiment increases when the negative labels are flipped to positive , as shown in fig . 3 .
results are presented in table 1 . the results are summarized in bold . we strongly suspect that the competitive nature of our approach leads to a better interpretability . however , the results are less clear than expected , indicating that our approach is less effective than other approaches . further , we suspect that our method may not be effective for the task at hand .
