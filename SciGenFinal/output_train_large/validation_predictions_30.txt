2 shows the performance of the treelstm model on the recursive framework and the iterative approach with the large movie review dataset . as table 2 shows , the recursive approach performs the best on the training dataset , while the iteration approach shows better performance on the inference dataset .
shown in table 1 , the balanced dataset exhibits highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to consider .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . we report the average number of hyper parameters in the validation set and the number of feature maps in the output set . the max pooling strategy consistently performs better in all model variations . moreover , the sigmoid model outperforms all the base models with different number of parameters . we also report the f1 score and the average value of the feature maps with the maximum number of iterations .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , but do not outperform these models in terms of f1 score . the comparison of our model with the strongest dependency path is shown in table 1 .
3 shows the performance of all the models compared to the original ones . for example , for y - 3 , the performance is significantly better than those of y - 2 because the gap between the average f1 and average r - f1 is narrower .
3 presents the results on the wordparagraph level . our proposed method achieves state - of - the - art results in terms of answering questions with a minimum of 50 % probability . the results are presented in table 3 .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph also exhibits lower performance , however , compared to the majority performance .
3 shows the performance of original and original models on each error generation . the results are presented in table 3 . original and original models are consistently better than the others , however , their performance is still slightly worse than original . for example , rouge - lstm is only slightly better than original in terms of accuracy , while it is more than 50 % better on accuracy .
1 compares the original and the cleaned versions of our e2e data . we report the number of distinct mrs , total number of textual references , and the average number of slot matching script mrs as measured by our data table , see section 3 . original and dev metrics have a total of 33 , 525 mrs and 39 , 525 ser ( 0 . 00 ) . the cleaned version has a total ranking of 39 . 67 .
3 shows the original and original test scores . original scores are presented in table 3 . the best performing ones are bleu , nist , rouge - l and cider . the original scores have been consistently better than the original scores , but still superior to the original ones . the sc - lstm model ( which takes cues from the original ) and tgen + are both slightly better than original but still inferior to original .
4 shows the absolute numbers of errors we found in the manual error analysis of tgen ( see table 4 ) . for the original dataset , we found 22 errors ( 17 . 6 % ) and 15 . 6 % of those were misclassified .
model < cao et al . , 2017 ) and parallelstm achieve state - of - the - art performance on all three models except for seq2seqk . the two models have completely different performance on the external and the single model , respectively .
2 presents the results on amr17 . gcnseq achieves 24 . 5 bleu points and achieves 27 . 5 epmu points . however , the biggest performance drop is on seq2seqb ( which takes the ensemble model and applies the same number of parameters to the final model ) .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing single model is the ggnn2seqb , which results in 43 . 4 % improvement on average compared to the previous state - of - the - art model , bow + gcn ( bastings et al . , 2017 ) . the only exception is the english - language variant , where we see that the single model performs better than the other two models .
5 shows the effect of the number of layers inside the dc stack on the performance of the model when we add the layers of layers that contribute to the overall performance . for example , we observe that of the four layers , only one is bigger than the others .
6 compares the performance of different gcns with residual connections . with residual connections , we observe that dcgcn2 ( 27 ) shows very similar performance to other baselines .
model f1 shows that dcgcn outperforms all the other models in terms of performance on both metric and bias metrics .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . the results are shown in table 8 . - { i , i , 4 } dense blocks have a significant impact on the model performance , which shows that removing the dense connections helps the model to improve performance .
shown in table 9 , the global encoder and the lstm decoder use the best performing dual encoder design . encoder modules use a gap of 22 . 9 % and 23 . 4 % respectively compared to the previous state of the art .
investigate the effects of different initialization strategies on probing tasks . our paper shows that our approach obtains the best performance with a gap of 10 . 5 % in the precision score for each initialization strategy .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . we observe that our approach obtains the best performance on every metric with a gap of 10 . 5 % in precision from the last published results .
1 shows the performance of all models trained on subj and mpqa . our model outperforms all the alternatives except cmp except for the one that has the worst performance . subj is only competitive with sst2 and sst5 on three of the four metrics . it is clear from the results that the hybrid model performs better on all the three metrics . however , it has the advantage of training on a larger corpus . we observe that cbow / 784 shows a slight improvement on performance over the previous state of the art model .
3 shows the relative change with respect to hybrid over unsupervised downstream tasks attained by our models . we observe that cbow and cmp perform similarly to each other on almost all downstream tasks , except for the ones where they are used more frequently .
8 presents the evaluation scores for all initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the alternatives except subj and mpqa except for the one that has the worst performance . it also fails to capture the best performance on the three downstream tasks , which means that the initialization strategies are inferior to the ones that belong to the original two .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts12 , sts14 and sts15 datasets . the worst performance is on sts16 .
1 shows the performance of all models trained on the cbow - r dataset . our model obtains the best performance on every metric with a gap of 10 . 5 % in precision from our last submission .
subj and sick - r are comparable in terms of performance on all three benchmarks . however , subj has the advantage of outperforming all the methods except cmow - c and cbow - r on all metrics except mrpc except for the one that it obtains from the united states ( e . g . , sst2 and sst5 ) . the difference between these methods is less pronounced on the mrpc metric , but still suggests some reliance on superficial cues . for example , for example , when trained on sst6 , the performance gap between the two improves .
system performance in [ italic ] e + per and e + misc scores are reported in table vii . the system performs better than all the systems except for the one that does not use org and misc . supervised learning models ( mil - nd and mil - nd ) result in significantly better performance than the systems described in the previous section . in both cases , the improvement is due to better interpretability of the data without sacrificing too many correct answers . as can be seen , the difference between the performance of the best systems is minimal , in theitalic case when only using one correct answer .
2 shows the results on the test set under two settings . our system outperforms all the models except mil - nd ( model 1 ) in terms of e + p score and f1 score . in both settings , the system performs better than the previous state - of - the - art models . moreover , the performance of the supervised learning model is comparable with that of the original model ( mil - nd ) .
6 shows the results of ref and ref for all models except for those that do not have ref ( table 6 ) . ref significantly outperforms ref in all but one of the cases where ref is used .
1 and table 2 summarize our results on the ldc2017t10 and ldc2015e86 datasets . the results are summarized in table 2 . the models perform significantly worse than the previous state - of - the - art models on both datasets .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms all the models except for the ones that are pre - trained with gigawords data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results of this study show that the use of bilstm improves the generalization ability of the model .
results are presented in table 4 . the results are summarized in terms of the average number of frames for each model , compared to the previous state - of - the - art models . note that the g2s models tend to have longer sentences , hence less precision is required for the model to achieve outstanding results . finally , for the 50 - 240 example , we observe that the average length of the sentences is shorter than the average of the other models , indicating that the model performs better on both occasions .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , is very small compared to the gold - based model , and is used in the comparison to derive the summaries from the reference sentences .
4 shows the performance of our approach with respect to target languages . we use word embeddings extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : upper bound encoder - decoder - based tags .
3 shows the performance of our system on three real - world datasets . our results tabulated in table 3 show that our system performs on par with the best performing state - of - the - art systems .
5 shows the accuracy with different layers of our uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
8 shows the performance of an attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 .
1 shows the performance of our training data on a single task . the performance gap between dial and sentiment is significantly narrower than in the other two scenarios .
2 shows the status of theected attribute leakage in the context of balanced and unbalanced data splits . the trained classifiers ( pan16 ) consistently show marked lower performance than the unbalanced classifiers . dial and sentiment also show lower performance , however , the difference is less pronounced for the two groups . sentiment and gender are the most prevalent classifiers ,
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . sentiment and language performance are similar across all datasets , with the exception of pan16 .
6 shows the performance of the protected attribute for different encoders . embedding leaky gets worse performance than embedded , and the rnn embeddings get worse performance .
3 shows the performance of our model on the # params and # finetune benchmarks . our model outperforms all the other models with a large gap in performance between thedynamic and finetune sets . the results are summarized in table 3 . this model performs on par with the original wt2 model , and on the largescale wt2 dataset . we observe that the performance gap between the original and the new state - of - the - art models is very small , however , this model performs better than other models we have evaluated before .
5 shows the performance of our model compared to previous models . the results are summarized in table 5 . we observe that our model significantly outperforms previous models in terms of training time , both on the acc and thebert datasets .
3 shows the performance of our model compared to other models . our model outperforms all the other models except for the one that is used in the amapolar time and full time settings . we observe that this model performs better on both datasets , with the exception of the amafull time dataset .
3 shows the bleu score of our model on the wmt14 english - german translation task . as seen in fig . 3 , our model obtains a significant improvement over the state - of - the - art model on both benchmarks with a gap of 1 . 5k training steps .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our # params model significantly outperforms other models in terms of match / f1 score . however , our model obtains a lower match / score score than other models because its parameter number is smaller .
6 shows the f1 score of our model on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . further , we observe that our model exhibits the best performance on both snli and ptb task .
3 shows the performance of the top 5 systems for each domain . all the systems trained on the table are considered in terms of system retrieval . the top five systems are : all of the systems are evaluated using the word " retrieval " . the system is significantly better than the previous state - of - the - art systems ,
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( k 2000 ) . the best performing system is seq2seq , which achieves an overall improvement of 2 . 3 points over human evaluation .
apply the best performing ensembles on the corpus and docsub datasets . our results are summarized in table vii . our proposed system outperforms all the other models except for the one that we use , namely , europarl and ted talks . it obtains the best performance on all the three datasets , apart from the one on corpus . we observe that for the docsub dataset , our proposed system performs slightly better than the best baseline .
are presented in table vii . our proposed system outperforms all the other systems except for the one that we use , namely , europarl , df and docsub . it obtains the best performance on all three of these metrics , outperforming both the baseline and the corresponding embeddings . we observe that for all three datasets , our proposed system performs slightly better than the baseline on all but one of the three .
apply the best performing ensembles on the corpus and docsub datasets . our results are summarized in table vii . our proposed system outperforms all the other models except for the one that we use , namely , europarl and ted talks . it obtains the best performance on all the three datasets , apart from the one on corpus . we observe that for the docsub dataset , our proposed system performs slightly better than the best baseline .
ributhe results are shown in table vii . our system obtains the best performance on every metric with a gap of 1 . 8 points from our baseline . our joint model outperforms all the other models except for the one that we use . we observe that our joint model significantly improves the joint model ' s performance on all the metric metrics . however , it does not exceed the europarl / maxdepth score by a significant margin .
ributhe results are shown in table vii . our proposed system outperforms all the base lines with a gap of 9 . 43 % in depthcohesion . we observe that our proposed system achieves the best performance on both metric metrics . on the other hand , our system performs slightly worse than our joint model on all metric metrics except for the one that contains the maxdepth and the averagedepth .
performance of our system on the validation set of visdial v1 . 0 . 0 is shown in table 1 . we observe that lf significantly outperforms the enhanced version of the original visdial model in terms of answer score sampling .
performance ( ndcg % ) of different ablative studies on different models is shown in table 2 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) . note that only applying p2 with the history shortcut can improve the results .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that their performance is significantly worse than those of other models that rely on bert or epm .
3 presents the results of our approach with respect to the direct assessment and eureor scores . the results are summarized in table 3 . the first set of results show that our approach significantly outperforms the baseline on both metrics with an absolute improvement of 3 . 6 points .
3 presents the performance of our baseline models on the sfhotel and smd datasets . our proposed bleu - 2 model outperforms all the baseline models with a gap of 3 . 5 points from the last published results ( table 3 ) .
performance of the models according to these metrics is reported in table 3 . the summaries are summarized in terms of leic scores ( which are obtained using bertscore - recall ) and spice ( which is derived from eureka et al . ( 2018 ) . the results are broken down in tables 1 and 2 .
3 shows the performance of all the models using the word embeddings . we observe that for all but m0 , there is a significant drop in performance compared to m1 , m2 and m3 due to different programming languages .
3 presents the results of all models tested on the validation set of hotpotqa in the distractor and fullwiki setting . the results are summarized in table 3 . all the models shown in the table show that the semantic preservation approaches significantly outperform the transfer quality ones . yelp , m6 and m7 are particularly bad for semantic preservation , however , the best results are obtained on the fullwiki setting , where we obtain the best scores . semantic preservation and transfer quality scores are significantly better than the sentiment scores of any other domain .
5 presents the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . these results are shown in table 5 . they show that the accuracy obtained by using these metrics can be significantly higher than the rate obtained by the machine .
3 shows the performance of all models with different level of supervision . we observe that for all but m0 , there is a significant drop in performance compared to m1 , m2 , m3 and m6 due to different training set size .
6 shows the results on yelp sentiment transfer . our best models achieve higher bleu than prior work at similar levels of acc ∗ ( see table 6 ) . however , the improvement is less pronounced for fu - 1 than for any other model we have used . also , multi - decoder is slightly better than simple - transfer , but still requires a lot of data to compile . we also observe that the language embedding technique can improve the interpretability of the generated sentences .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be in disfluency . the percentage of repetition tokens that were correctly predicted as n - disfluencies is 8 % overall .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens that contain a content word is shown in parentheses , however the fraction that contain that word is less than the number of tokens predicted in the original reparandum .
1 shows the performance of all models when using text + innovations . our model outperforms the single model in terms of dev and rewards , with an absolute improvement of 2 . 36 points over the best model .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance on the test dataset in the low - supervision settings . however , we do not have the best performance in the large - scale setting .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the accuracy ( % ) of our component models with and without attention . this shows the effectiveness of word attention and graph attention for this task . compared to ac - gcn , neuraldater performs significantly better on graph attention .
3 shows the performance of all models when trained on the same domain . our joint model outperforms all the other models except for trigger . the results are reported in table 3 .
3 shows the performance of all the methods for the validation set in table 4 . all the methods cause a significant drop in performance for all the models except for the one that belong to the " g & l " category . in general terms , all the method methods cause an extremely high drop in precision . we report both the identification and the trigger scores for each case . the method used in the g & l dataset is qualitatively different from the others in terms of both the original and the disambiguation set .
can be seen in table 1 , all the models trained on our system are comparable in terms of performance . all the models except for fine - tuned - lm are better than fine - tuned - lm on every metric . the difference is most prevalent in english , where the average number of frames per sentence is significantly higher than that of spanish - only .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev with only subsets of the code - switched data .
5 shows the performance of our model on the dev set and the test set . it can be seen that it performs slightly better than the monolingual model in terms of gold sentence prediction .
results in table 7 show that type - aggregated gaze features improved the precision and f1 - score for the three eye - tracking datasets tested on the conll - 2003 dataset . also , the drop in precision ( p < 0 . 001 ) and recall ( p > 0 . 01 ) since using the same gaze features improves the performance .
5 shows the precision , recall and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision ( p < 0 . 01 ) is statistically significant , with the exception of the one where we used the unlabelled gaze features .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the embeddings obtained by using autoextend rothe and schütze ( 2015 ) are used in wordnet 3 . 1 . however , they do not need to be re - trained as wordnet has already done so . we report the results on the original paper ( which was published in the united states in the same year ) . syntactic - sg vectors are already used on wordnet , but glove - extended vectors have been used on verbnet since the first published work .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . as table 2 shows , using oracle pp as the dependency parser results in a better performance than using any other oracle attachment predictor oracle . however , the best performance is obtained using lstm - pp ,
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . it shows the significant drop in ppa acc . from the baseline to the low baseline .
2 shows the performance of our model with domain - tuned captions and multi30k frames . we notice a drop in performance compared to en - de , indicating that the domain tuning approach is beneficial for image caption translation .
3 shows the performance of all subs1m models when domain - tuned . our model outperforms all the en - de models except for mscoco17 , whose results are shown in table 3 . the results are broken down in terms of a , b , c and e .
4 shows the bleu scores in terms of the models using the automatic captions . our model outperforms all the models except for the ones that have the best five frames .
5 compares the approaches for integrating visual information with our en - de embeddings . we observe that the best performing model is the one using multi30k + ms - coco + subs3mlm , and its enc - gate features are better than dec - gate .
1 shows the performance of subs3m with different visual features compared to subs6m on en - de and on flickr16 . sub - categories as adjectives antonyms and performer action have the highest performance on both datasets , but for the single - domain model , the performance is slightly worse than that of the other two models . with the exception of the one with the text - only features , our model achieves a performance comparable to the best on the two datasets ( mscoco17 and fig2048 ) .
3 shows the performance of all the models trained on the randcoref corpus compared to the original ones . the results are summarized in table 3 . we observe that for all the translations that do not rely on word embeddings , their performance is significantly worse than those of en - fr - rnn - ff .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the performance of our system with respect to the english , french and spanish vocabularies . our system outperforms all the other systems with a large margin .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) and ter metrics show significant performance drop due to high variation in rev performance .
2 shows the performance of our model compared to prior work on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . we observe that segmatch significantly outperforms the other two models in terms of recall scores .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model in terms of recall @ 10 and mean mfcc score .
1 shows the results for each classifier compared to the original on sst - 2 . for example , orig ( the original ) turns in a < u > screenplay that is curved at the edges and edges ; it ’ s so clever you want to hate it . also , rnn ( which is similar to orig but has been developed and tested on other languages ) and cnn ( which has similar features ) .
2 shows the part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these numbers indicate that some of the words possessed of goodness have been removed from the original sentence .
shown in table 3 , the change in sentiment between positive and negative sentiment is larger than those in the original sentence .
table 2 , we report the performance of our method with respect to word embeddings forpubmed and smartmed . results are summarized in table 2 . while the positive results are encouraging , the negative ones are less striking . our joint model outperforms all the other methods apart from sift on both metrics .
