2 : throughput for processing the treelstm model on our recursive framework , and tensorflow ’ s iterative approach , with the large movie review dataset as our training example . as table 2 shows , the recursive approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows better performance on training . further , the use of gpu exploitation improves the performance on the training dataset as well as on the inference tasks .
shown in table 1 show that the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset when the number of batch size increases from 1 to 25 .
performance for each model with different representation is shown in table 2 . the maximum pooling strategy consistently performs better in all model variations . as shown in fig . 2 , the size of the hyper parameters is the only factor that affects the performance of the model when using different representation . finally , the number of iterations of the validation set increases with each iteration . we can further see that using sigmoid as the model initialization set helps the model to learn the best features .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp . as shown in fig . 1 , macro - adapted models have the advantage of having shorter dependency paths . in fact , these models have higher f1 scores when using only sdp as dependency path . when using only one dependency path , the model achieves the best performance .
results are shown in table 3 . we observe that y - 3 outperforms all the other models except for those using the word " f1 " .
3 shows the performance of our model on the validation tasks . our model achieves the best performance on all three tasks . the results are presented in table 3 . the results of the best performing model are reported in tables 3 and 4 .
4 : c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
3 shows the performance of original and original training systems . the results are shown in table 3 . original is better than the original , while original is worse . the performance of tgen + is more consistent across all systems , with the exception of svm - lstm , which is more suitable for training on small datasets .
statistics for the original e2e data and our cleaned version are shown in table 1 . the original and the cleaned versions are comparable in terms of number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 .
3 shows the original and original test scores for each system . the original scores are shown in table 3 . original scores have been consistently better than the original scores since the introduction of tgen in 2003 . they also outperform all the other methods except for the one that relies on word embeddings . original results are summarized in tables 3 and 4 . these results are presented in tables 5 and 6 .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . the absolute numbers of errors we found ( added , missed , wrong values ) are shown in table 4 . in addition , we found a total absolute number of errors ( 17 , 21 , 22 ) which we found to be caused by slight disfluencies in the training set .
3 shows the performance of our models on the external and external datasets compared to the previous state - of - the - art models . our model outperforms all the other models on both datasets except for seq2seqk ( konstas et al . , 2017 ) and tree2str ( lei and bachard , 2018 ) . on the bias metric , our model achieves a performance improvement of 2 . 6 % on the single metric compared to previous work .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points and achieves 50 . 4 bleus points . as shown in table 2 , the model size in terms of parameters is smaller than those in the ensemble model . note that our model size is larger than those of other models .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing models are published in english , german , french , russian , turkish , russian and turkish . all the models except bow + gcn ( bastings et al . , 2018 ) show that the single model outperforms the other models in terms of performance in english - language and german . we observe that the difference in performance between single and multi - step models is minimal , however , it is encouraging to continue researching into the topic of concatenation .
5 shows the effect of the number of layers inside dc on the performance of the model when we add them all together . we observe that for every layer of dc , there are two layers that contribute strongly to the performance . table 5 shows that for all but one of the layers , there is a noticeable drop in performance .
6 compares gcn with baselines . + rc denotes gcns with residual connections . as shown in table 6 , when rc is applied to residual connections , the gcn shows a significant performance drop compared to previous models . rc + la also indicates that residual connections are important for gcn performance .
model 3 shows that dcgcn outperforms other models in terms of performance on all metrics when trained on a single dataset .
8 shows ablation study for the density of connections on the dev set of amr15 . the results shown in table 8 show that removing the dense connections in the i - th block reduces the performance of the model when removing the - th blocks .
9 shows ablation study for the graph encoder and the lstm decoder . the results are shown in tables 9 . encoder modules used in table 9 show that the global coverage mechanism helps the model achieve better interpretability . the multilingual encoder modules ( g & lstm ) achieve the best interpretability with a minimum of noise and a maximum of 50 % recall .
7 shows the performance of initialization strategies on probing tasks . our paper shows that our approach obtains the best performance on all three tasks . it achieves the best score on the subtasks with a minimum of 0 . 8 % precision .
3 shows the performance of our method in terms of depth and subtraction . our method outperforms all the other methods except for cbow / 400 , which shows the diminishing returns from mixing source and target .
3 shows the performance of our model compared to other methods . our model outperforms all the other methods except subj and mpqa except for cmp , which obtains the best performance . it also outperforms both sst2 and sst5 in terms of mrpc score . it is clear from the results that our model obtains superior performance on all three metrics when trained on the same single dataset .
3 shows the relative change with respect to hybrid over unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both hybrid and cmp in all downstream tasks . on the sts13 dataset , it achieves a better performance than cmp .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the other approaches except for the ones that do not use the word " sentiment " .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are obtained on the sts12 , sts14 and sts15 datasets , respectively . the worst performance is obtained on sts16 , which shows that cbow - r performs poorly on these tasks .
3 shows the performance of our method on the subtense and threshold metrics . our method outperforms all the other methods except for cbow - r , which shows the diminishing returns from mixing subtasks .
3 presents the results of our method on the subj and mpqa datasets . our model outperforms all the other methods except for the ones that do not use subj . we observe that cbow - r outperforms both the sst2 and sst5 baseline on both datasets . it is clear from the results that the combination of these methods improves the performance of subj by a significant margin .
3 shows the e + and per scores of all system models trained on the italic dataset . our system obtains the best results with a minimum of 50 % org and 50 % per scores . our model outperforms all systems except for the one that relies on name matching . we observe that the combination of domain name matching and multi - factor learning ( mil - nd ) significantly improves the performance of the system with respect to the human judgement . finally , we observe that combining all the features of the original domain leads to a better performance with only marginal variation .
results on the test set under two settings are shown in table 2 . our system outperforms all the models except for the one that trained on the original dataset ( mil - nd ) . in both settings , the system achieves 87 % confidence intervals of f1 score . supervised learning achieves 82 % e + p score and 94 % f1 scores . we observe that the automatic learning model , mil - nd ( model 2 ) achieves 94 % accuracy on the tests set under the two settings . with respect to recall , we observe that automatic learning performs better than model training on all three settings .
6 : entailment ( ent ) by ref and ref on the model compared to ref ( g2s - gin ) . the results are summarized in table 6 . ref significantly outperforms ref when compared to gen , and it is clear from table 6 that ref contributes more than ref .
3 shows the performance of models trained on the ldc2017t10 dataset compared to previous work by song et al . ( 2018 ) . our model outperforms all the other models except for the ones trained on ldc2015e86 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models with a large margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the model ' s performance by 3 . 5pp over the strong lemma baseline .
results are shown in table 3 . we observe that the model outperforms the models in terms of both sentence length and sentence length when compared to the baseline . the results are reported in tables 1 and 2 . in table 3 , we observe that g2s - gat exhibits significantly better sentence length than the model on both sets . finally , when compared with the baseline , the average sentence length is lower than the average of the two sets .
shown in table 8 , the fraction of elements that are missing in the output that are present in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . note that these tokens are used in the comparison of the reference sentences . they are used to compare the output of the two models that are already in the input graph . it is clear from table 8 that the use of token lemmas is important for the comparison .
4 shows the performance of our models trained with different target languages on a smaller parallel corpus ( 200k sentences ) . our model outperforms all the other models using features extracted from the 4th nmt encoding layer . our model achieves the best performance with 96 . 7 % recall on a single corpus .
2 : pos and sem tagging accuracy with baselines and an upper bound . accuracies are shown in table 2 . word2tag is the most frequent classifier using unsupervised word embeddings . it achieves the best upper bound score with a 96 . 41 % accuracy on baselines .
3 shows the performance of our model on the test set . our model outperforms all the other models except for those that do not use the word " parity " . our model obtains the best performance on all test sets .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as table 5 shows , when we only consider english as the language , our model obtains a 4 . 5 % boost in accuracy over the english baseline .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . on a training set 10 % held - out , the performance is 15 . 3 % better . on the training set 15 % more accurate , the difference between our performance is 14 . 3 % . on the other datasets , we observe that our training set contains 10 % more training instances .
1 shows the accuracies when training directly towards a single task . for pan16 training , we trained directly towards the single task of pan16 . the training results are shown in table 1 . gender and age are the most important factors in predicting the performance of a given task .
2 shows the performance of the classifiers when they are included in the conversation . the classifier trained on the pan16 dataset is able to detect instances of classifiers that are misclassified as having a negative effect on the conversation performance . when the classifier is included in conversation , the conversation splits into balanced and unbalanced states . when classifiers are added to the conversation context , the sentiment splits further into balanced states . this shows the effect of the word " gender " and " aggregation " .
performance on different datasets with an adversarial training is shown in table 3 . the performance on pan16 dataset is the difference between the attacker score and the corresponding adversary ’ s accuracy . in pan16 , the performance is significantly worse than those on the training data . sentiment and gender are the most important factors in the performance of the conversation , we observe that when the conversation is accompanied by a gender - neutral context , the conversation becomes easier to detect .
6 shows the accuracies of the protected attribute with different encoders . embedding leaky is easier than embeddings , and the performance gain is less pronounced for rnn .
3 shows the performance of our model compared to previous work on the topic . our model outperforms all the models except for the ones using finetune features . the results are summarized in table 3 . we observe that our model achieves the best performance on the two datasets when trained on the same training set . finally , we observe that this model performs better on the training set than the other two models . it is clear from the results that our approach is superior on both datasets .
3 shows the performance of our model compared to other models trained on the same dataset . our model obtains the best performance when trained on a single dataset . we report the average time taken to train the model , and the average number of seconds taken to complete the task . we also include the number of iterations in our model ' s training schedule , which we consider as a metric to compare against previous work on the dataset .
3 shows the performance of our model compared to other models trained on the same dataset . our model outperforms all the other models except ama , full time and amafull time by a significant margin . we also observe that our model performs better on both datasets when compared to the original ones .
3 shows the bleu score on wmt14 english - german translation task compared to previous stateof - the - art models . for reference , we show the performance of our model with respect to tokenized bleus . as expected , our model outperforms all the other models except for sru , which is more stable and requires fewer training steps .
4 shows the performance of our model on squad dataset compared to previous work on the model of wang et al . ( 2017 ) . as expected , our model obtains a higher match / f1 score than other models that use the same parameter number of base . the results published by wang et al . , ( 2017 ) indicate that our model is able to handle the task at hand without sacrificing too many parameters .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that lstm * denotes the reported result lample et al . ( 2016 ) . further , it can also be observed that lrn performs better than gru and sru in both ner tasks . as shown in table 6 , the parameter number of lrn is significantly lower than the performance of gru . the performance of sru is further improved by the use of additional features .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . the results are shown in table 7 .
results are shown in table 1 . word embeddings are used for system retrieval and system rerieval . sent attention is primarily used for word analogy task , word attention is used to improve the performance of both systems . the word attention methods ( mtr , mtr , r - 2 ) are effective for both systems when the attention is applied to the system . in combination with word attention , word attention and sentence learning methods ( emtd ) are beneficial for all systems except the systems . word attention alone is beneficial for the system ,
4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is highlighted in bold , with statistical significance marked with ∗ for ∗ and ∈ for all systems .
3 shows the performance of all models trained on the corpus dataset . our model outperforms all the baseline models except for the ones trained on docsub . the results are summarized in table 3 . for corpus , we observe that our model performs better than the baseline on three of the four datasets . for docsub , we see that the performance obtained by our model is comparable to that of the baseline model .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the ones trained on docsub . the results are summarized in table 3 . for corpus , we observe that our model performs better than the others on all three datasets except the one on which it is trained .
3 shows the performance of all models trained on the corpus dataset . our model outperforms all the baseline models except for the ones trained on docsub . the results are summarized in table 3 . for corpus , we observe that our model performs better than the baseline on three of the four datasets . for docsub , we see that the performance obtained by our model is consistent across all three datasets .
3 shows the performance of our models on the metric and schemas metric . our model achieves the best performance on both metric metrics . on the metric metric , we observe that the maxdepth of our model is comparable to that of the other two baselines , namely , docsub and hclust . on the other metric , our model achieves a performance improvement of 1 . 78pp over the previous state of the art on the metric metric .
3 shows the performance of our model compared to the baseline on every metric . our model achieves the best performance on all metric metrics , except for the metric of depthcohesion , which is comparable to the maxdepth of the other two baselines . we also observe that our model achieves a better performance on both metric metrics when compared to other baselines , such as the dsim metric .
performance ( ndcg % ) comparison for the experiments of applying our nlst principles on the validation set of visdial v1 . 0 . we note that the enhanced version of the nlst model performs better than the original lst model .
performance ( ndcg % ) of ablative studies on different models on the validation set of visdial v1 . 0 is shown in table 2 . using p2 indicates the most effective one ( i . e . , hidden dictionary learning ) . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - prec model outperforms all the other models except for the one that uses bert .
3 presents the performance of our model on the test set of ruse . the results are summarized in table 3 . our model outperforms all the test sets except for the ones that do not have ruse features . for example , our model obtains the best score on all test sets with a minimum of 0 . 5 .
3 presents the bagel and sfhotel scores on the setting of table 1 . the results are summarized in table 3 . the baseline bleu - 1 scores are significantly better than the baseline scores on both sets . as expected , the scores obtained by baselines are significantly worse than those obtained by the baseline . table 3 shows the performance of the baselines on all sets .
3 presents the metric and baseline scores of the models trained on the setting . the results are summarized in table 3 . the summaries are presented in bold . leic ( lei et al . , 2017 ) and spice ( 2017 ) show that the combination of elmo and p < 0 . 7 and 0 . 8 achieve the highest score on the three metrics . retrieving the summaries of the scores from the baseline improves the performance for all models except those using the word " mover " .
results are shown in table 3 . we observe that for all models except sim , our model performs better than the best on all three metrics except for the one where it relies on the word embeddings . as expected , when using the shen - 1 analogy , the model performs worse on sim than on pp .
3 presents the results of our model on the transfer quality and semantic preservation datasets . our model outperforms all the other models on both datasets with a large margin . the results are summarized in table 3 . we observe that the semantic preservation dataset is more stable than the sentiment preservation dataset with a small margin for error . semantic preservation datasets are further improved with the addition of semantic tags and semantic tags .
5 presents the results of human validation . the results are shown in table 5 . it is clear from the table that human evaluations have a generally positive effect on the performance of the model when compared to the machine evaluations . moreover , the percentage of machine and human judgments that match match sentence quality is high , indicating that the model performs well on sentence quality .
results are shown in table 4 . we observe that for all models except sim , our model performs better than the best on all three metrics except for the one where it relies on para + lang .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references are restricted to 1000 words . our best models achieve higher bleus than prior work at similar levels of acc ∗ . the results on simple - transfer are shown in tables 6 and 7 . these results are not included as they are worse than those by other classifiers in use . we also observe that the use of classifiers like delete / retrieve and multi - decoder underperforms previous work by a significant margin . finally , we observe that using the best model , yang2018unsupervised , is a better model than using any other classifier in the task .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be disfluent , followed by rephrase . it is observed that the frequency of repetition tokens is relatively low , indicating that the repetition tokens are more likely to contain errors .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - disfluent ) . the fraction of tokens predicted as errors is shown in table 3 . relative frequency of errors predicted as correct is reported in tables 3 and 4 , respectively .
results are shown in table 4 . we observe that when text + innovations are used as rewards , the model achieves the best performance on all test sets . in addition , we observe that using text and innovations as rewards improves the model ' s performance on the test set .
2 compares our model with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance when we agree to disagree with a topic and when we discuss it with the opposing party .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . compared to previous methods , burstysimdater performs significantly better .
3 : accuracy ( % ) of our model with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details . our model obtains a 3 . 2 % improvement on the performance of the word attention alone .
3 shows the performance of all models on the validation set . our model outperforms all the other models except for the ones that do not use the automatic embeddings feature . the results are shown in table 3 .
3 shows the performance of our method on event identification . our method outperforms all the other methods in terms of both event identification and event classification . for event identification , we use cross - event analysis ( cao et al . , 2018 ) . the method obtains an absolute advantage over all the methods except for the one that requires a single argument to classify the event . all the methods used for this analysis fail to achieve the best result .
results are shown in table 4 . all models except for the ones shown in bold displayed in bold have achieved the best performance on the test set . all except for english - only - lm are better than the others on every metric except for dev perp , which shows the performance of the model when trained on a single dataset . note that all models trained on the single dataset are comparable in terms of performance on all metrics .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data in the training set . we observe that fine - tuning achieves the best results on both the dev and test set .
5 shows the performance of our model on the dev set and the test set , compared to the model trained on the monolingual and code - switched gold sentence in the standard set . the performance of the model is shown in table 5 . it obtains the best performance when trained on both the dev and test set .
results for the three eye - tracking datasets are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features compared to type - free gaze features trained on the conll - 2003 dataset . note that type - based gaze features significantly improve recall ( p ≤ 0 . 05 ) , f1 score ( p > 0 . 01 ) and recall ( p ≤ . 01 ) . note that the improvement in precision is statistically significant , as shown in fig . 7 .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in precision ( p ≤ 0 . 01 ) over the baseline , as shown in fig . 5 . the improvement is statistically significant , with the exception of type combined features , which show statistically significant improvement .
results on belinkov2014exploring ’ s ppa test set . it uses syntactic - sg embeddings obtained by running autoextend rothe and schütze ( 2015 ) on glove . the results on the original paper are shown in tables 1 and 2 . we use the syntactic embedding of skipgram vectors retrofitted to wordnet 3 . 1 . further , we use syntactic features extracted from the original work ( lin et al . , 2014 ) . syntactic features are derived from the wordnet , verbnet , and wordnet datasets , they are used in wordnet as tokens , and are used to embed synset embedding . as shown in fig . 1 , syntactic information embedding is the most important part of the semantic embedding , and it is used to train wordnet models .
performance of our system with features coming from various pp attachment predictors and oracle attachments is shown in table 2 . the results from table 2 show that using oracle pp as dependency parser gives a significant boost to ppa acc . and ppa score . also , the results from our system are slightly better than those obtained using lstm - pp .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 . when the sensitivity is increased , the model exhibits a lower ppa acc .
2 shows the performance of our model when combined with domain - tuned captions . the results are shown in tables 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) with marian amun shows that the domain tuning improves the model ' s interpretability .
results are shown in table 4 . subdomain - tuned subs1m outperforms all models except for those using mscoco17 , which shows that domain - tuning improves the performance of the models when en - de tuned .
4 shows bleu scores in terms of automatic captions ( the best ones or all 5 ) . all results with marian amun are shown in table 4 . the results with en - de confirm that our model outperforms all the models except the ones with the best ones .
5 compares the performance of our strategies for integrating visual information with en - de and dec - gate . all results using multi30k + ms - coco + subs3mlm are shown in table 5 . we observe that the enc - gate approach outperforms all the other approaches except for the one using directlink .
3 shows the performance of subs3m compared to subs6m on en - de and on flickr16 . the results are summarized in table 4 . we observe that the combination of visual features and multi - lingual features improves the performance for all models except for those using text - only features . moreover , the clustering performance of all models is comparable to that of the single - language model , i . e . that the visual features alone do not improve performance .
performance on mtld compared to en - fr - ht is reported in table 1 . the results are reported in tables 1 and 2 . table 1 summarizes the results of our model on the test set . our model outperforms all the test sets except for the one in which it relies on the word embeddings .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 3 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) for the systems are shown in table 5 . the system reference obtains ter scores comparable to those of other reference systems .
results on flickr8k are shown in table 2 . the model trained on chrupala2017representations is comparable to the one trained on the same dataset . the results are summarized in tables 2 .
results on synthetically spoken coco are shown in table 1 . our model outperforms the previous stateof - the - art models in terms of recall @ 10 and mean mfcc score .
1 shows the effect of these classifiers on the performance of the original on sst - 2 . we report further examples in table 1 . for example , orig ( rnn ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . similarly , rnn ( which shows the ellipsis of the dialogues ) turns on a on ( in the margins ) . these are examples of the ways in which the different classifiers can be used to express hate speech . note that for rnn , the edges are edges and the curves are in the margins .
2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . these numbers indicate that there is no need to worry about the accuracy of the word choice .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . the results indicate that the sentiment increases when negative labels are flipped to positive , as shown in fig . 3 . similarly , sentiment decreases when the sentiment is flipped to negative .
results are presented in tables 1 and 2 . the results are summarized in table 1 . we observe that when we investigate positive and negative aspects of our model , our performance is significantly better than those of negative aspects . when we investigate negative aspects , we find that our approach is more effective than positive ones . in addition , when we compare our model with other approaches , our results are consistently better than the ones of positive , negative and positive .
