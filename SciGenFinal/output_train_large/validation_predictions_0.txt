< r > < c > batch size < c > throughput ( instances / s ) inference < c + throughput / s inference < c > throughput / sinference < b > inference / batch size < c + + throughput < batch size > batch sizes < c * throughput > inference ( instance / s ) inference > batch sizes > c > intrinsic < c < throughput , instances , inference < / c > training < d > training / c > tensorflow training < tensorflow training < c ) training < c4 throughput ( < c > ) training < s > < s ) < s > < c2 > iter < c > recur < c } iter < c > > fold < c . iter < fold < c > . recur 3 > fold < e > < a > 1 < c1 19 . 2 < c0 81 . 4 < c6 81 . 9 < c ] 16 . 5 < c – 2 . 0 < c - 4 . 8 < c > [ 9 . 0 < r > 3 > 10 < c > < 49 . 3 < c > < / 217 . 9 < c > ) 52 . 2 < c > 4 . 0 ( c > 4 . 2 < / c > 37 . 5 3 . 2 ( c ) 37 . 1 < r > < / c > 25 < c & 72 . 1 < c + 269 . 9 < / c > 61 . 6 < c > ) 5 . 5 < c > ( c > 3 . 6 3 . 7 < c > ( r > 54 . 7 < cap > table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorshop ’ d iterative approach , combined with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< r > < c > batch size < c > < / c > throughput ( instances / s ) balanced < c > throughput / s linear < c + + throughput - instances - s linear ( c > ) linear < d > linear < d > < d > ) linear < e > < r < c > 1 < c > [ 46 . 7 < c + 27 . 3 < c ) 7 . 6 < r > > < c < 10 < c > < 125 . 2 < c – 78 . 2 < c + 22 . 7 < r > 3 < c < c > 26 < c1 < c * < c2 < r > . < c4 < c . < c > . < d . < d > . < e . > < s . < r . < cap > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . the linear dataset exhibits the highest throughput .
< r > < c > [ bold ] representation < c > < / c > [ ] hyper parameters filter size < c + [ bold ] hyper parameters num . feature maps < b > [ c > hyper parameters hyper parameters activation func . < a > ( bold ) hyper parameters l1 reg . < b0 > [ fold ] hyper parameter l2 reg . . < bold > [ l2 ] hyperparameter l3 reg . [ c1 ] [ l3 ] [ c2 ] [ f1 ] l4 reg . l2reg . < c1 > [ dold ] l5 reg . learning rate < c2 > [ gold ] hyper parameters dropout prob . < d > [ sold ] f1 . ( avg . in 5 - fold ) with default values < c ] [ [ bolder ] f2 . ( av g . in 6 - fold ) . < d4 < c > < c > f1 . 0 < c . 0 . 79e - 02 < c > . 0 . 87 < c < c > 0 . 89 < / c > < cap > sigmoid < c4 4 - 5 < c > 1000 < c + + softplus < c6 1 . 15e + 01 < c0 1 . 14e - 03 < c > 1 . 13e - 04 < c > , 1 < c > [ c > 73 . 34 < / c > 74 . 49 < r > ) < c * sb < c3 > 4 - 3 < c > ( c > 806 < c & sigmoids < c ) 8 . 13c - 02 3 > 1 ( 79e + 03 < c > ) 0 . 86 < c = 72 . 83 < c > > [ bol ] 75 . 05 < r > ] < cx ud v1 . 3 < 3 > 5 < c • 716 < col > softplus 3 > 2 . 66e + 00 < c – 9 . 63e - 04 3 > 3 < c $ > 68 ( c > 69 . 57 < cap > table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations .
< r > < c > [ bold ] relation < c > < [ bold ] best f1 ( in 5 - fold ) without sdp < c > < / bold > < bold ) best f2 ( in 2 - fold , with sdp < / c > < sdp > < p > f1 < c < c > ( in 4 - fold ) , with sdk < c > [ bolder ] diff . < sdk > < t > < d > < l > < n > < a > > < c + + < c < c > c > 60 . 34 < c > [ 80 . 24 < c + + 19 . 90 < r > > < c } model - feature < c4 48 . 89 < c6 70 . 00 < c & + 21 . 11 < r > < r > [ part_whole < c1 > 29 . 51 < c2 70 . 27 < c $ + 40 . 76 < r > < / c > topic < c0 45 . 80 < c > > 91 . 26 < c > , + 45 . 46 < r & < c ) result < c > ( c > 54 . 35 < c > ) 81 . 58 < c = + 27 . 23 < r < c > compare < c / 20 . 00 < c > < 61 . 82 < c – + 41 . 82 < / r > 3 . 0 < cap > < m > macro - averaged < c * 50 . 10 < c ( ) 76 . 10 0c > + 26 . 00 < / cap > > table 1 : effect of using the shortest dependency path on each relation type .
< r > < c > [ empty ] < c > c - f1 100 % < c + f1 50 % ( c > f1 100 % ) < c - f2 50 % < c > f2 100 % < c > a - f3 100 % < / c > y - 3 : y < italic > r < / italic > , - 1 < c * 54 . 71 < c $ 66 . 84 < c > < 66 . 81 < c + + 26 . 28 < c1 > 37 . 00 < c . 37 . 35 < c , 37 . 40 < c > , 47 . 92 < r > > < c : y - 2 : y < italic > c < / italics > - 3 < c < 51 . 32 < c > [ 66 . 71 < / c > 26 . 18 < c > ) 37 . 18 > 37 . 19 < c > ( c > 35 . 86 < c ) 46 . 64 < r > < c > ( c ) y - 1 : y - 3 , y < tic > c < / nic > - 2 < c & 53 . 31 < c > ] 26 . 72 < c > : 26 . 65 < c6 26 . 66 < c2 > 36 . 80 < c4 37 . 80 < c > < 46 . 92 < r > ( r > y > 3 : c < / ic > - , 3 < b > 53 . 71 < c > [ 66 . 71 > c > 56 . 81 < / c > < 26 . 82 < c > < / c > 38 . 90 < / c > , 46 . 90 < r > < cx y - 4 : y < / italica > - 4 < c • < bold > 54 . 58 < / bold > < b + 67 . 66 < / c > < bold > [ 30 . 22 < / bold > . < c > > < bold < 40 . 30 < / bold > < / < c > . < bold > < 40 . 90 . < / bold > < / c > < < bold > < / 38 . 90 < / bold > 3 > < c > < bold + 50 . 51 < / bold < / r > > c > y – 3 : x < ic > s < / italic > - 1 < / c > [ 53 . 01 < c ] [ 66 . 31 < / c + 66 . 68 < c0 26 . 17 < c = 35 . 78 < c — 35 . 53 < c – 46 . 63 < r > [ c > c > y - 1 < b >
< r > < c > [ empty ] < c > paragraph level acc . < b > paragraph level c - f1 < c4 < c2 > paraph level r - f2 < c3 < c1 < d4 < e3 < e4 < a3 < a2 < e5 < e2 < a1 < e8 < e7 < e9 < e10 < e11 < e12 < e17 < e18 < e14 < e15 < e20 < e16 < e19 < e - 18 < a > essay level f1 < tay level a1 < f1 < c > essays level a - f0 < e13 < e1 < a17 < c > ) essay level f1 < e17 < e18 < e10 < e11 < e4 < e8 < e3 < e9 < e2 < e > < e > [ c > ] c > ( c > 100 % < c ) < c + + 100 % < / c > 10 % < e0 < a < e < ee > 0 < e & e > 1 . 29 < e00 < e . 00 < c > . 100 % 100 % > c > 50 % < d > < 3 > [ e17 ] < e25 < eem > < t > [ emty ] > c < 100 % < c > 100 % [ e00 ] < i > [ mst - parser < c > < c > [ wempty ] [ c > mst - parser < c ] 31 . 23 < c + 0 < c0 6 . 90 < c6 0 < c > 2 . 30 < c < 0 < a + 1 . 28 < c & 2 . 32 < c > > 2 . 17 < r > < c > . empty > [ p > [ apty ] [ e > ] empty ] < c . < c > : [ c + + ] < p > < w > [ ec > [ hemty ] [ empty ] ] < c > ] < c > [ c > < hem > < m > < empty > [ c ] 22 . 71 < c • 2 . 72 < c ] [ 12 . 34 < c = 2 . 03 < c $ 4 . 59 < c > ( c > 3 . 32 3 . 69 < c > < / c > 6 . 69 < / c > . . . empty ' < c * [ empy ] < t
< r > < c > [ empty ] < c + stagblcc < c > lstm - parser < r > < c > < essay < c > < / c > 60 . 62 ± 3 . 54 < c > 9 . 40 ± 13 . 57 < r > < / c > < c > paragraph < c > 64 . 74 ± 1 . 97 < c > ( c > 56 . 24 ± 2 . 87 < cap > table 4 : c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< r > < c > train < c > test < bold < c < c > [ bold ] test < d > [ cold ] < c0 < c1 < c2 < c > < c > test < c + + [ c > tgen < c4 < c > ) [ bold ] bleu < c6 [ bolder ] nist < c > : [ bol ] nist < c > . [ bond ] meteor < c . [ born ] met eor < bol > [ fold ] rouge - l < c > [ bold ) cider < c > , [ bod ] cider < c , cid er < c3 > [ gold ] add < c > ( bold ] [ bold ] ] miss < c > > [ bany ] wrong < c > < / c > 1 . 8181 < c < / c > 6 . 0217 < c - c > 36 . 85 < c > 5 . 3782 < c + 35 . 14 < c & 55 . 01 < c = 1 . 6016 < c / > 00 . 34 < c – 09 . 81 < c / 00 . 15 < c ) 10 . 31 < r > < c ’ s original < c • [ b • ] cleaned < c * tgen < c > 39 . 23 < c was 6 . 0117 < a > 00 . 00 < c 1 . 0118 < c < c 0 . 0119 < c 2 . 0120 < c 4 . 0121 < c 6 . 0319 < a > 36 . 97 < c > ] 55 . 52 < cx 1 . 7623 < c > 01 . 40 < c $ 03 . 59 < c ] [ 00 . 07 < c and 04 . 05 < r > > < c } original < c & [ bok ] cleaning < c : tgen + < c ( ) 40 . 25 < c — 6 . 1448 < c ' > 37 . 50 < col > 56 . 19 < t > 0 . 8180 < c9 00 . 21 < c > 01 . 99 < c = = 00 . 05 < / c > 02 . 24 < r & < st > original < st > < / bold ’ cleaned > c > sc - lstm < c64 23 . 88 < c96 3 . 9310 < c ] 32 . 11 < c
< r > < c > [ bold ] dataset < c > [ bold ] part < c > < c > ( bold ) part < c > [ cold ] mrs < c < bold , mrs > [ c ] < bold > mrs ( c ) < c2 > ( cold ) ( c > 2pt / 2pts ) < 1 . 5pt / 1pt ) < 2pt < c > original < c1 > train < c6 4 , 862 < c > [ 42 , 061 < c > < / c > 17 . 69 < r > < c } original < c1 > dev < c + 547 < c > ) 4 , 672 < c & 11 . 42 < r > > < c ’ original < t > 1 , 132 < c > ( c > 4 , 694 < c > . 11 . 49 < r > [ < c + + [ 0 . 5thpt / 3pts ] < 1pt ] cleaned < c * train < a > 8 , 362 < c > > 33 , 525 < c > , ( 0 . 00 ) < 10 . 00 < r > 3 > 0 . 05pt / 0 . 0 . 1pt > < 3 > < 0 . 2pt > cleaned < / c > test < b > 2 , 133 < c < 4 , 299 < c = ( 0 . * ) < 3 . 00 . < c4 < 3pt / 4pt ] < c0 > < t1pt / 2pt [ 0 . * ] < t2pt / 5pts > < s1pts > < s2pt > < 1stpts < s3pt > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< r > < c > train < c > test < bold < c < c > [ bold ] test < d > [ cold ] < c0 < c1 < c2 < c > < c > test < c + + [ c > tgen < c4 < c > ) [ bold ] bleu < c6 [ bolder ] nist < c > : [ bol ] nist < c > . [ bond ] meteor < c . [ born ] met eor < c > [ fold ] rouge - l < c > [ bold ) cider < c > , [ bod ] add < c > > [ bor ] cider < c , [ bon ] miss < c & [ bany ] wrong < c > ( bold , wrong < bor > original < c ) < c - lstm ( c > 39 . 11 < c > ] 5 . 6704 < c / > 36 . 83 < c > < / c > 50 . 02 < c < / c > 04 . 27 < r > < c ] original < born > [ tgen ] original [ c ] tgen + < c : 8 . 5565 < a > 45 . 07 < c + 7 . 7188 < cx 41 . 17 < c3 > 2 . 2253 < c5 < c8 < c9 < c7 > 6 . 2252 < 3 > 00 . 14 < c $ 0 . 00 < c00 > 01 . 00 < / c > 01 . 00 < c > 01 . 01 < c 0 . 05 < c 1 . 25 < c < c * 1 . 2256 < c 2 . 2681 < c 4 . 2257 < c 6 . 2621 < c10 < c15 < c16 < c19 < c18 < c20 < c12 < c17 < a1 > 1 . 2671 < c 10 . 5872 < c 9 . 5870 < c 16 . 73 < c14 69 . 73 < c > 2 . 2781 < c < 00 . 04 < c = 01 . 75 < c – 00 . 01 0c > 02 . 80 < r > > < c ’ original < / c > < / bold ] + original < t > sc - sstm < c ( ) 39 . 11 < c href 5 . 6704 < c4 36 . 82 < c100 < c30 < c36 . 84 < c200 < c01 <
< r > < c > [ bold ] training data < c > < c > ( bold ) < bold > [ cold ] add < a > [ aold ] < c > ] [ bold ] miss < c > [ bold ] + < c > [ aust ] correct < aust > [ cap > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set . < r < c > 0 < c > 22 < c + 0 < c > ) 14 < r > > < c & cleaned added < c < 0 < a1 < c1 < a4 < a2 > < a3 < c > . 23 < c > : 0 < b > 23 < a0 < c / > < c4 < c . < a > . < r > . < c : < c0 < a < c > . 2 < r > < / c > 3 < c < a . < bold > < e . < cap > table 4 . results of automatic error analysis for tgen based on a panel of 100 examples from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) .
< r > < c > [ bold ] < c > [ c > b < c < bold ) < c + + < c1 < c2 < c > , < c3 < c4 < bold > < c > c3 < b < c + < c0 < c > . b < r < c > seq2seqk ( konstas et al . , 2017 ) < 3 > - < c6 22 . 0 < r > < c & graphlstm ( song et al , 2018 ) < c > < - < c > 23 . 3 < r > < / c > gcnseq ( damonte and cohen , 2019 ) < r – < c - < c . < c * < c } < c < c , < c > ) < c ( c > 24 . 4 < r > . < c ) dcgcn ( single ) < 3 . 9 < c > ( c > 25 . 9 < r > < / c > dcgcna ( ensemble , 2016 ) < a > - < c - 23 . 2 < r > > < cg2str ( flanigan et al . 2016 ) < a > all < c : 23 . 6 < r / > < c / snrg ( song and cohen , 2016 , 2017 , 2018 , 2020 ) < b > all < c < 22 . 4 > < r > < c > pbmt ( pourdamghani et al , ' 2016 , 2017 ) , < c > : all < c < 26 . 9 3 > < r & c > tree2str < a < c > [ c > 23 . 5 < r4 < c ] snrg 2str ( lanigan and cohen et al , . 2017 ) . < c — all < r + 25 . 1 < r > [ c > 3 . 2m < c > ] < c < / c > snrg < a ( lansing et al . ' s , 2015 ) > c > 0 . 2 m < c $ 27 . 4 ( r > 0c > 1 . 2 m < c $ c > 2 . 2 ( c $ 2m < c $ 28 . 2 < / r > 1c > 4 . 2 < c > 0 . 1m < 1 . 1 * < r0 < 2 > 28 . 0 > < 1m 3 . 0 < r < c > 2m
< r > < c > [ bold ] model < c > < c > [ c > < bold > < cap > # p < c > [ c + + ] < c + + < c < c > # c > c < c1 < c < c2 < c3 < c4 < c * < c + < c0 < c > . < c > , seq2seqb ( beck et al . , 2018 ) < a > e < c > < / c > 28 , 4m < c > 21 . 7 < c > 49 . 1 < r > < c > dcgcn ( ours , 2018 ) < a > s < c > ) 28 . 3m < c > [ c > 27 . 5 < c > ] 53 . 5 < r > < a > seq 2seq b ( beck & al . , 2017 ) < a > s < c > 28 . 4m ( c > 26 . 6 < c $ 52 . 3 < s > < r > ) < c - seq3 ( ours ) < b > [ a > [ e ] 19 . 1m < t > 23 . 9 < c % 52 . 5 < / r > < / c > seq2 seqb < bold ] < a ( beck and cohen , 2017 ) < c ) < e > [ cap > [ gold ] 20 . 5 m < c : 27 . 3 < c > < 53 . 3 < / r > ( cap > table 2 : main results on amr17 . gcnseq < c . < c > : s < c > [ fold ] 19 . 1 m < bolder ] 27 . 9 3 > 57 . 2 < s ” < c ’ s < a ’ s ’ and “ e ” denote single and ensemble models , respectively . .
< r > < c > [ bold ] english - czech # p < c > [ bold ] [ czech ] type < d > [ cold ] [ dold ] < c + + [ c + + ] [ fold ] ( bold ) english - german < bold , english - english # p < c > [ bold ' english - french b < c4 [ c + c > ] bold ] . english - british # p > [ c > english - bold . english - cambridge < c > < c > ( c + + ) [ c $ ] english – czech type < c $ < c > < c > c $ < c $ > < d $ > c < c + gcn ( bastings et al . , 2017 ) < b > single < c – - < c1 > 12 . 2 < c > - < 3 > 16 . 1 < c0 - < 5 > 7 . 5 < c < r > 3 . 7 < c - < t > < r > < c * birnn - gcn < c & < c2 > < 3 . 3 < c > ) < c . < c > < / c > 4 . 4 < c > . < r < c > < birnc - gcng ( basting et al , 2017 ) < bastings et al . 2017 , 2018 , 2017 , c > < best ] single < a > - ( c > 10 . 1 < c > - < 1 > < 2 . 8 < c < / c > 9 . 6 < c > ( c > 6 . 3 < c < < c > , < r > < c > pb - smt ( beck et al , ' 2018 , 2018 ) < c ) single < c > - . c > 11 . 8 - < 2 > 43 . 2 < / c > < 3 > < 4 . 8 < c > . 8 . 6 3 > 36 . 4 3 < c < c > > seq2seqb ( beck and al . , 2018 ) 3 > single 3c > 41 . 4m < c6 15 . 5 3 > 40 . 8 = c > 39 . 1m < 3 > 8 . 9 < c > [ 33 . 3m < 4 > 8 . 1 m < 5 . 9 3 > 33 . 8 m < r > > < c = ggnn2sequ ( beck & al . , 2017 )
< r > < c > [ italic ] block < b > [ cap ] < c > [ cap > < cap > [ bold ] < a > [ r > [ c > ] < cap ] n < bold > [ sold ] [ cap ] ] < c < [ c ] < d > [ hold ] m < c ] b < c > < c < c2 < c4 < c < c1 < c > . 1 < c . 1 < c > 19 . 6 < c - 49 . 4 < r < c > ( c > 3 < c > ) 19 . 0 < c + + 50 . 5 < r > < c ) < c3 < c $ 1 < a > 20 . 0 < / c > 49 . 3 < r > > < c + 1 < 3 > 2 < c , 19 . 8 < c > , 49 . 1 < silence < c * < c & < c } < c0 1 < best < cat > 1 < t > 6 < c > [ 21 . 8 < c > 51 . 7 < selence < solence > < a3 < a1 < a2 > [ tank ] 23 . 5 < c > < 53 . 3 < r > ( r > > 1 < sold ) 23 . 4 < c < 1 < r2 < c > < 1 < 5 > 21 . 2 < sulence > 51 . 0 < / c > 0c > 4 < tank > 23 . 6 < c1 1 < 2 > 22 . 8 . c > 51 . 7 < r + < c6 1 < hold > < socious < cac > < tink > < 3 < tickets < c : the effect of the number of layers inside dc [ cap ] [ cap > table 4 : the effects of the number of layers in dc [ cap ] table 5 : the influence of the numbers inside dc
< r > < c > [ bold ] gcn + rc ( 2 ) < c > b 16 . 8 < c > c 48 . 1 < c > < / c > < rc ( 4 ) < c > < [ cold ] 20 . 7 < c > : [ bold ] gc n + rc + la ( 3 ) < a > b 18 . 3 < c6 < c > . + rc - la ( 4 , 5 , 6 ) < c > 19 . 9 < c > [ c > b 48 . 7 < / c > + rc ( 4 , 6 , 7 , 8 ) < [ c > 18 . 4 < c > ) < c > ] [ bol ] 21 . 2 < c > ( c > 25 . 5 < c . < cap > * rc ( 6 , 6 ) , [ c > 21 . 3 3 > 50 . 8 < / c > - rc ( 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 16 . 7 , 12 , 13 , 17 , 15 < c > , [ col ] 22 . 0 < c2 > 52 . 6 < r > < c1 > [ caln ] 50 . 7 ( 10 , 9 ) < 3 > [ rc ( 10 ) ] 21 . 1 ( c > 49 . 8 < c > < + rc + la ( 10 ) > 21 . 7 < c > ) [ col ] 52 . 9 , 11 < c > > < col ( 10 ) , [ c > ] 22 . 9 ( r > 53 . 5 , 11 ) , [ cap > 22 . 8 , c > 55 . 4 , 12 < c ] dcgcn2 ( 9 , 8 , 10 ) < b > 23 . 9 < c > 53 . 0 , < c + + dcgcgcn3 ( 9 ) 3 > 24 . 8 * c > 54 . 7 3 > dcgcng3 ( 27 , 8 ) , < c < 24 . 9 3 > 56 . 7 1r > dc gcn1 ( 28 ) < d > 24 . 8 < c > 54 . 8 3 > < r > > dcgcnf2 ( 18 ) < t > 24 - 2 < c + 54 . 4 < c > > dcgn4 ( 36 ) < st > [ dold ] 25 . 0 < c > [ bold ' 55 . 2 < cap > table 6 : comparisons with baselines . + rc denotes gcns with residual connections . + c denotes gc
