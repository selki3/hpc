2 shows the performance of our recursive framework on the large movie review dataset . the approach performs the best on inference with efficient parallel execution of tree nodes , while the iterative approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left .
2 shows the performance of the max pooling strategies for each model with different number of parameters . the performance of conll08 is shown in table 2 . softplus performs better than sigmoid and softplus in all model variations . the hyper parameters activation func . and l2 reg . achieve the best performance . retrieving the hyper parameters from the same representation consistently improves the model performance .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that macro - averaged models achieve the best f1 ( in 5 - fold ) without sdp dependency path . the results of using this approach are shown in table 1 . we can also see that the relative dependency path improves the f1 by a significant margin .
results are shown in table 3 . y - 3 outperforms y - 2 in terms of f1 and f1 score .
results are shown in table 1 . the results are presented in terms of the paragraph level and the number of instances in which the word embeddings are tested . in addition , the results of the best performing part - ofspeech method are reported in table 2 . all the results indicate that the method performs well on the validation tasks .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . the lstm - parser shows lower performance than the majority systems .
results are shown in table 1 . original and original results are presented in bold . the best performing system is sc - lstm , which is a state - of - the - art system that can be used to improve the interpretability of the word " original " . it is clear from the results that the original has a lot of performance improvement over the original .
statistics for the original e2e data and our cleaned version are shown in table 1 . the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . as expected , the original and the cleaned versions have significantly higher mrs than the original , but the difference is less pronounced for the cleaned version .
3 shows the original and original test scores . the original scores are shown in table 3 . original scores have been consistently better than the original scores . they are comparable in terms of bleu score , nist score , rouge score , and meteor score . original results are summarized in table 4 .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found significant numbers of errors ( added , missed , wrong values , slight disfluencies ) in the training data , as shown in table 4 . the number of errors we found was small but significant ( sometimes larger than expected ) .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous state - of - the - art models . all models perform better than all the other models except for seq2seqk ( konstas et al . , 2017 ) and snrg ( song et al . 2017 ) .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points . the results are shown in table 2 . as expected , the size of our model is small , but larger than seq2seqb .
results in english - czech are shown in table 1 . the best performing single model is ggnn2seqb , the results are reported in tables 1 and 2 . in english - german , the best performing single model is published in bow + gcn ( 2018 ) . the difference between the english - language and german - language models is minimal , however , as the difference in performance between the single and the multi - language model is significant . we observe that the single model performs better than the other two models in both languages .
5 shows the effect of the number of layers inside dc on the quality of the layers in table 5 . we observe that for every layer of dc , there are three layers that contribute to the overall quality of our system .
6 shows the performance of our models with residual connections . rc + la shows that the gcn has residual connections with multiple gcns . with residual connections , the average gcn is closer to its original state of the art state - of - the - art .
model 3 shows that dcgcn outperforms all the other models in terms of bias metric , showing that the high precision of the dcgcnn model leads to better results for all models .
8 shows the ablation study on the dev set of amr15 . the results are shown in table 8 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block , whereas { i , 5 } dense ones denotes removing them in the dense blocks .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the global coverage mechanism improves the results for both the standard and the gold encoder .
7 shows the performance of initialization strategies on probing tasks . our paper shows that the best performing initialization strategies are subtense , length , and topconst .
can be seen in table 4 the results of our method outperform the best state - of - the - art methods on every metric . for example , in the low - supervision settings , our h - cmow model obtains the best performance on all metrics .
1 shows the performance of our cbow model compared to other methods . our model outperforms all the other methods except for the one that cmp uses . cbow even outperforms sst2 and sst5 in terms of mrpc score , while sick - e performs better on mpqa score .
3 shows the relative change from hybrid to hybrid over unsupervised downstream tasks attained by our models . the results are shown in table 3 . the cbow model outperforms both hybrid and cmp in all downstream tasks .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the best performing initialization strategies are sst2 , sst3 and sst5 .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are obtained on the sts12 and sts14 tasks , respectively .
observe that cbow - r outperforms all previous methods in terms of depth and subtraction . the results are summarized in table 1 .
subj and sick - r are comparable in terms of mrpc performance . our cbow - r model outperforms all the other methods except for sst2 , which is superior in both categories .
3 shows the e + and per scores of all systems trained on the same dataset . our system obtains the best results ( out - of - the - box ) with a minimum of 50 % org and 50 % per scores . supervised learning improves the results for all systems except for those using name matching .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 score . the results are shown in table 2 . name matching improves the generalization ability of the system , it boosts e + p and f1 scores , supervised learning improves e − p and e + r scores ,
6 shows the results of ref and ref on the model compared to the original g2s - gin ( see table 6 ) . ref significantly outperforms ref in all but one of the cases where ref is used .
results are shown in table 1 . the models performed best on the ldc2017t10 dataset are highlighted in bold , indicating that the model performs well in low - supervision settings .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms the previous stateof - the - art models with a large margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the bilstm performs well on both development set .
results are shown in table 4 . the results are summarized in terms of average sentence length and average word diameter . in particular , the g2s - gin model had the best performance on the graph metrics , with an absolute improvement of 5 . 7 % on average compared to the previous state - of - the - art model .
shown in table 8 , the fraction of elements that are missing in the output that are in the reference sentence ( g2s - gin ) , for the test set of ldc2017t10 . it is clear from table 8 that the g2s models are better than the reference sentences in that their output is missing in many ways .
4 shows the sem and pos accuracy using the 4th nmt encoding layer . the results are shown in table 4 .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag has the best overall performance with a 90 % upper bound on baselines .
results are shown in table 4 . our system outperforms all the other methods except for the one that we used in table 1 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ , a difference of 2 . 2 % in accuracy on a training set with 10 % held - out .
1 shows the performance of our method when training directly towards a single task . the results are shown in table 1 . the performance of the word " task " is comparable across all training contexts .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced data splits . the classifiers trained on the pan16 dataset are named after the gender of the participants in the conversation , and the classifier trained on them is named pan16 . the classifier is trained on gender - neutral data , and is able to distinguish between balanced and balanced data splits depending on the conversation context .
performance on different datasets with an adversarial training is shown in table 3 . the difference between accuracy and leakage is the difference between the performance of the trained classifier and the corresponding adversary ’ s accuracy . sentiment and gender are the most important factors in predicting whether an object will reach the target . dial and gender - neutral features are important in predicting the task success .
6 shows the concatenation of the protected attribute with different encoders . embedding leaky is easier for both embeddings to perform , while the embedding is harder for rnn to perform .
results are shown in table 4 . the best performing model is lstm , which improves upon the state - of - the - art finetune on both wt2 and wt2 datasets . it achieves the best performance on both datasets with a minimum of 2 . 5x improvement on the original model . finally , it achieves the highest performance on the two datasets with an absolute boost of 2x on the final model .
results are shown in table 5 . the best performing model is the lstm , which takes the time to compute the acc andbert metrics . it achieves the best performance with a minimum of 260k iterations , which means that the time taken to compile the data is relatively low .
results are shown in table 4 . the results of our model outperform the best state - of - the - art models on all metrics , except for the one that is used in the amapolar time dataset . as expected , the results of this model are significantly better than those of the previous models .
3 shows the bleu score of our model on wmt14 english - german translation task . it can be seen that our approach improves the interpretability by a significant margin over the gold standard gnmt .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the results published by wang et al . ( 2017 ) show that our # params model outperforms all the base models except for the ones using elmo embeddings . as expected , our model obtains a better match / score score than most base models .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model performs better than the other models in the low - supervision settings .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with ptb setting . the results are shown in table 7 .
system retrieval and word embeddings are presented in table 4 . all the word embedding methods are used in this study . word embedding method ( mtr ) and word embedmings are used to train system - retrieval tools . the word embeddmings perform well on both systems , with the best results being obtained on the system retrieeval task . using word embedding method ( ovm ) improves the results for both systems .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 .
results are shown in table 4 . the results are summarized in terms of the average number of frames for each metric . our joint model outperforms all the other baselines except for the one that is used in corpus ( tables 1 and 2 ) . we observe that our joint model significantly outperforms the competition on many metrics , including df , docsub and tf .
results are shown in table 4 . the results are summarized in terms of the average number of frames for each metric . our joint model outperforms all the other baselines except for the one that is used in corpus . this confirms the competitiveness of our joint model .
results are shown in table 4 . the results are summarized in terms of the average number of frames for each metric . our joint model outperforms all the other baselines except for the one that is used in corpus : europarl , ted talks and docsub .
embeddings are shown in table 3 . our system achieves the best performance with a minimum of 3 . 5roots compared to the maxdepth of eurparl . our metrics are comparable to those of docsub , docmax and europarl , but are slightly better than their maxdepth counterparts .
embeddings are shown in table 1 . our system achieves the best performance on all metrics with a minimum of 3 . 5roots . our metrics are comparable to the best on every metric except for the metric of depthcohesion , which measures the depth of the relation with the nearest neighbours . our metric achieves the highest score with a maxdepth of 9 . 43 .
experimental results are shown in table 1 . the enhanced version of lf outperforms the original embeddings in terms of r0 , r2 and r3 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . the best performing method is p2 , which shows the most effective hidden dictionary learning .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the hmd - prec model outperforms all the other models except for the one that uses bert .
3 shows the performance of the baselines for direct assessment and bertscore - f1 . the results are summarized in table 3 . the best performing baselines are ruse ( * ) and f1 . these metrics show that the approach is effective at improving the test scores for both sets .
3 presents the bagel and sfhotel scores on the test set of sent - mover . the results are summarized in table 3 . the baselines show that the bertscore - f1 scores are significantly better than the bleu - 1 scores on both sets , indicating that the importance of word embeddings in the baseline is important .
3 presents the metric and baseline scores of our models . the results are summarized in table 3 . the summaries are presented in terms of leic scores and bertscore - recall scores . epm metrics are significantly better than those of word - mover , indicating that word - mover has superior recall ability .
results are shown in table 4 . we observe that for all metrics , para + para + lang improves the performance for all models except for those using m0 .
results are shown in table 4 . semantic preservation and transfer quality are the most important aspects of semantic preservation . the results are summarized in terms of the transfer quality scores for all three domains , with the exception of the semantic preservation scores . for semantic preservation , the results are markedly better than those for the other domains , indicating that semantic preservation has a high correlation with semantic preservation ( e . g . , semantic preservation ) . syntactic preservation scores are further improved with the addition of semantic tags .
5 shows the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . the results are shown in table 5 . it can be seen that the accuracy obtained by using these metrics is high , indicating that the quality of the sentence is high .
results are shown in table 4 . we observe that the m0 + para + lang model outperforms all the other models except for the one that relies on word embeddings . in particular , we observe that for the m1 + m0 + m2 model , the shen - 1 representation performs better than the other alternatives .
results on yelp sentiment transfer are shown in table 6 . the best models achieve higher bleu than those using simple - transfer or unsupervised embeddings . as table 6 shows , using only one classifier in the transfer setup can improve the acc ∗ score considerably . multi - decoder achieves the best overall acc score , but it is slightly worse than using the standard delete / retrieve classifier . finally , the best performing model is yang2018unsupervised .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be in the correct utterance . the number of repetition tokens is shorter than the number of utterances predicted in the original utterance , indicating that the recall function is well - trained .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses , indicating that the disfluencies in the reparandum are localized in the content word , not the function word .
results are shown in table 4 . we observe that when text + innovations are used in combination with text , the model achieves the best results with an absolute boost of 0 . 2 .
2 shows the performance of our model compared to state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance in terms of the number of instances in which the opposing utterances are discussed , and the percentage of instances that are unrelated is low .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing alternative is burstysimdater , which significantly outperforms all previous methods .
3 shows the performance of our method with and without attention . the accuracy ( % ) of our approach shows the effectiveness of word attention and graph attention for this task .
results are shown in table 1 . the best performing models are dmcnn , trigger and jrnn . the performance of these models is reported in tables 1 and 2 .
3 shows the performance of our method in the event of a catastrophic event . our method outperforms all the traditional methods in terms of both event identification and event classification . all the methods used in this study have a significant impact on the event classification performance .
results are shown in table 4 . all the models shown in the table are better than all the other models except for the ones that do not use the word embeddings .
4 shows the results on the training set and on the test set using discriminative training with only subsets of the code - switched data in the dev set .
5 shows the performance of our system on the dev set compared to the monolingual set of fine - tuned disc . the results are shown in table 5 . the performance of fine - tuneddisc is significantly better than that of the standard mono set ,
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 score for the three eye - tracking datasets tested on the conll - 2003 dataset .
5 shows the precision ( p ) , recall and f1 - score for using type - aggregated gaze features on the conll - 2003 dataset . the results are statistically significant ( p ≤ 0 . 01 ) on the precision - based metric , confirming the viability of the dual gaze approach .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings are derived from the original wordnet , and are used in wordnet 3 . 1 . glove - retro also uses syntactic sg embedding . the results on the original paper are shown in table 1 . it is important to note that syntactic embedding can improve the performance of wordnet without sacrificing syntactic features .
2 shows the performance of our rbg dependency parser with features from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 shows the results of domain tuning with respect to image caption translation . the results are summarized in table 2 . subdomain tuning improves the multi30k decoding performance by 3 . 5 % over the domain - tuned model , while domain tuning improves bleu % by 2 . 5 % .
results are shown in table 4 . subdomain - tuned subs1m outperforms all the other models except for the one that is used in the en - de setting . in particular , the results are particularly striking for the spanish - language model , which relies on domain - tuning to improve performance .
4 shows bleu scores in terms of multi30k captions compared to en - de ( the best one or all 5 ) . the results are shown in table 4 .
5 compares the performance of our approaches with prior approaches . we observe that enc - gate and dec - gate achieve the best results ( bleu % scores ) when using multi - dimensional decoding schemes .
1 shows the performance of subs3m and subs6m when combined with the visual features of the three languages . the results are summarized in table 2 . we observe that the multi - lingual approach outperforms the baselines in terms of overall performance , with the exception of the one with the word embeddings . in addition , the combination of the acoustic features and the semantic features improves performance by a noticeable margin . finally , we observe that combining the features improves the overall performance of the two languages .
3 shows the performance of our system compared to the original embeddings . our system outperforms the best state - of - the - art systems on all metrics except for the word count .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . the automatic evaluation scores ( bleu ) are significantly better than ter and en - es - rev , indicating that the system performs well in low - resource settings .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs model is significantly better than the segmatch model from chrupala2017representations . segmatch also improves the recall performance ,
results on synthetically spoken coco are shown in table 1 . the results are summarized in the row labeled vgs . we observe that the supervised model outperforms the adversarial embeddings by a significant margin .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig ( in the right hand hand hand ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . rnn ( out in the wrong hand ) also shows the importance of the edges in a screenplay screenplay .
2 shows the results of fine - tuning on sst - 2 . the results indicate that the number of occurrences in the original sentence has increased , decreased or stayed the same , indicating that there is no need to worry about goodness .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . the results are shown in table 3 . positive labels are slightly increased compared to negative labels .
results are presented in table 4 . the most striking thing about our results is that it is able to distinguish between positive and negative aspects of the word ( pmi ) without asking questions ( e . g . , whether it has been tested on sst - 2 or not ) .
