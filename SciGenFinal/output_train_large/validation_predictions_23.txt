2 shows the performance of our iterative approach on the large movie review dataset . the approach performs the best on inference with efficient parallel execution of tree nodes , while it requires significantly less computation .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
2 shows the performance of the hyper parameters optimization strategies for each model with different number of parameters . the best performing model is the ud v1 . 3 model , which performs better in all model variations . the max pooling strategy consistently outperforms all the other approaches except for the one that relies on the sigmoid embeddings . softplus also achieves the best performance with the maximum number of hyper parameters in each model .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . diff . model using only sdp gives the best performance . we can also see that the relative dependency path increases the f1 by a noticeable margin .
results are presented in table 3 . in particular , we observe that y - 3 significantly outperforms y - 2 in terms of f1 and r - f1 performance .
3 presents the results on the paragraph level . results are presented in table 3 . all models trained on mst - parser achieve state - of - the - art results on par with those on the essay level . all models except for those trained on the emty dataset are slightly outperform the others in terms of the number of instances in which they are tested .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) compared to the majority performances for the other two systems .
3 shows the performance of the original and the original methods for each variant . the results are presented in table 3 . original and original methods perform better than the others on all but one of these . they are slightly worse than the original on some of the subtasks , such as bleu , nist , rouge - l and meteor .
shown in table 1 , the original and the cleaned versions are comparable in terms of number of distinct mrs , total number of textual references and ser as measured by our slot matching script , see section 3 .
performance of original and original models is presented in table 4 . the results are presented in bold . original models generally perform better than the original on all tests except for the one that has the correct number of parameters . they also perform slightly worse on some tests than others . these include : bleu , nist , rouge - l < cao et al . ( 2017 ) and meteor ( 2017 ) .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found a total absolute number of errors ( 17 ) and a slight amount of misfias ( 14 ) .
model accuracies are presented in table 1 . the best performing dcgcn model is the all model , which achieves 25 . 2 % improvement over the state - of - the - art model on all external and internal cues .
2 shows the results on amr17 . our model achieves 24 . 5 bleu points and achieves 27 . 5 overall bleus points . the results are shown in table 2 , where " s " and " e " denote single model size , respectively .
3 shows the results for english - language and german , with the exception of english - czech , where we base our model on bach et al . ( 2018 ) . the results are shown in table 3 . the single model performs slightly better than the other models in english , german and french , the difference in performance between the single model and the multi - model model is minimal , however , the difference is significant in english - speaking german , where the gap between single and multi - model is narrower than in english ( table 3 ) .
5 shows the effect of the number of layers inside dc on the overall performance of the model . we observe that for every layer that has one layer , there are two more layers that have to be added to the network .
6 shows the performance of gcns with residual connections compared to baselines . rc + la ( 2 ) and dcgcn4 ( 6 ) show that the residual connections gcns have strong gcn performance . however , when gcn has residual connections , the performance drops significantly .
model a shows the performance of dcgcn models when trained on state - of - the - art data . the results are presented in table 1 . dcgcnn models generally perform better than other models in all but one of the three cases .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . the results are shown in table 8 . the dense blocks reduce the number of connections , but still give a significant performance improvement .
shown in table 9 , the models used in the graph encoder and the lstm decoder have varying performance depending on the coverage mechanism . encoder modules use the best multi - decoder design , but their performance is slightly worse than the other two models .
shown in table 7 , the initialization strategies performed best on probing tasks . our paper shows that the best performing method is glorot . it obtains the best performance on three of the four probing tasks : depth , coordinv and topconst . it achieves the best results on all three .
are presented in table 4 . we observe that our approach obtains the best performance when trained with a minimum of two parameters : depth , coordinv and topconst . as the results show , the threshold - based cbow / 400 model performs better than the other two methods on three of the four metrics .
are presented in table 3 . our model outperforms all the other methods except for the one that cmp uses . cbow shows significant performance improvement over both mpqa and sick - r . it also outperforms both the sst2 and sst5 models in terms of mrpc score . however , it is still inferior to both the original cbow / 784 model and the hybrid variant . this is mostly due to the smaller size of the training set and the higher mrpc scores .
results on unsupervised downstream tasks are shown in table 3 . our model outperforms both the hybrid and hybrid models in all but one of these tasks . on the sts13 and 15 datasets , it achieves the best performance .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms the best stateof - the - art approaches on three of the four tasks .
6 shows the performance for different training objectives on the unsupervised tasks . the cmow - c model outperforms the cbow - r model on all three tasks except for the one that is supervised .
can be seen in table 3 the performance of cbow and cbow - r on the subtense and coreference tasks . cbow shows significant performance improvement on both subtasks . on the coreference task , it obtains a significant improvement on the precision scores . it outperforms both somo and wc by a significant margin . on this metric , it achieves the best performance on all subtasks except the one that requires a significant increase in precision . we observe that cbow improves upon the strong performance of somo by a large margin .
subj and sick - r perform similarly on the mrpc and mpqa datasets . the best performances are obtained by cbow - r , which obtains the best performance on both datasets . it outperforms both sst2 and sst5 in terms of mrpc performance . on the other hand , it performs slightly worse on the ssts - b dataset , which underscores the competitiveness of the subj model . cbow improves upon the performance of sst - e by 3 . 8 points in mrpc , but still performs substantially worse on other datasets .
system performance in [ italic ] e + per and e + misc scores are reported in table 3 . supervised learning achieves the best results with a minimum of 50 % org and 50 % per scores , all org scores are computed using the best state - of - the - art features , and the best performing feature set is the combination of name matching and multi - task learning . we observe that all the systems trained on the original org dataset perform similarly in theitalic setting , with the exception of mil - nd .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 score .
6 shows the results of ref and ref for all models except those that do not use ref ( table 6 ) . ref significantly outperforms ref in all but one of the cases .
3 presents the results on the ldc datasets . the results are presented in table 3 . the models performed slightly worse than the state - of - the - art models on all but one of the 10 datasets .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model obtains the best performance on both sets . it outperforms all the models except for konstas et al . ( 2017 ) by a noticeable margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results of this study show that the use of bilstm improves the model ' s performance by a noticeable margin .
results are presented in table 4 . we observe that the g2s model outperforms the other models in terms of sentence length and sentence length on both occasions . sentence length and average sentence length are both relatively high , indicating that the model is more suitable for the task at hand .
shown in table 8 , the fraction of elements that are missing in the output that are present in the generated sentence ( g2s - gin ) , is much smaller than those in the input , indicating the importance of token lemmas in the design .
4 shows the performance of the two approaches using the 4th nmt encoding layer . the results are shown in table 4 .
2 shows the pos and sem tags accuracy with baselines and an upper bound . unsupemb and word2tag achieve the best results with a lower bound on baselines .
results are presented in table 4 . table 4 shows that the accuracy obtained by our method significantly outperforms the performance obtained by other methods .
5 shows the accuracy with different features from different layers of the uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos is 87 . 9 % accurate and res is 88 . 9 % .
performance on different datasets is shown in table 8 . the attacker scored significantly worse on the training set than the corresponding adversary on both datasets . the difference between the attacker score and the corresponding target is significant .
1 shows the accuracies when training directly towards a single task . for pan16 , we trained directly towards this task .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced data splits . it is clear from table 2 that the presence of this protected attribute can help the model to better interpret the messages generated by the messages .
performance on different datasets is shown in table 3 . for the classifier pan16 , mention is the difference between the performance of the trained classifier and the corresponding adversary ’ s accuracy . the difference between trained classifiers is significant , sentiment and gender are the most important factors in predicting whether an object will reach the target and whether the target will be reached .
6 shows the performance of different encoders when embeddings are trained on the same model . embedding is strictly supervised by rnn , while embedding is supervised by leaky . the results are shown in table 6 .
results are shown in table 2 . the first group shows that the lstm model outperforms the other models in many aspects . it achieves the best results with a minimum of 2 . 5x performance on the training set and on the finetune set . we observe that this model performs better on the two sets of training sets than the other ones .
performance of our model on the acc andbert datasets is reported in table 5 . the results are presented in tables 5 and 6 . we observe that the lstm model significantly outperforms the other models in both the acc / bert and the time - to - params tasks .
can be seen in table 4 the performance of our model compared to other models using the same time - based approach . our model obtains the best performance on both datasets . on the one hand , it performs slightly worse than the other two models in terms of both err and full time . this is due to the fact that the amapolar time model requires significantly more data than the original lstm model , and therefore requires considerably more data to model . this shows the performance gain on the two datasets when combined with the additional features of theolar time model .
shown in table 3 , the tokenized bleu scores on wmt14 english - german translation task are significantly better than those on the gold - adapted tesla p100 . further improvements are also evident in the performance of our model when trained with only one sentence decoding .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the model obtains the best performance with a parameter number of 2 . 67m and achieves the best f1 score with a gap of 1 . 36 points . however , the performance is still significantly worse than those of other models with larger parameter numbers .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly outperforms the other models in terms of parameter number in the evaluation .
performance on snli task with base + ln setting and test perplexity on ptb task with the same set of parameters as in table 7 .
system retrieval is presented in table 4 . word embeddings are used for both human and system tasks . all systems trained on the word - based systems retrieeval are significantly better than the systems trained only on the systems using the word " retrieval " . word - based system learning methods ( mtr , mtr , r - 2 ) outperform human on all three systems except for the one that is used on the system retriaeval dataset . sent attention is applied to both systems with the maximum number of frames per sentence , with the exception of the one using the system referral complexity ( strs ) . word attention is used to improve the general performance of both systems ,
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performing system is seq2seq , which achieves a 3 . 2 overall improvement over human evaluation .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the one that we use on the df dataset : europarl , term , and ted talks . we observe that term significantly outperforms both the original and the alternative approaches on many of the datasets , including df , docsub , and docsub .
3 shows the performance of all the models trained on the corpus dataset . the results are summarized in table 3 . our model outperforms all the other models except for europarl , which obtains the best performance on all three datasets . we observe that for all but one of the three datasets , our model performs slightly worse than the others on both datasets . for the other two datasets , we see that the difference in performance between the best performing models is less pronounced for the eurparl model .
3 shows the performance of all the models trained on the corpus dataset . our model outperforms all the other models except for the one that is used on the df dataset , namely , europarl , ted talks and docsub . the results are slightly worse than those on docsub , but still comparable to the results on other datasets . for example , our model performs slightly better than the others on three of the four datasets .
embeddings are shown in table 1 . they are based on the best performing metric on the corpus dataset , namely , depthcohesion . they complement each other well on the other metric , such as df , docsub and hclust . however , they do not exceed the maxdepth of our model on all metric , which shows the diminishing returns of our approach . our model achieves the best results with a maxdepth ratio of 1 . 78 to 1 . 1 on a scale of 1 , 000 to 1 , 025 . on the other hand , europarl achieves the worst result with a depth ratio of 2 . 1 / 1 . 1 . this suggests that our approach is more effective than our baseline model .
embeddings are shown in table 1 . our system achieves the best performance with a minimum of 3roots on each metric compared to the maxdepth of europarl . this is reflected in the averagedepthcohesion score of our model , which is slightly better than our maxdepth score .
experimental results are shown in table 1 . the enhanced version of lf outperforms the enhanced version by a noticeable margin .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . the best performing model is lrv , which relies on hidden dictionary learning .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . for the hard alignments , we see that wmd - prec significantly outperforms both hmd - f1 and wmd
performance on the direct assessment and eureor metrics is presented in table 3 . the results are presented in terms of bertscore - f1 score , which significantly outperforms other approaches on both metrics . for example , the following table shows the performance of all approaches except for the one that directly targets the learner .
3 presents the bagel and sfhotel scores on the test set . the results are summarized in table 3 . we observe that the baseline bleu scores significantly outperform the baseline on all metrics except for those in the range of " inf " and " qual " .
performance of the models according to these baselines is reported in table 3 . the morph - mover model obtains the best performance on three of the four metrics . it follows leic and spice metrics , which show that the combination of elmo and p < 0 . 001 gives a performance improvement of 0 . 005 on the m1 and 2 sets .
3 shows the performance of the models trained on sim and pp datasets . we observe that for all models that rely on word embeddings , the model performs better than the original model on all three datasets except for the one that relies on syntactic or semantic information .
results are presented in table 4 . semantic preservation and transfer quality are the most important aspects of the semantic preservation and semantic preservation tasks . syntactic preservation tasks are further improved with the addition of semantic preservation features . for semantic preservation , we see a drop of 3 . 5 points in performance compared to the previous stateof - the - art model .
5 shows the human evaluation results . we show the results of human evaluation using the [ italic ] ρ b / w negative pp and human ratings of fluency . the results are shown in table 5 . it can be observed that the accuracy obtained by using these metrics is high , indicating that the quality of the sentence is high .
3 shows the performance of the models trained on sim and pp datasets . we observe that the m0 model outperforms all the other models except for the m1 model , which is more likely to rely on syntactic or semantic features such as shen - 1 .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than those using simple - transfer and unsupervised embeddings . however , the improvement is less pronounced for the more recent model , yang2018unsupervised , which shows the diminishing returns from mixing multiple classifiers in the same sentence . we also observe that the use of the " replace / retrieve " classifier severely limits the model ' s ability to achieve acc ∗ . this is reflected in the fact that the transfer model is limited to 1000 sentences and contains 1000 human references . it is clear from table 6 that there are some limitations to using this classifier in the transfer setup , which may account for some of the variation in performance between classes .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted to be disfluent is slightly less than the number predicted as nested , but still considerably higher than the rate at which repetition tokens were predicted .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is in parentheses , indicating that the accuracy of the prediction is high for both instances .
experimental results are presented in table 4 . we observe that text + innovations significantly improve the model ' s performance over the single model by 3 . 5 points on the dev and test set . text + innovations also boosts the model ’ s performance by 0 . 2 point over the best state - of - the - art model .
performance on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance on both test datasets . it exhibits the best performance in terms of both the accuracy and the number of instances in which it can be trained .
2 shows the performance of different approaches for the document dating problem on the apw and nyt datasets . the best performing approach is attentive neuraldater .
3 shows the performance of our method in terms of word attention and graph attention . it achieves the best performance with 61 . 8 % and 63 . 9 % accuracy , respectively , compared to the performance with ac - gcn .
model performance in table 1 shows that all models trained on the jvm are comparable in performance to the state of the art on all stages except for the one that is used in combination with trigger . we observe that for all models , the performance gap between trigger and jvmee is less than that on other stages .
experimental results are shown in table 4 . all methods used for this task are described in terms of the three stages of the event . in all but one of the cases , the method has achieved the best results with a significant margin . all the methods used in this data are statistically significant with respect to both the identification and the event classification .
can be seen in table 4 the results for english and spanish are presented in table 5 . all but fine - tuned models appear to be better than all the other models except for those that do not use the word " attention " . all but one show that fine - tuned models perform better on the dev and test sets .
4 shows the results on the train dev and test set . fine - tuning achieves the best results with only subsets of the code - switched data in the dev set .
performance on the dev set and the test set is shown in table 5 . fine - tuned - disc achieves the best performance on both sets , outperforming monolingual and code - switched modes . however , it is still inferior to fine - tuned - disc in both sets .
results are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over type - aggregated gaze features trained on the conll - 2003 dataset and tested on the same three eye - tracking datasets .
5 shows the precision and f1 scores for using the type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and correlation ( g ) are all statistically significant improvements over the baseline ( p < 0 . 001 ) . f1 - score is also statistically significant ( p ≤ 0 . 01 ) for using type - based features .
experimental results on belinkov2014exploring ’ s ppa test set . the glove embeddings are derived from the original wordnet ( see table 1 ) , and they use syntactic - sg embedding as the base embedding for wordnet 3 . 1 . the results on the original paper are shown in table 1 . it is clear from the output that the syntactic embedding via skipgram gives the best performance . however , the performance drop is not significant , and it is not statistically significant ( see fig . 1 ) .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp helps rbg to achieve outstanding results .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect is minimal , however it does have a significant effect on ppa acc . it is clear from table 3 that removing the sense and context sensitive features hurts the model ,
2 shows the performance of domain - tuned models compared to multi30k on the largerickr16 dataset . subdomain tuning improves the image caption performance by 3 . 5 points over the model of en - de , while domain tuning improves it by 2 . 4 points .
results are shown in table 4 . the models trained on the flickr16 dataset are slightly outperform the subs1m model on all metrics except for those using domain - tuned h + ms - coco . table 4 shows the results for both sets . in the en - de setting , the h + m models outperform subs1ms in terms of performance , but on the large - scale datasets , the improvements are less pronounced on the small - scale ones .
4 shows bleu scores in terms of the automatic captions added after adding the best ones or all 5 models . the results are shown in table 4 .
5 compares the approaches for integrating visual information with traditional embeddings . we use multi30k + ms - coco + subs3mlm , and detectron mask surface . results are summarized in table 5 . we observe that enc - gate and dec - gate have similar performance , but the improvement is less pronounced on larger datasets .
1 shows the performance of subs3m with different visual features compared to subs6m on the en - de and on the wider - de datasets . in particular , we see that the combination of visual features and the multi - lingual lm detectron improves the performance for both sets . however , the improvements are only noticeable on the larger datasets where the visual features alone do not improve the performance . this is mostly due to the large size of the training dataset and the fact that the embeddings are small ( e . g . , small clusters ) , making the model more suitable for multiple tasks .
performance on mtld is reported in table 3 . we observe that the best performing model is en - fr - rnn - ff , which results in significantly better performance on the mtld test set compared to the original embeddings .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the english , french and spanish vocabularies used for our models . our model outperforms the best performing french model by a significant margin .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) and ter are significantly better than those by non - compliant systems ( e . g . , en - fr - rnn - rev ) .
results on flickr8k are shown in table 2 . pretrained vgs models achieve the best performance with a 10 % recall rate .
experimental results on synthetically spoken coco are shown in table 1 . the model trained on the mirrored embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model .
shown in table 1 , one of the classifiers for cnn ( unk ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . as shown in the second example , the other classifier for cnn also makes use of the edges edges edges of the original .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results are shown in table 2 .
shown in table 3 , the sentiment changes in sst - 2 when negative labels are flipped to positive . this shows that the effect of the flipped sentiment signal is positive , but negative sentiment is negative .
results are presented in table 4 . we observe that the transfer learning ability ( pmi ) is beneficial for both positive and negative aspects of the model ( see table 4 ) . however , it does not improve significantly for both negative and positive aspects . this suggests that more research should be done to further refine the model for future use . table 4 shows the results for both approaches .
