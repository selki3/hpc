2 shows the performance of our recursive framework on the large movie review dataset , and tensorflow ’ s iterative approach , which shows better performance on inference with efficient parallel execution of tree nodes .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the model .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . softplus achieves the best performance with the maximum number of hyper parameters and the number of feature maps . the max pooling strategy consistently performs better in all model variations . finally , the sigmoid model performs better with the different number of hyper parameters in the pooling setup . the performance of our model is shown in table 2 . we also evaluated the f1 and f1 metrics , which show the performance obtained when pooling the multiple hyper parameters into the correct representation . the results are shown in tables 2 .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that our model achieves the best f1 ( in 5 - fold ) with sdp as the dependency path , and the macro - averaged model improves the f1 by 21 . 11 % in the standard task formulation . the comparison of our model with the strongest dependency path is presented in table 1 .
results are presented in table 3 . the performance of y - 3 compared to y - 2 shows that , when combined with the effective recall function , the performance of the two models increases significantly .
results are shown in table 1 . the results of the best performing model are presented in the table 1 . our model significantly outperforms the competition in terms of both word level and entity level .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) compared to the majority performances on the runs given in table 2 . the difference in performance between the two systems shows that the quality of the paragraph system can be improved with a reasonable selection of the correct paragraph and the correct utterance .
performance of original and original models on the test set is presented in table 4 . the results are presented in tables 1 and 2 . table 4 shows that the cleaned and corrected models perform better than the original on all tests , with a slight improvement on bleu score .
results for the original and the cleaned versions are shown in table 1 . the cleaned version has the highest number of distinct mrs and the average number of instances as measured by our slot matching script , see section 3 . as table 1 shows , the original e2e data is more than 50 % better than the original , but the cleaned version is over 50 % worse .
performance of original and original models on the test set is presented in table 2 . original models generally perform better than the original on all tests , however the differences in accuracy between original and original are significant . this result is reflected in the bleu score ( p < 0 . 001 ) and rouge - l scores ( p ≤ 0 . 01 ) . original models perform slightly better than original ones , but on the rare occasions when the error level is low , such as during training on the sc - lstm ( p ≈ 0 . 05 ) dataset , the performance of the original model is sometimes considered to be significant , but this analysis fails to account for these differences .
results of manual error analysis are shown in table 4 . the total number of errors we found was 22 ( out of 100 ) , with a slight amount of mispelling ( 17 % ) . we also found some slight disfluencies in the training data as well , which we labeled as " added , missed , and misclassified " .
model the performance of our dcgcn model is presented in table 1 . the best performances are achieved on the external and the external datasets , with a gap of 3 . 5 % in performance between the published and unpublished results .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points , which means that it has comparable performance to the ensemble model of seq2seqb .
results are shown in table 1 . the best performing model is the ggnn2seqb in english - koch , while the best performing one is in german . we notice a drop in performance between single and single models , the difference between the performance between the two is minimal , however , the gap between the average number of entries in the single model is large , bow + gcn ( bastings et al . , 2017 ) consistently outperforms the other models in both languages , this is evident from the large difference in performance in the english - language and german - language datasets , both the single and the multi model perform better in the low - supervision settings , in particular , this is due to the high overlap between the features of the multi and single model , as the results of our model indicate , both the single and the multi - step model perform similarly ,
5 shows the effect of the number of layers inside the dc stack on the performance of the model when we add the layers of layers that contribute to the overall effect of dc expansion . we observe that for example , the size of the layer that contributes the most to the performance is 17 . 6 % larger than that of the other layers , which shows the diminishing returns from adding layers to the stack .
results are shown in table 6 . rc + la also denotes gcns with residual connections , and dcgcn4 ( 27 ) shows that it has residual connections with multiple gcns . however , when gcn is considered as a standalone model , its performance drops significantly , the results show that the residual connections gcn has a significant performance drop when we consider the age of the connections and the number of gcn nodes in the network , as can be seen , the size of residual connections decreases with age ,
model f1 shows that dcgcn outperforms the other models in terms of both performance and model b on both benchmarks , as shown in table 1 , when the number of models is used in the model , the model b model suffers from overfitting and overfitting , which results in a reduction in model b performance .
8 shows the ablation study results for amr15 . it can be seen that dense blocks reduce the performance of the model when removing the dense connections in the i - th block .
results in table 9 show that the global encoder and the lstm decoder have similar performance on the graph encoder . the results of an ablation study for the two types of encoder shows that the multi - factor design leads to a better interpretability performance , with the exception of the dcgcn4 decoder .
investigate the effects of different initialization strategies on probing tasks , we show in table 7 . our proposed method outperforms the previous stateof - the - art method on both probing tasks . it achieves high precision on both the depth and the topconst scores , indicating that the initialization strategies are well - equipped to handle the task .
are presented in table 4 . the first group shows the performance of our method compared to other methods . our cbow / 400 model achieves the best performance on both subtense and subtense metrics , while on the other hand , it has the worst performance on the subtense metric .
1 shows the performance of our model compared to other models . our cbow model outperforms all the other models except for the one that has been tested on sst2 and sst5 . our model has the best performance on both mr and sick - e tests , while it has the worst performance on mpqa . we observe that the hybrid model performs on par with the original model , but on the mrpc test set , where the difference between the performance between the hybrid and hybrid modes is less pronounced .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp on all downstream tasks , though it has the advantage of being more stable . on the sts13 dataset , cbow shows a slight improvement over the performance of cmp .
results for initialization strategies on supervised downstream tasks are shown in table 8 . our paper shows that our approach improves the performance by 3 . 8 points over the best state - of - the - art model on mpqa , and outperforms the sst2 and sst5 baseline on all three tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms the other models on all three tasks , except for the one that the training objectives are on . on the sts13 dataset , it achieves 27 . 6 % higher performance compared to the other two models .
can be seen in table 3 the performance of cbow - r and its variants on the subtense subjnum . as the table shows , the method obtains a significant performance boost when trained on a larger dataset with a greater diversity of parameters , such as the coordinv and the topconst scores . these results show that cbow has superior performance on both subtense and subtense contexts compared to somo , indicating that the performance gain comes from a better understanding of the context .
subj and sick - r are comparable in terms of performance on all benchmarks , with the exception of mpqa . our model obtains the best performance on both benchmarks , outperforming both the sst2 and sst5 baseline by a margin of 3 . 6 points on the mrpc metric , while surpassing sst3 by 3 . 4 points on ssts - b . these results show that the cbow - r method can significantly improve the performance of the s - st2 baseline by improving the recall scores of the benchmark models .
system performance in [ italic ] e + per and e + misc scores are reported in table 3 . supervised learning models ( mil - nd ) outperform all systems except for the one that does not use the org metric ( misc ) . as can be seen , all the models trained on the data are significantly better than the original ones . in fact , the difference between the original and the debiased model is less pronounced in theitalic setting than in the original case , when using the misc metric .
2 presents the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals and 94 % e + f1 score . the results are shown in table 2 . supervised learning model mil - nd ( model 2 ) achieves the highest e + p score and the best f1 scores with 82 % . the performance of the supervised learning model is shown in tables 2 and 3 .
table 6 , we can see that ref and ref have a significant impact on the model ' s performance : ref significantly outperforms ref , while ref substantially boosts the performance of ref models .
results are presented in table 3 . the results of our model outperform all the other models except for the one that has achieved the best performance on the ldc2017t10 dataset .
results on ldc2015e86 test set are shown in table 3 . our model outperforms the previous state - of - the - art on both gigaword and external datasets .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results show that the use of bilstm improves the model ' s performance by 3 . 8pp over the strong lemma baseline .
results are presented in table 3 . the results are summarized in table 4 . we observe that the g2s - gin model outperforms the other models in terms of both sentence length and sentence length . on the larger scale , we notice lower precision on the graph diameter and the average number of frames is shorter than those on the smaller scale . finally , the improvement on average is less pronounced on the large scale , indicating that the model performs well on both the larger and smaller scales .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , is slightly larger thanadded , but still comparable to the output of ldc2017t10 , indicating that the model has more value in the production setting .
4 shows the performance of our method with respect to target languages . it achieves the best performance with 96 % accuracy using the 4th nmt encoding layer , and 96 % on the smaller parallel corpus ( 200k sentences ) .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; word2tag : the most frequently classifier using unsupervised word embeddings .
results are presented in table 4 . our results tabulated at table 4 show that our method significantly outperforms the competition in terms of both accuracy and precision . on the other hand , our results are significantly better than those of the previous methods , indicating that our approach has superior generalization ability .
can be seen in table 5 , the accuracy with different layers of our uni / bidirectional / residual nmt encoders improves over all non - english target languages , averaged over all four layers , and then decreases with the number of iterations .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in p - value < 0 . 05 . on a training set with 10 % held - out participants , the average p - value is 9 . 7 and the average roc score is 10 . 2 .
results in table 1 show that the training directly towards a single task can improve the performance for both groups .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the findings are summarized in table 2 . dial models show that the presence of the protected attribute can further improve the task performance for both groups , though it is harder to detect instances that are balanced . sentiment models seem to have little effect on the results , however it does have a significant impact on the overall performance of the models .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the performance of the trained and the corresponding adversary is significant , in particular , it is significant for the classifier that is trained on pan16 , and it is less likely to fail the task in the adversarial setting .
6 shows the performance of the embeddings for different encoders . embedding guarded is qualitatively different from the case when embedded is trained on rnn , and the performance for rnn is asymmetric .
results in table 2 show that the lstm model performs better than the other models on both datasets when trained and tested on the largescale wt2 and the finetune wt2 datasets . the results of our model outperform both the original and the debiased wt2 dataset by a significant margin . our model achieves a performance improvement of 2 . 36 % over the original model on both the wt2 / wt2 datasets and the finetune dataset by using the same number of parameters . table 2 shows the results of the second study . our model outperforms the previous work by a margin of 3 . 48 % on the final dataset , which shows the performance gain from training on a larger corpus .
results are presented in table 5 . the results of experiment 1 show that our model significantly outperforms previous models in terms of both acc andbert time , as the results show , the model performs better on both datasets when the training time and the total number of iterations are used to compute the model ' s final tasks .
results are presented in table 4 . the results of experiment 1 show that our model significantly improves upon the state - of - the - art model on both yelp and amapolar time datasets . it also improves on the amafull time dataset , as the results show , the model performs better on both datasets when using the same time - based schedule .
3 shows the bleu score on the wmt14 english - german translation task , measured by the number of frames used to encode one sentence in the newstest2014 dataset . the accuracy on this task is measured in seconds , which shows the diminishing returns from training on a single training batch compared to the gold - standard gnmt .
4 shows the model ' s performance on squad dataset . the results published by wang et al . ( 2017 ) show that the parameter number of our model improves the match / f1 score over the baselines . however , the improvements remain slim , the model obtains a f1 score of less than 2 . 5 / 3 . 5 on average , which indicates that the model performs well on a larger dataset than other models .
6 shows the f1 score on conll - 2003 english ner task . it can be seen that the lstm model significantly improves upon the reported result by lample et al . ( 2016 ) by increasing the parameter number to 87 . 46 .
results on snli task with base + ln setting and test perplexity with base setting are shown in table 7 .
results are shown in table 1 . word embeddings are used in the system retrieval tasks and are used as part of word2vec task . table 1 shows the results for all systems with different attention levels . all the systems using the word " retrieval " are evaluated using the multi - reg task ( mtr ) set , which consists of two stages : the first stage of the task and the second stage . the attention level is set at 1 . 11 , with a maximum attention level of 3 . 05 . when the attention is applied to the system , the attention level drops significantly and the average number of attention drops significantly . in the final stage , we use word " recent " to describe the results of the system and the attention mechanisms used to achieve these results . retriever is used as a state - of - the - art on all systems , with the focus being on the attention mechanism .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is highlighted in bold , with the highest standard deviation being 1 . 0 for k 100 and the lowest being 2 . 3 for k 1000 .
can be seen in table 4 , the results for english and spanish are summarized in table 1 . our proposed model outperforms the competition on all three datasets , with the exception of docsub . on the df dataset , our model performs slightly better than the other two baselines on three of the four datasets , while the performance on the docsub dataset is slightly worse on the other four datasets .
can be seen in table 2 , the results for english and spanish are summarized in table 1 . our joint model outperforms all the competition on both datasets , with the exception of docsub . on the df dataset , our joint model performs slightly better than both the baseline and the corresponding embeddings . the difference between our model and the baseline is most striking when we compare it with the previous state - of - the - art on the corpus dataset , where our model performs better than the baseline on all three datasets .
can be seen in table 3 the performance of all the models compared to the baseline on the corpus and docsub datasets . our proposed model outperforms both the baseline and the embeddings on all three datasets except for the one on corpus , where it obtains the best performance on the docsub dataset . the results are presented in tables 1 and 2 .
metrics are presented in table 1 . our system achieves the best performance on both metric metrics with a gap of 1 . 8 % on the metric compared to the previous best performance . our joint model outperforms both the baseline and the corresponding embeddings on every metric , with the exception of docsub contributing less than 1 . 5 % on each metric . our model achieves the highest level of depthcohesion , with an absolute improvement of 3 . 3 % compared to our joint model .
are presented in table 3 . our proposed system improves upon the previous stateof - the - art on all metrics with a gap of 9 . 43 % on the dsim metric compared to 1 . 1 % on europarl . our system achieves the best performance on both metrics with an absolute improvement of 2 . 57 % on both metric and on the depthcohesion metric , which shows the diminishing returns of our approach .
performance of our enhanced model on the validation set of visdial v1 . 0 is shown in table 1 . we observe that lf significantly outperforms the enhanced model in terms of answer score sampling , and consequently , r1 , r2 and r3 denote significant loss in the ranking performance . surprisingly , the enhanced version of the model shows lower performance than the original one .
performance ( ndcg % ) of the ablative studies on different models is shown in table 2 . using p2 shows the most effective model ( i . e . , hidden dictionary learning ) compared to using p1 .
results on hard and soft alignments are shown in table 5 . the results are slightly better than those on hard alignments , but still inferior to the strong lemma baseline on the soft alignment . on the hard alignment , our hmd - prec model outperforms all the other models except for the one without bert .
performance on the direct assessment and eureor metrics is presented in table 4 . the results are summarized in terms of ruse scores , which show that the effectiveness of our approach can be further improved with a better understanding of the human resource .
performance on the sfhotel and bagel metrics is presented in table 1 . the results are summarized in terms of bleu - 1 scores , which are significantly better than the baseline on both sets . the results also indicate that the approach is beneficial for both groups , with the exception of the case of sfhotel , where the improvements are statistically significant even under the difficult requirement of a low - supervision baseline .
performance of the models according to these metrics can be seen in table 2 . the results are summarized in table 1 . leic scores are significantly better than those of sent - mover on m1 and m2 while spice scores are slightly worse on m2 . the results of bertscore - recall are presented in tables 1 and 2 .
results are shown in table 2 . as can be seen , all models trained on the shen - 1 dataset are significantly better than those trained on simulov et al . ( 2018 ) . however , for the m3 model , the performance drop is still significantly worse than those using pure word embeddings ( i . e . m0 + 1 ) .
results are presented in table 4 . we observe that the transfer quality and transfer quality scores are the most important components in the semantic preservation and semantic preservation tasks . semantic preservation is the most difficult part of the preservation task to solve , yelp is the only one that achieves the best performance on both datasets . the semantic preservation scores obtained during the experiments are significantly better than those obtained on the single - domain dataset ( m0 , m7 ) . however , the difference between the performance of the two sets is less pronounced for the two datasets ( m6 and m7 ) than for the other one , indicating that yelp models are more effective in both cases . as shown in the second group of tables , we further compare our model with yelp , showing that yelp has a better handle on transfer quality .
5 shows the human evaluation results . it can be seen that both the quality of the acc and the human ratings of fluency are high , indicating that the model has good generalization ability . the results also indicate that the semantic preservation ability of the model can be improved with a reasonable selection of the correct sentence , and that the generated sentences have a high quality of conversational accuracy . we also evaluated the effectiveness of the ρ b / w negative evaluation , as shown in table 5 , by evaluating the performance of the models with respect to sentence quality .
results are presented in table 4 . the results are summarized in table 5 . we observe that the m0 model performs better than the m2 model on all metrics , with the exception of the shen - 1 metric , which shows the diminishing returns from mixing word embeddings .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than those using simple - transfer and unsupervised embeddings , and the highest acc ∗ score is achieved using the multi - decoder model . however , the difference between the performance of the two models is less striking , because the training set is broader and the number of classifiers in use is small . we also observe that the transfer set is less diverse than the original set , indicating that the model is more suitable for the task .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens for each repetition token , while the number of repetition tokens is shorter . the number of reparandum tokens that are correctly predicted to be disfluent is 8 .
3 shows the percentage of rephrases correctly predicted to contain a content word in the reparandum and the repair ( content - content ) . the fraction of tokens predicted to belong to each category is in parentheses , as shown in table 3 , the number of tokens in the reparandum is less than the number in the repair , indicating that the function contains a lot of content word . table 3 shows that the accuracy of the prediction is relatively high for the two types of documents ,
results are shown in table 4 . the best performance on the single test is achieved by text + innovations , while the best results are obtained using innovations . we observe that the use of text + raw model improves the model ' s performance on both the single and the multi - test test , the results are presented in table 5 .
performance of our model compared to state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with the help of a low - supervision threshold , and significantly improves the micro - f1 score over the adversarial baseline .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models , and maxent - joint even outperforms the best previous model .
3 shows the performance of our method in terms of word attention and graph attention . it achieves a performance comparable to that of ac - gcn , which shows the effectiveness of the attention method .
results are shown in table 1 . the best performances are obtained on the 1 / 1 and 2 / n test sets . our model outperforms all the models except jrnn except for the one that embedding + t embeddings can handle . on the 2 / 2 test set , we observe that the jvmnn model performs better than both the original dmcnn model and the trigger model on both test set .
3 shows the performance of our method on the event threshold . our method outperforms all the other methods in terms of both event and event identification . in particular , we see that the method has the best performance in both scenarios . all the methods in table 3 show that the identification method has a high precision on both the event and the trigger .
can be seen in table 1 , all the models trained on the spanish - only - lm outperform their counterparts on the dev perp and test acc tasks . the results are presented in tables 1 and 2 , respectively . all but the fine - tuned - lm model achieved the best results on both metrics , with the exception of the english - only model .
results on the dev set and the test set are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev with only subsets of the code - switched data in the train dev set .
5 shows the performance of our model on the dev set and the test set , compared to monolingual embeddings . fine - tuned - disc achieves a comparable performance on the test setting , though it has the advantage of training on a gold sentence instead .
results in table 7 show that type - aggregated gaze features improved the precision and f1 - score for the three eye - tracking datasets tested on the conll - 2003 dataset .
5 shows the precision , recall and f1 - score for using type - aggregated gaze features on the conll - 2003 dataset . the performance improvement is statistically significant , with a drop of 0 . 3 % in f1 score compared to the previous experiment .
results on the ppa test set are shown in table 1 . the embeddings obtained by using autoextend rothe and schütze ( 2015 ) are used in wordnet 3 . 1 . they use syntactic - sg embedding , and the glove - retro embedding is derived from the original wordnet paper ( by faruqui et al . , 2015 ) . however , the syntactic embedding used in the original paper can be further improved with a slight improvement in performance .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that incorporating oracle pp features improves the model ' s performance and boosts ppa acc . acc . performance as well .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect is minimal , however it does improve the ppa acc . by 9 % in the simple model , which shows the diminishing returns from removing context sensitivity .
2 shows the results with domain - tuned captions . our model outperforms all the other models with a large margin . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the multi30k model by 3 . 8 points in terms of bleu % score .
results are shown in table 4 . the models trained on the flickr16 dataset are slightly better than those using the domain - tuned h + ms - coco but still perform worse than the subs1m model on the largerickr17 dataset . table 4 shows that when domaintuned , the improvements are minimal but consistent , with the exception of when using untuned h + mcoco embeddings . in the larger flickr17 dataset , the performance improvements are modest but consistent with the improvements on the large - scale training dataset ( mscoco17 ) .
4 shows the bleu scores in terms of the model captions added using the automatic captions . the results with the best model are shown in table 4 . after adding the multi30k model in the first set , our model improves in all but one of the comparisons .
5 compares our approach with prior works on en - de embeddings ( mscoco17 et al . , 2017 ) . the results are summarized in table 5 . enc - gate and dec - gate achieve remarkably similar results ( bleu % scores ) on both datasets , with the exception of flickr16 , where enc - gate achieves a better performance . as expected , the multi30k + ms - coco + subs3mlm model outperforms both the original and the detron mask surface ,
1 shows the performance of subs3m on the en - de dataset compared to the subs6m model on the flickr16 dataset . the results are presented in table 2 . we observe that the multi - lingual model , when combined with the text - only model , achieves an overall improvement of 3 . 36 % on the overall performance . further improving performance by incorporating the visual features and the augmented reality features improves the performance by 3 . 48 % overall .
performance on mtld compared to en - fr - rnn - ff is reported in table 1 . as expected , the performance of these models on the mtld test set is significantly less than those of the original embeddings . table 1 shows that the translations performed on the test set are significantly less accurate than the original ones .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the performance of our method in terms of training vocabularies for english , french and spanish . the results are presented in tables 2 and 3 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) show that the re - rev systems perform better than the original ones , showing that the redundancy removal method can reduce the error of re - scoring .
results on flickr8k are shown in table 2 . we can see that the visually supervised model improves the performance by 3 . 8 points over the naive state of the art rsaimage model .
results on synthetically spoken coco ( vgs ) are shown in table 1 . the model achieved the best performance with a roco score of 9 . 7 % on the test set , compared to the previous best performance by audio2vec - u .
can be seen in table 1 the difference in the use of these classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . in the same way , dan also shows the effects of the additional classifiers on the screenplay .
2 shows that the number of occurrences have increased , decreased or stayed the same since fine - tuning has been completed . similarly , for punct . ( p - value < 0 ) and rnp ( p < 0 ) we see substantial decreases in pos scores indicating that fine - tune has not impacted the quality of the word .
shown in table 3 , the change in sentiment from positive to negative is less pronounced in sst - 2 than in the case where negative labels are flipped to positive . this indicates that the effect of the flipped sentiment signal is positive .
results are presented in table 2 . attractively , the performance gap between positive and negative ( p < 0 . 001 ) indicates that our approach is more effective than the approaches suggested by jim . however , the results are less striking when compare with other methods of leveraging word embeddings , such as sift , pubmed and schemas . table 2 presents the results of re - scoring our summaries with respect to the impact of the additional cost term on the performance of our joint model . it is clear from table 2 that the use of sift improves the interpretability of the documents , but does not help the generalization of the findings . this is evident from the large difference in performance between the two approaches , i . e . when compare against jim , when investigate , whether the impact is less pronounced , when compare , and when compare . in addition , it is easier to refine the findings when comparing with jim without having to re - phrase the documents .
