2 : throughput and training on the recursive treelstm model on the large movie review dataset . the recur approach performs the best on inference with efficient parallel execution of tree nodes , while the iterative approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , when the batch size increases from 1 to 25 , it does not improve as well as the linear dataset .
2 shows the performance of the max pooling strategy for each model with different number of parameters . it achieves the best performance in all models with different representation size . as shown in table 2 , the maximum pooling approach performs better in all model variations . the hgn outperforms the sigmoid and softplus models in all aspects .
1 shows the effect of using the shortest dependency path on each relation type . our model obtains the best f1 ( in 5 - fold ) with sdp and the best diff . with sdp , it obtains a 2 . 36 % f1 boost over the baseline .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models .
3 presents the results of our method in terms of paragraph level . the results are presented in table 3 . our method outperforms all the methods except mst - parser except for the one that achieves the highest f1 score . on the other hand , our method achieves the best performance on the essay level .
4 shows the performance of the two systems at the essay vs . paragraph level . the lstm parser performs better than the baselines , but is slightly worse than paragraph .
3 shows the bleu scores of the original and the original tgen models . the results are presented in table 3 . the original model outperforms all the other models except for tgen , whose cleanliness is more consistent . on the other hand , it is harder to find errors in the original than the original model . this is mostly due to the fact that the error detection accuracy is relatively high on the original dataset .
1 compares our original and our cleaned e2e data with the original ones . the results are summarized in table 1 . the original and the cleaned versions have the highest number of distinct mrs and the highest percentage of concatenated mrs as measured by our slot matching script , see section 3 .
3 shows the bleu scores of original and tgen models trained on the original dataset . the results are summarized in table 3 . original model outperforms all the other models except for the one that has the best performance . it is clear from table 3 that the original model is superior to the tgen model in all but one of the cases where it has the worst performance .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set of tgen . we found that adding errors caused slight disfluencies in the training set . adding errors caused a slight imbalance in the learning set , leading to incorrect learning . corrected misferences caused disferences in training set , as shown in table 4 .
3 shows the performance of our dcgcn model on external and external datasets compared to the previous state - of - the - art models . our approach outperforms all the other models except for snrg , which performs better on the external dataset compared to snrg .
2 : main results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points in terms of parameters , compared to the previous best state - of - the - art model , ggnn2seqb ( beck et al . , 2018 ) .
3 shows the english - german b and english - czech c scores . the best performing model is bow + gcn ( bastings et al . , 2018 ) . the results are summarized in table 3 . we show that the single - language model outperforms the other models in both languages , with the exception of german , where it is more difficult to distinguish between single and multi - language models . as expected , the results are slightly better in english - language than in german , although the difference is less pronounced in german .
5 shows the effect of the number of layers inside dc on the performance of the network layer . for example , when we add layers of layers , we get a 2 - fold boost in performance compared to the baseline .
6 shows the performance of gcns with residual connections . rc + la ( 2 ) and gcn + rc ( 6 ) show significant performance improvement . with residual connections , gcn + rc ( 7 ) shows that the gcn has residual connections with the baselines .
3 shows the performance of our dcgcn model compared to the previous state - of - the - art models in terms of bias metric .
8 shows the ablation study results for amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block , whereas - { 4 } dense ones denotes removing them in the ii - th .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are presented in table 9 . encoder modules have the highest correlation with graph attention , and their coverage mechanism ( 24 . 9 % vs . 25 . 9 % ) leads to better interpretability .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our method obtains the best performance when the gap is less than the gap between the two . it obtains a better performance than glorot and subjnum .
3 presents the results of our method on the subtraction test set . the results are presented in table 3 . as expected , our method obtains the best performance on both subtraction and subtraction tests . it also achieves the best results on both precision test set as well as subtraction .
3 presents the results of our method on the mrpc test set . our model outperforms both the sst2 and mpqa datasets in terms of mrpc score . on the other hand , it is slightly better than both the mroc and sick - e datasets in both cases . cbow / 784 achieves the best performance on both metrics , while sst5 performs better on both datasets .
3 shows the relative change with respect to cmp in unsupervised downstream tasks attained by our models . the cbow model outperforms the hybrid model in all but one of these downstream tasks .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the sst2 and sst5 initialization strategies outperform all the baselines except for the one that has the worst performance on the supervised downstream task .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms both the supervised and supervised baselines on both tasks .
results are shown in table 3 . the cbow - r method outperforms all the other methods except for the one that does not have a significant effect on the performance . it is clear from the table that cbow is better at subtraction and subtraction . it obtains a better performance on the subtraction task than somo because it obtains the best performance on both subtraction tasks . on the other hand , its performance is comparable to somo , with a gap of 3 . 5 points between the two .
3 shows the performance of subj and sick - r on the mrpc test set . subj outperforms both sst2 and sst5 in terms of mrpc score . on the other hand , it is slightly better than sst3 and mpqa score , both on mrpc and on ssts - b test sets .
3 shows the e + and per scores of our system in [ italic ] and ( italic ) systems . our system obtains the best e + org scores and the best per scores . our model obtains both e + loc and e + per scores , which shows that the best performing model is the one that can do the best job on both datasets . the results are presented in table 3 . name matching is the most difficult part of the system to do , with a gap of 3 . 5 points in org score and an absolute improvement of 2 . 3 points in per score . the best performance is obtained by using a combination of e + loc and e − per scores to name matching . our approach obtains a better e + org score than any other system in the system .
2 shows the results on the test set under two settings : name matching and supervised learning . the results are shown in table 2 . name matching improves the e + p score by 2 . 5 points and f1 score by 3 . 3 points . in both settings , the system performs better than the previous state - of - the - art model in all but name matching .
6 : entailment ( ent ) and ref ( g2s - gin ) are the most important metrics for model success . ref significantly outperforms ref in all but one case , when ref is used with ref .
results are presented in table 3 . the best performing models are s2s , g2s - ggnn and ldc2017t10 , respectively .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm significantly improves the performance of the model on the development set by 3 . 5 points .
results are presented in table 4 . the model outperforms all the models except g2s - gin in terms of average sentence length . in particular , the g3s - ggnn model shows the best performance on sentence length with a 5 . 2 % improvement on average compared to the previous state - of - the - art model .
shown in table 8 , the fraction of elements missing in the output that are present in the input ( g2s - gin ) that are in the generated sentence ( miss ) . these tokens are used in the comparison to derive the summaries from the reference sentences .
4 shows the accuracy of our approach with respect to target languages extracted from the 4th nmt encoding layer . it is clear from table 4 that our approach obtains the best performance with the best accuracy with pos and ar tags .
2 : pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag is the most frequent classifier , followed by word1tag , which is the second most frequent .
results are presented in table 4 . our system outperforms all the other methods except for the one in which it performs best . the results are shown in bold .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is caused by the training set held out .
1 shows the accuracies when training directly towards a single task . for pan16 , the training data are presented in table 1 . sentiment is the most important part of the conversation , and it is the only one that interacts with the conversation .
2 shows the effect of classifiers on the balanced and unbalanced task averages . when classifiers are included in the conversation , the effect is less pronounced . when they are not included , the task averages are balanced , but unbalanced . this shows that classifiers have a significant effect on the performance of the conversation .
3 shows the performance on different datasets with an adversarial training . the performance on pan16 dataset is the difference between the attacker score and the corresponding adversary ’ s accuracy . sentiment and gender are the most important factors in predicting whether an object will reach the target and whether it will reach that target . sentiment is the most predictive factor , and gender is the least predictive factor .
6 : accuracies of the protected attribute with different encoders . the rnn embeddings outperform the embedding guarded ones .
3 shows the performance of our lstm model compared to the previous state - of - the - art model on the wt2 and wt2 datasets . the results are summarized in table 3 . this model outperforms both the original wt2 model and the finetune model on both datasets . however , it does not achieve the best performance on the two datasets . this is due to the large size of the training dataset and the relatively small size of training dataset .
results are shown in table 5 . the best performing model is the lstm , which has the shortest response time and the highest bert response time .
results are shown in table 4 . the lstm model outperforms all the other models except for the one that is used in the amapolar time dataset .
3 shows the bleu score of our model on wmt14 english - german translation task . it is clear from table 3 that our approach is able to decode one sentence in a fraction of the time when trained on the original gnmt dataset . in addition , it is easier to encode sentences in german than in french .
4 : exact match / f1 score on squad dataset with respect to the parameter number of base . rnet achieves the best performance with a f1 score of 69 . 83 / 83 . 83 on the model with a 2 . 44m parameter number . with respect to elmo , we observe that the lstm model is better than the sru model in terms of parameter number , but it is still better than sru .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , while sru is the only one that can be used as a parameter . it is clear from the table 6 that the use of # params leads to better performance .
7 shows the performance of our model with base + ln setting and test perplexity on ptb task with base setting setting .
2 shows the performance of the word " sent " . word embeddings ( mtr ) w / oracle retrieval and mtr ( r - 2 ) are the most effective methods for both systems , with the best performance on both systems . the word " sent " is used to describe the sentiment generated by the system when the system is re - evaluated . it is clear from the results that the word ' sent ' has a positive effect on the system ' s performance . sent attention is beneficial for both human and machine learning tasks , as it helps the system to better interpret the data . word attention and word attention are beneficial for the human task as well . in the machine learning task , word attention ( rtr ) is beneficial , as the word attention is more effective for machine learning . as expected , when the systems are reevaluated , the overall performance is better than the system . using word attention , the system learns to predict the next action .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . our system outperforms all the systems except seq2seq ( k 1000 ) .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that we trained on corpus : europarl ( p < 0 . 005 ) and the other ones that trained on docsub ( p > 0 . 05 ) . the results of our model are summarized in table 4 .
3 shows the performance of all the models trained on the corpus dataset compared to the previous state - of - the - art models . the results are summarized in table 3 . our model outperforms all the other baselines except for the one that is used in df . our approach outperforms both the df and docsub datasets by a significant margin .
3 shows the performance of all the models trained on the corpus dataset compared to the previous state - of - the - art models . the results are summarized in table 3 . our model outperforms all the baselines except for the one that is used on docsub . it is clear from the results that our approach is superior to both the original corpus and the original docsub datasets .
3 shows the performance of our model on the metric with respect to depthcohesion . our model achieves the best performance on both metric with an absolute improvement of 1 . 78 and 3 . 46 points over the previous state - of - the - art model . on the other hand , it performs worse than our model with a gap of 3 . 55 points over our baseline .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance on both metric metrics , with a 2 . 42 % boost on the metric compared to our baseline . on the other hand , our model performs slightly worse than our baseline on all metric metrics except for the metric of maxdepth .
1 shows the performance ( ndcg % ) of our enhanced model on the validation set of visdial v1 . 0 . lf is the enhanced version of the visdial model , and r0 , r2 , r3 denote regressive loss , weighted softmax loss , and generalized ranking loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on visdial v1 . 0 validation set . the best performing model is the one using the hidden dictionary learning method .
5 shows the performance on hard alignments compared to soft alignments . the hmd - prec model outperforms all the other models except for the one that relies on bert . this shows that bert improves the recall performance for both hard and soft aligned alignments as well .
3 presents the performance of our model on the direct assessment task . our model outperforms all the baselines except for the one with the highest bertscore - f1 score . the results are summarized in table 3 .
3 presents the bagel and sfhotel scores on the validation set of bleu - 2 and bertscore - f1 . these scores are based on the best bert score of 0 . 267 and 0 . 174 respectively , compared to the baseline score of 1 . 005 .
3 presents the metric and bert scores of the models trained on the m1 and m2 datasets . the results are summarized in table 3 . the leic scores are presented in bold , indicating that the model performs well on both metrics . however , the bert score - recall metric is slightly worse than the leic score - mover on both m2 and m1 datasets .
results are shown in table 3 . the m1 model outperforms the m2 model by a margin of 3 . 8 points on the acc + sim test set and by 3 . 6 points on pp test set .
3 presents the results of our model on the transfer quality and semantic preservation datasets . the results are summarized in table 3 . we show that our model has the best performance on the semantic preservation dataset , with a slight advantage over the syntactic preservation dataset . as expected , the results are slightly better than those of yelp , indicating that the model is more suitable for semantic preservation . however , the difference is less pronounced in the semantic preserving dataset , which indicates that the modeling has to rely on semantic preservation to achieve the best results .
5 shows the results of human sentence - level validation . the results are shown in table 5 . sim and human ratings of semantic preservation are the most important metrics for the task , while pp is the only one that requires human ratings to match .
results are shown in table 4 . the m1 model outperforms the m2 model by a margin of 3 . 5 points on the acc + sim test set . as expected , the performance drop is due to the large size of the gap in the training set , in addition , the m3 model performs better than the m4 model on both sim and sim test sets .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the 1000 sentiment schemas dataset . however , the results are slightly worse than those obtained by simple - transfer . multi - decoder outperforms all the classifiers except for the one that is used in the sentiment transfer dataset ( yelp sentiment embeddings ) . in addition , the performance of the simple transfer model is significantly worse than the one using the human references dataset . the results on the 90 sentimentschemas datasets are summarized in tables 6 and 7 .
2 shows the number of tokens that are correctly predicted to be disfluent . reparandum length is the smallest of the three types , with a gap of 3 - 8 tokens between repetition and rephrase .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted to contain the content word is in table 3 , while the fraction predicted as containing the word is small .
results are shown in table 4 . the best results are obtained by combining text and innovations with the best dev scores . adding text + innovations improves the model ' s performance by 0 . 2 and 1 . 2 points respectively compared to the single model .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . our model achieves the best performance on both test datasets .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the attentive neuraldater significantly outperforms all previous methods except burstysimdater .
3 shows the performance of our method with and without attention . it is clear from table 3 that it is effective for word attention , graph attention , and word attention tasks .
3 shows the performance of the jvmee model compared to the previous state - of - the - art models . on the one hand , it performs better than all the other models except for jrnn , which performs worse on the argument stage . however , on the other hand , its performance is comparable to the other two models .
3 shows the performance of our method in cross - event . it is clear from the table that the method performs well in the event of a single event , with the exception of the one in which the event is co - ordinated with a single entity . the method obtains a significant advantage in the identification and classification task due to the high precision of the mechanism . in addition , it obtains an advantage in cross - event classification task as well . in both cases , the identification task and the classification task are the most important factors in predicting the event .
results are shown in table 4 . all but the spanish - only - lm outperform all the other models in terms of dev acc and test wer , respectively .
4 shows the results on the train dev and test set with only subsets of the code - switched data . fine - tuning achieves the best results with only a 25 % train dev improvement and a 75 % train test improvement .
5 shows the performance on the dev set and the test set , compared to the monolingual state of the art . the results are summarized in table 5 .
shown in table 7 , type - aggregated gaze features trained on the three eye - tracking datasets and tested on the conll - 2003 dataset show significant performance improvement . precision ( p ) , recall ( f1 ) and f1 - score ( g ) are both statistically significant improvements over type combined gaze features , as shown in fig 7 .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show statistically significant improvements over type combined gaze features ( table 5 ) .
results on the original wordnet test set are shown in table 1 . the hpcd approach relies on syntactic - sg embeddings , and it uses the semantic skipgram embedding obtained by the original research paper . it uses syntactic sg embedding as a base for wordnet and wordnet 3 . 1 . the results on wordnet are summarized in tables 1 and 2 . glove - retro refers to the syntactic embedding of the original wordnet vectors , and is used to embed the synset embedding from the original work .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model .
2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) with subsfull domain tuning ( marian amun et al . , 2018 ) improves the results for all models except for those that use domain tuning . adding multi30k data improves the bleu % score by 3 points .
3 shows the performance of the subs1m model in en - de mode . the results are summarized in table 3 . with domain - tuned models , the results are slightly better than those of the other models . in particular , the improvements are more pronounced on the flickr16 dataset , where the h + ms - coco model performs better than the other subs1ms models .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . our model outperforms all the models except for the ones with the best ones . in fact , the model with the worst bleus scores is better than the one without .
5 shows the performance of our approach with respect to en - de and dec - gate . the results are summarized in table 5 . we observe that the enc - gate approach outperforms the other approaches in terms of bleu % scores , indicating that it is more effective to integrate visual information into the network .
3 shows the performance of subs3m compared to subs6m in the en - de setting . the results are presented in table 4 . sub - domain embedding improves the performance by 2 . 5 points over subs2m . adding visual features improves performance by 3 . 3 points . multi - lingual embeddings improve performance by 4 . 4 points .
3 shows the performance of our model on the mtld test set compared to the en - fr - ht baseline . the results are presented in table 3 . as expected , our model outperforms the best state - of - the - art on all metrics except for the i / ttr metric .
1 : number of parallel sentences in the train , test and development splits for the language pairs we used and the number of development splits in the training splits .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that rev has the best performance on both systems , with the exception of the one in which it obtains a better performance .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs model is significantly better than the segmatch model from chrupala2017representations .
results on synthetically spoken coco are shown in table 1 . the vgs model achieves the best performance with a 3 . 5 % recall rate and a 4 . 3 % overall recall rate .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . dan < c > shows the same results as it does in the original . rnn also shows that the edges of a screenplay are so clever that you want hate hate it to turn on a on ( in the margins of the screenplay ) . for rnn , the edges are very clever and the curves are clever as well . for cnn , the turns in the on ( in the margins ) are clever , but for rnn it is more clever .
2 presents the results of fine - tuning on sst - 2 . the first row shows the percentage points in which the number of occurrences have increased , decreased or stayed the same . the second row indicates the extent to which the quality of the word " nouns " has not changed .
3 : sentiment changes in sst - 2 from positive to negative . the results are shown in table 3 . when negative labels are flipped to positive , the sentiment increases .
results are presented in table 3 . the results are summarized in bold . the most striking thing about the results is that they are strongly positive ( p < 0 . 001 ) and negative ( p > 0 . 01 ) . as expected , the performance of sift is significantly better than those of pimi , indicating that the quality of the research has improved over time .
