2 shows the performance of our recursive approach on the large movie review dataset compared to the iterative approach , which performs better on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset .
2 shows the results for each model with different representation . the max pooling strategy consistently performs better in all model variations . as shown in table 2 , the hgn model with the smallest number of parameters has the highest f1 score .
1 shows the effect of using the shortest dependency path on each relation type . the results are shown in table 1 . we observe that the macro - averaged model achieves the best f1 ( in 5 - fold ) without sdp . however , when using sdp as a dependency path , the model performs slightly worse than the baseline model .
results in table 3 show that y - 3 significantly outperforms y - 2 in terms of f1 and r - f1 scores .
results are reported in table 3 . the results of the second study are summarized in table 1 . we observe that mst - parser outperforms the previous methods in terms of paragraph level and f1 .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph is lower than the majority performance for both systems ; however , the difference is larger than the difference between the majority and majority performances .
results are shown in table 3 . the original and the original results are presented in table 4 . the results are summarized in bold . original results show that the tgen + model performs better than the original on both test sets . however , the results are slightly worse on the original than on the test set .
shown in table 1 , the original e2e data and our cleaned version are comparable in terms of the number of distinct mrs , total number of textual references and ser as measured by our slot matching script , see table 1 .
results are shown in table 1 . original and original results are presented in bold . original results show that tgen + performs better than tgen − in all but one of the three cases , while the other two show lower performance .
results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . we found that adding incorrect values to the training set caused slight disfluencies ( see table 4 ) . we also found that removing incorrect values caused a slight drop in accuracy ( from 0 to 0 ) .
table 3 , we present the results of our joint model on the external and external datasets . the results are summarized in table 3 . the dcgcn model outperforms all the other models except for seq2seqk ( konstas et al . , 2017 ) on both the external dataset and the external datasets , with the exception of snrg .
2 shows the bleu score on amr17 compared to the previous state - of - the - art gcnseq model . the results are summarized in table 2 . we observe that the model size in terms of parameters is comparable to seq2seqb ( beck et al . , 2018 ) .
3 presents the results for english - german and english - czech . the results are presented in table 3 . the results show that the single model outperforms the other models in both languages . in english , the results are reported in tables 1 and 2 . we observe that the single model performs better than the multi - task bow + gcn on both english - language and german - language datasets .
table 5 shows the effect of the number of layers inside dc on the performance of the model . we observe that when we add layers of layers , the model achieves a better performance than those obtained by bold .
6 shows the performance of gcns with residual connections compared to baselines . rc + la ( 2 ) shows that the gcn has residual connections with the residual connections . however , the results are not statistically significant , indicating that residual connections are beneficial .
3 shows the performance of dcgcn in relation to the bias metric . the results are summarized in table 3 . we observe that dcgcnn achieves the best performance on both metrics , with a gap of 2 . 5 % in performance between the previous state - of - the - art models .
8 shows the ablation study for amr15 . the results are shown in table 8 . - { i } dense blocks denote removing the dense connections in the i - th block , whereas - { 4 } dense connections denotes removing the layers of dense connections . the results also show that removing the dense blocks in the dev set is beneficial for the model .
table 9 shows the ablation study for the graph encoder and the lstm decoder . the results are summarized in table 9 . encoder modules have the best performance with a gap of 3 . 5 points between the two decoders .
7 presents the results for initialization strategies on probing tasks . our paper shows that our approach obtains the best results on the two tasks . the results show that our method obtains a superior performance on the second task .
results are presented in table 3 . table 3 shows the results of our method on the subtraction and subtraction tasks . the results show that our method obtains the best results on both subsjnum and subjnum metrics . the results also show that the h - cbow model obtains a superior performance on both subtraction metrics .
3 presents the results of our method on the subj and mpqa datasets . the results are summarized in table 3 . the subj model outperforms both sst2 and sst5 on both mrpc datasets . cbow / 784 achieves the best performance on both datasets . however , it does not improve on the mrpc dataset . this is due to the large variation in the performance between the two approaches .
results on unsupervised downstream tasks attained by our models are shown in table 3 . the cbow model outperforms both hybrid and cmp in terms of downstream performance .
8 shows the results for initialization strategies on supervised downstream tasks . our paper shows that glorot outperforms subj and mpqa in terms of mrpc performance .
6 shows the performance for different training objectives on the unsupervised downstream tasks . cmow - r outperforms cbow - c on the sts12 and sts14 tasks . however , it does not achieve the best performance on sts15 , which indicates that the training objectives are more complicated .
results are shown in table 3 . the best results are obtained by applying the cbow - r method on the subjnum and the coordinv datasets . the results are summarized in table 4 . as expected , the method obtains the best results on the subjnum dataset .
3 presents the results of our method on subj and mpqa datasets . the results are summarized in table 3 . we observe that cbow - r outperforms both sst2 and sst5 in terms of mrpc performance . however , it is still superior to sick - e on both mrpc and socks - b datasets . on the subj dataset , it obtains the best mrpc score on the mrpc dataset .
3 shows the e + org and per scores for all systems trained on the same dataset . in [ italic ] all org scores are reported in table 3 . with respect to misc scores , we observe that the system performs better than the previous state - of - the - art model on all three datasets . however , the results are not statistically significant , indicating that the performance gap between the two approaches is small .
2 shows the results on the test set under two settings . name matching improves the e + p score by 3 . 5 points compared to the previous state - of - the - art model . supervised learning improves the f1 score by 2 . 3 points . in [ italic ] e + f1 scores are shown in table 2 .
6 : entailment ( ent ) and ref ( neu ) results are shown in table 6 . we observe that s2s and g2s - ggnn outperform all the other models in terms of ref , and that ref is more effective than ref . further , we observe that the model performs better on the ref test set compared to other models .
results are presented in table 3 . the results are summarized in table 1 . we observe that the ldc2017t10 model outperforms all the previous models in terms of bleu and meteor scores .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . bilstm achieves the best performance on both sets .
results are shown in table 3 . we observe that g2s - ggnn has the best overall performance on both datasets , with a slight improvement on the graph diameter metric compared to the previous state - of - the - art model .
shown in table 8 , the fraction of elements missing in the output that are present in the generated sentence ( g2s - gin ) is slightly larger than the fraction in the input ( miss ) , indicating that the model is more suitable for the task at hand .
4 shows the accuracy of our approach with respect to target languages extracted from the 4th nmt encoding layer . our model outperforms the previous state - of - the - art approach by 3 points .
2 shows the accuracies of unsupemb and word2tag with baselines and an upper bound . the results are shown in table 2 . the mft model outperforms the word embeddings in both baselines .
results are shown in table 3 . table 3 shows that the accuracy obtained by applying the best performing method is comparable to that obtained by using the f1 method . the results are summarized in table 4 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the attacker scored 10 % higher on the training set compared to the trained adversary .
1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . as expected , pan16 performs better than pan16 on the dial task .
2 shows the performance of our classifier in the balanced and unbalanced task splits . the results are shown in table 2 . the classifier has the highest performance on the balanced task splits , while the unbalanced one has the smallest performance gap .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . as expected , the training performance on pan16 is significantly worse than pan16 .
6 shows the performance of embedded and guarded with different encoders . the results are shown in table 6 . embedding leaky is slightly better than embedding guarded ,
results are shown in table 3 . we observe that the lstm model outperforms the previous state - of - the - art models in both base and finetune settings . however , it does not achieve the best performance on the wt2 dataset . in fact , it performs worse than the previous model on both datasets . the results of the second study are summarized in table 4 . further , the results of this study are not significant , as the results are not statistically significant .
results are presented in table 5 . we observe that the lstm model outperforms the previous state - of - the - art model in terms of training time . the results are summarized in table 6 . it is clear that the time taken to train the model is important for the model to achieve the best performance . as expected , the average training time is slightly longer than the original model , however , the difference is less pronounced when training the model .
results are shown in table 4 . we observe that our model improves upon the state - of - the - art lstm on both yahoo and yelp datasets . however , it performs worse than our model on both datasets .
3 shows the bleu score on wmt14 english - german translation task compared to the previous state - of - the - art model on tesla p100 . we observe that sru performs better than gnmt and sru in terms of decoding one sentence . however , sru does not achieve the best performance on the german translation task .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our model obtains a better match / f1 score than the previous state - of - the - art model . however , the sru model performs worse than our model .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result , and sru shows the performance of the model on the same task . it is clear from table 6 that the use of lrn has a significant effect on the performance .
results on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are shown in table 3 . word embeddings ( mtr ) w / oracle retrieval [ b - 2 ] and mtr ( r - 2 ) are the most effective methods for system retrieeval . the results are reported in table 4 . as expected , word embedders ( mtr ) are more effective than human methods , but are less effective than the human method . in particular , the results are less consistent with human methods ( e . g . , mtr , r - 2 , mtr ) and human methods . we observe that human methods perform better than human ones ,
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 .
results are shown in table 3 . the results are summarized in table 1 . we observe that our approach outperforms both the df and tf embeddings by a margin of 0 . 5 - 0 . 7 points .
results are shown in table 3 . the results are summarized in table 1 . we observe that our approach outperforms both the df and tf embeddings by a significant margin . however , it is still inferior to the approaches proposed by the previous methods .
results are shown in table 3 . the results are summarized in table 1 . we observe that our approach outperforms both the df and tf embeddings by a margin of 0 . 5 - 0 . 7 points .
results are shown in table 3 . the results are summarized in table 1 . we observe that the maxdepth of europarl is comparable to that of docsub , but closer to the baseline . however , the gap between maxdepth and maxdepth is larger than that between docsub and docsub . as a result , our results are less significant than those of the other baselines .
results are shown in table 3 . we observe that the maxdepth and maxdepth of europarl are comparable to those of both corpus and docsub . however , the gap between the max depth and max depth is larger than that of corpus , docsub is comparable to that of docsub , we also observe that our maxdepth is comparable with that of the original corpus model .
1 shows the performance ( ndcg % ) on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned , and r0 , r2 , r3 and r3 denote regressive loss , weighted softmax loss , and generalized ranking loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . using p2 as the base indicates the most effective ablative model .
5 presents the results on hard and soft alignments . the results are summarized in table 5 . we observe that hmd - prec + bert significantly improves the performance on hard alignments by 0 . 8 % compared to 0 . 7 % .
3 presents the results of the direct assessment task on the test set of ruse and sent - mover . the results are summarized in table 3 . table 3 shows the results for both sets . as expected , the results are significantly worse than those obtained by bertscore - f1 ( p < 0 . 005 ) . further , the performance of both sets is slightly worse than the results obtained by ruse .
3 presents the bagel and sfhotel scores on the test set . the results are summarized in table 3 . we observe that bleu - 1 achieves the best bertscore - f1 score on both sets . however , it does not achieve the best f1 score , indicating that the baselines are too weak .
3 presents the metric and baseline scores of the models trained on the m1 and m2 datasets . the results are summarized in table 3 . we observe that the leic score is significantly better than the f1 score , indicating that the model is more suitable for the task at hand . however , leic scores are still significantly worse than f1 scores .
results are shown in table 3 . we observe that m0 [ italic ] + para + lang significantly improves the performance of the model when compared to the previous state - of - the - art model .
results are presented in table 3 . we observe that the transfer quality and transfer quality scores are comparable across all domains , the results are summarized in table 4 . in the semantic preservation setting , we observe that semantic preservation improves over semantic preservation , while transfer quality decreases . semantic preservation improves with semantic preservation . as expected , the results of semantic preservation outperform semantic preservation on both datasets .
5 presents the results of human sentence - level validation . the results are shown in table 5 . we observe that the accuracy obtained by sim is comparable to that obtained by human . however , the difference is less pronounced for human evaluation .
results are shown in table 3 . we observe that m0 [ italic ] + para + lang = 0 . 818 on a single test set compared to the previous state - of - the - art on the sim dataset . as expected , the performance of m0 ( m1 + 2d ) is lower than that of m2 ( m2 + 2c ) on the sim dataset , but still comparable to the results on the pp dataset .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the same 1000 sentences compared to prior work . however , the results are slightly worse than those obtained by simple - transfer .
2 shows the performance of nested disfluencies in terms of the number of reparandum tokens that are correctly predicted to be disfluent .
3 shows the percentage of tokens correctly predicted to contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted as disfluent is shown in table 3 . as expected , disfluencies in the reparandum are larger than those in the repair , indicating that the function is more likely to contain the content word .
results are shown in table 3 . we observe that text + innovations outperform text + text in the early and late stages of the model , in particular , the results are better than those obtained in the late stages . in addition , the best results are obtained by adding innovations to the model . text + innovations also improve the dev mean by 0 . 2 points .
2 shows the performance of our model on the fnc - 1 test dataset . our model achieves the best performance with the state - of - art word2vec embeddings . however , it does not improve the micro f1 score by much .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the attentive neuraldater significantly outperforms all previous methods .
3 shows the accuracy ( % ) of our method for word attention and graph attention compared to the previous state - of - the - art model .
3 shows the performance of our model on the argument and argument stages . we observe that the jnn model outperforms the previous state - of - the - art on all three stages except the one in which it performs worse .
3 presents the results of our method on the event identification task . our method obtains the best results with a precision of 68 . 7 % and a f1 of 44 . 1 % , respectively . the results are presented in table 3 . table 3 shows the results for both event identification and event classification .
results are shown in table 3 . all the models shown in the table 3 outperform all the models except for the spanish - only model . the results are summarized in table 4 .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data in the training set .
5 shows the performance on the dev set and on the test set , compared to the monolingual model .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset shows statistically significant improvement in precision ( p ) , recall ( f1 ) and f1 - score ( f2 ) .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) and recall ( f1 ) are shown in table 5 . the improvement in precision ( p < 0 . 01 ) is statistically significant , indicating that the combination of the two features improves recall and recall .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the glove - retro embeddings are derived from the original paper , and are used in wordnet 3 . 1 . however , they are not syntactic - sg - based , and do not have the syntactic embedding features obtained by using autoextend rothe and schütze ( 2015 ) .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 .
2 shows the results of domain tuning for image caption translation . the results are summarized in table 2 . subdomain tuning improves the multi30k performance by 3 . 5 points compared to the previous state of the art model .
3 shows the performance of subs1m overdomain - tuned models on en - de and flickr16 . the results are summarized in table 3 . the results of domain - tuning are shown in bold . we observe that the results are consistent across all models , with the exception of mscoco17 , which shows that domain tuning improves performance . subdomain tuning improves the results for all models .
4 shows bleu scores in terms of multi30k captions . the results are summarized in table 4 . the best results are obtained by adding automatic image captions ( dual attn . ) in the final set . however , the best results with mscoco17 captions are obtained with the exception of the multi30k one .
5 compares the performance of our approaches with those using en - de and dec - gate . the results are summarized in table 5 . we observe that the enc - gate approach outperforms the other approaches in terms of bleu % scores .
3 shows the performance of subs3m on the en - de dataset compared to subs6m . subensemble - of - 3 achieves the best performance on both datasets , with the exception of mscoco17 , which achieves a better performance on the text - only dataset . also , the performance improvement on the multi - lingual dataset is modest , with a gap of 0 . 5 points between the performance between subs6 and subs7m . this is due to the large number of visual features in the dataset , and the large variation in performance between the two datasets .
results are shown in table 3 . we observe that the en - fr - ff model outperforms both the original and the original ones on the mtld test set . the results are summarized in table 1 .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used for the training splits is shown in bold .
2 shows the performance of our model on the english , french and spanish datasets . the results are summarized in table 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system performs better than the previous state - of - the - art rev system .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs model achieves the best performance when trained on a hierarchical supervised model .
results on synthetically spoken coco are shown in table 1 . we observe that the visual supervised model performs better than the acoustic supervised model by a margin of 3 . 9 points .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that has edges at the edges ; it ’ s so clever you want to hate it . similarly , rnn turns in an u . screenplay screenplay of < u . edges edges edges shapes so clever that you want hate hate it to be removed .
2 presents the results of fine - tuning on sst - 2 . the results are presented in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same through fine tuning . however , the percentage of occurrences that have not changed with respect to the original sentence has not increased .
3 shows the change in sentiment between positive and negative labels with respect to sst - 2 . the results are shown in table 3 . positive labels are flipped to positive and vice versa .
results are presented in table 3 . the results are summarized in table 4 . we observe that the performance gap between positive and negative is relatively small ( 0 . 54 % ) compared to sst - 2 . as expected , the difference between positive vs . negative pmi is significant . however , it is not clear whether the difference is due to the small size of the corpus or to the fact that it is difficult to distinguish between the two .
