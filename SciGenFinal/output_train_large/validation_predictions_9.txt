2 shows the performance of the treelstm model on training and inference with the large movie review dataset . throughput improves on inference with efficient parallel execution of tree nodes , while the recur approach shows better performance on training . inference improves on inference and inference , respectively .
table 1 shows the performance of the treernn model when the batch size increases from 1 to 25 , and when the number of iterations increases from 10 to 25 . the overall performance is comparable to that of the linear model , which exhibits higher throughput .
2 shows the performance of the max pooling strategy for each model with different representation . the maximum pooling approach consistently outperforms all models with different number of hyper parameters . as shown in table 2 , the max pooling strategy consistently performs better in all model variations . in addition , the sigmoid and softplus model outperform all the other models using the same representation .
1 shows the effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) with sdp shows the f1 of the relation type without sdp . in the macro - averaged model , the best relation f1 is achieved in five - fold . we also observe that the shorter dependency path can improve f1 by 3 . 5 points .
3 shows the performance of the y - 3 model on the f1 and r - f1 test sets compared to the previous state - of - the - art models .
3 shows the performance of our mst - parser on the essay level . the results are presented in table 3 . the best performance is achieved on the paragraph level f1 and f1 scores . we also show the performance on the f1 score . as expected , the results are significantly better than those obtained on the test set . in addition , the performance is significantly better on the validation set than on the final set of questions .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 and 57 . 24 ± 2 . 87 respectively , compared to the majority performances of the other two systems .
3 shows the performance of the original and the original models on the test set . the results are presented in table 3 . the original model outperforms all the other models except for tgen , which is more accurate . in addition , the original model performs better than the original on all the test sets except for the one in which it is better than tgen .
shown in table 1 , the original e2e data and the cleaned version are comparable in terms of number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see table 1 .
3 presents the results of the original and the original test sets . the results are presented in table 3 . the original and original test sets outperform all the other test sets by a significant margin . table 3 shows the performance of all the test sets in terms of bleu and nist scores . as expected , the original scores are significantly better than the original ones .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found a total absolute number of errors of 0 , 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 13 , 14 , 15 , 19 , 20 , 21 , 22 , 28 , 21 and 20 , respectively . in addition , we also found a significant number of disfluencies in the original training set .
3 presents the results of our model on the external and external datasets . the results are presented in table 3 . all models outperform all the other models except for seq2seqk ( konstas et al . , 2017 ) and snrg ( pourdamghani and cohen , 2018 ) by a margin of 0 . 2m compared to 0 . 1m in the external dataset . graphlstm ( song et al . 2017 ) achieves a comparable performance to snrg and pbmt on the external datasets .
2 shows the bleu score on amr17 . the model size of the ensemble models is shown in table 2 . as expected , the model size is comparable to the size of seq2seqb ( our model ) in terms of parameters , respectively .
3 presents the results for english - german and english - czech . the results are presented in table 3 . our model outperforms the previous best - performing models in both languages by a significant margin . we observe that the single and the multi - language models outperform the best - aligned models in english - language and german - language .
5 shows the effect of the number of layers inside dc on the overall performance of the system . we observe that when we add layers of layers , the effect is less pronounced on the performance of dc than it is on the other layers .
6 shows the performance of baselines on gcns with residual connections . the results are shown in table 6 . with residual connections , the gcn performs better than all the baselines except for dcgcn2 .
3 shows the performance of the dcgcn models in terms of number of participants compared to the previous state - of - the - art models . the results are summarized in table 3 . we observe that all models are comparable in performance across all three baselines , with the exception of acgcn .
8 shows the results of ablation study on amr15 . we show that removing the dense connections in the i - th block reduces the number of connections . the results are shown in table 8 . adding the 3 dense connections decreases the density of the connections .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules are used in the two encoder models . in addition , they have the advantage of co - ordination , which improves the performance of the two decoder models by 3 . 5 points . the two encoders have the same coverage mechanism as the other two decoders . however , the difference between the two is less pronounced .
7 shows the performance of our initialization strategies on probing tasks . our model outperforms the previous state - of - the - art approaches in terms of depth and length . the results are shown in table 7 .
3 presents the results of our method on the subsjnum and subjnum datasets . the results are presented in table 3 . the results show that our method outperforms all the other methods except for the two that we use . as expected , the results are slightly worse than those of the other two methods . we also observe that the h - cmow / 400 model outperforms the other models in terms of depth and subtense .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other models except for subj , which is closer to the best - performing model . it also outperforms both the sst2 and sst5 datasets in terms of performance .
3 shows the relative change with respect to hybrid compared to cbow on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both the hybrid and the hybrid models by a significant margin .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the other approaches except subj and mpqa . it also outperforms both the sst2 and sst5 baseline by a margin of 3 . 5 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performance is on the sts12 and sts14 tasks , which are supervised by cbow - r ( see table 6 ) .
results are presented in table 3 . the best performing models are cbow - r and somo - r , both of which outperform the previous models in terms of depth and subjnum . as expected , the best performing ones are the ones with the shortest length and the highest subjnum , respectively .
3 presents the results of our model on the subj and mpqa datasets . our model outperforms all the other models except for sst2 and sst5 , which are superior in terms of performance . we also outperform all the models except sick - e , which is comparable in performance to sst1 .
3 shows the e + org and per scores for all systems in [ italic ] e + and e + e + per scores . all org scores are reported in table 3 . in ( italic ) e + misc scores are shown in table 4 . name matching and misc are the most important metrics for e + loc scores , misc is the only one that is more important than org . the results are summarized in table 5 . our system outperforms all the other systems except for the one that does not use org or per . we observe that all the systems that do not rely on org are better than all the ones that do . as expected , all the system that does rely on the org score is better than any other system except those that rely on misc score .
2 shows the performance of our system on the test set under two settings . our system outperforms all the previous models in terms of e + p and f1 scores . the results are shown in table 2 . we observe that the system performs better than the previous model in all three settings . in all three settings , the system achieves the best performance . our model outperforms the previous state - of - the - art model in e + f1 score .
6 : entailment ( ent ) and ref ( ref ) in the model outperform all the other models except for s2s - ggnn , which outperforms all the models except ref . in addition , ref outperforms ref in all but one case , ref is more effective than ref on all models except g2s .
3 shows the performance of the models on the ldc2017t10 and ldc2015e86 datasets compared to the previous state - of - the - art models . the results are shown in table 3 . our model outperforms all the other models on both datasets except for ldc2016e86 , which is comparable in terms of performance .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 .
4 shows the ablation results of the ldc2017t10 development set on the development set . the results show that bilstm significantly improves the performance of the model compared to the previous state - of - the - art models .
results are shown in table 3 . the model outperforms the g2s - ggnn model in terms of sentence length and sentence length . in particular , the model shows a significant drop in sentence length from 0 - 20 compared to the previous state - of - the - art model , which shows a drop of 3 . 5 % compared to 0 - 7 . 5 % .
8 shows the fraction of elements in the output that are missing in the input ( added ) and the fraction of elements that are not present in the generated sentence ( miss ) . in the ldc2017t10 test set , the model outperforms the reference sentences in terms of fraction and miss , respectively .
4 shows the accuracy of our approach with different target languages on a smaller parallel corpus ( 200k sentences ) . our approach outperforms the previous state - of - the - art approaches in terms of accuracy .
2 shows the accuracy of our model with baselines and an upper bound . the results are shown in table 2 . our model improves upon the previous state - of - the - art model by 3 . 5 points .
results are presented in table 4 . table 4 shows the performance of our models in terms of pos tagging accuracy and f1 scores . our model outperforms the previous state - of - the - art models in both terms of accuracy and precision . we observe that our model significantly outperforms both the original model and the original one .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional nmt encoders , averaged over all non - english target languages .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . in the case of pan16 , the difference is 10 . 2 % compared to 10 . 3 % for pan16 .
1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . as expected , pan16 outperforms pan16 in terms of recall .
2 shows the effect of the protected attribute leakage on the balanced and unbalanced task averages . the results show that the presence of the protected attribute leakage leads to a significant drop in the balanced task averages compared to those of the unbalanced ones .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in the case of pan16 , the difference is 0 . 5 points ( 0 . 01 ) compared to 0 . 01 points for pan16 .
6 shows the concatenation of the protected attribute with different encoders . embedding rnn and rnn shows the performance of the encoder with different parameters .
results are shown in table 3 . our model outperforms all the other models in terms of base and finetune models . the results show that our model improves upon the state - of - the - art lstm model by a significant margin . we observe that the model improves the performance of both the original and the original models by significantly improving the performance on the original model .
results are presented in table 5 . our model outperforms all the previous models in terms of time and distance . the results show that the lstm model improves upon the previous state - of - the - art model by a significant margin . in addition , it improves the performance of the previous model by 3 . 5 % over the baseline model .
3 presents the results of our model on the amapolar time and yahoo time datasets . the results are presented in table 3 . our model outperforms all the other models on both datasets except for amafull time . the results show that our model significantly improves the performance of amafull time compared to the previous state - of - the - art lstm model . as expected , the results are slightly worse than those of our previous model , but still comparable to our previous work model .
3 shows the bleu score on wmt14 english - german translation task . the model outperforms all the other models except sru and gru in terms of decoding one sentence . sru outperforms both gru and sru in both languages by a significant margin . gru also outperforms sru by a margin of 0 . 99 % and 1 . 67 % , respectively , compared to the previous state - of - the - art sru . in addition , sru performs better than sru on both languages .
4 shows the performance of our model on squad dataset . our model outperforms all the models except lrn and sru . we observe that our model improves match / f1 score by 2 . 67 % compared to the previous state - of - the - art model . in addition , our model obtains a boost of 1 . 59 % compared with the baseline model .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in the reported result . sru also shows the performance of the lstms in the same task as lrn and lrn . lrn also shows a performance improvement over sru and sru . the performance of sru is comparable to that of lrn ,
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting . our model outperforms the previous state - of - the - art snli model on both tasks .
3 presents the results of the word - based system retrieval task . word - based systems ( mtr ) outperform human systems ( r - 2 , r - 2 ) in terms of system retrieval . sent - based models outperform both human and system - based methods . the word - aligned systems ( svm ) are more effective in system retrievance task , in particular , svm and svm are better than human ones . svm outperforms human systems in system evaluation task in both cases , the svm model outperforms the human one . in the human case , the combination of svm with svm improves the performance of the system . as expected , the human model performs better than the system - aligned ones .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is shown in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . as expected , the best performance is achieved on the k 1000 and k 1000 datasets , respectively , with the highest standard deviation being 1 . 2 ( appr ) .
results are shown in table 3 . our model outperforms all the other models in terms of performance . the best performing models are df and docsub , both of which are comparable in performance to df .
results are shown in table 3 . we show the performance of our models on all three test sets . our models outperform all the other test sets except for the one on the df test set . our model outperforms both the df and df test sets by a significant margin . the results of our model outperform the other two baselines in terms of performance .
results are shown in table 3 . our model outperforms all the other models in terms of performance . the best performing models are df and docsub , respectively , while the worst performing ones are df , docsub and docmax . we observe that the best performing model is df , with the best performance being df .
embeddings are shown in table 1 . the results show that the maxdepth and maxdepth of our models are comparable to those of the other three baselines . we also observe that our model outperforms the other baselines in terms of numberroots and depthcohesion , respectively .
embeddings are shown in table 1 . the maxdepth of our system is significantly higher than the maxdepth on the other two models . our system outperforms both the df and df models by a significant margin . we observe that our system achieves the best performance on the df model , while the df models perform worse .
1 shows the performance of our system on the validation set of visdial v1 . 0 . our enhanced model outperforms the original visdial model by a significant margin . the difference in performance between baseline and the enhanced version is due to the fact that the enhanced model is able to learn hidden dictionary learning . in addition , the difference between the enhanced and enhanced visdial models is less pronounced , indicating that the improved model is more suitable for the task .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . our model outperforms all the other models in terms of hidden dictionary learning ( i . e . , using the history shortcut ) .
5 shows the performance of hmd - prec and wmd - f1 on hard alignments and soft alignments . the results are summarized in table 5 . we observe that the hmd prec model outperforms all the other models except for ruse .
3 presents the performance of our model on the direct assessment test set . our model outperforms all the other models except ruse and sent - mover by a significant margin . as expected , our model significantly outperforms the baselines on both direct assessment and bertscore .
3 presents the baselines for bagel and sfhotel . our model outperforms all the other baselines except bertscore - f1 and meteor by a significant margin . the baselines are significantly better than the bleu - 1 baseline , but are slightly worse than the baseline .
3 presents the metric and baselines scores for each model . the metric scores are shown in table 3 . leic scores are reported in bold , indicating that the model is more suitable for the task at hand . bertscore - recall shows that the models are better than the baselines on the m1 and m2 datasets .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms both the m2 model and the m3 model by significantly improving the performance of both models . as expected , the difference between m0 and m3 shows in terms of performance .
3 presents the results of our model on the transfer quality and transfer quality datasets . the results are summarized in table 3 . we show that our model outperforms the previous state - of - the - art models in transfer quality . as expected , the results are significantly better than those of the previous model .
5 shows the results of human sentence - level validation . the results are shown in table 5 . sim and human ratings of semantic preservation are significantly better than those of the previous generation . moreover , the performance of sim and pp is comparable to that of the other two methods .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model by a significant margin . in particular , it outperforms both the m2 model and the m3 model by significantly improving the performance of the sim model .
6 shows the bleu scores on yelp sentiment transfer compared to prior work . the best models achieve the highest acc ∗ scores compared to the best ones . multi - decoder outperforms all the other models except fu - 1 , which achieves the best acc score . in addition , the best models outperform all the others in terms of sentence embedding . our best model outperforms both the best and the worst ones in both cases . we also observe that the difference between the two models is small , but the difference is large .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent , compared to the percentage that were predicted as nested . reparandum length [ bold ] 2 - 8 shows the average number of tokens that are correctly predicted to be disfluent . the number of repetition tokens is slightly larger than the number of disfluencies .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted to contain the content word is shown in table 3 . in both cases , the number of tokens that contain the word is significantly higher than the number predicted for the function .
3 presents the results of our model on the test set . our model outperforms all the other models in terms of dev and innovations . in addition , our model improves upon the best state - of - the - art model by 0 . 2 % on average . the results are shown in table 3 .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on cnn - based sentence embedding . our model outperforms all the other models on the test dataset except for word2vec .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . our unified model significantly outperforms all previous methods except burstysimdater .
3 shows the accuracy ( % ) of the component models with and without attention compared to the word attention and graph attention for the task of word attention .
3 shows the performance of our model on each stage of the test set . our model outperforms all the other models in terms of performance on all stages except for the argument stage , where our model performs best .
3 presents the results of cross - event training . our method outperforms all the previous methods in terms of identification and classification . we observe that cross - event feature - based training improves the performance of our method by 3 . 5 points over the previous method .
results are shown in table 3 . all models outperform all the models except for those that do not use the word embeddings . all models show the best performance on dev perp , test acc and test wer , respectively . in english , all models show better performance than all models except the ones that use word embedding . the difference between english - only and spanish - only - lm is less pronounced .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev and full train test .
5 shows the performance on the dev set compared to the monolingual set , and on the test set , respectively . our model outperforms both the two approaches .
7 shows the performance of type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show statistically significant improvement over the previous state - of - the - art model .
5 shows the precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . note the significant improvement in recall ( p < 0 . 01 ) compared to type combined gaze features ( p = 0 . 05 ) . the f1 score shows that type combined and type combined features improve recall by a significant margin .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings are used in wordnet and wordnet 3 . 1 , and they are used for embedding synset embedding . glove - retro is used to embed the synsets of wordnet , and it uses syntactic skipgram to embed them . the results on the original paper are shown in table 1 .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . we show that our system outperforms all the other models in terms of ppa accuracy .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model .
2 shows the effect of domain tuning on image caption translation ( bleu % scores ) . subsfull and domain - tuned embeddings significantly improve the performance of the multi30k model , which shows that domain tuning improves the translation performance of image caption .
3 shows the performance of subs1m on en - de and en - fr . the results are shown in table 3 . subdomain - tuned subs1ms outperform all other models except for the ones that do not have domain - tuning . we observe that all models with domain tuning performance are comparable to subs2m , but the difference in performance is less pronounced for the two models that use domain tuning .
4 shows bleu scores in terms of automatic captions ( only the best one or all 5 ) . the results with marian amun are shown in table 4 . the best ones with the best ones ( dual attn . ) and multi30k ( concat ) show that the best image captions are the ones that contain the best captions . as expected , the results with all 5 captions show that using the best five captions is beneficial .
5 shows the bleu % scores of the four strategies for integrating visual information . we show the results of multi30k + ms - coco + subs3mlm with enc - gate and dec - gate as the key features . enc - gate outperforms dec - gates in both visual information and decoding ( bleu % score ) . in the en - de setting , we observe a significant drop in performance compared to the enc - gated setting . the difference in performance between enc - gates and dec + gate is due to the large number of frames in the ende setting .
3 shows the performance of subs3m on en - de and in - de , respectively , compared to subs6m and subs7m , respectively . sub - ensemble - of - 3 shows performance comparable to subs2m , but significantly better performance than subs4m . in - de shows performance similar to subs3 , but with the exception of the multi - lingual feature set , which shows superior performance . also , the performance is comparable to the subs6ms model , which has the advantage of multi - language feature set .
results are shown in table 3 . we observe that the en - fr - ff model outperforms all the other models in terms of translation performance . in addition , we observe that en - rnn - ff improves translation performance by 3 . 7 % compared to the previous best state - of - the - art models .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the training vocabularies for the english , french and spanish data used for our models .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is more suitable for the task at hand . as expected , the system performs better than the previous state - of - the - art rev system .
2 shows the performance of our visually supervised model on flickr8k . the vgs model outperforms all the other models except segmatch , which is more accurate .
results are shown in table 1 . as expected , the vgs model outperforms all the other models in terms of recall @ 10 and chance @ 10 , respectively .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , dan turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . for cnn , we show that the edges of the screenplay are very clever . for rnn , the edges are so clever that it shows the shape of the dialogues . for dan , we see that it is very clever to show the shapes of dialogues in the edges . we also see that rnn turns on a on ( in the the the edges ) and a curve ( in the the corners ) .
2 shows the effect of fine - tuning on the number of words in sst - 2 . the results are shown in table 2 . as expected , the average percentage of words that have been added to the original sentence is 69 . 0 % compared to 69 . 5 % in the previous state of the art . the difference in percentage points is due to the increased number of occurrences of words with respect to original sentence .
3 shows the change in sentiment with respect to the original sentence in sst - 2 . the positive and negative labels are flipped to positive and vice versa . as shown in table 3 , the positive label is flipped to negative and the negative one to positive .
results are presented in table 3 . the results are summarized in terms of the percentage of positive and negative responses ( pmi ) compared to positive ones ( sst - 2 ) . in addition , the performance of sift is comparable to that of pimi ( p < 0 . 001 ) . the performance of ppmi is comparable with pimi ( p > 0 . 01 ) . pimis are comparable to pimpi ( p = 0 . 005 ) . we observe that ppmis are similar to the average of ppmi in both cases . we also observe that the difference in performance between the two cases is due to the small size of the corpus . in turn , we observe the difference between the performance between pimms and pima ( p ≤ 0 . 05 ) . as expected , the difference is small .
