2 shows the performance of the treelstm model on our recursive framework , with the large movie review dataset performing the best on training . the recur approach performs better on training than the recursive approach .
table 1 shows the performance of the treernn model when the batch size increases from 1 to 25 . the results are shown in table 1 . the balanced dataset exhibits the highest throughput compared to the linear dataset . however , it does not improve as well as the balanced dataset .
table 2 shows the performance of the max pooling strategy for each model with different representation . the maximum pooling approach consistently performs better in all model variations . we also compare the performance for all models with different representations .
table 1 shows the effect of using the shortest dependency path on each relation type . the results are shown in table 1 . our model achieves the best f1 ( in 5 - fold ) without sdp . we also observe that our model achieves better f1 with sdp than our model does without .
results are presented in table 1 . y - 3 : y , y , and y ( y - 3 ) have the highest f1 scores , with the exception of r - f1 , which has the lowest f1 score . the results are shown in table 2 .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other models except for mst - parser , which outperforms both our model and our model . we also outperform our model by a significant margin . for example , we outperform all the models except the model that outperforms our model , but we still outperform the model .
4 shows the c - f1 scores for the two indicated systems . the results are shown in table 4 . the best performing system is the lstm - parser .
results are presented in table 1 . the results are summarized in table 2 . the original and the original are shown in bold . the new system outperforms the original in all but one of the three cases . the best performing system is the sc - lstm system , which outperforms all the other systems except for the original .
results are presented in table 1 . we compare our original e2e dataset with our cleaned version . the results are shown in bold . we also compare our clean version with the original , which has the highest number of distinct mrs and the lowest number of errors .
results are presented in table 1 . the original and the original are shown in table 2 . the original and the original are the only two systems that perform better than the original on the test set . the second system performs slightly worse than the first , but still outperforms the second system .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . the results are summarized in table 4 . we found that adding errors can cause disfluencies in the training set . adding errors can also cause disfusion , which can lead to disfluency in training . we also found that removing errors can improve training performance .
results are presented in table 1 . all models outperform all the other models except for the ones that have the best performance . the results are shown in table 2 .
results on amr17 are presented in table 2 . our model achieves 24 . 5 bleu points , which is significantly higher than our model size . our model size is comparable to that of seq2seqb .
results are presented in table 1 . our model outperforms the previous best - performing models in both english - german and english - czech . the results are shown in table 2 .
table 5 shows the effect of the number of layers inside dc on the overall performance of the dc layer . we observe that dc layers have a significant effect on the performance of dc layers , as shown in table 5 . the effect of these layers on dc layer performance is shown in fig . 5 .
6 shows the performance of baselines on gcns with residual connections . the results are shown in table 6 . we observe that the baselines have a significant effect on gcn performance .
results are presented in table 1 . we observe that the dcgcn model outperforms the previous models in terms of performance . the results are shown in table 2 .
8 shows the results of ablation study on the dev set of amr15 . the results are presented in table 8 . we observe that the dense blocks are significantly less dense than the dense ones .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are presented in table 9 . we observe that the model outperforms all the other models in terms of coverage , with the exception of dcgcn4 , which has the best coverage .
7 shows the performance of our initialization strategies on probing tasks . our results are shown in table 7 . our model outperforms all the other approaches except for the ones we used in the previous study . we also outperform all the others .
results are presented in table 1 . the results are summarized in table 2 . we observe that our method outperforms all the other methods except for h - cmow and h - cbow . we also observe that the method performs better than the other two methods .
results are presented in table 1 . our model outperforms all the other methods except for subj , which outperforms both subj and subj by 0 . 2 % and 0 . 4 % respectively . we also outperform all other models except sick - e and sst2 , which both outperform subj . however , we still outperform the other two methods .
3 shows the performance of our models on unsupervised downstream tasks attained by our models . the results are shown in table 3 . cbow scores are significantly lower than hybrid scores , indicating that cbow is more likely to perform better than hybrid on downstream tasks . we also observe that the difference in performance between cbow and hybrid is significant .
8 shows the performance of our initialization strategies on supervised downstream tasks . our model outperforms all the other approaches except for sst2 and sst5 , which both outperform all the others . we also outperform most of the other models on the supervised downstream task .
6 shows the results for different training objectives on the unsupervised downstream tasks . the results are presented in table 6 . the cmow - r model outperforms the cbow - c model on all three tasks except sts13 and sts14 . the cbowr model performs better than the other two models .
results are presented in table 1 . the results are shown in table 2 . the best performing method is cbow - r , which outperforms all the other methods except for the best performing ones . it also outperforms the best ones by a significant margin . however , it does not outperform all the others .
results are presented in table 1 . the results are summarized in table 2 . we show that our model outperforms all the other models except for sst2 and sst5 . we also show that the model performs better than the other two models in terms of performance . however , the performance of our model is slightly worse than those of our other models .
results are presented in table 1 . our system outperforms all previous systems in e + org , e + per , and e + e + misc . we also outperform all other systems except for mil - nd , which outperforms the previous system by a significant margin . the results are shown in table 2 . in e + loc , we outperform the previous systems by a considerable margin .
results on the test set under two settings are shown in table 2 . the results are presented in the table 2 . our system outperforms all three models except for mil - nd ( model 2 ) . our system performs better than the other two models in all three settings . our model performs better on the e + p and e + f1 tests .
6 : entailment ( ent ) results are presented in table 6 . the results are shown in bold . the model outperforms all the other models except for s2s - ggnn , which outperforms the model by a significant margin .
results are presented in table 1 . the results show that the model outperforms the model in terms of performance . we also observe that the models outperform the models in both the model and the model .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are presented in table 3 . our model outperforms all other models except for those trained with gigawords .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the ablation results show that bilstm can significantly improve the performance of the model on the development set .
results are presented in table 1 . our model significantly outperforms the previous model in terms of sentence length and sentence length . the results are shown in table 2 . we also observe that our model significantly improves sentence length over the previous models .
results are shown in table 8 . our model outperforms all the other models except for the ones that are missing in the input graph .
4 shows the accuracy of our model on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our model outperforms all the other models in terms of semantic accuracy .
results are presented in table 2 . our model outperforms all previous models except for word2tag , which has the highest accuracy with baselines .
results are presented in table 1 . the pos tagging accuracy scores are reported in table 2 . the average score is 91 . 9 % on average , which is slightly better than the average score of 91 . 8 % on the average . our results are summarized in table 3 .
5 shows the accuracy of our 3 - layer uni / bidirectional / residual nmt encoders over all non - english targets .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ' s accuracy is shown in table 8 .
table 1 shows the results of training directly towards a single task . the results are presented in table 1 . the results show that the training method significantly improves the performance of pan16 and pan16 .
table 2 shows the results of the protected attribute leakage experiments . the results are presented in table 2 . the results show that we have a significant drop in the number of instances where we are able to distinguish between balanced and unbalanced instances .
3 shows the performance of the adversarial training on different datasets with different training settings . the performance of pan16 is shown in table 3 . the performance on pan16 shows the difference between the attacker score and the corresponding adversary score . for pan16 , the performance is significantly worse than pan16 . however , the difference is less pronounced for pan16 and pan16 with the same training set .
6 shows the performance of the protected attribute with different encoders . the results are shown in table 6 . the best performing encoder is rnn , followed by rnn and rnn .
results are presented in table 1 . our model outperforms all the other models in terms of performance . the results are shown in table 2 . we also observe that our model performs better than all the models except for the model that outperforms both the model and the model .
results are presented in table 5 . our model outperforms all the previous models in terms of time and accuracy . we show that our model performs better than all the other models except for gru and sru .
results are presented in table 1 . our model outperforms all the other models in terms of err . the results are shown in table 2 . we show that our model improves upon the performance of the previous model by a significant margin . this model improves on both the yahoo err and the amapolar time err by a considerable margin .
3 shows the bleu score of our model on wmt14 english - german translation task . our model outperforms all the previous models in terms of bleus score .
4 shows the performance of our model on squad dataset . our model outperforms all the other models in terms of match / f1 scores . we also observe that our model performs better than all the models except for the model that outperforms the model . the results are shown in table 4 . as expected , the model performs worse than all models except sru and sru .
6 shows the performance of our model on conll - 2003 english ner task . our model achieves a f1 score of 90 . 94 , which is significantly better than the average score of 89 . 94 . we also observe that our model performs better than our previous model on our ner tasks . the results are shown in table 6 . lstm achieves a better performance than our original model . however , our model still performs worse than the previous model .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting setting .
results are presented in table 1 . word and word are used to improve the performance of the system . the word and the word are better than the word , but the word performance is worse than the system performance . in particular , word and word are more effective than the system retrieval performance .
4 shows the results of human evaluation on grammaticality , appropriateness , and content richness . the best results are shown in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the second best result is candela , which is ranked in the top 1 or 2 .
results are presented in table 1 . the results are shown in table 2 . our model outperforms most of the other models in terms of performance . we also observe that our model performs better than the best on all three models . however , the performance of our model is slightly worse than those of our other models .
results are presented in table 1 . the results are shown in table 2 . we observe that the results are comparable to those of the previous models . our model outperforms all the other models in terms of performance . we also observe that our model performs better than the previous model , but still performs worse than our model .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other models in terms of performance . we also observe that our model performs better than the best on all the models except for our model .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other models in terms of depthcohesion . our models outperform all the others except for our model , which outperforms our model by 1 . 78 points .
results are presented in table 1 . the results are summarized in table 2 . our model outperforms all the other models in terms of depthcohesion . our models outperform all the models except for our model , which is better than our model . however , our model is worse than our best model . we also outperform both our model and our model by a significant margin .
results are presented in table 1 . our model performs better than the original visdial model on the validation set of visdial v1 . 0 . the results are shown in the table 1 .
2 shows the performance of the ablative studies on different models on visdial v1 . 0 validation set . the results are shown in table 2 . the best performing ablative study is on the baseline model . the worst performing ones are on the second model .
5 shows the performance on hard and soft alignments compared to hard alignments . the results are summarized in table 5 .
results are presented in table 1 . our model outperforms all the other models except for ruse and w2v . the results are shown in table 2 . as expected , the models outperform all the models except ruse . we also observe that the baselines are significantly better than the baseline .
results are presented in table 1 . the bleu - 2 model outperforms all the baselines except for the bertscore - f1 model . the baselines show that the model is more accurate than the baseline , which indicates that it is more likely to outperform the baseline .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other models except for wmd - 1 , w2v , and w3v . we also observe that our model improves upon the previous model by 0 . 7 points .
results are presented in table 3 . the results show that the m0 model outperforms the m1 model by a significant margin . we also observe that m0 models outperform m1 and m2 models by a large margin .
results are presented in table 1 . the results show that our model outperforms all the other models in terms of transfer quality and transfer quality . we also observe that the models outperform all other models except for the ones that do not have transfer quality ( e . g . , m2 and m7 ) . we observe that these models are significantly better at transfer quality than those that do .
5 shows the results of human sentence - level validation . the results are shown in table 5 . the results of the human sentence level validation are comparable to those of the machine and human . we also observe that the human sentences are more accurate than the machine ones , indicating that the sentence level is better than the human ones .
results are presented in table 1 . the results show that the m0 model outperforms the m2 model by a significant margin . we also observe that m0 models outperform m2 models in terms of performance , with the exception of m1 models which outperform both m2 and m2 .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu than any prior work , but it is significantly worse than our best model . our best models also achieve the highest acc ∗ score . the results are presented in row 6 . we also show that we can improve our model by using different classifiers in the same sentence .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . the average number of disfluencies is 6 - 8 , and the number of repetition tokens is 8 - 8 .
3 shows the percentage of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the percentage of tokens predicted to contain the content word is shown in table 3 .
results are presented in table 1 . the results show that our model outperforms all the other models in terms of dev mean and innovations mean . the results are shown in table 2 .
2 shows the performance of word2vec on the fnc - 1 test dataset . our model outperforms the state - of - art word2vec embeddings on both test datasets . however , our model still performs worse than the state of the art word - processing algorithms .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the best performing method is burstysimdater .
3 shows the performance of our model with and without attention . the results are presented in table 3 . the average score is 63 . 9 % for word attention compared to 61 . 8 % for graph attention . the best performance is on word attention .
results are presented in table 1 . our model outperforms all the other models except for jrnn and jnn , which outperform all the models except jnn .
results are presented in table 1 . our method outperforms all previous methods in terms of identification and classification . our approach outperforms the previous methods by a significant margin . our model outperforms both the previous method and the previous one by a considerable margin . we also outperform the previous two methods in both classification and identification . in both cases , we outperform our previous method by a large margin .
results are presented in table 1 . all models are shown in the table . all models have the best performance on test perp , test acc , and test wer . however , all models only have the worst performance on dev perp .
4 shows the results on the dev set and the test set using discriminative training with only subsets of the code - switched data .
5 shows the performance on the dev set compared to the monolingual set on the test set . the results are shown in table 5 . our model achieves the best performance on both sets .
results are shown in table 7 . for the conll - 2003 dataset , we trained the type - aggregated gaze features trained on all three eye - tracking datasets and tested them on the same three datasets . we found that the precision and recall scores were significantly better than the recall scores for all three .
5 shows the results for the conll - 2003 dataset . the results are shown in table 5 . the precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the same dataset are significantly better than the baseline ( p ) .
results on belinkov2014exploring ’ s ppa test set are presented in table 1 . the hpcd system is based on the original paper , and it uses syntactic skipgram to embed embeddings in wordnet and verbnet . the results are shown in the table 1 . glove - retro is derived from the original work , and is based upon the original research paper ( 2014 ) . the results show that using syntacticskipgram improves the performance of wordnet , verbnet and wordnet .
results are presented in table 2 . the rbg dependency parser outperforms all the pp attachment predictors and oracle attachments in uas .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model .
results are presented in table 2 . subdomain tuning improves image caption translation performance by 3 . 5 % compared to multi30k ( bleu % scores ) . domain tuning improves translation accuracy by 2 . 5 % .
results are presented in table 1 . we observe that subs1m outperforms subs2m in both en - de and in - de settings . the results are shown in table 2 . subdomain - tuned models outperform subs3m models in all but one of the three cases . our model outperforms all subs1ms models except for the ones with domain tuning .
4 shows the bleu scores for all models with marian amun as the model . the results are presented in table 4 . the best ones are shown in bold , while the worst ones are displayed in bold .
5 shows the performance of the three strategies for integrating visual information . our model outperforms all the other approaches except for the enc - gate and dec - gate . the results are summarized in table 5 . we observe that the combination of dec - gates and enc - gate improves the bleu % scores for all three strategies .
results are presented in table 1 . we observe that subs3m is better than subs6m in terms of multi - lingual features . the results are shown in table 2 . our model outperforms all the other models except for subs2m , which outperforms the model by a significant margin .
results are presented in table 1 . the results are shown in table 2 . our model outperforms all the other models except for en - fr - t - ff and en - rnn - ff , which both outperform all the others . we also observe that our model performs better than the other two models .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . we also used the same language pairs in the development splits .
2 shows the performance of our training vocabularies for the english , french and spanish datasets .
5 shows the performance of the rev systems on the bleu and ter scores . the results are shown in table 5 . we observe that the automatic evaluation scores are significantly better than those of the manual evaluation scores , indicating that the system is more suitable for the task at hand .
results on flickr8k are presented in table 2 . the vgs model is the visually supervised model from chrupala2017representations , and the rsaimage model is trained on the same dataset as the original model .
results are presented in table 1 . our model outperforms all the previous models in terms of recall @ 10 and recall @ 15 , respectively . the results are shown in the table .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a screenplay that < u > at the edges ; it ’ s so clever you want to hate it . dan < c > and rnn ( in the same sentence ) turn on the edges of the screenplay . dan also turns on the edge of the script , which is so clever that it makes it easier to hate the script . we also show that dan turns on a on ( in the the edges ) .
2 presents the results of fine - tuning on sst - 2 . the results are presented in table 2 . we observe that the number of words in the original sentence has increased , decreased or stayed the same through fine tuning . we also observe that there is a significant overlap with the number in the previous sentence .
3 shows the change in sentiment in sst - 2 compared to the original sentence . the results are shown in table 3 . the results show that the positive and negative labels are more likely to be flipped to positive .
results are presented in table 1 . the results are summarized in table 2 . the best results are shown in bold , while the worst ones are in italics . the worst results are highlighted in red . we also see that the results are more likely to be positive than negative , indicating that the performance of the model is less likely to improve . however , we see that it is more likely that the model to improve performance will improve .
