2 shows the performance of our recursive approach on the large movie review dataset compared to our iterative approach , which shows better performance on training compared to the recur approach .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset when the batch size increases to 25 .
2 shows the results for each model with different representation . we show that the max pooling strategy consistently performs better in all model variations . we also show the performance of sigmoid and softplus models with different representations .
1 shows the effect of using the shortest dependency path on each relation type . our model outperforms the macro - averaged model in terms of f1 , diff . and f1 score . we observe that our model obtains the best f1 ( in 5 - fold ) with sdp and the best diff . score ( in 10 - fold ) . we also observe that the approach obtains better f1 scores than the macro alternative .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models in terms of f1 and f1 scores .
3 shows the performance of our model on the essay level . the results are presented in table 3 . our model outperforms all the other models in terms of performance . we show that our model achieves the best performance on the paragraph level . we also show that the model achieves a high performance on both the test set and the evaluation set . we observe that the best results are achieved on the validation set . the best performance is obtained on the final set , where the average number of entries is closer to 50 % than the average level . this shows that the mst - parser model performs well on both test set .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level , and lstm - parser level . note that the mean performances are lower than the majority performances for both systems .
results are shown in table 3 . we show the results of the original and the original tests on the test set . the results are presented in tables 1 and 2 . our system outperforms all the other test sets except for the one in which it is completely clean . we also observe that our system performs better than the original on all test sets . we observe that the performance of our system is more consistent across all test set , with the exception of the ones in which the performance is less consistent .
shown in table 1 , we compare our original and our cleaned e2e data with the original ones . the results are shown in tables 1 and 2 . we show that the original and the cleaned versions have the highest number of distinct mrs and the highest ser as measured by our slot matching script . we also show that our cleaned version has higher mrs than the original .
results are shown in table 3 . the original and the original embeddings show that the tgen + models perform better than the original on all test sets except for the one in which they are used . in addition , the svm performs better than both the original and the original in terms of bleu score .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that removing incorrect values from the training set significantly reduces the number of errors . we also found that adding incorrect values significantly improves the accuracy of the training .
3 shows the performance of all models compared to the previous best state - of - the - art models . for example , all models outperform all models except for snrg ( song et al . , 2017 ) in all but one of the cases , while snrg outperforms all the other models in the other two .
2 shows the results on amr17 . our model achieves a bleu score of 25 . 5 on the model size in terms of parameters , compared to the previous best state - of - the - art model , ggnn2seqb ( beck et al . , 2018 ) . the results are summarized in table 2 . we observe that our model size is comparable to that of seq2seqb , but larger than our ensemble model .
3 shows the results for english - german and english - czech . the results are presented in table 3 . our model outperforms all the other models except for bow + gcn ( beck et al . , 2018 ) in terms of english - language performance . we show that our model performs better than the best in both languages . the results of our model are summarized in table 4 . we show the results of the single - language model in english - korean and german , respectively . in both languages , we show that the model performs best in the multi - language setting .
table 5 shows the effect of the number of layers inside dc on the performance of the model . we observe that in the first case , the layers inside the dc are larger than those inside the other layers . in the second case , there is a significant drop in performance compared to the previous state of the art model .
6 shows the performance of gcns with residual connections compared to baselines . gcn + rc ( 2 ) shows that the gcn has residual connections with the residual connections . however , gcn + rc ( 4 ) shows lower performance than the baselines , indicating that residual connections are important .
3 shows the performance of dcgcn models in relation to the bias metric . the results are presented in table 3 . we observe that the model outperforms all the other models in terms of performance . in particular , dcgcgcn ( 1 ) achieves a better performance than the other two models in both cases .
8 shows the ablation study for amr15 . - { i , 4 } dense blocks denotes removing the dense connections in the i - th block , while - { 3 , 4 } , dense blocks denote removing the layers of dense connections . table 8 shows that - { 2 , 3 , 4 } } dense blocks have the highest correlation with the dev set , and - { 4 } dense connections have the lowest correlation .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . encoder modules have the best correlation with the graph attention dataset , and the best co - ordination model has the worst correlation . the graph attention module has the most coverage , with a gap of 3 . 5 points .
7 shows the performance of our initialization strategies on probing tasks . our paper shows that our approach outperforms the state - of - the - art approaches on all three tasks . the results are clear in table 7 : our approach obtains the best performance on the depth task . our method obtains a better score than glorot , but is slightly worse than our approach .
results are presented in table 4 . we show the results of our method on the subtraction and subtraction metrics . our method outperforms all the other methods except for h - cmow / 400 . we observe that our method obtains the best performance on all metrics except for the subjnum metric . it obtains a better performance than the other approaches . we also observe that the h - cbow model obtains better results than the best ones on the other metrics .
3 shows the performance of our cbow / 784 model compared to other approaches . our model outperforms all the other models except subj and mpqa except for sst2 , which is better than subj . it also outperforms both sick - e and sst5 in terms of mrpc performance . it is clear that our model is more suitable for the hybrid and hybrid contexts . it obtains better performance than both subj ( which obtains a better mrpc score ) and subj , but it obtains lower mrpc scores than sst1 .
3 shows the performance of our model on unsupervised downstream tasks attained by our model . cbow shows the relative change with respect to hybrid compared to cbow . the results are shown in table 3 . the cbow model outperforms both hybrid and cmp in all downstream tasks .
8 shows the performance of our initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the state - of - the - art approaches except for subj and mpqa . subj outperforms both sst2 and sst5 in all aspects except for mrpc . our approach shows that subj performs better than subj on all downstream tasks , with a slight improvement over sst1 on the mrpc task .
6 shows the results for different training objectives on the unsupervised downstream tasks . we show that cbow - c outperforms cbow on all three tasks except sts12 , sts14 , and sts15 . the results for both the supervised downstream tasks are shown in table 6 .
results are presented in table 3 . the best performing models are cbow - r and somo - r , both of which show strong performance on the subtraction and subtraction metrics . however , the best performance is on the subjnum metric , which shows the performance of the best performing method on both datasets . the results are shown in table 4 . the cbow model outperforms all the other methods except for somo , which is better at subtraction . we observe that the best performances are on the subsjnum metrics , where the performance is better than somo and wc .
3 presents the results of our model on subj and sick - r . we show that our model outperforms all the other methods except for subj . we observe that subj outperforms both the sst2 and sst5 models in terms of mrpc performance . we also observe that cbow - r outperforms subj in both mrpc and mpqa performance . it is clear that our approach obtains better results than subj , but it does not achieve the best results . we note that our method obtains superior results than the other approaches .
3 shows the e + org and per scores for all systems . our system outperforms all the systems except for the one that does not use the org feature . the results are shown in table 3 . our model outperforms the best state - of - the - art systems in e + loc and e + per scores . it is clear that our model performs better than the best in all three metrics . we also observe that our approach obtains better results than all the other approaches .
2 shows the results on the test set under two settings . our system outperforms all the models in e + p , e + r and f1 scores . the results are shown in table 2 . our model achieves the best performance in all three settings , with 95 % confidence intervals of f1 score . we observe that the system performs better than all models except for the model trained on the original model , which is more suitable for the task of name matching .
6 shows the results of ref and ref on the model compared to the previous state - of - the - art models . the results are summarized in table 6 . ref outperforms ref in all but one of the three cases . ref is more effective than ref , but ref does not outperform ref .
3 shows the results of models trained on the ldc2015e86 dataset . the results are presented in table 3 . our model outperforms all the other models except for the ones trained on ldc2017t10 and ldc2016e86 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms all the other models except for the one that is trained with the additional data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms all the other models in terms of size and feature size .
results are shown in table 3 . we observe that the model significantly outperforms the g2s - gin model in terms of sentence length and sentence length . in particular , the model obtains lower sentence length than the model , which indicates that it is more likely to derive sentences with shorter sentences .
shown in table 8 , the fraction of elements in the input that are missing in the output that are present in the generated sentence ( g2s - gin ) . this shows the performance of the model in the ldc2017t10 test set compared to the previous state of the art model .
4 shows the performance of the two approaches using the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our approach outperforms the previous state - of - the - art approaches in terms of semantic accuracy .
2 shows the accuracy of our classifier with baselines and an upper bound . the results are shown in table 2 . our model outperforms all the other models except word2tag , which is more likely to use baselines .
results are presented in table 3 . our results show that our method outperforms all the other methods in terms of accuracy . our model outperforms the best performing ones in all but one of the four categories . we observe that the accuracy of our method is significantly better than the performance of the other two .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we show that the best performing uni model is res , while res has the worst performance .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown by the percentage of trained responses that are trained on a training set with 10 % held - out .
1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . our model outperforms all the other models except pan16 , which shows that the training data are more suitable for the task .
2 shows the state of the art in terms of classifier leakage . we observe that the word " race " has the most significant effect on the performance of the classifier , leading to a drop in performance for both classifiers .
3 shows the performance on different datasets with an adversarial training . the performance on the training datasets is shown in table 3 . we observe that the training dataset with the most training data has significantly higher performance than the one with the least training data .
6 shows the concatenation of the protected attribute with different encoders . we show the performance of rnn over embedded and guarded embeddings . the results are shown in table 6 . the asymmetric nature of the encoded attribute is evident in the fact that rnn is more sensitive to leaky than rnn .
3 shows the performance of our model on the two datasets . our model outperforms the state - of - the - art lstm on both datasets . we observe that our model performs better than the best on the other datasets . this shows that our approach is more suitable for the task at hand . the results are shown in table 3 . the results of our work show that our system outperforms all the other models in terms of training performance .
results are presented in table 5 . we show the performance of our model in relation to the time taken to train the models . our model outperforms all the other models in terms of training time , with the exception of the lstm model , which performs better on the time - to - model task .
3 shows the performance of our model compared to previous work on the amapolar time dataset . the results are presented in table 3 . our model outperforms all the other models in terms of err performance . we observe that our model performs better on the yahoo time dataset than on the amafull time dataset , which shows that the model is more suitable for the task at hand .
3 shows the bleu score on wmt14 english - german translation task . our model outperforms all the other models except sru , which is better at decoding one sentence . we observe that sru performs better than sru and sru in both english and german translation tasks . however , sru does not perform as well in both languages .
4 shows the performance of our model on squad dataset . our model outperforms all the models except lrn and sru in terms of match / f1 score . as expected , our model performs better than all models except for lrn , which performs worse than sru .
6 shows the f1 score on conll - 2003 english ner task . the lstm model outperforms all the models except sru and sru in terms of parameter number . the sru model achieves the best performance with a f1 of 90 . 94 .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
results are shown in table 3 . word embeddings are used to improve the performance of system retrieval . sentiment is beneficial for both human and system retrieval , with the exception of the human model , which is more suitable for system retrieeval . the results are summarized in table 4 . human models outperform human models in terms of performance on both systems . the results of human models are presented in table 5 . we show that word embedding improves the performance for human models as well as the human models .
4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance is shown in bold , with statistical significance marked with ∗ and p < 0 . 0005 . our system outperforms all the other systems except seq2seq and retrieval .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models except for the one we use in the lab . we observe that our approach outperforms both the df and tf models in terms of performance . the df models outperform both the tf models and the tf ones , but we observe that the df model performs better than the tf model . for the tf and tf datasets , we see that our model is more suitable for both datasets .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models except for the one we use in corpus . we observe that our model performs better than most of the models in corpus and is comparable in terms of performance . however , it does not perform as well in corpus as our model does in docsub .
results are shown in table 3 . the results are presented in tables 1 and 2 . our model outperforms all the other models except for the one we use in the lab . we observe that our approach outperforms both the df and tf models in terms of performance . we also observe that the df model performs better than the tf model , but is slightly worse than tf and tf model .
3 shows the performance of our model on the metric compared to the previous best state - of - the - art model . our model outperforms all the other models except for the one we use in table 3 . we observe that our model performs better than the best on all metrics except the metric of depthcohesion . this shows that our approach is more effective than our best model .
3 shows the performance of our model on the metric compared to the baseline . our model outperforms all the other models on both metric and metric metrics except for the metric of depthcohesion . we observe that our model performs better than the baseline on all metric metrics , but is slightly worse than our baseline on metric metrics . the results are summarized in table 3 . our approach shows that our approach achieves the best performance on metric terms , with a slight improvement over our baseline over our approach .
1 shows the performance of our model on the validation set of visdial v1 . 0 . lf outperforms the original visdial model in terms of r0 , r2 , r3 , and weighted softmax loss .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . using p2 indicates the most effective ablative model , while using the history shortcut indicates the least effective one .
5 shows the performance of hmd - prec and wmd - f1 on hard alignments and soft alignments . the results are summarized in table 5 . the hmd model outperforms all the other models except for ruse , which obtains a better performance on soft alignment .
3 presents the performance of our model on the direct assessment test set . our model outperforms all the baselines except for those using ruse and sent - mover . the results are summarized in table 3 . we observe that our model significantly outperforms the baseline on all metrics except for one metric : direct assessment . our model obtains the best performance on every metric except for the one where it obtains only the best score .
3 presents the bagel and sfhotel scores on the test set . our model outperforms all the baselines except for bleu - 1 , which obtains the best scores . the baselines are significantly better than the baseline on both sets . for example , our model obtains a score of 0 . 005 and 0 . 072 on the baseline , respectively , compared to the baseline score of 1 . 005 .
3 presents the metric and baseline scores of our models . our model outperforms all the baselines except for spice , which is closer to leic and spice . the results are summarized in table 3 . we observe that our model obtains the best metric score on both the m2 and m2 datasets . it obtains a score of 0 . 939 and 0 . 749 on the leic dataset , respectively , compared to the f1 baseline .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in all but one of the four scenarios . in both cases , the model performs better than the previous state of the art model .
3 presents the results of our model on the transfer quality and transfer quality metrics . we show that our model outperforms the state - of - the - art on all three metrics except for the semantic ones . we observe that the semantic preservation metrics are significantly better than the semantic one , indicating that semantic preservation is more important than semantic preservation . we also observe that both semantic preservation and semantic preservation are comparable across all three domains .
5 shows the results of human sentence - level validation . the results are shown in table 5 . sim and human ratings of semantic preservation are significantly higher than sim and pp , indicating that the human evaluation is more accurate than the machine evaluation .
results are shown in table 3 . we observe that the m0 model outperforms the m1 model in all but one of the four scenarios . in the sim model , we observe that para + para + lang improves the performance of the model on the sim metric .
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ scores . our worst model outperforms all the other models except for the one that uses the best classifier , delete / retrieve . we observe that the best model performs better than the best in all three cases . the best model is yang2018unsupervised , which shows that it is more likely to use the same classifier in the same sentence . it is clear from table 6 that this is a result of different classifiers in use in the sentiment transfer domain . in addition , it is clear that using the correct classifier helps the model to better interpret sentiment .
2 shows the number of tokens that were correctly predicted as disfluent , compared to the number predicted as nested disfluencies . reparandum tokens are generally shorter than repetition tokens , indicating that the repetition tokens are more likely to be misfluent . for example , the average number of repetition tokens is 3 - 5 , while the average length is 6 - 8 .
3 shows the percentage of tokens correctly predicted to contain a content word in both the reparandum and the repair ( table 3 ) . the fraction of tokens that contain the content word is shown in table 3 . the number of tokens predicted as disfluent is in parentheses , indicating that the disfluencies in the reparandum are more likely to contain content word than in the repair .
results are presented in table 3 . we show the results of our model in the best - performing setting . our model outperforms all the other models in terms of both dev and innovations . in addition , we observe that our model has the best performance on the single - sample test compared to other models . we observe that the best performing model is the text + innovations model .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art embeddings on the cnn - based test dataset . our model achieves the best performance on the micro f1 test set , with an accuracy of 28 . 53 % and an f1 of 82 . 43 % .
2 shows the performance of different approaches on the apw and nyt datasets for the document dating problem . the best performing approach is burstysimdater , which significantly outperforms all previous methods .
3 shows the accuracy of our method for word attention compared to the output of graph attention . the results are shown in table 3 . neuraldater outperforms both graph attention and ac - gcn in word attention . it is clear that neuraldater is better at word attention than graph attention , but it is harder to distinguish between the two .
3 shows the performance of our models in each stage . our model outperforms the state - of - the - art models in all but one of the stages . we observe that the jvmee model performs better than all the other models except for the one in which it performs worse . this suggests that our model is more likely to perform better than the others in all stages .
table 3 , we show the performance of our method in cross - event learning . our method outperforms all the other methods in terms of both identification and classification . we show that our method obtains the best results in both domains , with the exception of the trigger . we observe that our approach obtains better results in the single event learning task than any other method . in both cases , we observe that the system obtains a better understanding of the event than the other approaches . in the event of a single event , we use a single domain learning task to train our model . this suggests that the method is more suitable for cross - event learning .
results are shown in table 3 . all models are comparable in terms of performance on dev perp , test acc and test wer . we observe that all models outperform all the models except for those that do not use the word embeddings . however , all models perform better than all models except the ones that do . we notice that the english - only model outperforms all the other models in both metrics .
4 shows the results on the train dev set and on the test set using discriminative training with only subsets of the code - switched data . we show that fine - tuned train dev outperforms the state - of - the - art in both train dev and train test .
5 shows the performance of our model on the dev set and the test set , compared to monolingual and code - switched models . the results are shown in table 5 . we observe that our model performs better than the monolingually trained model on both dev and test sets .
7 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . note the significant improvement ( p < 0 . 001 ) in precision ( p ≤ 0 . 01 ) compared to the baseline , and the f1 score ( p > 0 . 05 ) .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the results are shown in table 5 . type - aggregation features significantly improve recall ( p < 0 . 01 ) and recall ( f1 ) . the precision ( p > . 01 ) is comparable to the baseline , but the f1 score is significantly lower .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the glove embeddings are derived from the original wordnet and wordnet , and are used in wordnet 3 . 1 . they have the best performance on syntactic - sg embedding , and the best f1 score on wordnet 2 . 0 . they also have the worst performance on semantic embedding . we note that syntactic embedding improves the performance of wordnet by a significant margin , but the performance drop is still significant .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 . we show the results of our model using the best performing ppa accreditation ( hpcd ) feature .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we observe that the removal of context sensitivity reduces the ppa accomplishment by 3 points .
2 shows the results of domain tuning for image caption translation . our model outperforms all the models except for those using domain - tuned embeddings . we observe that domain tuning improves the translation performance for multi30k , while domain tuning decreases translation performance .
3 shows the performance of subs1m overdomain - tuned models on en - de and in - de . the results are presented in table 3 . we observe that the performance obtained by domain - tuning is comparable to the performance achieved by subs2m . in - de models , we observe that domain tuning improves the performance by a significant margin . in - de models outperform all the other models in terms of performance .
4 shows bleu scores in terms of multi30k captions compared to the best ones in the en - de dataset . the results are shown in table 4 . the results with the best captions show that the model is more likely to outperform the model using only one or all 5 captions . as expected , the model outperforms all the models using only the best features .
5 shows the performance of our approach with respect to decoding visual information . we observe that enc - gate and dec - gate embeddings significantly improve the bleu % scores for both approaches . in particular , we observe that decoding the visual information leads to better interpretability . in addition , decoding the information improves interpretability and improves performance .
3 shows the performance of subs3m compared to subs6m in terms of multi - lingual features . our model outperforms all the other models except for the one with text - only features . we observe that the combination of word embeddings and semantic features improves the performance for all models except the one without . the combination of semantic features boosts performance for the models without the need to rely on syntactic features .
results are shown in table 3 . we show the performance of our model on the mtld test set compared to the previous state - of - the - art model . we observe that our model outperforms all the other models except for en - fr - ht , which obtains the best performance .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of lines in each language pair is shown in bold .
2 shows the results for the english , french and spanish data used for our model . the results are shown in tables 2 and 2 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) show that the system is more suitable for the task of re - evaluation than the original rev .
2 shows the vgs performance on flickr8k . the results are shown in table 2 . the vgs model outperforms the standard rsaimage model in terms of recall @ 10 and f1 score .
results are shown in table 1 . our model outperforms all the previous models except for the one that has the highest recall @ 10 .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . rnn shows the same results as the original . dan shows that the edges of the screenplay are more interesting than the edges . rnn also shows that it is so clever to show the edges in the screenplay . it shows that if you want hate hate hate , you need to hate hate it . for rnn , the edges are much more interesting and the curves are much wider . for cnn , we show that using the edges is more appealing than using the curves . for the rnn example , we use the edges edges edges .
2 shows the results of fine - tuning on sst - 2 . the results are shown in table 2 . we observe that the number of occurrences have increased , decreased or stayed the same for the same number of words . we also see that the accuracy of the word " nouns " has increased , but the percentage of words that have not changed significantly .
3 shows the change in sentiment from positive to negative in sst - 2 compared to the original sentence . the results are shown in table 3 . the results indicate that the sentiment increases when the negative labels are flipped to positive and vice versa .
results are presented in table 2 . the results are summarized in table 1 . our model outperforms all the other approaches except for sst - 2 , which shows that our approach is more effective than other approaches . our approach outperforms both the positive and negative aspects of our approach . it is clear that our model is better than both the negative and positive aspects of the approach , and that it is more likely to improve over the negative aspects .
