2 shows the performance of our iterative approach on the large movie review dataset . with the expansion of the batch size from 1 to 25 , the system performs the best on training , while it requires fewer iterations to converge .
1 shows the overall performance of the treernn model implemented with recursive dataflow graphs . the balanced dataset exhibits the highest throughput , but suffers from the smallest room of performance improvement , w . r . t parallelization .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . the results for conll08 show that the max pooling strategy performs better in all model variations . the hyper parameters activation func . gives a 0 . 87 f1 score over the baseline . the sigmoid model gives a 1 . 66 f1 score over the best performing variant .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that the macro - averaged model achieves the best f1 ( in 5 - fold ) with sdp , while it has the smallest f1 with pdp . the relation types that rely on sdp have the worst performance .
results in table 3 show that for non - y - 3 models , the average f1 is closer to 50 % , while for y - 3 , the gap is close to 50 % .
results are presented in table 1 . all models trained on the word " paragraph " receive high scores on the essay level . all models except for those trained on mst - parser are above average in terms of word quality .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) compared to the majority performances for the other two systems .
performance of original and original models on test set is presented in table 1 . the results are presented in tables 1 and 2 . all system performance is statistically significant even under the difficult requirement of pre - trained tgen models . the best performance is obtained by sc - lstm , which performs on par with the original .
results for the original e2e data and our cleaned version are shown in table 1 . the original and the cleaned versions have the highest number of distinct mrs as measured by our slot matching script , see section 3 . the cleaned version has the highest overall number of mrs and the highest average number of concatenated instances .
performance of original and original models on test set is presented in table 4 . original models generally perform better than tgen + on all test sets except for those that are pre - trained . the best scores are obtained by bleu , rouge - lstm and cider . the worst scores obtained by error reduction are those obtained by random chance .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . the total number of errors we found was 22 ( out of 100 ) , so we found no significant difference in the overall numbers .
model the performance of all models is presented in table 1 . the most representative models are the dcgcn ( single ) and tree2str ( discounted in the distractor and fullwiki setting ) all models perform better than all the other models except for seq2seqk , which is more stable .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points and achieves 27 . 5 overall e model size . the results also show that dcgcn behaves similarly to the ensemble model ,
results are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the results are presented in tables 1 and 2 . we observe that the single model performs better in english - language than the multi - step model , the difference in performance between single and single is minimal , however , the difference between the average bias metric and the average cias metric is significant , we notice that the gap between the single and multi - step models is relatively small , this suggests that the differences between the performance of the two models are not significant .
5 shows the effect of the number of layers inside dc on the overall performance of the layers in table 5 . the first group shows that for all layers , there is a significant effect on the performance . for example , of the 10 layers in the corpus , there are 3 that are more than 50 %
6 compares gcn with baselines with residual connections . rc + la ( 2 ) and dcgcn4 ( 14 ) show that the residual connections gcn has better performance than the baselines .
model f1 shows the performance of dcgcn models when combined with the number of training instances . the results are presented in table 1 . the first group shows that dcgcnn models perform well on both test sets .
8 shows the ablation study results for amr15 . the results show that removing the dense connections in the i - th block severely decreases the performance .
ablation study for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the global network and the multi - decoder have similar performance on graph attention .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our method outperforms the best state - of - the - art methods on all probing tasks .
observe that our method outperforms all the other methods except for cbow / 400 , which shows the diminishing returns from mixing source and subjnum .
cbow / 784 improves upon the best state - of - the - art model subj and mpqa by 3 . 8 % in terms of mrpc score . it outperforms both sst2 and sick - e by a noticeable margin . it also outperforms sst3 and sst5 by a margin of 3 . 6 % . note that the differences in performance between the two methods are mostly due to the size of the training set , not the type of training set .
results on unsupervised downstream tasks attained by our models are shown in table 3 . the best performances are on the sts12 , sts14 and sts15 datasets , which show the relative change with respect to hybrid . cbow shows the greatest performance . compared to cmp , the difference is less pronounced .
results for initialization strategies on supervised downstream tasks are shown in table 8 . our paper shows that the best performing initialization strategies are sst2 , sst5 and sick - b .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms the other two methods on all three tasks .
observe that cbow - r outperforms all the methods except for the one that does not use subjnum and coordinv embeddings .
subj and sick - e perform comparably to other methods on the mrpc and mpqa datasets . cbow - r also outperforms both sst2 and sst5 in terms of mrpc performance . on the other hand , it has the advantage of training on a larger corpus , which suggests that it has better chance of outperforming both methods .
system performance in [ italic ] e + per and e + misc scores are reported in table 1 . supervised learning models outperform all supervised learning models except for mil - nd , which obtains a better e + misc score . all org and per scores indicate that the supervised learning model performs well in all aspects of the system , with the exception of name matching .
results on the test set under two settings are shown in table 2 . our system achieves the best results with 95 % confidence intervals of f1 scores . supervised learning improves the e + p scores by 2 . 5 % in [ italic ] e + f1 score and by 3 . 3 % in [ mil - nd ] . the performance of the supervised learning model ( model 1 ) is shown in bold . with the exception of name matching , all supervised learning models achieve the best performance .
6 shows the results of ref and ref compared to the original model ( g2s - gat ) . ref significantly outperforms ref in all but one case , when ref is used with ref pre - trained .
results in table 1 show that the models trained on the ldc2017t10 outperform the best state - of - the - art models on all metrics , except for the " bleu " metric , which is more difficult to detect .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models by a noticeable margin .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results of this study show that the use of bilstm improves the model ' s performance .
results are presented in table 4 . the results are summarized in terms of the average number of frames per sentence , divided by the number of words in the sentence . note that the g2s - gin model obtains significantly better results on average compared to the baseline model . sentence length and average sentence length have a generally positive effect on sentence length and sentence length , however , it is less significant on average on the 50 - 240 δ δ table 4 shows the results for all models except those that obtains the best results .
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gat ) . these tokens are used in the comparison to derive the summaries from the reference sentences . however , for the ldc2017t10 test set , the token lemmas are used .
4 shows the performance of the two approaches using the 4th nmt encoding layer . it can be seen that both embeddings have high performance on the parallel corpus ( 200k sentences ) .
2 : pos and sem tagging accuracy with baselines and an upper bound . word2tag is the most frequent classifier using unsupervised word embeddings . it improves upon the performance of word2verd by 9 . 5 % in pos and 91 . 45 % in sem .
results in table 1 show that the accuracy obtained by applying the best performing method is superior to that obtained by using the standard schemas algorithm . table 1 also highlights the performance of our model on the word - level tasks .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that for bi , pos is 87 . 9 % better than res , while res is 88 . 9 % .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is 10 . 2 % on a training set 10 % held - out . on the other datasets , gender is 8 . 7 % more accurate .
results in table 1 show that training directly towards a single task can improve the performance for both groups .
2 shows the effect of the additional cost term on the balanced and unbalanced data splits . the classifiers trained on pan16 seem to have little effect on the data splits , however , it does have a significant impact on the unbalanced task performance . dial models trained on the pan16 dataset are able to detect instances of gender - neutral sharing in the balanced dataset , as shown in table 2 .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is significant , in pan16 , the performance is significantly worse than that of the corresponding trained classifier .
6 shows the performance of different encoders when embeddings are trained on the same protected attribute . embedding is more difficult than embedding , and the performance is less consistent when trained on rnn .
results in table 2 show that the best performing model is the lstm model developed by yang et al . ( 2018 ) . it further improves upon the strong lemma baseline on the wt2 dataset by 2 . 5 points . the results of " dynamic " and " firmetune " models outperform both the ' basic ' model and the ' programmable ' model by a noticeable margin . finally , the performance improvement on the " programmable " model by 3 . 4 points over the ' other ' baseline by the same margin .
performance of this model compared to previous models is reported in table 5 . the results of experiment 1 show that the lstm model significantly outperforms both the baseline and the work model in terms of training time . further , the improvements in training time are statistically significant ( i . e . , 2 . 87m vs . 2 . 41m ) on the base acc and full time datasets , the difference in performance between these models is less pronounced on the full time dataset .
3 shows the performance of our model compared to previous work on the amapolar time and full time datasets . the results of experiment 1 show that our model significantly outperforms the alternatives in terms of both err and polar time . on the other hand , it does not have the best performance on both datasets .
3 shows the bleu score on wmt14 english - german translation task . it is clear from table 3 that the tokenized approach has far superior performance to the state - of - the - art model on gold - standardized goldstandard translation tasks .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our model improves the match / f1 score by 2 . 67 % over the baseline model . further , the improvements by rnet * show that the parameter number of base does not harm the model performance .
6 shows the f1 score on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number . lample et al . ( 2016 ) also observe that the use of # params improves the performance of the model .
performance on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
results are shown in table 1 . word embeddings ( mtr ) w / system retrieval and word symmetric schemas ( rlsc ) outperform human on all metrics except system retrieeval . the word - based approach , mtr - 2 , achieves the best results with a minimum of 0 . 05 b - 2 and 2 . 05 r - 2 compared to the previous state - of - the - art . doc - based learning methods ( emtd ) do not rely on superficial cues .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all systems is highlighted in bold . the results of retrieval are shown in table 4 .
results are presented in table vii . our proposed system outperforms all the competition except for the one that embeddings pre - trained on . europarl , on the other hand , performs slightly worse than the others on many of the sub - categories . for example , it obtains the best performance on docsub while the others perform slightly worse .
are presented in table vii . the results are summarized in terms of the best performing ensembles . our joint model outperforms all the other models except for the one that we use , namely , europarl . on the other hand , our joint model performs slightly worse than the others on many of the test sets . for example , we see that eurparl and ted talks have better performance on all test sets , with the exception of docsub .
3 shows the performance of all models trained on the corpus dataset . our joint model outperforms the baseline on all three datasets except for the one that is used on the docsub dataset . on the other hand , it performs slightly worse on the other two datasets , namely , docsub and tf - news . europarl significantly outperforms both the baseline models on both datasets .
metrics are shown in table 1 . they are broken down in terms of metric and depthcohesion . europarl achieves the best overall score with a gap of 1 . 78 points , compared to 1 . 86 points for the other two systems . our joint model is better than both the baseline and the maxdepth of our model .
metrics are shown in table 1 . they are broken down in terms of metric and depthcohesion . europarl achieves the best score with a score of 1 . 5 , 1 . 1 and 1 . 2 . these metrics are used to compare to the official score of eurparl . however , their performance is slightly worse than that of docsub due to the high coverage of the metric .
performance of our enhanced model on the validation set of visdial v1 . 0 is shown in table 1 . compared to the original embeddings of qt and lf , the enhanced model performs better than the original one .
performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . the best performing model is lrv , which relies on hidden dictionary learning .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 .
3 presents the results of direct assessment and bertscore - f1 on the test set of eureor . the results are summarized in table 3 . the most representative of these metrics are the scores of " direct assessment " and " ruse " scores , which are significantly better than those of " f1 " .
3 presents the bagel and sfhotel scores . the results are summarized in table 3 . the baseline bleu - 1 scores are significantly better than those of bertscore - f1 , while the difference between those of smd is much smaller .
performance of the models according to these metric and baseline metrics is reported in table 4 . the results are summarized in bold . the summaries are presented in low - supervision settings ( e . g . , wmd - 1 + elmo + p < 0 . 749 ) . they are broken down in terms of morph - mover scores , which are based on pre - trained word - mover scores . epm scores are broken into categories depending on the training context . the most representative of these categories is word - movers , which is based on the elmo and p scores obtained by spice ( which is used to train sentence prediction ) . the best scores are obtained using the bertscore - recall algorithm .
results in table 3 show that for all models that rely on word embeddings , the model performs better than the original model on all metrics except for the one that relies on syntactic or semantic information . in particular , the shen - 1 model performed worse than the m0 model on sim because it had less data to encode .
results are presented in table 4 . semantic preservation and transfer quality are the most important aspects of semantic preservation , while semantic preservation is the least important . syntactic preservation and semantic preservation are further improved with the help of yelp , indicating that the semantic preservation approach can further improve the transfer quality of the models . the semantic preservation baseline is significantly better than the syntactic preservation baseline δsim due to the higher transfer quality and the wider semantic preservation set . finally , semantic preservation improves with the addition of semantic features .
5 shows the human evaluation results . we show the results of the second stage of human evaluation . it is clear from the table that the method requires a considerable amount of effort to match the quality of the sentences generated by the machine and human judgments that match the generated sentences .
results in table 5 show that for all models that rely on word embeddings , the model performs better than the original model on all metrics except for the one that relies on syntactic or semantic information . in particular , for the shen - 1 model , the performance drop is much worse than that of m0 ( m1 + m2 ) .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer and unsupervised embeddings . they also outperform the best models in terms of both acc ∗ and epm by a margin of 3 . 6 points .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted to be disfluent is 8 % , which means that the accuracy obtained by rephrase is less than that obtained by repetition .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word in the reparandum is 15 % less than the fraction predicted as containing a function word . table 3 also shows the distribution of the tokens predicted as contained in the repair .
results are shown in table 4 . text + innovations significantly improve the model ' s performance over single model by 0 . 2 points over the best state - of - the - art model by all metrics , in addition , text + text + innovations also boosts model performance by 1 . 8 points over simeshu et al . ( 2018 ) .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves a state - of - art performance on the test dataset , which is almost entirely due to the ability to distinguish between opposing and opposing utterances .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is attentive neuraldater .
3 shows the performance of our method with and without attention . the results show the effectiveness of both word attention and graph attention for this task . compared to ac - gcn , neuraldater achieves a better performance .
performance of all models on the test set is reported in table 1 . embedding + t model outperforms trigger on every metric , while jrnn performs better on all metric except for the one that is pre - trained .
ert and f1 measures are shown in table 1 . all methods used for this task are described in terms of event identification . the method is described in section iv . in all but one case , the method has achieved the best results with a significant margin . all methods cause a significant drop in the precision of the event , both in event and in event .
results in table 1 show that all the models trained on the spanish - only - lm subset are comparable in terms of performance . all but fine - tuned - lm models perform better on the dev perp and test wer datasets , while all the other models perform worse on the test perp .
results on the dev set and on the test set are shown in table 4 . fine - tuned train dev with only subsets of the code - switched data in it . this results show that fine - tuning gives a significant improvement over the performance on the train dev set .
5 shows the performance of our model on the dev set and the test set , compared to fine - tuned - disc . the results are shown in table 5 . it is clear that fine - tuneddisc has superior performance to the monolingual model in both sets .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets tested on the conll - 2003 dataset . note that the drop in precision between baseline and pre - trained gaze features indicates that the model performs well on both eyetracking datasets .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the baseline , indicating that the use of the pre - trained gaze features boosts recall .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings are the most useful for wordnet , and it improves upon the syntactic features obtained by using autoextend rothe and schütze ( 2015 ) . glove - retro also improves on the original wordnet by 9 . 5 % in ppa .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp improves upon the strong lemma baseline by 3 . 8 points in uas .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 shows the results with domain - tuned and multi30k decoding data . the results with subsfull decoding data are shown in table 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) results with marian amun et al . , 2018 . domain tuning improves the model performance by 3 . 8 points over the en - de model , though it is less effective .
results are shown in table 4 . subdomain - tuned subs1m models outperform all models except for those using mscoco17 . the results are presented in the en - de setting , which means that the models trained on the flickr16 domain are more likely to have good performance on the fixed domains . table 4 shows the results for all models without domain - tuning . the results for both groups are very similar : for the english - speaking subset , the h + ms - coco improves by 2 . 6 points in performance . for the spanish - speaking sub - set , the improvements are modest but significant , with a slight improvement in performance over the subs - based baseline .
4 shows bleu scores in terms of the automatic captions added to the models . the best results with mscoco17 are shown in table 4 . adding the best multi30k image captions ( the best one or all 5 ) shows that the model can easily distinguish between the best two captions .
5 compares the performance of different approaches for integrating visual information . we use multi30k + ms - coco + subs3mlm , enc - gate and dec - gate embeddings . the results are summarized in table 5 . the first study shows that the enc - gate approach significantly improves the bleu % scores for the visual information layer .
performance of subs3m compared to subs6m is presented in table 4 . the best performances are obtained using the en - de embeddings layer of theitalic model , while the best performance on flickr16 is achieved using the combination of text - only and multi - lingual features . sub - text - only performance is comparable to that of subs4m , but on the larger scale , the performance is superior . adding the visual features improves the performance by 2 . 5 points .
performance on mtld compared to en - fr - ff is reported in table vii . the results of the best performing system are reported in tables vii and viii . the results are summarized in table viii . we observe that for all translations that do not rely on word - of - speech embeddings , the quality remains the same .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) show that the re - rev systems perform better than the other systems when using ter .
2 shows the vgs model from chrupala2017representations . the results on flickr8k are summarized in table 2 . the visually supervised model outperforms the standard rsaimage model by a significant margin .
results on synthetically spoken coco are shown in table 1 . the visually supervised model outperforms the similarly supervised audio2vec - u model by a significant margin .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . similarly , for cnn ( 2017 ) , the edges edges of the screenplay are very clever ( see table 1 ) . for rnn , the edges are so clever , you want hate hate hate , love it , hate it , and hate it .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . these results are shown in table 2 .
3 shows the change in sentiment from positive to negative . it can be seen that the flipped switch in sst - 2 shows that the sentiment increases with the growth of positive sentiment .
results of experiment 1 are presented in table 1 . the most striking thing about the results is that it is easier to detect positive states than negative ones . it is less difficult to distinguish between positive and negative states . on the other hand , it is harder to distinguish whether a statement is positive or negative . in addition , cross - training ( sst - 2 ) improves the interpretability of the statement .
