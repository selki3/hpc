2 shows the performance of our iterative approach on the large movie review dataset . the approach performs the best on inference with efficient parallel execution of the tree nodes , while the iteration approach shows better performance on training .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . the max pooling strategy consistently performs better in all model variations . softplus also outperforms sigmoid in the multi - model setup with different number of parameters . as shown in table 2 , the combination of the four hyper parameters with the best performance on the single representation performs better than the dual representation approach . adding the three hyper parameters boosts the model ' s learning rate and boosts the f1 .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp as well as the best diff . as shown in fig . 1 , the models trained on the strongest dependency path have the smallest f1 and the highest diff . these models outperform the models using only sdp .
results in table 3 show that y - 3 significantly outperforms the previous state - of - the - art models in terms of f1 and r - f1 .
3 presents the results for the essay and paragraph level . results are presented in table 3 . all the methods trained on the proposed method achieve the highest level of performance , with 50 % achieving the highest score .
4 shows the c - f1 scores for the two indicated systems ; for the lstm - parser system , it is 60 . 62 ± 3 . 54 and 58 . 24 ± 2 . 87 respectively , compared to the majority performances of the other systems .
results are shown in table 4 . original and original train performance are presented in the best - performing state - of - the - art system , while the other two systems perform slightly worse . for example , original is better than tgen + on bleu and on rouge - lstm , but the improvement is slim .
shown in table 1 , the original e2e data and our cleaned version are comparable in terms of the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 .
3 shows the results for original and original test . the results are presented in table 3 . original and original test scores are shown in bold . the best performing system is tgen + , which results in a better bleu score than any other system on the test set .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . in table 4 , we show the absolute numbers of errors we found ( added , missed , slight disfluencies ) and the percentage of correct ones in the training set .
model the performance of all models is reported in table 1 . the best performance comes from the external model , which improves upon the state - of - the - art dcgcn ( single ) model by 3 . 8 points .
2 shows the model size in terms of parameters . our model achieves 24 . 5 bleu points , which shows the performance of the ensemble model compared to the smaller seq2seq model .
3 shows the results for english - german and english - czech . the results are presented in table 3 . we observe that the single model outperforms the other models in both languages , with the exception of english - korean , where the gap between the two is narrower . as expected , the english - language model performs slightly worse than the german model , with a gap of 3 . 5 points in the overall performance .
5 shows the effect of the number of layers inside dc on the performance of the model in table 5 . as table 5 shows , for every layer of dc , there are two layers that contribute strongly to the overall performance .
6 shows the performance of models with residual connections . rc + la ( 2 ) and gcn + rc ( 6 ) show significant performance improvement . with residual connections , dcgcn + dcgcn2 ( 27 ) shows a significant performance drop compared to previous models .
model a shows the performance of dcgcn models when trained on a single set of data . the results are presented in table 1 . the models performing best on the three sets of data are highlighted in bold . however , for the four sets , the results are slightly worse than those on the two sets .
8 shows the ablation study results for amr15 . it can be seen that removing the dense connections in the i - th block leads to a lower performance than the previous model .
9 shows the ablation study for the graph encoder and the lstm decoder . encoder modules used in table 9 show that the multi - decoder approach achieves the best results with a gap of 2 . 5 % in coverage .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our approach obtains the best performance on all probing tasks . it achieves the best results on the subtense and subtense tasks , while outperforming the other approaches .
1 and table 2 summarize our results on the subtense and subtense subtasks . we observe that cbow / 400 has the best performance on both subsampling and subtasking tasks . it also outperforms all the other methods except for the one that has the worst performance . it is clear from table 2 that the h - cbow model is superior to both the previous state - of - the - art models .
3 shows the performance of our model compared to other methods . our model outperforms all the other methods except for the one that cmp relies on . cbow even outperforms sst2 and sst5 in terms of mrpc score . on the other hand , it has the advantage of training on a larger corpus , which underscores the competitiveness of cmp over mpqa and sick - e .
results on unsupervised downstream tasks attained by our models are shown in table 3 . hybrid models outperform both cbow and cmp in all but one of these tasks .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms the best state - of - the - art models on all three tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performance is on the sts13 task , which is supervised by the cbow - r model .
observe that cbow - r outperforms all the methods except for the one that requires more training data . it obtains the best performance on every metric when trained on a single domain . it also achieves the best results on the subtense metric , as measured by coordinv and subjnum . it also outperforms somo and wc by a margin of 3 . 5 points .
subj and sick - r are comparable in terms of performance on all three benchmarks . however , for sst2 and sst5 , the best performance is achieved on both benchmarks . cbow - r outperforms all the methods except for the one that is used on the sst3 dataset . this confirms the viability of subj as an alternative to mpqa in the low - supervision settings .
system performance in [ italic ] e + per and e + misc is reported in table 3 . our system outperforms all stateof - the - art systems in all aspects except for the org metric , which shows the performance of the system when trained with only one domain name . supervised learning systems ( mil - nd ) consistently outperform all state of the art systems in terms of both e + and per metrics .
2 shows the results on the test set under two settings . the system trained on the model of mil - nd improves upon the performance of previous models with 95 % confidence intervals . supervised learning improves the e + p score by 0 . 9 % in [ italic ] e + f1 score and by 1 . 8 % in [ mil - nd ] . the system also improves the generalization ability of the model with respect to name matching .
6 shows the results of ref and ref on the model compared to ref alone . ref significantly outperforms ref in all but one of the cases when ref is used only for ref .
results in table 3 show that the models trained on the proposed ldc2017t10 outperform the state - of - the - art models on all metrics , with the exception of meteor .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model outperforms the previous stateof - the - art models on all three benchmarks .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the use of bilstm improves the model ' s performance by a significant margin .
results are presented in table 4 . we observe that for all models , the average number of frames per sentence is significantly lower than that of g2s - gin , indicating that the model is more suitable for shorter sentences .
shown in table 8 , the fraction of elements that are missing in the output that are present in the reference sentence ( g2s - gin ) , for the test set of ldc2017t10 . it is clear from table 8 that the use of token lemmas in the model is important to improve the picture quality .
4 shows the performance of the two approaches using the 4th nmt encoding layer . it is clear from table 4 that both approaches improve the tagging accuracy for the target languages .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag also exhibits the best performance . note that the use of unsupervised word embeddings does not improve the results for word2tag .
results are presented in table 4 . table 4 shows that our approach significantly improves the performance of our model compared to previous methods .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . note that for bi , pos is 87 . 9 % better than res , while res is 88 . 9 % .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ , a difference of 2 . 7 % in the performance of pan16 .
results in table 1 show that training directly towards a single task can improve the performance for the task in question .
2 shows the status of theected attribute leakage in the context of multi - task conversation . the trained classifiers ( pan16 ) consistently show balanced and unbalanced data splits . dial and sentiment classifiers consistently show higher instances of misclassification , indicating that the classifiers are trained to detect instances of gender - neutral speech .
performance on different datasets with an adversarial training set is shown in table 3 . the performance on pan16 is the difference between the performance of the trained classifier and the corresponding adversary ’ s accuracy . sentiment and gender classification are the most important factors in predicting whether an object will reach the target .
6 shows the performance of the embeddings for different encoders . embedding is more difficult than embedding , as shown in fig . 6 . for example , rnn performs slightly better than embedded , but is slightly worse than rnn .
results in table 4 show that our approach outperforms the previous stateof - the - art models on all three subsets . the results of our model outperform all the models except for the one that is used on the wt2 dataset . our approach achieves the best performance on both subsets , with a gap of 2 . 5 % on the ptb and wt2 datasets .
performance of our model compared to previous models is reported in table 4 . the results of experiment 1 show that our model significantly outperforms previous models in terms of both training time and model time .
3 shows the performance of our model compared to the previous stateof - the - art models . our model outperforms all the other models except for the one that is used in the amapolar time dataset . this model also improves the recall on the yelp dataset as well as the amafull time dataset ,
3 shows the bleu score on the wmt14 english - german translation task . our model outperforms all the state - of - the - art models except for the one that uses the gold - based sru embeddings .
4 shows the performance of our model on squad dataset . our model improves the match / f1 score by 3 . 5 points over the previous state - of - the - art model . further improving the model by 2 . 4 points .
6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) . it also indicates the performance of our model in the low - supervision settings . it can be observed that our model significantly outperforms other models in terms of parameter number .
shown in table 7 , the performance of elrn with base + ln setting and test perplexity on snli task with base setting .
system retrieval and system re - evaluation are presented in table 4 . word and multi - task learning methods ( mtr ) outperform human in all aspects . the word - based approach is particularly effective for system retrieeval , with a boost of 2 . 5 % over the previous state - of - the - art systems . multi - tasklearning methods ( mrtr ) and mtr are particularly useful for system evaluation . in particular , the word " retrieval " and " multi task learning " methods are particularly beneficial for system evaluations .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 8 . the best performance among all systems is reported in table 4 .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that is used on the df dataset . for df , our model performs slightly better than the others . for europarl , the performance is slightly worse than that of the other two models .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that is used on the df dataset . for df , our model performs slightly better than the others . for europarl , the performance is slightly worse than that of docsub , but still comparable to df .
3 shows the performance of all the models trained on the corpus dataset . the results are presented in table 3 . our model outperforms all the other models except for the one that is used on the df dataset . for df , our model performs slightly worse than the others . for europarl , the performance is significantly worse than that of the other two models .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance on all metrics with a gap of 1 . 78 points in the truedepth metric compared to 1 . 86 points for europarl . on the other hand , our model performs slightly worse than our maxdepth metric , which shows the diminishing returns of our approach .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best performance on all metrics with a gap of 1 . 5 points in the dimensioncohesion metric . it outperforms both the df and docsub metric by a significant margin .
performance ( ndcg % ) on the validation set of visdial v1 . 0 . the enhanced version of lf outperforms the enhanced version as shown in table 1 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performance is seen in section 5 , where p2 is implemented as the most effective one .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - prec model outperforms all the other models except for those using bert .
3 presents the performance of our approach on the test set of eureor . our approach obtains the best performance on all metrics with a minimum of 0 . 5 bertscore - f1 score . the results are presented in table 3 .
3 presents the bagel and sfhotel performance on the test set . the results are summarized in table 3 . the baseline bleu - 1 improves significantly over the baseline on all metrics with a gap of 0 . 3 points from baseline .
performance of the models according to these baselines is reported in table 4 . the results are summarized in table 5 . the summaries are presented in terms of leic scores ( p < 0 . 001 ) and bertscore - recall ( p > 0 . 005 ) . the results of the summaries indicate that the combination of elmo and p scores significantly improves the m2 score , but does not improve the f1 score .
results in table 3 show that for all models that rely on word embeddings , the model performance is comparable to that of sim , with the exception of m3 , which relies on para + para + lang .
results are presented in table 4 . semantic preservation and transfer quality are the most important aspects of the semantic preservation dataset for both datasets . the proposed model outperforms all the other models on both datasets in terms of both transfer quality and semantic preservation . for semantic preservation , the proposed model achieves a 3 . 5 - fold improvement over the previous stateof - the - art setup on all the three scenarios .
5 shows the results of human validation . the performance of our method is shown in table 5 . it is clear from the table that the human evaluation has a significant impact on the quality of the sentence , as measured by the number of instances in the corpus that match the machine and human evaluations .
results are shown in table 4 . the performance of m1 and m2 models on the acc test set is slightly better than those on the sim test set . however , the performance drop is still significant on the pp test set , with the exception of m3 , where the shen - 1 model performs better than the other models .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than those using simple - transfer and unsupervised embeddings . however , this is less than the best results on previous work , which is mostly due to different classifiers in use . multi - decoder models are comparable to the best state - of - the - art models , but do not achieve the highest acc ∗ score . sentiment transfer is restricted to 1000 sentences and human references , so it is less useful to train models with multiple classifiers . the best model is yang2018unsupervised .
statistics for nested disfluencies are shown in table 2 . reparandum length is the average number of tokens predicted to be in disfluent state . it is observed that the frequency of repetition tokens is relatively low , meaning that the accuracy obtained by rephrase is less than the rate at which repetition tokens are predicted .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is shown in parentheses .
results are shown in table 4 . we observe that for all models that use text + innovations , the model outperforms the single model in terms of dev and test mean .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance on the test dataset . it also achieves the best performance in the unrelated ( micro - f1 ) and micro f1 ( % ) tests .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models except for burstysimdater .
3 shows the performance of our method in the context of word attention and graph attention . the results show that neuraldater performs substantially better than the state - of - the - art model in both instances .
3 shows the performance of all models trained on the same training set . our model outperforms all state - of - the - art models except for the one that performs on the " trigger " stage . the results are reported in table 3 .
3 shows the performance of our method in the event of a single event . our method outperforms all state - of - the - art methods in terms of both event identification and event classification . in particular , the method has the best performance on the event classification task .
can be seen in table 4 . all models trained on the spanish - only - lm setup outperform all the other models except for the ones that do not use the word " wait " in the setup .
4 shows the results on the training set and the test set using discriminative training with only subsets of the code - switched data . fine - tuned results show that fine - tuning reduces the training performance for both train and test sets .
5 shows the performance on the dev set and the test set , compared to the model trained on fine - tuned - disc . the results are shown in table 5 . the performance improvement over the monolingual model is almost entirely due to the sensitivity of the gold sentence in the set , i . e . the cs - only - lm model achieves a better performance than the model using fine - tuneddisc .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets tested on the conll - 2003 dataset . the improvement in precision ( p ≤ 0 . 01 ) is statistically significant , as shown in fig . 7 .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all statistically significant improvements over the baseline ( p ≤ 0 . 01 ) using type - based gaze features .
results on belinkov2014exploring ’ s ppa test set . the hpcd approach uses syntactic - sg embeddings as the base for wordnet , and it uses glove - retro embedding as the embedding layer . the results on the original paper are shown in table 1 . it is clear from the results that the syntactic embedding mechanisms are superior to the semantic embedding methods obtained by using autoextend rothe and schütze .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results from table 2 show that the combination of oracle pp and lstm - pp pre - trained models improves the model ' s performance .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 . with the attention removed , the model improves ppa acc .
2 shows the performance of domain - tuned models compared to multi30k models . we observe that domain tuning improves the image caption translation performance , but does not improve the bleu % scores . adding subtitle data and domain tuning , however , does improve results .
results are shown in table 4 . subdomain - tuned subs1m models outperform all models except for those using mscoco17 , which shows the diminishing returns from domain - tuning . table 4 shows the performance of the models trained on the flickr16 dataset , with the exception of the one that is used on the largerickr17 dataset . domain tuning improves the results for all models , with only marginal improvements for the larger flickr15 dataset .
4 shows the bleu scores in terms of multi30k captions on the large scale en - de datasets . the results are shown in table 4 . the best results with only the best one or all 5 captions is shown in the table 4 .
5 compares the performance of different approaches for integrating visual information . we observe that enc - gate and dec - gate achieve the best results ( bleu % scores ) on the largerickr16 and mscoco17 datasets . in the larger flickr17 dataset , we observe that the enc - gated approach outperforms all the other approaches except for the one using directlink embeddings .
results in table 3 show that subs3m is better than subs4m in terms of multi - lingual features . sub - domain features improve the performance for all models except for those using the en - de embeddings . moreover , the improvements on the flickr16 dataset are larger than those on the largerickr17 dataset , indicating the effectiveness of multiple layers of the system .
performance on mtld compared to en - fr - ff is reported in table 4 . the results are reported in tables 4 and 5 . table 4 summarizes the results of our system on the word analogy task . we observe that for all models that rely on word analogy tasks , their performance is comparable to that of en - rnn - ff .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in tables 2 and 3 .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) for the system reference are shown in table 5 . the system reference obtains ter scores comparable to that of en - fr - rnn - rev , although en - es - mt - rev has higher ter scores .
2 shows the vgs performance on flickr8k . the model trained on chrupala2017representations is significantly better than segmatch , confirming the importance of word embeddings in network design .
results on synthetically spoken coco are shown in table 1 . the visual supervised model outperforms the similarly supervised audio2vec - u model by a margin of 3 . 9 points .
1 shows the results of the different classifiers compared to the original on sst - 2 . for example , dan ( in the the edges ) turns in a < u > screenplay that is very clever at the edges ; it ’ s so clever you want to hate it . similarly , rnn ( out in the the corners ) has the same results as the original , but in the edges .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning .
shown in table 3 , the effect of the flipped switch on sentiment is less pronounced in sst - 2 than in the original sentence .
results of experiment 1 are presented in table 1 . it is clear from table 1 that the use of sift improves the interpretability of pubmed and sst - 2 . however , this does not improve the results for the aim of improving interpretability .
