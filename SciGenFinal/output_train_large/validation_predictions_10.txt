2 shows the performance of our recursive approach on the large movie review dataset compared to our iterative approach , which performs the best on training .
shown in table 1 , the balanced dataset exhibits the highest throughput , but at the same time exhibits the smallest amount of performance improvement .
2 presents the results for each model with different representation . we show that the max pooling strategy consistently performs better in all model variations . we also show the performance of sigmoid and softplus models with different number of parameters . the performance of the two approaches is summarized in table 2 . in the first case , the maximum pooling approach performs better than the other approach .
1 shows the effect of using the shortest dependency path on each relation type . we show the f1 f1 with sdp in 5 - fold and f1 without sdp . the results are shown in table 1 . our model achieves the best f1 ( in 5fold ) in relation to the relation type , we can see that our approach has a positive effect on f1 and diff .
3 shows the performance of the y - 3 model compared to the previous state - of - the - art models in terms of f1 and f1 scores .
results are presented in table 1 . the performance of our model is summarized in terms of the number of instances in which the word " paragraph " is used in relation to the sentence " f1 " . our model achieves the best performance on both the test and the submission level .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph scores are shown in table 4 . the best performance for the essay system is 60 . 62 ± 3 . 54 , while the worst performance is 57 . 24 ± 2 . 87 .
results are presented in table 1 . the original and the original are shown in bold . the original is better than the original in all but one of the three cases . as expected , the performance of the original is much worse than the other two . in addition , the errors are much worse in both cases .
shown in table 1 , we compare our original and our cleaned e2e data with the original ones . the results are presented in tables 1 and 2 . we can see that the original and the cleaned versions are comparable in terms of number of distinct mrs and the number of instances of textual references .
3 presents the results of the original and the original test sets . the results are presented in table 3 . the original and the original test sets are shown in bold . the original test set has the best performance on both sets , however , it is slightly worse than the original . table 3 shows the performance of the test set on each set .
4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that adding errors caused slight disfluencies in the training set , which caused the errors to be added to the original training set .
results are presented in table 1 . all models are significantly better than the previous state - of - the - art models in all but one of the three categories . table 1 shows the performance of all models on the external and external datasets . the performance of the dcgcn ( single ) is comparable to that of snrg ( konstas et al . , 2017 ) . table 2 shows that all models are comparable in performance to snrg .
2 presents the results on amr17 . our model achieves a bleu score of 27 . 5 , which indicates that it can be used to improve upon the performance of seq2seqb .
3 presents the results for english - german and english - czech . the results are presented in table 3 . our model outperforms the previous best - performing models in both languages . in english , the results are summarized in table 4 . we observe that the single - language model performs better than the two - language models in english - language .
5 shows the effect of the number of layers inside dc on the performance of the layer representation . table 5 shows that we can see that the layers inside the dc have a significant effect on the quality of the layers .
6 shows the performance of the baselines with residual connections . the results are shown in table 6 . as expected , the gcn has residual connections with the residual connections ( see table 6 ) . however , the results are slightly worse than the previous state - of - the - art gcn , indicating that residual connections are important .
results are presented in table 4 . we observe that the dcgcn model outperforms all the other models in terms of performance . the results are summarized in table 5 . in particular , we observe that when the models are combined , the performance drops significantly .
8 shows the ablation study for amr15 . the results are shown in table 8 . table 8 shows that removing the dense connections in the i - th block leads to a reduction in the number of connections . in addition , removing the layers of dense connections decreases the performance of the model .
9 presents the ablation study for the graph encoder and the lstm decoder . the results are presented in table 9 . we observe that the global encoder has the best coverage , with a gap of 3 . 5 % in performance between the two models .
7 presents the results for the initialization strategies on probing tasks . our paper shows that our approach obtains the best performance on all three tasks . the results are summarized in table 7 .
3 presents the results of our method on the subtraction test set . the results are presented in table 3 . table 3 shows that our method obtains the best results for each subjnum . as expected , the results are slightly worse than our previous method , but still comparable to our best method .
3 presents the results of our method on the subj and mpqa datasets . the results are presented in table 3 . our method outperforms all the previous methods except for subj , which is closer to the best performing model .
3 shows the relative performance of our models on unsupervised downstream tasks attained by our models . the results are shown in table 3 . we observe that the cbow model outperforms the hybrid model in terms of downstream performance .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that the best initialization strategies are glorot and sst2 , while the best is subj .
6 shows the results for different training objectives on the unsupervised downstream tasks . the results are presented in tables 6 and 7 . we observe that the cbow - r model outperforms the other two models in terms of performance .
results are presented in table 1 . the best performing models are cbow , cbow - r and somo . we observe that cbow has the best performance in terms of depth and subtraction , while somo has the worst performance .
3 presents the results of our method on the subj and mpqa datasets . the results are presented in table 3 . the subj model outperforms all the other methods except for the cmow - c model , which is superior in terms of performance . the sick - e model is comparable in performance to both the sst2 and sst5 datasets , but it is inferior in the mrpc dataset .
3 presents the results of our system in table 3 . the results are presented in tables 1 and 2 . our system obtains the best e + and per scores in all org scores are reported in table 1 . as expected , our model obtains a better e + score than the previous state - of - the - art model . we also observe that our model outperforms all the other models in terms of e + loc scores . in addition , we observe that all the models trained on the original domain are better than the original model in all but one of the cases .
2 presents the results on the test set under two settings . name matching and supervised learning are shown in table 2 . the system achieves the best e + p score and the best f1 score . in both settings , the system performs better than the previous state - of - the - art model . finally , the performance of the system is comparable across all three settings , the performance of both models is comparable .
6 presents the entailment ( ent ) results for all models except those that do not have ref . the results are presented in table 6 . as expected , all models with ref are significantly better than those without ref ( except g2s ) .
results are presented in table 3 . the results are shown in bold . our model outperforms all the other models except for the ones that do not belong to the ldc2017t10 class . we also observe that the model performs better than the previous state of the art models .
3 presents the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . we observe that the model performs better on the external model than the internal model .
4 presents the results of the ablation study on the ldc2017t10 development set . the results are summarized in table 4 . bilstm has a significant impact on the performance of the model .
results are presented in table 1 . we observe that the average number of sentences in the sentence is significantly larger than the average length of the sentence , indicating that the model is more suitable for the task at hand .
shown in table 8 , the fraction of elements in the input that are missing in the output that are present in the generated sentence ( miss ) , for the test set of ldc2017t10 . the gold lemmas are used in the comparison with the reference sentences . in the g2s - gin comparison , we find that there are a significant number of missing elements in both the input and the generated sentences , which indicates that the model is more suitable for the task .
4 shows the performance of our model on a smaller parallel corpus ( 200k sentences ) with different target languages trained with different nmt encoding layers .
2 shows the performance of our classifier with baselines and an upper bound . the results are shown in table 2 . we use unsupervised word embeddings as the classifier , and word2tag as the upper bound encoder - decoder .
results are presented in table 1 . table 1 shows the performance of our models on each of the four datasets . our model outperforms all the other baselines except for the one in which it performs best . we observe that our model performs better than all the baselines in terms of accuracy and precision .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 . in the case of pan16 , the attacker has the advantage of training with 10 % held - out .
1 presents the results of training directly towards a single task . the results are presented in tables 1 and 2 . as expected , the training results are significantly worse when trained directly towards the single task , as shown in fig . 1 .
2 presents the results of the study on the balanced and unbalanced data splits . the results are presented in table 2 . we observe that the word " race " has a significant effect on the performance of the data splits , the word " gender " has the most significant effect , gender is the most important part of the conversation , while gender is the least important . the data splits are reported in tables 2 and 3 , respectively .
3 shows the performance on different datasets with an adversarial training . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . in particular , we see that the gender - neutral responses are the most important ones for predicting the performance of a task . similarly , the age of the target is less important , indicating that the target has a higher chance of reaching the task .
6 shows the concatenation of the protected attribute with different encoders . embedding leaky is easier than embedding guarded , but it is more difficult to distinguish between the two .
results are presented in table 3 . our model outperforms the previous state - of - the - art models in terms of base and finetune performance . the results of our model are summarized in table 4 . we observe that our model performs better on the two sets of models compared to the previous ones . in particular , it performs better than the previous models in the base set ,
results are presented in table 5 . our model outperforms all the previous models in terms of time and distance . the results are shown in table 6 . we observe that our model has the best performance on both datasets . as expected , it has the worst performance on all datasets .
results are presented in table 4 . the results of our model are summarized in table 5 . our model outperforms the previous state - of - the - art models in terms of err and time . we observe that our model performs better on the yahoo time dataset than on the amapolar time dataset . table 5 shows the performance of the model on the amafull time dataset compared to the original lstm model .
3 shows the bleu score on the wmt14 english - german translation task . in addition , we show the performance of our model in terms of time in seconds per training batch measured from 0 . 2k training steps on the newstest2014 dataset . we also observe that our model performs better than the previous state - of - the - art model on both english and german translation tasks . finally , we observe that the training batch size of the sru is comparable to that of the original sru .
4 shows the performance of our model on squad dataset . the model performs better than the previous state - of - the - art models in terms of match / f1 score . our model outperforms all the baselines except for the lstm model , which performs worse than the baseline model . we observe that the sru model is more accurate than the base model , and that it performs better on the f1 score of the model .
6 shows the f1 score of our model on conll - 2003 english ner task . the lstm model is significantly better than the previous state - of - the - art models in terms of parameter number . it is clear that our model is able to improve the ner performance by a significant margin . in addition , it can improve the performance of our system by a considerable margin .
7 shows the performance of our model on snli task with base + ln setting and test perplexity on ptb task with base setting .
results are presented in table 1 . word and sentence embeddings are used in all instances except system retrieval . the word embedding technique ( mtr ) is used in most instances , however it can be used in some cases as well . sentiment word embedding methods are used to improve the performance of the system . in some cases , the word embedded embedding method can improve performance for both human and machine learning tasks . it can be seen that human embedding techniques can help the system to better interpret the data .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all the automatic systems is shown in bold , with the highest standard deviation of 1 / 2 . as expected , the performance of the best human evaluation is reported in table 4 .
results are presented in table 4 . the results are summarized in table 5 . we observe that the best performing english language is the english language , while the worst performing spanish language is docsub . however , we observe that both the english and spanish language are better performing than the spanish language .
results are presented in table 4 . the results are summarized in table 5 . we observe that the best performing english language is the english language , while the worst performing spanish language is docsub . however , we observe that both the english and spanish language are better performing than the spanish language .
results are presented in table 4 . the results are summarized in table 5 . we observe that the best performing english language is the english language , while the worst performing spanish language is docsub . as expected , the performance of both english language and spanish language word embeddings is significantly worse than those of the other two languages .
results are presented in table 1 . our results show that our model achieves the best performance on both metric and depthcohesion metrics . our model outperforms both the df and tf metrics by a significant margin . we also observe that our approach is more accurate than the df metric , but is less effective than the tf metric .
results are presented in table 1 . we observe that the maxdepth and maxdepth of our models are comparable to those of the other three baselines . our model achieves the best performance on both metric and metric metrics , however it is slightly worse than our baseline .
shown in table 1 , the performance of our model on the validation set of visdial v1 . 0 is shown in the table 1 . we observe that the enhanced version of our system performs better than the original one .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the results are shown in table 2 . p2 is the most effective one ( i . e . , hidden dictionary learning ) compared to p1 .
5 presents the results of our model on hard and soft alignments . the results are summarized in table 5 . the hmd - f1 model outperforms all the other models except for the one that uses bert .
3 presents the results of the direct assessment and bertscore - f1 tests for each test set . the results are presented in table 3 . the performance of the test set is summarized in bold . we observe that the baselines for both sets are significantly better than those for the other two sets . as expected , the performance of both sets is significantly worse than that of the other set .
3 presents the bagel and sfhotel scores on the test set . the baselines shown in table 3 show that the bertscore - f1 score is significantly better than the bleu - 2 score on both sets . as expected , the baselines for both sets are significantly worse than those for the other two sets .
3 presents the metric and baseline scores of our models . the baselines are summarized in table 3 . our model achieves the best metric score on both m1 and m2 . it achieves a bertscore - recall score of 0 . 939 and 0 . 749 on the m2 and the m1 scores respectively . we observe that our model has the best performance on both metrics .
results are presented in table 4 . the results are shown in bold . we observe that the m0 + para + lang model outperforms the previous state - of - the - art models in terms of performance . as expected , the performance of the m2 + lang models is significantly worse than those of m3 + lang .
3 presents the results of our model on the transfer quality and semantic preservation datasets . the results are summarized in table 3 . we show that the proposed model outperforms the previous state - of - the - art model in both transfer quality metrics . in the semantic preservation dataset , we observe that the semantic and semantic aspects of the preservation data are significantly better than the semantic ones . as expected , the performance of the proposed semantic preservation data is significantly worse than those of the previous model , indicating that semantic preservation is more difficult to achieve .
5 presents the results of human sentence - level validation . the results are shown in table 5 . as expected , the performance of our model is significantly worse than that of the human model . we also observe that our model performs better than the previous state - of - the - art model ,
results are presented in table 4 . the results are shown in bold . we observe that the m0 + para + lang model outperforms all the other models except for the m6 + m0 + m1 model , which shows that the model is more suitable for the task at hand . as expected , the performance of the m1 + m2 model is lower than the m2 + m3 model ,
results on yelp sentiment transfer are shown in table 6 . our best model achieves the highest bleu and the highest acc ∗ on the same 1000 sentences compared to prior work . we also note that our best model , yang2018unsupervised , achieves the best acc ∇ score on the test set , but it is less than the best model in the classifier category .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent , compared to the percentage that were incorrectly predicted as repetition tokens . the average number of repetition tokens is 6 - 8 , while the average number is 8 - 10 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . the fraction of tokens predicted to contain the content word is shown in table 3 . it is clear that the disfluency predictions are accurate , but that the accuracy is not high enough to predict the correct number of tokens for each category .
results are presented in table 4 . the best performing models are shown in bold . we observe that the best performing model is the text + innovations model , in addition , the best - performing model is text + text + nomenclature . in particular , we observe that in the early and late stages of the model , the innovations model outperforms the single model in terms of dev mean , while in the late stages , the model is better than the other models .
2 shows the performance of our model on the fnc - 1 test dataset compared to the state - of - art word2vec embeddings . our model achieves the best performance on both test sets , with the exception of the one that is unrelated .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing method is burstysimdater , which significantly outperforms all previous methods . similarly , the best performing alternative is maxent - joint ,
3 presents the performance of our method with and without attention . the results are shown in table 3 . it is clear that our approach is effective for word attention , and graph attention is beneficial for graph attention .
3 presents the performance of our model on each stage of the test set . the results are presented in table 3 . we observe that our model performs better than the previous state - of - the - art models on all stages except for the argument stage .
3 presents the results of our method with respect to identification and classification . our method obtains the best results with the best performance on both trigger and classification tests . the results are presented in table 3 . in the case of the trigger test , we use a single argument to classify the events in the event . in the event of the classification test , the event is classified as a non - trivial event , which indicates that the event has a significant impact on the event classification .
results are presented in table 1 . all the models shown in the table have the best performance on dev perp , test acc and test wer . the best performance is achieved on all models except for those that have the worst performance .
4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 .
5 shows the performance on the dev set compared to the monolingual set . the best performance is achieved on the test set , when the gold sentence is used as a gold sentence in the standard set .
7 shows the performance of type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( g ) are shown in table 7 . the precision ( p ≤ 0 . 05 ) is significantly better than the f1 score of type combined , indicating that type combined gaze features are beneficial for the human judgement .
5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are statistically significant improvements over the baseline . the performance of the type combined gaze features is also statistically significant ( p < 0 . 001 ) . the f1 score is statistically significant , indicating that the type combination features are beneficial for the model .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , and glove - retro is used in verbnet 3 . 1 . the hpcd ( full ) is derived from the original paper ( 2014 ) and is based on syntactic skipgram embedding .
2 presents the results of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 . we show that our system has the best ppa accomplishment score on the full uas dataset .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of attention removal is evident in the ppa acc . it is clear that attention removal reduces the sensitivity of the model to context sensitivity .
2 shows the results of domain tuning for image caption translation ( bleu % scores ) . subdomain tuning improves the performance of the multi30k model by 3 . 5 % compared to the previous state - of - the - art model , while domain tuning improves performance by 2 . 5 % .
3 shows the performance of the subs1m in en - de and in - de settings . the results are presented in table 3 . subdomain - tuned models outperform the other models in terms of performance . in - de models perform better than the other two models , but the performance is slightly worse than the others .
4 shows the bleu scores in terms of automatic image captions . the results are shown in table 4 . as expected , the automatic captions outperform the traditional ones . in particular , the multi30k model outperforms the traditional captions , in fact , it is the only model that performs better than the traditional one .
5 presents the results of our approach on the en - de dataset . the results are summarized in table 5 . we observe that enc - gate and dec - gate are the most effective approaches for integrating visual information . in particular , we observe that the enc - gated approach has the best performance on both datasets ( bleu % scores ) . we also observe that dec - gates has the worst performance on all datasets ,
3 shows the performance of subs3m and subs4m on the en - de dataset compared to the subs6m dataset . the performance of the two models is comparable to that of the other models , however , the performance is slightly worse . we observe that the performance improvement over the subs2m model is due to the increased number of visual features , the performance gap between the two modes is small , but the performance increase is significant . in addition , we observe that subs3ms has the advantage of having the ability to distinguish between text and non - text contexts .
results are presented in table 1 . we observe that the en - fr - ff model outperforms all the other models in terms of translation performance . the results are shown in tables 1 and 2 . in table 1 , we compare the performance of the two models in relation to the original ones .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 presents the results of our training on the english , french and spanish vocabularies for our models .
5 presents the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) are shown in table 5 . the automatic evaluation scores show that the re - rev system performs better than the previous state - of - the - art system .
2 shows the performance of our visually supervised model on flickr8k . the results are shown in table 2 . we observe that the vgs model performs better than the previous state of the art rsaimage model .
1 shows the performance of our visually supervised model on the synthetically spoken coco dataset . the results are presented in tables 1 and 2 .
1 shows the results of the different classifiers compared to the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . similarly , rnn also shows that the edges of a screenplay are so clever that it can be used to hate hate it .
2 presents the results of fine - tuning on sst - 2 . the results are presented in table 2 . we observe that the number of occurrences in the original sentence has increased , decreased or stayed the same for the last three years . the results also show that the amount of words that have been added has not changed .
3 shows the change in sentiment between positive and negative sentiment in sst - 2 compared to the original sentence . the results are shown in table 3 .
results are presented in table 1 . the results are summarized in bold . in general , the positive and negative aspects of the results are less pronounced than the negative ones . however , the negative aspects are more pronounced . as expected , the performance of the sift model is significantly worse than the positive ones , indicating that it is more likely to improve upon the sst - 2 dataset . table 1 summarizes the results of our joint study .
