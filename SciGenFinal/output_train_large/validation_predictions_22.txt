2 shows the performance of our iterative model on the large movie review dataset . we use the recur and iterative approaches , respectively , to train the treelstm model . the recur approach shows better performance on training than the iterative approach .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset .
2 shows the performance of the max pooling strategies for each model with different representation . our system performs better in all model variations with different number of parameters . the hyper parameters dropout rate consistently outperforms softplus and sigmoid , both in terms of performance and f1 score . moreover , the boost function performs significantly better in the multi - model setup .
1 shows the effect of using the shortest dependency path on each relation type . it can be seen that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , and the best dff score ( in 10 - fold ) .
results in table 3 show that the y - 3 model outperforms the previous state - of - the - art models in terms of f1 score .
results are shown in table 1 . the results of the best - performing models are presented in the table . our model achieves the highest score with 50 % of the entries on the essay level .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) over the runs given in table 2 . the mean performances for both systems are lower than those for the other two systems , however .
results are shown in table 4 . the original and the new systems perform better than the original on all tests except for those where the error is caused by incorrect label selection . table 4 shows the results for all systems except for the one that contains incorrect labels . the two that do not belong to the original ( tgen + and tgen + are the only ones that are able to correct the error . the other two that are not part of the original dataset are testing sc - lstm ( which is the only one that does not contain incorrect labels ) and meader ( which contains errors ) . the results are presented in table 6 . all the other systems that perform poorly on the original dataset are statistically significant ( table 6 ) .
results in table 1 show that our original and our cleaned versions are comparable in terms of the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 . our cleaned version is slightly worse than the original , but still comparable to our original .
results are shown in table 1 . original and original test scores are presented in bold . the best performing system is the sc - lstm , which performs on par with the original . however , it is slightly worse than the other two systems . for example , the bleu score of tgen + is slightly better than the original score on all but one of the two cases .
results of manual error analysis on a sample of 100 instances from the original test set ( see table 4 ) . we found a total absolute number of errors ( 17 ) in the correction data , which we found to be small but significant ( 14 ) . further , we found a large number of misclassified instances ( 17 % ) .
3 shows the performance of our dcgcn model on the external and external datasets compared to the previous state - of - the - art models . for the external dataset , we see that all models perform better than all the other models except for the one that relies on a single entity ( table 3 ) .
2 shows the model size in terms of parameters . our model achieves 24 . 5 bleu points , which significantly outperforms other models in amr17 ( see table 2 ) .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing model is bow + gcn , which performs better in english - language than it does in german . we observe that the single model significantly outperforms the other two models in terms of performance .
5 shows the effect of the number of layers inside dc on the overall performance of the model . we observe that for every layer that has two layers , there are three layers that have three layers each .
6 shows the performance of our models with residual connections . rc + la ( 2 ) and dcgcn4 ( 9 ) show significant performance improvement . however , when gcn has residual connections , the performance drops significantly .
model f1 shows that dcgcn outperforms other models in terms of performance . the results are reported in table 1 .
8 shows the ablation study results for amr15 in terms of density of the connections in the dev set . the results show that removing the dense connections severely decreases the performance of the model .
shown in table 9 , the models used in the graph encoder and the lstm decoder have remarkably similar performance . however , the differences in performance between the two models are less pronounced for the two .
7 shows the performance of initialization strategies on probing tasks . our paper shows that it is possible to improve the performance by adding more context to the initialization strategies . the results are shown in table 7 .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . our model outperforms all the other methods except for the one that requires a greater precision . it obtains the best performance on every metric with a gap of 3 . 5 points . it also achieves the best score on all metrics with an absolute improvement of 2 . 4 points over the previous state of the art .
1 shows the performance of all models except cmp . our model outperforms all the other models except for the one that cmp uses . it also performs better than both mpqa and sst2 . it even outperforms both sick - e and trec in terms of mrpc score . however , it is slightly worse than sick and sick .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms the hybrid model on all downstream tasks , except for the one that is supervised by cmp . cbow shows the relative decrease in performance with respect to hybrid .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that it improves upon the state - of - the - art model by 3 . 8 points on the mrpc task . it outperforms the sst2 and sst5 baseline on all three tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the best performances are on the sts12 and sts14 datasets , which show that cbow - r performs well on both tasks . however , the performance drops significantly when training on the supervised tasks . this suggests that the reliance on supervised tasks may have a negative effect on the performance of the model .
observe that the cbow - r method outperforms all the other methods except for the one that do not use the word " depth " . the results are summarized in table 1 . the results show that the method performs well on both subtense and subtense contexts , and that it obtains the best performance on the subtense / tense contexts . it also outperforms somo and wc - r in both cases .
subj and sick - r perform similarly on the mrpc and mpqa datasets . cbow - r outperforms both sst2 and sst5 in both categories , however it has the advantage of training on a larger corpus . it also performs better on the sst1 dataset , while sst - b performs slightly worse on the repqa dataset . these results are mostly due to the smaller size of the training corpus .
3 shows the e + and per scores of all system models trained on the italic dataset ( including all org and per ) and the e + misc scores computed using the system ' s multi - factor learning ( mil - nd ) . the results are summarized in table 3 . the system performs well in all cases , with the exception of the case of the one in which it does not have the best e + or per scores . it obtains a significant improvement over the previous state - of - the - art model in all three cases ( except for the one where it has the worst e + org score .
2 shows the results on the test set under two settings . the first set shows the performance of the supervised learning model ( mil - nd ) in terms of e + p score and f1 score . supervised learning models show lower performance than supervised learning models . however , the difference in performance between supervised learning and supervised learning is less pronounced in [ italic ] than in the case of supervised learning . the results of the second set are shown in table 2 . we observe that the automatic learning model outperforms the model in all but one of the cases .
6 shows the results of ref and ref compared to the original model ( g2s - gin ) . the results are summarized in table 6 . ref significantly outperforms ref in all cases except for those where ref is used .
table 3 , we present the results of models trained on the ldc2017t10 dataset . the results are presented in table 3 . the models performed slightly worse than the previous state - of - the - art models on all 10 datasets .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms all the other models except for the one that is trained with only external data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the use of bilstm improves the performance of the model .
results are shown in table 1 . we observe that the g2s model significantly outperforms the other models in terms of sentence length and sentence length . in particular , we observe that when the sentence length is increased , the average sentence length decreases .
shown in table 8 , the fraction of elements missing in the output that are present in the generated sentence ( g2s - gin ) is much smaller than the fraction in the input , indicating that the model is more suitable for the task .
4 shows the performance of our models trained with different target languages . our model outperforms the previous stateof - the - art models in terms of both semantic and target tagging accuracy .
2 shows the pos and sem tags accuracy with baselines and an upper bound . accuracies are shown in table 2 . we use unsupervised word embeddings as the classifier , and word2tag as the upper bound encoder - decoder . we observe that the accuracy obtained using unsupemb tags is significantly higher than those using word1tag .
results are presented in table 4 . our proposed method outperforms all the other methods except for the one that significantly improves the accuracy . we observe that the accuracy obtained by our method is substantially better than the others .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we find that res and bi have comparable performance , but bi has the worse performance .
8 shows the performance of an attacker on two different datasets . the performance is shown in table 8 . the average age of the participants is 9 . 7 % compared to the corresponding adversary ’ s age of 10 . 2 % .
results in table 1 show that training directly towards a single task can improve the performance for both models . for pan16 , the training data is significantly more accurate .
2 shows the effect of the additional cost term on the balanced and unbalanced data splits . the classifiers trained on the pan16 dataset are named after the gender of the participants in the conversation , and are sometimes associated with gender - neutral tweets , as shown in fig . 2 . the classifier trained on pan16 has the worst performance on both datasets .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is significant ( p < 0 . 001 ) . in the other two datasets , the training set ( pan16 ) significantly boosts the performance for the classifier .
6 shows the concuracies of the protected attribute with different encoders . embedding leaky is easier for rnn to learn , but it is harder for embedded to learn . the rnn model outperforms both the original embeddings .
results in table 2 show that our approach outperforms the previous stateof - the - art models on both base and finetune models . we observe that our lstm model achieves the best performance on both datasets . the results of our model are shown in table 1 . our model outperforms both the original wt2 model ( which has been tested on multiple datasets ) and the finetuned wt2 dataset ( which is tested on two datasets ) . we also observe that this model performs better on the two datasets than the other two models .
5 shows the performance of our model compared to previous models . the results are summarized in table 5 . our model outperforms all the models except for the one that takes the time to train . this model has a 4 . 36 % increase in performance compared to the previous model . when training with lstm , it has a 2 . 42 % boost in performance .
3 shows the performance of our model compared to other models . our model improves upon the best stateof - the - art models on three of the four datasets . the results are summarized in table 3 . this model outperforms all the other models in terms of err and f1 scores .
3 shows the bleu score on the wmt14 english - german translation task . our model obtains the best performance with a 2 . 67 % improvement over the state - of - the - art model on the gold - standard2014 dataset . it also outperforms other models like sru , gru and atr .
4 shows the performance of our model with respect to match / f1 score on squad dataset . our model obtains the best performance with a parameter number of 2 . 44m . the results published by wang et al . ( 2017 ) show that the parameter number that interacts with the model significantly improves match / score over other models with the same parameter number .
6 shows the f1 score on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number . however , it does not achieve the best performance .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . the results are shown in table 7 .
results are shown in table 1 . word models trained on the word - based system retrieval ( mtr ) outperform human in terms of both system and word evaluation ( r - 2 ) . word - based systems are particularly effective for the task , with the best performance on the oracle dataset . the word " retrieval " and " supervising " systems significantly improve the results for both human and machine learning systems .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . our system outperforms all the other systems except seq2seq ,
3 shows the performance of all the models trained on the corpus dataset . our joint model outperforms all the other models except for the one that we use , namely , europarl . the results are summarized in table 3 . we observe that our joint model performs slightly better than the others on most of the datasets . however , it is slightly worse than the other two models . for example , we observe that the " ted talks " dataset , which relies on word - of - the - word embeddings , is significantly worse than " p " and " tables " by a significant margin .
3 shows the performance of all the models trained on the corpus dataset . our joint model outperforms all the other models except for the one that we use on the df dataset , namely , europarl . the results are slightly worse than those on the other two datasets . however , our joint model performs better than the others on both datasets . we observe that the best performing model is the " ted talks " model , which significantly outperforms the others .
results are shown in table 1 . the most representative models are en and pt , followed by p < 0 . 005 . these models outperform all the other models except for the ones that do not use the word " tables " . they also perform slightly worse than df , docsub and tf . however , their performance is slightly better than df and tf on all three metrics .
are shown in table 3 . our system achieves the best performance with a maxdepth of 1 . 78 on the metric compared to the baseline . our joint model outperforms all the other models except for the one that has the smallest truedepth . we observe that our joint model has the highest truedepth of the three .
are shown in table 1 . our system achieves the best performance with maxdepth and maxdepth on both datasets . our joint model outperforms all the other models except for the one that has the smallest truedepth . we observe that our joint model performs better on all datasets except for those that have the highest maxdepth . our model achieves the highest score with a maxdepth of 9 . 43 .
experimental results are shown in table 1 . the enhanced version of lf outperforms the enhanced version we used in the experiments of applying our principles on the validation set of visdial v1 . 0 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is lrv , which relies on the history shortcut . note that applying p2 also improves the rva score considerably .
5 presents the results on hard and soft alignments . the results are summarized in table 5 . we observe that wmd - recall + bert significantly improves the performance on hard alignments over those without .
performance on the direct assessment and eureor datasets is reported in table 4 . the results are summarized in table 5 . the most representative models are ruse ( * ) and bertscore - f1 ( * ) which significantly outperform the baselines on both metrics . for example , the ablation scores of noreg ( p < 0 . 001 ) are significantly better than those of pi - en and ruse ( p > 0 . 005 ) on both metric . however , the superior score of " direct " and " eurek " on the euror dataset is slightly worse than " direct assessment " .
3 presents the bagel and sfhotel scores on the validation set . our proposed bertscore - f1 improves upon the previous state - of - the - art baseline by 3 . 5 points .
performance of the models according to these baselines is reported in table 4 . the results are summarized in bold . the summaries are presented in tables 1 and 2 . the leic scores ( p < 0 . 001 ) consistently show that the combination of elmo and p scores significantly boosts performance . however , these scores do not improve significantly when combined with bertscore - recall scores .
results are shown in table 3 . we observe that for all models that use the word " shen - 1 " , the performance drops significantly . however , for the m1 model , the performance remains the same , with the exception of the m2 model , which performs slightly better on pp .
results are shown in table 4 . we observe that the semantic preservation and transfer quality scores are the most important aspects of semantic preservation . semantic preservation scores are significantly better than those of the other two baselines , indicating that semantic preservation is more important than transfer quality . the semantic preservation scores computed using yelp data are significantly worse than those computed using m2 and m7 datasets .
5 shows the human evaluation results . we show that our method significantly improves the performance for both sim and human when compared to previous methods .
results are shown in table 4 . we observe that for all models except m1 , there is a significant drop in performance compared to the previous state - of - the - art models . in particular , for m2 , we observe that the shen - 1 model performs slightly better than the other models .
results on yelp sentiment transfer are shown in table 6 . our best model achieve higher bleu than those using simple - transfer and unsupervised embeddings . however , the improvement is less pronounced for those using only one classifier , namely , the classifier delete / retrieve . we find this to be a significant drop from previous work ( which used multiple classifiers to achieve the best acc ∗ score ) . we also observe that the use of " unsupervised " classifiers severely affects the model ' s ability to interpret sentiment . this suggests that more sophisticated classifiers are needed to improve interpretability .
statistics for nested disfluencies are shown in table 2 . the percentage of repetition tokens that were correctly predicted to be disfluent is slightly higher than the rate at which repetition tokens were predicted as disfluency .
3 shows the percentage of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens predicted to contain a content word is in parentheses , indicating that the accuracy of the prediction is high for both instances .
results are shown in table 4 . we observe that the text model outperforms the single model in terms of dev and rewards , in addition , the innovations model shows the best performance on the test with a boost of 0 . 2 . the results are presented in table 5 . the text model with the best innovations score outperforms all the other models with an increase of 1 . 4 .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance with the help of rnn - based sentence embedding . however , it does not achieve the best performance on the micro - f1 dataset .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models except for burstysimdater .
3 shows the performance of our method with and without attention . it obtains a 3 . 2 % improvement over the previous state of the art model on word attention and graph attention .
3 shows the performance of all models trained on the same training set . our model outperforms all the other models except for the one that performs on the " trigger " stage . the results are reported in table 3 . we observe that for all models that perform on the trigger stage , the performance drops significantly .
3 shows the performance of our method in the event of a single argument . our method outperforms the previous state - of - the - art method in terms of both event identification and event classification . all the methods used for this task are statistically significant , with the exception of the one that used the l2r method .
can be seen in table 1 , all the models trained on the spanish - only - lm outperform all the other models except for the ones that do not use the word " wait " in the setup . the results are summarized in table 2 . all the models that rely on this feature have lower performance on the test perp and test wer datasets . however , the results are slightly better than those using the plain - text - based alternative .
results on the train dev and test set are shown in table 4 . fine - tuned models outperform fine - tuned models using only subsets of the code - switched data .
5 shows the performance on the dev and test set compared to the monolingual model of the gold sentence in the test set . the results are shown in table 5 . for the dev set , fine - tuned - disc achieves a better performance than monolingually trained - disc models . however , it is slightly worse than fine - tuned ,
results in table 7 show that type - aggregated gaze features significantly improve recall ( p ≤ 0 . 01 ) and f1 - score ( p > 0 . 05 ) for the three eye - tracking datasets tested on the conll - 2003 dataset .
5 shows precision and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) on the single - tailed test set , while f1 - score is significantly lower ( p > 0 . 05 ) .
experimental results on belinkov2014exploring ’ s ppa test set . the glove embeddings are derived from the original wordnet ( see table 1 ) , and they use syntactic - sg embedding . they have been recently shown to perform well on wordnet 3 . 1 . the results on the original paper are shown in table 1 . type and semantic embedding are the most promising features for wordnet , but they have been found to be less promising for syntactic contexts . we observe that the semantic - sg bridges that allow the syntactic embedding of wordnet vectors are particularly promising for semantic contexts . however , this does not account for the fact that they are not suitable for semantic use in wordnet .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear from the table that removing the sense and context sensitive features hurts the model negatively .
2 shows the performance of domain - tuned models compared to multi30k models . the results are shown in table 2 . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) improves the results for both models . domain tuning also improves the interpretability of the captions .
results are shown in table 4 . subdomain - tuned models outperform all the other models except for those using word - augmented captions . in particular , the results are particularly striking for the en - de model , which results in significantly better performance than the subs1m model . table 4 shows that the domain - tuning method performs well on both datasets , with the exception of the flickr16 dataset , where the h + ms - coco performs poorly .
4 shows bleu scores in terms of automatic captions ( the best one or all 5 ) . the results are shown in table 4 . the models using en - de also outperform the models using the multi30k model .
5 compares the results of different strategies for integrating visual information ( bleu % scores ) . we use multi30k + ms - coco + subs3mlm with dec - gate as the decoding layer . we observe that the best performing model is the img w , which improves upon the strong leuu % baseline on all metrics ( except enc - gate ) .
1 shows the performance of subs3m with different visual features compared to subs6m . the results are summarized in table 2 . sub - coding features ( i . e . , word embeddings ) and multi - lingual features ( e . g . , " gn2048 " , " gn2048 " and " mscoco17 " respectively . the performance of the two models is slightly improved with the addition of text features , however , the performance remains the same when using only text - only features , as shown in fig . 2 . the results of the combined models are slightly worse than those of the other two models .
results are shown in table 3 . we observe that the best performing model is en - fr - rnn - ff , which results in significantly better performance than the other two models . table 3 shows that for all but one of the comparisons , the performance of the two models is significantly better than the others .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . we used en – fr as the training language , and en – es as the development language .
2 shows the performance of our models trained on the english , french and spanish datasets . the results are shown in table 2 . our model outperforms all the other models with a large margin .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) consistently show lower performance than ter , indicating that the model is more suitable for production use .
results on flickr8k are shown in table 2 . we observe that the visually supervised model outperforms the supervised model from chrupala2017representations .
results on synthetically spoken coco are shown in table 1 . our model outperforms the previous stateof - the - art models in terms of recall @ 10 and mean mfcc score , both of which are statistically significant ( paired t - test , p < 0 . 01 ) .
1 shows the results of different classifiers compared to the original on sst - 2 . for example , orig ( in the right hand hand hand ) turns in a < u > screenplay that is curved at the edges ; it ’ s so clever you want to hate it . similarly , dan shows similar results . the difference in the results between the original and the original is less pronounced .
2 shows that fine - tuning has indeed increased the number of occurrences with respect to the original sentence ( from 69 . 5 % to 70 . 5 % ) . however , this decrease is less pronounced for punct . ( p < 0 . 01 ) than for other words .
shown in table 3 , the sentiment changes in sst - 2 when the negative labels are flipped to positive . this shows that the effect of the flipped sentiment signal is very positive . however , this does not seem to have significant effect on the sentiment score .
results are presented in table 1 . we observe that the competitive nature of our method significantly improves upon the strong lemma baseline ( sst - 2 ) . however , it is still difficult to confirm whether a statement has been found to be positive or negative . our joint model ( sift - 2 ) significantly outperforms other methods in terms of ppmi score ( upadhyay et al . , 2018 ) on both positive and negative evaluations .
