2 : throughput for processing the treelstm model on our recursive framework and tensorflow ’ s iterative approach , with the large movie review dataset as our training example . as table 2 shows , the recursive approach performs the best on the training dataset , while the iteration technique shows better performance on the inference dataset . further , the use of gpu exploitation improves the performance on training datasets ,
1 shows the overall performance of the treernn model implemented with recursive dataflow graphs . the balanced dataset exhibits the highest throughput , but at the same time suffers from the high degree of parallelization .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . we choose to use conll08 as our max pooling strategy , and use sigmoid as our dropout rate . the hyper parameters initialization method performs better in all model variations . softplus also learns rate and the number of parameters in the validation set is slightly higher than softplus .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp . however , macro - adapted models have the worse f1 score , indicating that there is a need to design more dependency paths to improve the f1 scores .
3 shows the performance of the three models compared to each other on the test set . the results are summarized in table 3 . for brevity we only report results of r - f1 and f1 , with the exception of those of y - 3 . in general terms , the performance is better on both sets .
3 shows the paragraph level and the essay level . the results are presented in table 3 . all the methods trained on the proposed mst - parser are significantly better than the state - of - the - art method .
4 shows the c - f1 scores for the two indicated systems at the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph , on the other hand , has higher performance .
3 shows the bleu score on the original and symmetric test sets . the results are shown in table 3 . original is better than all the other systems except for tgen , whose cleanliness is more consistent . the difference is less pronounced on rouge - l , but still significant on mouge ( which takes the pre - trained tgen and applies the cleaned tab on top of the original ) .
1 compares our original and our cleaned versions . we find that the cleaned version has the highest number of distinct mrs and the average number of errors as measured by our slot matching script , see section 3 .
3 shows the test set on the original and original datasets . the results are presented in table 3 . original and tgen scores are shown in bold . original scores have higher bleu and cider scores , while the original ones have higher f1 scores . the rouge - l method is slightly better than both the original and the original but is slightly worse than the original .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found a total of 22 errors ( 17 . 6 % ) in the total number of errors we found ( 14 . 6 % ) .
model the performance of our dcgcn model is reported in table 1 . all models are significantly better than the state - of - the - art tree2str model on both external and external cues .
2 shows the results on amr17 . gcnseq achieves 24 . 5 bleu points and achieves 25 . 5 overall bleus points . the model size in terms of parameters is slightly larger than seq2seqb ( beck et al . , 2018 ) . the ensemble model is larger than both the single and ensemble models , however , the difference is less pronounced for the ensemble model .
3 shows the results for english - german and english - czech . the results are shown in table 3 . the best performing single model is ggnn2seqb ( beck et al . , 2018 ) . the difference in performance between single and single is minimal , however , in english - language , both the average bias metric and the average eq . metric are significantly higher than in other languages . we observe that the single model performs better in both languages , with the exception of english - koch .
5 shows the effect of the number of layers inside dc on the performance of the final layer . we observe that for every layer that has two layers , there are three more layers that have three layers .
6 shows that with residual connections , gcn has comparable performance to baselines in terms of residual connections . rc + la also improves gcn performance in the low - resource settings .
model d is presented in table 4 . the results are summarized in table 5 . the first set of models shows that dcgcn models perform well in all three scenarios . however , when the number of models is increased , the performance drops significantly , the second set shows severe overfitting of models with fewer data .
8 shows the ablation study results for amr15 . the results are shown in table 8 . - { i , 4 } dense blocks denote removing the dense connections in the i - th block . - ( i , 3 ) dense blocks denotes removing the layers of dense connections that are present in the dense blocks . the reduction of the dense blocks by 2 . 5 % shows the diminishing returns from using the dense blocks .
shown in table 9 , the global encoder and the lstm decoder use the best performing dual encoder design . encoder modules use a remarkably similar set of features to the ones used in the original encoder . however , the differences in coverage are less pronounced for the two encoder modules .
investigate the effects of different initialization strategies on probing tasks . we show in table 7 the results for each initialization strategy that we base it on . our paper shows that our method obtains the best performance .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . the results are summarized in table 2 . our method obtains the best performance on every metric with a gap of 10 . 5 % in precision . it also achieves the best score on the subtense test set with a 58 . 3 % improvement on the precision score . we observe that the two approaches converge on the same level of performance .
1 shows the performance of our method compared to other methods . our cbow model outperforms all the other methods except mpqa except for the one that cmp uses . cbow even outperforms sst2 and sst5 in terms of mrpc score . however , it has the advantage of training on a larger corpus . our model performs better on both mrpc and sick - e datasets . the difference between cbow / 784 and cmp is less pronounced in the case of the hybrid model , i . e . , it has a 0 . 2 % overall improvement over the cbow baseline .
3 shows the relative change from hybrid to hybrid on unsupervised downstream tasks attained by our models . our model obtains the best performance on sts12 , sts15 and sts16 compared to hybrid .
8 shows the performance for initialization strategies on supervised downstream tasks . our paper shows that our approach improves upon the state - of - the - art model on mpqa and sst2 by 3 . 8 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms the other two methods on both benchmarks .
can be seen in table 3 the performance of all methods that converge to the required threshold . cbow - r shows the best performance . it obtains the highest accuracy on both subtense and subtense subtasks , while it achieves the worst performance on subtense .
subj and sick - r are comparable in terms of mrpc score . our cbow - r model outperforms all the other methods except for sst2 and sst5 except for the one that allows mpqa to exceed its mrpc baseline . also , sick performs slightly better than sst3 and sts - b on both mrpc and subj benchmarks .
3 shows the e + org scores of all system models in ( italic , e + per and supervised learning ) . our system obtains the best e + org scores and the best misc scores . we observe that , in all but one case , the system performs better than the previous state - of - the - art model on all three scenarios .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 score . supervised learning improves the e + p scores and the f1 scores are shown in table 2 . we observe that the automatic learning model mil - nd outperforms the model in all three scenarios . name matching and supervised learning achieve the best e − p scores , respectively . finally , the performance of the supervised learning model in [ italic and ouritalic is improved with the addition of cross - training .
6 shows the results of ref and ref compared to gen on the symmetric keyphrases . ref significantly outperforms ref when compared to ref , indicating that ref alone does not harm the model ' s performance in general .
1 and table 2 summarize our results on the " bleu " and " meteor " metric , respectively . the results are summarized in table 2 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . our model obtains the best performance on both sets . the results are reported in table 3 . note that the training set size significantly increases the bleu score , which means that more models need to learn to program with additional data .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be seen that the use of bilstm improves the generalization ability of the model .
results are shown in table 4 . we observe that the comparison models generally have better performance on sentence length and sentence length compared to the baseline models .
shown in table 8 , the fraction of elements that are missing in the input graph that are present in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . note that the use of token lemmas in the comparison is strictly related to the reference sentences . however , it is harder to find those elements in the output than it is to find them in the mined sentences .
4 shows the performance of our method with respect to target languages . we use the 4th nmt encoding layer , trained with different target languages and tested on a smaller parallel corpus ( 200k sentences ) . we observe that the accuracy obtained using pos is comparable to that obtained using encoding layer v .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag has the best overall performance and is more likely to classifier with unsupervised word embeddings . it also improves the upper bound of the word tags by 3 points .
3 presents the results of our method . our results tabulated at the end of table 3 shows that our proposed method outperforms the competition on all three metrics . the results are reported in tables 1 and 2 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model shows that our res model is better than the original bi model , and therefore more accurate .
8 shows the performance of an attacker on two different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 .
1 shows the accuracies when training directly towards a single task . for pan16 training , the training method obtains the best performance . sentiment performance is consistently better than dial , indicating that the training context is important .
2 shows the status of the protected attribute leakage in the balanced and unbalanced datasets . the results are shown in table 2 . the trained classifier ( pan16 ) consistently leads to balanced & unbalanced data splits . in both cases , the classifier is able to detect instances of gender - neutral speech that are in conflict with the balanced one .
3 shows the performances on different datasets with an adversarial training set . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 3 . sentiment and gender are the most important factors in predicting whether an object will reach the target . sentiment is particularly difficult to predict , with an average age of 57 . 5 and an average gender of 58 . 5 .
6 shows the performance of different encoders in the setup when the protected attribute is added to the embeddings . embedding is strictly supervised by rnn , while the embedding is supervised by leaky . the results are reported in table 6 .
3 presents the results of our second study . our model outperforms the previous stateof - the - art models on both base and finetune tests . the results are summarized in table 3 . we observe that our lstm model achieves the best performance on both datasets with a minimum of training time of 22m . however , the results are slightly worse on the wt2 dataset with a maximum of 15m training time . finally , we observe that the improvement on the final model is more consistent with the strong lemma baseline on wt2 , indicating that the model performs better on both tests when trained and tested .
5 shows the performance of our model on the test set of hotpotqa in the distractor and fullwiki setting . the results are summarized in table 5 . our model obtains the best performance on both test set with a minimum of time to train , and the average number of iterations to train .
3 shows the performance of our model on the divided test set of yelppolar time and amafull time . results are summarized in table 3 . our model obtains the best performance on both datasets . on the one hand , it gets a 4 . 42 % improvement on average compared to the previous state - of - the - art model . this model also gets a 3 . 53 % boost in average amapolar time and a 2 . 45 % boost on average on yelppule time . finally , we get the best overall performance on the two datasets . we report the results of experiment 1 and experiment 2 using the best performing model , gru .
3 shows the bleu score on wmt14 english - german translation task . our model improves upon the state - of - the - art model by 3 . 5 % in bleus score compared to the previous best model , gru .
4 shows the performance of our model on squad dataset . the results published by wang et al . ( 2017 ) show that our # params model can significantly improve match / f1 score over the base model . however , it is still inferior to the lstm model in terms of match / score . we observe that our model obtains the best performance when the parameter number of param is increased .
6 shows the f1 score of our model on conll - 2003 english ner task . it can be seen that the use of # params improves the performance by 0 . 31 points over the strong lemma baseline .
7 shows the performance of our model with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 .
3 shows the results for both systems . word embeddings w / system retrieval and word2vec both systems are state - of - the - art . all the word models trained on the systems are double - tailed , with the exception of mtr , which is used on the human dataset . the word - based approach is particularly effective for system retrieeval , with an absolute improvement of 2 . 5 % over previous methods .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . our system obtains the best overall score ( k 1000 ) . the results are highlighted in bold . we note that automatic evaluation results are slightly higher than those of random evaluation , indicating that the selection process is more accurate .
are presented in table vii . our proposed system outperforms the previous state - of - the - art systems on three of the four datasets : en , librispeech , and europarl . it closely matches the performance of both df and docsub . on the other hand , it performs slightly worse than the other two baselines on all three datasets except for the one with the exception of corpus .
are presented in table vii . our proposed system outperforms the previous state - of - the - art systems on all three datasets except for the one that we use , namely , the ensembles of europarl and librispeech . it obtains the best performance on both datasets , with the exception of the one on corpus . we observe that our proposed docsub wrapper outperforms all the other baselines except for eurparl , whose performance is significantly worse on the three datasets .
are presented in table vii . our proposed system outperforms the previous state - of - the - art systems on all three datasets except for the one that we use , namely , df . europarl is better than both libris and docsub , confirming the competitiveness of our proposed domain - aware embeddings . we observe that the difference in performance between the two approaches is minimal , however , when we switch to the more realistic " ted talks " and " p " modes .
3 shows the performance of our model compared to the previous best state - of - the - art models . our model achieves the best results on both metric metrics . our joint model is better than both the baseline and the maxdepth metrics . on the other metric , our model performs slightly worse than the other two baselines . our maxdepth metric is slightly better than our joint model , but europarl has the advantage of having a greater number of iterations .
3 shows the performance of our model compared to the previous best state - of - the - art systems . our model exceeds both the maxdepth and maxdepth of our baseline in terms of both metric and depthcohesion . our joint model outperforms both the baseline and the corresponding embeddings . we observe that our joint model has the best overall score on both metric metrics , with the exception of the exceptional case of docsub .
experimental results are shown in table 1 . the enhanced version of lf outperforms the enhanced version we used in the experiments of applying our principles on the validation set of visdial v1 . 0 .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . the results are shown in table 2 . using only p2 indicates the most effective one ( i . e . , hidden dictionary learning ) .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . our model significantly outperforms the strong lemma baseline on hard alignments and fi - en , indicating that ruse relies on selective attention .
3 presents the results of our approach with respect to direct assessment and eureor . the results are summarized in table 3 . the first set of metrics shows that our approach significantly improves the results for both groups .
3 presents the bagel and sfhotel scores on the test set of hotpotqa . the results are summarized in table 3 . our proposed bleu - 2 improves the scores by 3 . 5 points over the baseline on both sets . however , the improvements are not significant enough to exceed the baseline scores by any significant margin .
3 presents the metric and baseline scores . the results are summarized in table 3 . the summaries are presented in bold . they appear to be slightly better than the leic score on m1 and m2 . however , leic scores are significantly worse than those on m2 and m3 .
3 shows the performance of all models using the word embeddings . we observe that for all but m0 , the model performs better than the previous state of the art on all three metrics except for the shen - 1 metric .
3 presents the results of our final model on the transfer quality and transfer quality tests . we show that all the models tested on the datasets are in the best state of the art . semantic preservation and semantic preservation are the most difficult tasks to solve . yelp , on the other hand , is the better - equipped and more stable one . the semantic preservation scores are significantly better than those of the other two baselines . complicated test sets result in significantly worse performance for both semantic and syntactic preservation .
5 shows the human evaluation results . we apply our [ italic ] ρ b / w negative pp and human ratings of fluency . we show in bold the results for each metric , and the percentage of machine and human judgments that match ( p < 0 . 001 ) . the results tabulated in table 5 show that our method significantly improves the sentence quality .
3 shows the performance of all models using the word embeddings . we observe that the m0 baseline performs better than the m1 baseline on both sim and pp , indicating that the improvement is due to the high quality of the shen - 1 representation .
6 shows the results on yelp sentiment transfer , where bleu is between 1000 and 1000 sentences and human references are restricted to 1000 words . our best model achieves the highest acc ∗ score ( 31 . 4 % higher than yang2018unsupervised ) , but that is only because the classifiers in use in the past are worse than those in the present . we also observe that simple - transfer and abstractive word embeddings achieve the best acc scores ( 29 . 3 % and 31 . 4 % ) compared to the previous best state - of - the - art model . finally , we observe that the use of the classifier in the transfer domain limits the performance of our model , so we do not include it in table 6 .
2 shows the true number of repetition tokens that were correctly predicted as disfluencies . reparandum length is reported in table 2 . for nested disfluency , the average number of reparandum tokens is 8 .
3 shows the relative frequency of rephrases correctly predicted as disfluent for both the reparandum and the repair ( table 3 ) . the fraction of tokens that contain a content word is in parentheses , indicating that the function contains a lot of content word .
results are shown in table 4 . we observe that the text + innovations model outperforms the single model in terms of dev and test scores . the results are broken down in α by the number of examples in each test set , with the exception of text + raw having the best effect .
2 shows the performance of our model compared to the state - of - art algorithms on the fnc - 1 test dataset . our model achieves the best performance in the two scenarios .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the performance of our method in terms of word attention and graph attention . the results show that neuraldater performs better than both the original method and the graph attention model .
3 shows the performance of all models on the test set of hotpotqa in the distractor and fullwiki setting . the results are presented in table 3 . embedding + t models perform better than jvmnn and dmcnn on both test set , but do not outperform trigger on all test set .
3 shows the method and the type of object used for the identification task . all methods cause a significant difference in the performance of the models when trained and tested on the same dataset . we include the method in table 4 . in all but one case , the error threshold and the number of tokens used to classify the models have been reported as significant ( p < 0 . 001 ) . the method has been shown to handle both the argument and the threshold for the classification task . the method is described in section iv .
can be seen in table 4 the results for english and spanish are summarized in table 5 . all except for fine - tuned - lm are better than all the other models except for the one that we included in the comparison set .
4 shows the results on the dev set and the test set using discriminative training . our fine - tuned model achieves the best results with a 25 % train dev and a 75 % train test score .
5 shows the performance on the dev set and the test set . it can be seen that the language - switched approach improves the performance over the monolingual approach , but does not outperform the code - switching approach .
results in table 7 show that type - aggregated gaze features significantly improve recall and f1 - score for the three eye - tracking datasets tested .
5 shows precision , recall and f1 - score for using type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) for type combined with the drop in f1 score from the previous experiment , confirming the viability of our method .
1 shows the results on the ppa test set . our hpcd approach uses syntactic - sg embeddings obtained by running autoextend rothe and schütze ( 2015 ) on glove . the results on table 1 show that using syntactic sg embedding can improve wordnet 3 . 1 by a significant margin . syntactic sg - based wordnet vectors have been recently used in the development set of verbnet , and it has been recently adapted to wordnet 2 . 1 .
2 shows the results from our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . as table 2 shows , using oracle pp as the dependency parser results in significantly better ppa acc . than using lstm - pp .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . it is clear from the table that removing the sense and context sensitive features hurts the model considerably .
2 shows the results with domain - tuned and multi30k decoding . the results with subsfull are very similar , but the improvement is slim . adding subtitle data and domain tuning for image caption translation ( bleu % ) .
3 shows the performance of subs1m models using domain - tuned word embeddings . our model outperforms all the en - de models except for the case of flickr16 . the results are slightly worse than those of mscoco17 , but still superior than that of the previous models .
4 shows bleu scores in terms of the automatic captions added after adding the best ones or all 5 models . we also include the results in table 4 , which shows that adding multi30k captions improves the results for all models .
5 compares the strategies for integrating visual information and dec - gate . we use transformer , multi30k + ms - coco + subs3mlm , and directron mask surface . results are summarized in table 5 . we observe that the enc - gate approach significantly improves the bleu % scores for visual information , enc - gate also improves the mscoco17 baseline by 2 % .
1 shows the performance of subs3m on the en - de and on the flickr16 datasets . we observe that the visual features alone do not improve the performance , so we choose to rely less on the word embeddings and rely more on the semantic features alone . moreover , the combination of syntactic and semantic features further improves the performance .
3 shows the performance of our system compared to the previous state - of - the - art on mtld . the results are summarized in table 3 . we observe that our system performs better than the alternatives on both mtld and en - fr - ff .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 shows the performance of our method in terms of training vocabularies for english , french and spanish . the results are shown in table 2 . our model obtains the best results with a 3 . 66 % increase in precision .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores ( bleu ) and ter are both below - normal , indicating that automatic evaluation results are beneficial .
2 shows the vgs evaluation results on flickr8k . the results tabulated in table 2 shows that our supervised model outperforms the strong lemma baseline by a significant margin .
experimental results on synthetically spoken coco are shown in table 1 . the model trained on audio2vec - u is comparable to the visually supervised model from chrupala2017representations . however , the difference is narrower , in that the learned embeddings are more consistent .
1 shows the results for each classifier compared to the original on sst - 2 . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that is slightly curved at the edges ; it ’ s so clever you want to hate it . rnn ( 2016 ) shows that it is easier to hate the word " hate it " when the edges are curved . for cnn ( 2016 ) , the same thing is true for rnn .
2 shows the results for part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning . the results are shown in table 2 .
3 shows the change in sentiment from positive to negative . the results are shown in table 3 . essentially , negative sentiment is replaced by positive sentiment . this shows that the effect of flipped sentiment is positive .
3 presents the results of experiment 2 . our joint model improves upon the strong lemma baseline by 10 % onpubmed . our results are slightly positive , but significantly worse than expected by our joint model . this indicates that our approach is more effective for target task .
