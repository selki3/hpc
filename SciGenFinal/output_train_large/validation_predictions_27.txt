2 : throughput for processing the treelstm model on the recursive framework , and tensorflow ’ s iterative approach , with the large movie review dataset as our training dataset . as table 2 shows , the recursive approach performs the best on the training dataset , with efficient parallel execution of the tree nodes . further , throughput ( instances / s ) and training ( potential scalability / potential ) achieve the best results on the instructional and training dataset , which shows the value of finetuning over time .
shown in table 1 , the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization . however , it does not improve as well as the linear dataset , because there is only a small room of performance improvement left to train the system .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . our system achieves the best results with different number of parameters and the max pooling strategy consistently performs better in all model variations . the hyper parameters activation func . is the most efficient in all scenario . it reduces the error of selecting the best hyper parameters and reduces the number of iterations . moreover , it boosts the model performance with the boost of prediction accuracy .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models ( ours ) achieve the best f1 ( in 5 - fold ) with sdp . however , these models do not outperform the strong baselines in terms of f1 score , indicating that the dependency path alone does not improve the model ' s performance in the task .
results are presented in table 3 . in general terms , the performance of y - 3 compared to y - 2 is significantly better : on the f1 metric , it achieves 50 % and 50 % f1 scores respectively , while on the r - f1 metric it achieves 60 % .
results are presented in table 4 . the results of the best - performing models are summarized in table 5 . all the models except mst - parser have achieved high scores on the essay level , which shows the diminishing returns from using word embeddings .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . paragraph also shows lower performance , however , compared to the majority performance .
results are shown in table 4 . the original and the new systems perform better than the original on a single test set . the difference is most prevalent in bleu ( which takes the pre - trained tgen and tgen + from the original ) and on the variant of sc - lstm . these systems are comparable in terms of performance on both test sets . however , their performance is slightly worse on the original set than the variant .
1 compares our original and our cleaned versions . the results are shown in table 1 . the original and the cleaned versions have the highest number of distinct mrs , total number of textual references , and the number of instances as measured by our slot matching script , see section 3 . however , the cleaned version has the highest percentage of instances , which shows the diminishing returns from mixing multiple tokens .
performance of original and original models on the test set of hotpotqa in the distractor and fullwiki setting . the results are shown in table 4 . original models generally perform better than the original on all test sets except for the one that the original relies on . this result is sometimes misclassified as a result of incorrect programming ( e . g . , tgen + tgen − < 0 . 001 , 0 . 005 ) . the sc - lstm ( which relies on syntactic or semantic information ) performs slightly worse than original but is comparable in terms of accuracy .
results of manual error analysis of tgen on a sample of 100 instances from the original test set . the error numbers shown in table 4 show that the added errors caused by incorrect values are relatively small ( i . e . , no errors were detected , no wrong values were detected ) .
model the performance of our dcgcn model compared to other state - of - the - art models on the external and internal datasets is reported in table 1 .
results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points ( paired t - test ) and achieves 27 . 9 bleus points ( a significant improvement over the previous state of the art model , dcgcn ) .
results of experiment 1 are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the difference in performance between single and single is minimal , however , in english - german , the gap between the best performing models is 15 . 5 % vs . 25 . 5 % . the most striking thing about the english - language model is that it relies on single - language programming instead of multi - language models .
5 shows the effect of the number of layers inside the dc stack on the performance of the model in table 5 . the first group shows that when layers are added to a dc stack , the effect is less pronounced for the other layers .
results are shown in table 6 . with residual connections , gcn models generally outperform baselines in terms of overall performance . rc + la also improves gcn performance .
model f1 shows that dcgcn models outperform the state - of - the - art models on all metrics except bias metric .
8 shows the ablation study results for amr15 . the results show that removing the dense connections places the most important importance for the model .
shown in table 9 , the models used in the graph encoder outperform the lstm decoder in terms of coverage . the ablation study shows that the global encoder design improves upon the strong lemma baseline by 2 . 8 points in the bias metric .
results for initialization strategies on probing tasks are shown in table 7 . our paper shows that our method significantly improves the performance over the strong baselines .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . the results are summarized in table 2 . our proposed method outperforms the previous state - of - the - art methods on every metric by a significant margin . in particular , it has the best performance on the subtense metric , while the other two have lower performance .
1 shows the performance of our method compared to other methods . our cbow model outperforms all the other methods except for the one that cmp uses . cbow even outperforms mpqa and sst2 in terms of mrpc score . however , it has the advantage of training on a larger corpus , which results in lower mrpc scores . moreover , the difference between cbow / 784 and sick - r shows that cbow models have superior recall scores when trained on larger corpus .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both the hybrid and hybrid models in terms of overall performance . on the sts13 dataset , it achieves a lower performance than cmp . however , it is comparable with the performance of cmp in the low - supervised settings .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that it is possible to improve upon the performance by leveraging the best performing state - of - the - art model on mpqa and sst2 tasks .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms the other methods on three out of the four tasks .
1 and table 2 summarize our results on the subtense and subtense subtasks . our proposed method outperforms both cbow and cbow - r by a significant margin .
3 shows the performance of subj and sick - r models compared to other methods . the subj models outperform all the other methods except for cbow - c , which obtains the most consistent performance . however , it does not outperform sst2 and sst5 in terms of mrpc score . on the other hand , it has comparable performance with sst3 and sts - b on all three metrics .
3 shows the e + and per scores of all systems trained on the same domain . our system ( mil - nd ) significantly outperforms all the systems except for the one that relies on cross - domain learning ( misc ) . the results of the best performing system are shown in table 3 . the system performs well in all aspects , with the exception of name matching . it obtains a significant improvement over the previous state of the art on all org and per metrics .
2 shows the results on the test set under two settings . our system achieves the best results with 95 % confidence intervals of f1 scores . supervised learning improves the e + p score by 0 . 59 points in generalization , while supervised learning improves e + f1 by 2 points . the performance of the supervised learning model is shown in table 2 .
table 6 , we report the results of ref and ref experiments on the entailment ( ent ) task . the results are summarized in table 6 . ref significantly outperforms ref when compared to ref , and again when compared with ref alone .
results in table 3 show that the models trained on the proposed ldc2017t10 outperform the state - of - the - art models on all metrics , except for the keystroke of the ldc2015e86 .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are summarized in table 3 . our model obtains a significant improvement in performance over the previous state - of - the - art model on a single test set .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . it can be observed that the impact of bilstm on the model is less pronounced than that of a similar ldcstm .
results are presented in table 4 . the results are summarized in table 5 . in particular , the results show that the g2s models significantly outperform the other models in terms of sentence length and average sentence length .
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) . these tokens are used in the comparison to derive the summaries from the reference sentences . however , for the ldc2017t10 test set , the token lemmas are used instead of the gold ones .
4 shows the performance of the two approaches using the 4th nmt encoding layer . it can be seen that both approaches significantly improve the target language performance , with the exception of the strong lemma baseline .
2 shows the pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb embeddings are the most frequent tags ; word2tag is the most sophisticated encoder - decoder .
results are presented in table 4 . our proposed method outperforms the previous state - of - the - art methods on three of the four subsets . the results are reported in tables 1 and 2 .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model shows marked improvement over the previous state of the art on three of the four layers .
performance on different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . on a training set with 10 % held - out participants , the average age of the participants is 9 . 7 % ( p = 0 . 0088 , double - tailed ttest ) . in pan16 , an attacker is 18 . 3 % more likely to miss an assignment because the training set contains 10 % more data .
1 shows the accuracies for training directly towards a single task . the training data generated by pan16 shows that the training data have a high correlation with human judgement .
2 shows the effect of the additional cost term on the balanced and unbalanced task averages . the classifiers trained on the pan16 dataset are named after the gender - neutral tweets ( hence , there is no such thing as a balanced or unbalanced dataset ) . dial models trained on pan16 data are able to detect instances of classifiers that are potentially harmful to the model ( e . g . , age , gender ) . the pan16 model shows marked performance improvement over the baseline .
performance on different datasets with an adversarial training set is shown in table 3 . the difference between the performance of the trained classifier and the corresponding adversary is measured in δ . in pan16 , the classifier trained on gender - neutral training data is more likely to classify the target as an attacker , although the difference is less pronounced for the corresponding training data .
6 shows the performance of the embeddings for different encoders . embedding leaky is easier for rnn to perform than it is for embedded ( see table 6 ) .
results in table 2 show that our proposed lstm outperforms the previous state - of - the - art models on both base and finetune tasks . the results of our model outperform the strong baselines on both datasets , in particular , it achieves the best performance on the largescale wt2 dataset , on the other hand , it performs slightly worse than the strong baseline wt2 model on both subsets . finally , the results of the second study seem to indicate that the best performing model is the lrn model , which performs on par with the strong lemma baseline from the previous work .
results in table 5 show that our approach significantly outperforms previous models in terms of training time . the results of our model outperform all the other models except for the one that takes the time to train . in addition , the difference in training time is less pronounced for this model , which shows the diminishing returns from using a pre - trained model . table 5 shows the performance of our approach when training on a single dataset .
3 shows the performance of our model compared to other models . our model improves upon the state - of - the - art on three of the four datasets . on the other four datasets , it gets a 4 . 42 % improvement on average compared to yelppolar time . this model works better on both datasets .
3 shows the bleu score on wmt14 english - german translation task . our model improves upon the state - of - the - art model by a noticeable margin .
4 shows the model ' s performance on squad dataset . the results published by wang et al . ( 2017 ) show that rnet models significantly outperform other models in terms of match / f1 score . however , for the lstm model , the parameter number of base is only 2 . 67m , which shows the diminishing returns from mixing parameter number with model number . finally , the sru model performs slightly better than the other models .
6 shows the f1 score of our model on conll - 2003 english ner task . the lstm model significantly outperforms the other models in terms of parameter number . it can also be observed that , when using lrn as a parameter , the model achieves a significantly better performance .
shown in table 7 , the performance of elrn on base + ln task with base setting is significantly worse than that of glrn , indicating that the model is more suitable for ptb task .
results are shown in table 4 . word - based systems retrieval and word - based learning methods ( mtr ) outperform human on all metrics . the top - level systems , specifically , the oracle - based mtr , are significantly better than human - based system , word based systems are more effective for both systems , with the exception of system retrieeval , which is used in the distractor and distractor task .
4 : human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is highlighted in bold . our system outperforms all the systems except seq2seq on the k 100 and k 1000 datasets .
results are summarized in table vii . our proposed system outperforms the previous state - of - the - art systems on three of the four datasets : english , spanish , french , dutch , russian and russian . it closely matches the performance of the other two baselines , namely , the df model , europarl , and ted talks . it obtains the best performance on all three datasets , with the exception of the english one , where the difference between the performance between the two sets drops significantly .
results are summarized in table vii . our proposed system outperforms the previous state - of - the - art systems on all three datasets except for the one that embeds the word " democracy " . it obtains the best performance on the three datasets , outperforming all the alternatives except the original ones . for example , on the corpus dataset , our proposed system performs better than both the original and the alternative approaches .
results are summarized in table vii . our proposed system outperforms the previous state - of - the - art systems on three of the four datasets : english , spanish , french , dutch , russian and russian . it closely matches the performance of the other two baselines , namely , the df model , europarl , and ted talks . it obtains the best performance on all three datasets , with the exception of the english one , where the difference is narrower .
are presented in table 3 . our system achieves the best performance with a minimum of 3 . 5roots compared to the maxdepth of europarl . on the other hand , our system performs slightly worse than the others on many metrics , such as df , docsub and hclust scores . in particular , we observe that our system has the worst performance on the three metrics .
metrics are shown in table 1 . our system achieves the best performance with a minimum of 3 . 5roots compared to the maxdepth of europarl . on the other hand , our system performs slightly worse than the others on many metrics , such as df , docsub and hclust scores . in particular , we observe that our system has the worst performance on depthcohesion .
experimental results are shown in table 1 . the enhanced version of lf outperforms the enhanced version by a noticeable margin . we observe that the loss function , when combined with the hidden dictionary learning function , severely reduces the performance .
performance ( ndcg % ) of different models on the validation set of visdial v1 . 0 is shown in table 2 . the best performing model is lrv , which relies on hidden dictionary learning . note that applying p2 also improves the performance of coatt and rva .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . they show that the hmd - prec model significantly outperforms the alternatives on hard alignments , while fi - en performs slightly better .
3 presents the performance of our approach on the direct assessment and eureor metrics . the results are summarized in table 3 . the first set shows that our approach significantly outperforms the baselines on both metric metrics .
performance of the baselines on the sfhotel and smd datasets is reported in table vi . the results are summarized in table vii . our proposed bertscore - f1 model outperforms the baseline on every metric by a significant margin . in particular , it improves the bagel score by 3 . 8 points over the baseline .
performance of the models according to these baselines is reported in table vi . the results are summarized in table vii . the summaries displayed in bold indicate that the performance obtained by the models exceeds the leic score by a significant margin . leic scores are broken down into five categories : m1 , m2 , w2v and word - mover . these scores are computed using the weighted average of pre - trained word utterances ( emo ) to obtain the best score . the results of the best performing models are reported in tables vii .
results are shown in table 4 . the performance of all models that use the word embeddings is presented in table 5 . table 5 shows that the m0 model performs better than the m1 model on all metrics except for the shen - 1 score , which shows the diminishing returns from using syntactic or semantic information .
results are shown in table 4 . semantic preservation and transfer quality scores are broken down in terms of the quality of the data according to the baselines . in particular , the results show that the semantic preservation approaches significantly outperform the transfer quality ones . the semantic preservation approach , δpp , improves the results for all models with a drop of 0 . 05 or 0 . 01 compared to the previous state - of - the - art .
5 shows the human evaluation results . the results are shown in table 5 . it is clear from the table that human evaluation has a significant impact on the quality of the sentences generated by the model , as measured by the number of errors in the generated sentences and the percentage of error generated by human evaluation .
results are shown in table 6 . the performance of all models that use the word " shen - 1 " is slightly worse than that of m1 , m2 , m3 and m6 , but still comparable to the performance of sim .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than prior work at similar levels of acc ∗ ( see table 6 ) . however , the improvement is less striking for those using unsupervised embeddings . the results on simple - transfer are not included as they are worse than in previous work , which is typical of sentiment transfer . we also observe that the use of classifiers in the transfer domain hurts the model , as it is harder to distinguish between the quality of the transferred and unlabeled sentences .
statistics for nested disfluencies are shown in table 2 . the percentage of tokens that were correctly predicted to be disfluent is reported in tables 2 and 2 .
3 shows the percentage of rephrases correctly predicted as disfluent for each category . the fraction of tokens predicted to contain a content word in each category is shown in table 3 . as the table shows , the proportion of tokens that contain a word in both the reparandum and the repair function is significantly less than the fraction predicted as contained in the function .
results are shown in table 4 . the results are broken down in terms of the best and average results for each model , with the exception of the case of " late " when text is used with innovations . in addition , the text model with innovations as the feature set achieved the best results on the single test . table 4 shows the results for the three scenarios . text + innovations significantly improve the model ' s performance on the two scenarios .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model shows state - of - art performance on the test dataset . the accuracy ( % ) agree with , disagree with , and sometimes completely agree with the models . finally , the accuracy ( % ) disagree with is significantly higher than the rnn - based model .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the unified model significantly outperforms all previous models .
3 shows the performance of our method with and without attention . the performance shown in table 3 shows that it is possible to improve the interpretability by using graph attention for word attention .
results are shown in table 1 . the best performances are obtained on the 1 / 1 and 1 / n test sets , respectively . the models trained on jvmee outperform all the other models except for the one that had the most training data . the results are presented in tables 1 and 2 .
experimental results of all methods are shown in table 1 . all methods cause a significant difference in the identification and trigger performance for all models . in particular , the method that relies on the word " trigger " has the most significant impact on the event identification performance . the method used in the experiments shows severe overfitting of the event with a large number of parameters .
can be seen in table 4 , all the fine - tuned models trained on the spanish - only - lm outperform all the other models except for the ones that do not use concatenated word embeddings . the results are summarized in table 6 .
results on the dev set and the test set are shown in table 4 . fine - tuned train dev outperforms fine - tuned train dev with only subsets of code - switched data .
5 shows the performance of our system on the dev set and the test set , compared to fine - tuned - disc . the results are shown in table 5 . our system obtains the best performance when trained with gold sentences instead of code - switched ones .
results for the conll - 2003 dataset are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) show significant improvement over type - aggregated gaze features trained on the three eye - tracking datasets and tested on the same single dataset .
5 shows the performance of type - aggregated gaze features for the conll - 2003 dataset . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are shown in table 5 . the improvement of precision ( p ≤ 0 . 01 ) over the previous state of the art model is primarily due to the improved recall scores .
results on belinkov2014exploring ’ s ppa test set . the hpcd approach relies on syntactic embeddings obtained through autoextend rothe and schütze ( 2015 ) and wordnet 3 . 1 . the results on the original paper are shown in table 1 . glove - retro is an alternative to skipgram on wordnet , and it uses the syntactic - sg embedding obtained by fauqui et al . ( 2015 ) . it closely matches the original wordnet design , but it uses syntactic features obtained by using autoextension rothe . it improves the performance by 9 . 8 points over the glovec embeddings obtained by the previous work .
performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . table 2 shows the results of the models using the best performing system , hpcd .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing context sensitivity is shown in table 3 .
2 : adding subtitle data and domain tuning for image caption translation ( marian amun et al . , 2018 ) with subsfull embeddings improves the results for all models except for those using mscoco17 . adding domain - tuned data improves the multi30k performance by 3 . 8 points .
results are shown in table 4 . subdomain - tuned subs1m models outperform the en - de models in terms of performance on both datasets . the results are presented in tables 4 and 5 , respectively .
4 shows bleu scores in terms of automatic captions added by marian amun ( marian amun et al . , 2018 ) . as expected , the better multi30k model outperforms all the other models except for the one with the best one or all 5 captions .
5 compares the performance of different approaches for integrating visual information . we use transformer , multi30k + ms - coco + subs3mlm , detectron mask surface and word2vec enc - gate . results are summarized in table 5 . enc - gate and dec - gate achieve remarkably similar results ( bleu % scores ) with respect to the original embeddings ( table 1 ) .
results are shown in table 4 . the subs3m model outperforms all the other models except for the one that relies on word embeddings . in particular , the improvements on the flickr16 dataset are more striking than those on the largerickr17 dataset , sub - categories such as augmented reality ( aoco ) and multi - lingual features have resulted in an overall improvement of 2 . 36 points over the previous state - of - the - art model on both datasets .
results are shown in table vii . the best performing translation systems are the word - adaptive ontonotes ( emt ) and the variant - based mtld . our system outperforms all the other systems except for the one that requires more training data . the results are presented in tables vii and viii .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used .
2 : training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) show that the re - rev systems perform better than the other systems when trained using ter .
2 shows the vgs model trained on flickr8k . the results are summarized in table 2 . the marked vgs rank is the result of a supervised model design using the best performing neural network architecture .
results on synthetically spoken coco are shown in table 1 . the model trained on the embedded embeddings of chrupala2017representations is comparable to the similarly supervised audio2vec - u model . however , the difference is less pronounced for rsaimage .
1 shows the results for different classifiers compared to the original . for example , orig < cao et al . ( 2017 ) turns in a < u > screenplay that shows the edges at the edges ; it ’ s so clever you want to hate it . also , dan ( 2016 ) shows that it is easier to hate hate it when the edges are in the right position . similarly , cnn ( 2016 ) , shows the same results .
2 shows the percentage points in which the number of occurrences have increased or stayed the same since fine - tuning . the results are shown in table 2 . these numbers indicate that the quality of the word has not decreased , decreased or stayed in the same range .
shown in table 3 , the change in sentiment from positive to negative is larger than that in the original sst - 2 sentence .
table 2 , it can be seen that the competitive advantage of posemi over other methods is less pronounced for positive and negative evaluations . on the other hand , for negative evaluations , it is much more likely to improve . our joint model outperforms both the competitive and negative approaches .
