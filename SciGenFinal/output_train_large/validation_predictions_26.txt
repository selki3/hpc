2 : throughput and training on the recursive framework with the large movie review dataset in table 2 . our iterative approach improves the performance on training with efficient parallel execution of tree nodes , while the recursive approach shows better performance on inference .
1 shows the performance of the balanced and linear datasets when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left to train the model in w . r . t parallelization . the balanced dataset exhibits the highest throughput thanks to the high degree of parallelization , but it does not improve as well as the linear dataset .
2 shows the performance of the hyper parameters optimization strategies for each model with different representation . our approach achieves the best performance with the maximum number of hyper parameters and the number of feature maps . the max pooling strategy consistently performs better in all model variations . the combination of sigmoid and softplus models with different number of parameters performs better than any other approach .
1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models achieve the best f1 ( in 5 - fold ) with sdp , but do not outperform this model in terms of f1 score . the comparison of our model with the strongest dependency path is shown in table 1 .
performance of y - 3 compared to y - 2 is reported in table 3 . in general terms , the results are significantly better than those obtained by y - 4 : y - 3 achieves 50 % f1 score , while f1 50 % achieves 50 % .
results are presented in table 4 . the results of the best performing model are presented here in terms of the paragraph level . all the models except mst - parser have achieved the highest scores on the test set . in addition , the average number of frames per word is significantly higher than the average of the other models .
4 shows the c - f1 scores for the two indicated systems ( the lstm - parser and the paragraph system ) over the runs given in table 2 . the result is lower than the majority performances over the three indicated systems , however still comparable to the best performances by both systems .
results are shown in table 4 . original , tgen + and sc - lstm have been shown to perform better than the original on all tests except for one . the results are presented in tables 4 and 5 . table 4 shows that the cleaned model outperforms all the other methods except for the one that has been shown in the past . table 4 also shows the results of re - scoring all the test sets for each of the four scenarios . the only exception is the one in the original case where tgen − was shown to have been misclassified as abusive .
1 compares our original and our cleaned versions with the original e2e data . our cleaned version has the highest number of distinct mrs , total number of textual references , and the number of slot matching scripts as measured by our data table , see section 3 .
performance of original and tgen + on the test set is presented in table 4 . the results are presented in tables 4 and 5 . original results are summarized in bold . they include : bleu score , average error rate , f1 score , rouge scores , average ranking scores and average ranking of test sets . they also include the results of re - scoring the original scores for each sub - category . the best performing model is sc - lstm .
results of manual error analysis of tgen on a sample of 100 instances from the original test set is shown in table 4 . the percentage of errors we found was very small ( 0 . 01 % ) , so we found no significant trends in the error numbers .
1 and table 2 summarize our results on the external and external keyphrases . our proposed dcgcn model outperforms all the base models except for seq2seqk ( konstas et al . , 2016 ) and tree2str ( mikolov and schmidhuber , 2017 ) . note that the hierarchical nature of our model results in a significant drop in performance compared to previous models .
2 presents the results on amr17 . our model achieves 24 . 5 bleu points ( paired t - test ) compared to the previous best state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . the model size in table 2 shows that the ensemble model performs better than the single model ,
results of experiment 1 are shown in table 1 . the best performing model is bow + gcn ( bastings et al . , 2017 ) . the difference in performance between single and multi is minimal , however , the difference between the average bias metric and the average czech score is significant , we notice that the single model significantly outperforms the other models in terms of both english - language and german ,
5 shows the effect of the number of layers inside the dc stack on the performance of the model in table 5 . as table 5 shows , for every layer , there are two layers that contribute to the overall performance . for example , of the 3 layers in our model , we have 17 . 5 layers .
6 compares gcn with baselines with previous models . rc + la ( 2 ) and parallelism ( 4 ) show that gcns with residual connections have higher gcn performance than those without . however , compared to dcgcn2 ( 6 ) , gcn has higher residual connections .
model f1 shows that dcgcn ( 2 ) outperforms all state - of - the - art models in terms of bias metric , and in general terms , the improvement is modest but significant , reaching a high of 62 . 5 % in bias metrics . in addition , dcgcnn ( 3 ) achieves a comparable performance improvement of 48 . 5 % .
results in table 8 show that removing the dense connections in amr15 improves the performance for the model by 3 points .
table 9 , we show the ablation study results for the four types of decoder modules used in the lstm decoder . the results are presented in table 9 . the hierarchical structure of the encoder modules ( 25 . 9 % ) leads to a significantly better performance than the other four types .
results for initialization strategies on probing tasks are shown in table 7 . our proposed method outperforms the best state - of - the - art method on three of the four probing tasks .
1 and table 2 summarize our results on the hidden test set of cbow / 400 . we observe that our approach obtains the best performance on every metric with a gap of 2 . 5 points in precision . it also improves the performance on all metrics with a boost of 3 . 3 points in accuracy .
1 shows the performance of our method compared to other methods . our model outperforms all the alternatives except subj and mpqa except for sst2 , which has the advantage of training on a larger corpus . it also outperforms sst3 , sst5 and sick - r . cbow / 784 shows significant performance improvement over both the original and hybrid models . however , it is still inferior to both sst1 and sst - r in terms of mrpc score .
results on unsupervised downstream tasks attained by our models are shown in table 3 . our model outperforms both hybrid and cmp on all downstream tasks , except for sts13 , which shows the relative change in performance with respect to cmp . cbow also exhibits a lower performance relative to hybrid , though still performing better than cmp in some downstream tasks .
results for initialization strategies on supervised downstream tasks are shown in table 8 . our paper shows that the best performing model is the sst2 model , which improves upon the performance of mpqa and sst3 by 3 . 8 points .
6 shows the performance for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms all the alternatives except sts12 , sts14 and sts15 .
1 shows the performance of our method compared to cbow - r . our method obtains the best performance on every metric with a gap of 3 . 5 points from the last published results . it achieves the best results on all metrics with a 17 . 4 % improvement on average . it also outperforms the previous best performances by a large margin .
subj and sst5 perform comparably to other classifiers , such as cbow - r , sst2 , sts - b and sick - b . our model outperforms all the classifiers except mpqa except for sst - e , which obtains a better ranking . it also outperforms the sst3 and its variants . however , it has the advantage of training on a larger corpus . it obtains the best mrpc score of 39 . 9 % and 35 . 4 % respectively compared to the previous best state - of - the - art .
1 shows the e + and per scores of all systems trained on the same data ( including all org ) and the corresponding e + per scores . supervised learning models ( mil - nd and mil - nd ) perform better than all supervised learning models except for the one in [ italic ] e + org and misc , which shows the performance of the supervised learning model when trained on a single data structure . the results of the best performing model are reported in table 2 .
2 presents the results on the test set under two settings . our system improves upon the strong lemma baseline by 3 . 5 points in f1 score . the results are shown in table 2 . our model outperforms all the base models except mil - nd ( model 1 ) in terms of e + p score , while it improves e + f1 scores by 3 points .
table 6 , we can see that ref and ref significantly outperform ref when combined with ref , indicating that the model is more effective in generation of ref than ref alone .
1 and table 2 summarize our results on the ldc2015e86 and ldc2017t10 datasets , respectively . our model outperforms all the base lines except for the one that has the worst bleu score .
3 shows the results on ldc2015e86 test set when trained with additional gigaword data . our model obtains a significantly better performance on the external model than on the internal model .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . the results of this study show that the use of bilstm improves the model ' s performance on the development set .
results are presented in table 4 . the most striking thing about the g2s - gin model is that it is significantly longer than the average number of sentences in the model , indicating that the model is more suitable for the task at hand . sentence length and average sentence length have a generally positive effect on sentence length and sentence length , but on average it has a negative effect . we observe that for the 50 - 240 example , the average length of the sentence is only 5 . 5 % , which shows the diminishing returns from using word embeddings .
shown in table 8 , the fraction of elements in the output that are not present in the input graph that are missing in the generated sentence ( g2s - gin ) , for the test set of ldc2017t10 . note that the use of the token lemmas in the model results in a larger performance gap than those of the reference sentences .
4 shows the performance of our method with respect to target languages . our model outperforms both the traditional embeddings and pos tags with a large corpus ( 200k sentences ) .
2 : pos and sem tagging accuracy with baselines and an upper bound . word2tag is the most frequent classifier using unsupervised word embeddings . it improves upon the performance of word2link using a lower bound encoder - decoder .
results are presented in table 4 . our proposed system outperforms all the base methods except for the one that requires significantly more data . our system obtains the best performance on three out of the four scenarios . our model significantly outperforms both the traditional methods in terms of accuracy and test set .
5 shows the accuracy with different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our uni model improves accuracy with features from bi to res , and res to bi .
performance on two different datasets is shown in table 8 . the difference between the attacker score and the corresponding adversary ’ s accuracy is reported in δ . in pan16 , the average age of the tokens is 9 . 7 % and the average gender is 8 . 7 % . on the other training set , there is a significant drop in performance due to training set size .
1 shows the performance of our method when training directly towards a single task . accuracy is significantly higher than that of pan16 , indicating that the training target has a high quality of training .
2 shows the status of the protected attribute leakage in the context of balanced and unbalanced data splits . the classifiers trained on the pan16 dataset are named after gender - neutral tweets , and the classifier trained on this data is named after the genderneutral tweets . sentiment and language tags have a generally positive effect on the task performance , however it is harder to detect instances of imbalance in the multi - task dataset than those in the single task dataset .
performance on different datasets with an adversarial training set is shown in table 3 . sentiment is the difference between the attacker score and the corresponding adversary ’ s accuracy . when trained with a trained adversarial framework , we see significant performance drop ( p < 0 . 001 ) due to low training performance . sentiment also has a significant impact ( p > 0 . 01 ) on performance ( p ≈ 0 . 05 ) .
6 presents the performance of the embeddings for different encoders . embedding leaky is easier for rnn to perform than it is for embedded . the performance gap between rnn and leaky is narrower , but still significant .
results reported in table 1 show that our approach outperforms the previous stateof - the - art models on both base and finetune tasks . however , it does not improve significantly over the dynamic model , the results of our model seem to indicate that it is better suited to the task at hand . we observe that the combination of psi and tf2 significantly improves the model ' s performance over the strong lemma baseline on the wt2 dataset , as a result , we have decided to continue using our model as our training set ,
performance of our model compared to previous models is presented in table 5 . the results of experiment 1 show that our model significantly outperforms previous models in terms of both acc andbert time . as expected , our model has a significantly better performance on both datasets when training with the same number of iterations . further improving performance by using the lstm model
3 shows the performance of our model compared to previous models . our model improves upon the best state - of - the - art model on three of the four datasets . on the other four datasets , it gets a 4 . 42 % improvement on average compared to the previous model . this model also improves on the yelppolar time err and the amafull time model by 2 . 36 % on average .
shown in table 3 , the tokenized bleu score on wmt14 english - german translation task measured from 0 . 2k training steps on tesla p100 . it also measures time in seconds to decode one sentence measured on newstest2014 dataset , compared to the previous best stateof - the - art model , sru . finally , for german translation task , our model obtains a comparable score to the sru of 205m .
4 shows the performance of our model with respect to match / f1 score on squad dataset . the model obtains the best performance with a parameter number of 2 . 67m and achieves the best f1 score with a score of 82 . 83 / 83 . 86 . 03 . however , our model significantly outperforms other models with higher parameter numbers , such as lstm and sru .
6 shows the f1 score of our model on conll - 2003 english ner task . lstm * denotes the model with the highest parameter number . it can be observed that our model significantly outperforms the other models in terms of number of parameter number in the low - supervision settings .
7 shows the performance of our model with base + ln setting and test perplexity on snli task with base setting . it can be seen that our model performs better than the other models on both snli and ptb task .
results are shown in table 4 . word models trained on the oracle retrieval data are presented in table 5 . the word models ( mtr ) are primarily used for system evaluation . table 5 shows the results for both human and system using the word " retrieval " . while the word model is more suitable for system evaluations , it should not be dismissed as it does not perform well for human use . using word models like word2vec and word3vec ( which relies on word embeddings ) , both the human and the system - based approach are better for the task . when using word - based learning models , the word models are more effective for both systems . in general terms , word models with different learning rates perform better than the other two . as shown in the table , the human model is better at system evaluation , with a higher rate of error reduction .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among all systems is reported in table 4 . our system is ranked in the top 1 or 2 for each of the four aspects .
3 shows the performance of all the models trained on the corpus dataset . our joint model outperforms all the base models except for the one that we use , namely , europarl . the results are slightly worse than those on df , docsub and tf , but still superior to those on eur - trans . we observe that our joint model significantly outperforms the competition on all three datasets , apart from the one on tf .
3 shows the performance of all the models trained on the corpus dataset . our joint model outperforms all the base models except for the one that we use , namely , europarl . the results are slightly worse than those on df , docsub and tf , but still superior to those on eur - trans . we observe that our joint model significantly outperforms the competition on all three datasets , apart from the one on tf .
3 shows the performance of all the models trained on the corpus dataset . our joint model outperforms all the base models except for the one that we use , namely , europarl . the results are slightly worse than those on df , docsub and tf , but still superior to those on eur - trans . we observe that our joint model significantly outperforms the competition on all three datasets , apart from the one on tf .
embeddings for our dataset are shown in table 4 . our model obtains the best performance on every metric with a gap of 1 . 5 points from our metric metric to 1 . 3 points on the df metric . europarl also improves on this metric by 1 point . 7 points over our baseline .
metrics are presented in table 4 . our proposed system improves upon the best stateof - the - art on three of the four metrics . our joint model outperforms all the base models except for the one that embeds the maxdepth in our system , namely , docsub . europarl also improves on the depthcohesion metric by 2 . 03 points , though it does not improve on the hclust metric .
performance of our enhanced model on the validation set of visdial v1 . 0 is shown in table 1 . we observe that lf significantly outperforms s and d in terms of r0 , r2 , r3 and gsm learning .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the validation set of visdial v1 . 0 . the best performing model is lrv , which can be further improved with the addition of the history shortcut .
5 compares the performance of these models on hard and soft alignments . the results are summarized in table 5 . the best performing model is wmd - f1 + bert . it obtains a better performance on hard alignments than fi - en and ruse .
3 provides exact scores on the direct assessment and eureor metrics for each metric . our proposed model improves the performance by 3 . 8 points over the baseline on both sets .
performance of our bleu - 2 model on the sfhotel test set is presented in table 4 . the results are summarized in bold . our proposed bertscore - f1 model outperforms the baseline on every metric by a significant margin . however , the results are still significantly worse than those by smd + w2v .
performance of the models according to the leic score - recall metric is presented in table 4 . the results are presented in bold . leic scores are significantly worse than those of word - mover and wmd - 1 , indicating that the reliance on elmo and primer have a significant impact on performance .
results are shown in table 4 . we observe that the m0 model outperforms the m1 model on all metrics except for the shen - 1 metric , which means that it has better performance on pp compared to m1 .
results of experiment 1 are presented in table 2 . we observe that the transfer quality and transfer quality scores are the most important factors in the semantic preservation and semantic preservation scores . semantic preservation scores are significantly better than those of the other two baselines , indicating that the model performs well in the deep semantic preservation settings . complications are sometimes associated with semantic preservation , however , this is mostly due to the high accuracy of the feature sets in the multi - domain setup ( e . g . , m0 , m7 ) and m7 ( table 2 ) . complicating matters for both datasets is the fact that the clustering quality of the data tends to be less important for semantic preservation than that of semantic preservation . relis and transformer feature sets are less important than those in the singledomain setup
5 presents the results of human validation using the [ italic ] ρ b / w negative pp and human ratings of fluency . the results of this validation seem to indicate that it is possible to improve sentence quality with a reasonable selection of the correct sentence quality , and that the generated sentences have good interpretability .
results are presented in table 4 . the results are summarized in table 5 . we observe that the m0 + m0 model performs better than the m1 model on all metrics except for the shen - 1 metric , which shows that the model is more effective when trained with more than pure word embeddings .
results on yelp sentiment transfer are shown in table 6 . our best model achieves higher bleu than any other model using simple - transfer or n - word embeddings . however , this improvement is less striking than those obtained using the more sophisticated multi - decoder model . the results on simple - sentiment transfer are not included as they are worse than the best model using the same classifier in previous work . we also observe that the use of classifiers in the transfer settings hurts the model ' s performance , as it is more likely to misclassify the inputs as errors .
statistics for nested disfluencies are shown in table 2 . the percentage of tokens that were correctly predicted to be disfluent was 8 % , which shows that the accuracy obtained by rephrase is relatively high .
3 shows the percentage of tokens predicted to contain a content word in both the reparandum and the repair ( table 3 ) . the fraction of tokens in each category is in parentheses , indicating that the function contains a significant amount of content word . the fraction in parentheses show that the fraction containing the word contains a large number of tokens .
results of experiment 1 are presented in table 2 . we observe that the best performing model is text + innovations , improving the model ' s performance by 0 . 2 points over the single model .
performance of our model on the fnc - 1 test dataset is shown in table 2 . our model achieves the state - of - art performance on the test dataset with a 3 . 43 % increase in accuracy compared to the previous best model , sentence embedding .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem . the best performing model is attentive neuraldater . on the nyt dataset , it obtains 62 . 2 % higher accuracy .
3 shows the performance of our method with and without attention . it achieves the best performance with a 61 . 8 % f1 score and a 65 . 6 % roc score .
results are shown in table 1 . the best performing models are jvmee , dmcnn , jrnn and trigger . all models perform better than the baseline on all stages except for the one that is pre - trained . we observe that for all models that do not perform well on the replication test set , the performance is significantly worse on the validation test set than on the baseline .
3 shows the test set for each event . all methods used for this analysis are presented in table 4 . the method used in this study has the best performance on both types of event . the method is described in terms of object identification and the number of tokens used for the event classification . table 4 shows the method ' s performance on each of the four scenarios . in particular , the method has the worst performance on the two types of events .
can be seen in table 4 the results of all models trained on the same single - domain learnerbase . all models except for fine - tuned - lm outperform the baseline on all metrics except for the dev perp and test wer . the results are presented in table 5 . the best results are obtained using the concatenated word embeddings of the extracted sentences . the only exception is the english - only - lm model , which performs slightly better than the spanish - only model .
results on the dev set and the test set are shown in table 4 . fine - tuned train dev with only subsets of the code - switched data in it is used as training data . this results show that using discriminative training , the train dev has a better chance of success .
5 shows the performance on the dev set and the test set , compared to the monolingual model of fine - tuned - disc . it is clear that fine - tuning has a significant impact on the performance , but it does not have a significant effect on the model performance .
results for the conll - 2003 dataset are shown in table 7 . precision ( p ) , recall ( f1 ) and f1 - score ( f ) are all improvements from using type - aggregated gaze features trained on the same three eye - tracking datasets . the improvement from the baseline to the current state of the art is statistically significant ( p < 0 . 001 ) .
5 shows precision ( p ) , recall ( f1 ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . type - aggregation features significantly improve recall ( p ≤ 0 . 01 ) , but do not improve f1 score ( p > 0 . 00 ) .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . the hpcd approach relies on syntactic embeddings obtained through autoextend rothe and schütze ( 2015 ) for wordnet 3 . 1 . however , it does not use glove - retro as a model , and it uses syntactic skipgram instead . the results on the original paper show that it is better to rely on the syntactic - sg approach than on the lstm - pp approach . we notice a drop in performance between the original ( 83 . 3 % ) and wordnet ( 87 . 8 % ) on this set , which suggests that future work may need to consider further adaptation .
2 shows the performance of our rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are presented in table 2 .
3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the ppa acc . ( normalized by the number of frames ) is close to zero , however it is still significant .
2 shows the performance of domain - tuned models compared to the en - de model on multiple datasets . subsatellite tuning improves the multi30k model by 3 . 8 points in bleu % scores . domain tuning also improves the image caption translation performance by 2 . 4 points .
results are shown in table 4 . subdomain - tuned subs1m models outperform the en - de model on all metrics except for the performance on flickr16 . the results are presented in tables 4 and 5 , with the exception of mscoco17 .
4 shows bleu scores in terms of the model captions added using the best five models . our model outperforms all the models except for the one that had the best three models in the en - de dataset .
5 compares the performance of our approach with prior approaches on en - de and flickr16 . enc - gate and dec - gate achieve similar results ( bleu % scores ) on both datasets , as table 5 shows , using the multi30k + ms - coco + subs3mlm model improves the performance for both visual information and enc - gate . however , the improvement is slim , we notice that the enc - gating model has a lower bleu % score than the other approaches we tried .
1 shows the performance of subs3m with different visual features compared to subs6m on the en - de dataset . sub - categories as adjectives antonyms and performer action have the most significant effect on performance , but do not dominate the ensemble - of - 3 model . the best performances are obtained using the multi - lingual model ( which relies on word embeddings ) , while the best performing using the text - only model ( gn2048 ) achieves a better performance .
3 shows the performance of the best performing en - fr - ff models compared to the alternatives on the randcoref test set . the results are summarized in table 3 . as expected , the results are slightly worse than those obtained by the traditional enfr - ht model . table 3 shows that the performance obtained by using the best - performing model is significantly better than those by the alternative .
1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . as table 1 shows , for each language pair we used the greatest number of pairs , our average number of sentences is 1 . 48 .
2 shows the vocabularies for the english , french and spanish data used for our models . the results are summarized in table 2 . our model obtains the best performance on every metric with a gap of 3 . 5 points .
system reference bleu and ter scores for the rev systems are shown in table 5 . automatic evaluation scores ( bleu ) consistently show lower performance than ter and en - fr - rev , indicating that the model is more suitable for production use .
2 shows the performance of our supervised model compared to the standard rsaimage embeddings . our model improves upon the strong lemma baseline by 3 . 8 points in f1 score .
results on synthetically spoken coco ( vgs ) are shown in table 1 . the model trained on the embeddings of chrupala2017representations is significantly better than the similarly supervised audio2vec - u model . the difference is less pronounced for rsaimage , but still significant for vgs .
in table 1 we report further examples in the appendix . for example , cnn turns in a < u > screenplay that is slightly wider than the original on sst - 2 and has edges edges edges . this indicates that the need to hate the word " hate " as much as possible .
2 shows the effect of fine - tuning on the rnp score . note that the number of occurrences in sst - 2 has increased , decreased or stayed the same , indicating that there has been a considerable effort to refine the word selection process .
results in table 3 show that sentiment has been flipped from positive to negative . this shows that the effect of the flipped sentiment signal is having a positive effect on the sentiment score .
results of experiment 1 are presented in table 1 . it is clear from the table that the use of sift improves the interpretability for verbs . however , it does not improve interpretability . table 1 shows that our proposed method significantly improves interpretability without a drop in performance . our proposed method outperforms the competition on three out of four scenarios .
