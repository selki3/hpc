2 shows the performance of the treelstm model on training with the large movie review dataset . the recur approach performs the best on inference with the best performance on training , while the iterative approach performs better on training .
shown in table 1 , the balanced dataset exhibits the highest throughput compared to the linear dataset when the batch size increases to 25 .
2 shows the performance of the hyper parameter optimization strategies for each model with different representation size . the maximum pooling strategy consistently performs better in all model variations . the hyper parameter optimization results are shown in table 2 . the model with the highest number of hyper parameters is the one with the smallest number of representations .
1 shows the effect of using the shortest dependency path on each relation type . our model achieves the best f1 ( in 5 - fold ) with sdp and the best diff . we also observe that our model performs better than the macro - averaged model when using only sdp as dependency path . this shows that the model is able to improve the f1 by a significant margin .
results are shown in table 3 . the y - 3 model outperforms all the other models in terms of f1 and r - f1 .
results are shown in table 1 . the results of the test set are presented in tables 1 and 2 . our model outperforms all the other methods except mst - parser and mate . the results show that our model performs better on the test sets than those of other methods .
4 shows the c - f1 scores for the two indicated systems ; essay vs . paragraph level is shown in table 4 . for the lstm - parser system , the average performance is 60 . 62 ± 3 . 54 compared to the majority performances for the other two systems .
results are presented in table 1 . the original and the original models are shown in bold . the original model has a significantly better performance than the original model , and is more accurate at predicting errors than the wrong ones .
shown in table 1 , we compare the original and the cleaned e2e data with the original ones . we show that the original model has the highest number of distinct mrs , while the cleaned version has the lowest number of instances .
results are shown in table 1 . the original and the original embeddings are presented in bold . our system outperforms all the other methods except the original ones . we observe that our system performs better than the original on all the test sets except for the ones that are wrong .
4 shows the results of manual error analysis on a sample of 100 instances from the original test set . we found that removing incorrect values from the training set significantly reduces the number of errors . we also found that adding incorrect values significantly reduces disfluencies .
results are presented in table 1 . our model outperforms all the other models in terms of both external and external metrics . we observe that our model significantly improves upon the state - of - the - art dcgcn on both the external and the external metrics ( table 1 ) .
results on amr17 are presented in table 2 . the model size is shown in terms of bleu points . we observe that the model size of the ensemble models is significantly larger than those of the single - model models .
results are presented in table 1 . the english - german model outperforms all the other models except for the one in english - czech . the results are shown in table 2 . we observe that the english - language model performs better than the german model in terms of performance , with the exception of english - korean , where the performance is worse .
5 shows the effect of the number of layers inside dc on the performance of the model . the effect of layers in dc is shown in table 5 . for example , when we add layers of layers to dc , we see a drop of 2 . 5 layers compared to the previous state of the art .
6 : comparisons with baselines . gcn + rc ( 2 ) and gcn + la ( 4 ) show that gcns with residual connections have higher performance than those without . the results are shown in table 6 . the results show that the gcn has the best performance on the residual connections compared to the baseline .
results are shown in table 3 . we observe that the dcgcn model outperforms all the other models in terms of performance . in particular , it outperforms both the acgcn models by a significant margin .
8 shows the ablation study on the dev set of amr15 . - { i } dense blocks denotes removing the dense connections in the i - th block , while - { 4 } dense connections denote removing the ones in the dense blocks .
9 shows the ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the model used in the graph encoder outperforms all the other models except for the one that used in this study . in addition , the model with the most coverage mechanisms outperforms the other two models in terms of performance .
7 presents the results for initialization strategies on probing tasks . our paper shows that our method outperforms all the other initialization strategies except for the one we use in the first stage . the results are shown in table 7 .
results are presented in table 3 . the best performing models are the h - cbow and h - cmow / 400 models . these models outperform all the other models in terms of depth and subtraction .
results are presented in table 3 . we observe that the cbow / 784 model outperforms all the other models except for subj and mpqa . subj outperforms both sst2 and sst5 on the mrpc test set , while subj obtains the best performance on all the test sets .
3 shows the relative performance of our models on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms both hybrid and cmp in both instances . the difference in performance between the two is significant .
8 shows the performance of initialization strategies on supervised downstream tasks . our paper shows that our approach outperforms all the other approaches except for subj and sst2 , which are more difficult to predict .
6 shows the performance for different training objectives on the unsupervised downstream tasks . the cmow - r model outperforms all the other methods except for the cbow - c model . the results are shown in tables 6 and 7 .
results are presented in table 3 . the best performing models are cbow - c and cmow - r , both with respect to depth and subjnum . the worst performing ones are the ones with the highest accuracy . these models are significantly worse than those with the least accuracy .
results are presented in table 3 . we observe that the cbow - r model outperforms both the subj and sst2 models in terms of performance . the subj models outperform both the mrpc and mpqa models by a significant margin .
results are shown in table 3 . in [ italic ] e + loc and e + per , the system outperforms all the other models except for the one with the best org score . the system also outperforms the system in terms of e + org scores . we observe that the system performs better than the system with respect to name matching , and that the performance improvement is due to the fact that it can be trained on a larger corpus .
results on the test set under two settings are shown in table 2 . name matching improves the e + p score by 2 . 5 % compared to the previous state - of - the - art system . supervised learning improves the f1 score by 3 . 5 % . in [ italic ] e + f1 scores are shown by a 2 . 3 % improvement compared to previous state of the art systems .
6 : entailment ( ent ) and model ( ref ) results are shown in table 6 . the model outperforms all the other models except for s2s and g2s - ggnn , which both outperform all the others .
results are shown in table 3 . the models outperform the models in terms of performance . we observe that the model outperforms both the ldc2017t10 and ldc2015e86 by a significant margin . however , the model performs worse than the model in both cases .
3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . our model outperforms all the other models except for the ones trained with gigawords .
results of the ablation study on the ldc2017t10 development set are shown in table 4 . we observe that bilstm significantly improves the performance of the model compared to the original model .
results are shown in table 3 . the model outperforms the other models in terms of sentence length and sentence length . in particular , the model significantly improves sentence length compared to the previous model .
shown in table 8 , the fraction of elements in the output that are not present in the input ( added ) and fraction of those that are missing in the generated sentence ( miss ) are used in the comparison with the reference sentences .
4 shows the performance of the models trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our model outperforms all the other models using the 4th nmt encoding layer .
2 : pos and sem tagging accuracy with baselines and an upper bound . mft : most frequent tag ; unsupemb : most frequently tag ; and word2tag : the most frequently tagged word embeddings .
results are shown in table 3 . the results of the pos tagging accuracy test are presented in table 1 . we observe that the accuracy of our models is significantly better than those of the other models . our model outperforms all the other methods except for the one used in the original study .
5 shows the accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
8 shows the performance of the attacker on different datasets . the difference between the attacker score and the corresponding adversary ’ s accuracy is shown in table 8 .
1 : accuracies when training directly towards a single task . the results show that the training method significantly improves the performance of the task when trained directly towards it .
2 presents the results of the protected attribute leakage experiments in table 2 . the results are shown in bold . for the balanced task splits , we observe that the word " race " is the most frequently used word in the conversation , while " gender " is more frequently used in the task splits .
3 shows the performance on different datasets with an adversarial training . the performance on the task dataset is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is explained by the difference in the number of tokens assigned to the target .
6 : accuracies of the protected attribute with different encoders . embedding leaky is more difficult than embedding guarded and embedded , but the performance improvement is less pronounced when embeddings are used .
results are shown in table 3 . the model outperforms all the other models in terms of base and finetune performance . we observe that the lstm model performs better than all the models except for the one that performs best on the wt2 dataset . in addition , it performs better on the tf2 dataset than the other two models .
results are presented in table 5 . our model outperforms all the previous models in terms of time and accuracy . the results show that our model significantly improves the performance of the models when compared to previous models .
results are presented in table 3 . the model outperforms all the other models in terms of err performance . we observe that the lstm model significantly improves the performance on the yahoo time dataset compared to the previous state - of - the - art model .
3 shows the bleu score on the wmt14 english - german translation task . the model outperforms all the other models except sru and gru in terms of decoding one sentence per training batch . in addition , the model performs better on the german translation task than the sru .
4 : exact match / f1 score on squad dataset . our model outperforms all the models except for the ones with the higher parameter number . we observe that the model performs better than the model with the parameter number of 2 . 67m .
6 shows the f1 score of our model on conll - 2003 english ner task . the lstm model outperforms all the models except sru and sru in terms of parameter number . sru also outperforms the other models in ner tasks .
7 shows the performance of our model on snli task with the base + ln setting and test perplexity on ptb task with base setting .
results are shown in table 1 . word embeddings are used in all systems except system retrieval . sentiment is used in both human and machine learning settings . in the machine learning setting , word embedding is used to improve the performance of the system . as expected , the system - based approach outperforms the human approach by a significant margin . for the human model , we use word embedding in the system as well as in machine learning tasks .
4 presents the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 . the best performance among automatic systems is shown in bold , with statistical significance marked with ∗ and 1 . 2 .
results are shown in table 3 . the results of the experiments are presented in tables 1 and 2 . the results are summarized in table 1 . our model outperforms all the other models in terms of performance . we observe that the best performing model is europarl , which outperforms both the df and tf models .
results are shown in table 1 . the results of the experiments are presented in tables 1 and 2 . we observe that the performance of our models is comparable to those of the other two models , namely , the df and docsub models .
results are shown in table 3 . the results of the experiments are presented in tables 1 and 2 . the results are summarized in table 1 . our model outperforms all the other models in terms of performance . we observe that the best performing model is europarl , while the worst performing is docsub .
results are shown in table 3 . the results are presented in table 1 . our model achieves the best performance on both metric and depthcohesion metrics . our model outperforms all the other models except for the one we use in corpus and europarl . in corpus , our model achieves a better performance than those of the other two models .
results are shown in table 1 . the results are presented in the table below . our model achieves the best performance on both metric and depthcohesion metrics . our model outperforms all the other models except for the one we use in corpus and europarl . in corpus , our model achieves a better performance than the other two models .
shown in table 1 , we compare the performance of the models on the validation set of visdial v1 . 0 with respect to the original visdial model . the results are shown in the table 1 . the enhanced model outperforms the original model by a significant margin .
2 shows the performance ( ndcg % ) of different ablative studies on different models on the visdial v1 . 0 validation set . our model outperforms all the other models in terms of accuracy .
5 shows the performance on hard and soft alignments compared to hard alignments . the results are shown in table 5 .
results are presented in table 1 . metrics and baselines are shown in bold . for example , our model outperforms all the other models in the direct assessment test set except for those that do not have the ability to compare the performance of our model with those of other models .
results are presented in table 1 . the baselines for bagel and sfhotel are shown in bold . we observe that the baselines are significantly better than those for the other baselines . for example , the bleu - 1 model outperforms all the baseline models in terms of accuracy .
3 presents the metric and baselines scores for the two models . the metric scores are shown in table 3 . the baselines are derived from the leic score and the bertscore - recall score . the summaries are presented in bold , with the exception of the summaries for the m1 and m2 models , which are derived using bert score - recall scores . these results show that the baselines for both models are more accurate than those for the other models .
results are shown in table 3 . the results show that the m0 model outperforms the m1 model by a significant margin . as expected , the performance of the m2 model is significantly worse than the m3 model . in particular , the difference in performance between m0 and m6 model is significant .
results are presented in table 3 . we present the results of our model on the transfer quality and transfer quality datasets . the model outperforms all the other models on both datasets except for the one with the highest transfer quality . for the semantic preservation dataset , we observe that both the syntactic and semantic preservation datasets are significantly better than those of the other datasets .
5 presents the results of human sentence - level validation . the results are shown in table 5 . the results of the human sentence level validation are presented in terms of acc and pp scores , respectively . we observe that the human ratings of semantic preservation are higher than those of the machine and human judgments that match .
results are presented in table 3 . the results show that the m0 + para + lang model outperforms all the other models in terms of performance . in particular , it outperforms both the m1 + and m2 + models by a significant margin . moreover , the performance of both models is comparable to those of the previous models .
results on yelp sentiment transfer are shown in table 6 . the best models achieve higher bleu than the best ones in previous work . in addition , the model outperforms all the other models in terms of acc ∗ . multi - decoder outperforms both the best and worst models in the classifier category .
2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . for the nested disfluencies , we show that the repetition tokens are more likely to be misfluent than the disfluency tokens . the number of repetition tokens is shown in table 2 .
3 : relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and the repair ( content - content ) . table 3 shows the fraction of tokens predicted to contain a word in each category . the fraction predicted for each category is shown in table 3 .
results are shown in table 3 . the model outperforms the best - performing models in both the dev and the test set . in particular , the models outperform the models in terms of both dev and test set , in addition , the model performs better in the early and late stages of development .
2 shows the performance of the word2vec embeddings on the fnc - 1 test dataset . our model significantly outperforms the state - of - art models in terms of the number of topics discussed and the percentage of topics that are unrelated .
2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous methods .
3 : accuracy ( % ) of the component models with and without attention . the results show the effectiveness of word attention and graph attention for this task . for example , the oe - gcn of neuraldater shows a 61 . 8 % improvement over the previous state - of - the - art model with attention .
3 shows the performance of our model on each stage of the test set . our model outperforms all the other models except for the one that performs the best on the first stage . the results are shown in table 3 .
3 presents the results of our method on the trigger and classification task . our method outperforms all the other methods in both domains . we observe that the method has a significant effect on the identification and classification task . the method obtains the best results for both domains , with the exception of the trigger task , where the identification task is more difficult to classify .
results are presented in table 3 . all models are shown in the table . all models have the best performance on dev perp and test acc . the best performance is achieved on dev acc and dev wer ↓ the worst performance is obtained on test acc and test wer , respectively .
4 shows the results on the dev set and the test set using discriminative training with only subsets of the code - switched data .
5 shows the performance on the dev set compared to the monolingual set . the best performance is achieved on the test set , when the gold sentence is code - switched . the worst performance is obtained on the multi - sentence set .
shown in table 7 , type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in bold . the precision ( p ) , recall ( f1 ) and f1 - score ( f2 ) are shown for all three datasets .
5 shows the precision ( p ) , recall ( f1 ) and f1 scores for using type - aggregated gaze features on the conll - 2003 dataset . the precision ( p ≤ 0 . 001 ) and recall scores are shown in tables 5 and 6 .
results on belinkov2014exploring ’ s ppa test set are shown in table 1 . syntactic - sg embeddings are used in wordnet , verbnet , and glove , and are used for wordnet embedding . the syntactic embedding of wordnet vectors is the most important part of the hpcd setup , and it uses syntactic skipgram embedding as the base embedding layer . it is important to note that syntactic sg embedding gives the best performance on wordnet and verbnet .
2 presents the results of the rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 .
3 : effect of removing sense priors and context sensitivity ( attention ) from the model .
2 shows the results of domain tuning for image caption translation ( bleu % scores ) and domain tuning ( table 2 ) . the results are shown in table 2 . subdomain tuning improves the performance of the multi30k model by 3 . 6 % compared to the previous state - of - the - art model .
3 shows the performance of subs1m on en - de and in - de settings . the results are shown in table 3 . the subs 1m outperforms all the other models except for the one with domain - tuned features . in - de models outperform all the others in terms of performance . subdomain tuning improves performance by 2 . 5 % compared to subs1ms on en de settings . on the other hand , subs1m outperforms subs1
4 shows bleu scores in terms of automatic image captions . the results with marian amun are shown in table 4 . the best ones with the best captions are those with only the best one or all 5 .
5 compares the performance of two strategies for integrating visual information : enc - gate and dec - gate . the results are shown in table 5 . we observe that the enc - gated approach outperforms all the other approaches except for the one that encodes the information .
3 shows the performance of subs3m on the en - de dataset compared to subs6m . the results are shown in table 3 . sub - lingual models outperform all models in terms of performance , with the exception of the one with text - only features . in addition , the performance is comparable to the subs2m model , with a slight improvement in the performance on the multi - language dataset .
results are shown in table 3 . we observe that the en - fr - ht model outperforms all the other models in terms of translation performance . in particular , we observe that en - rnn - ff has the best performance on the mtld test set compared to the enfr - trans - ff model .
shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of pairs in each language pair is shown in bold .
2 : training vocabularies for the english , french and spanish data used for our models . table 2 shows the performance of our model on the english and french datasets .
5 shows the bleu and ter scores for the rev systems . automatic evaluation scores are shown in table 5 . the automatic evaluation scores ( bleu ) show that the system is more suitable for the task at hand .
results on flickr8k are presented in table 2 . the vgs model is the visually supervised model from chrupala2017representations . it is shown in bold in the table 2 .
1 : results on synthetically spoken coco are presented in table 1 . we observe that the vgs model outperforms all the other models in terms of recall and chance .
1 shows the different classifiers used in the original on sst - 2 . we report further examples in table 1 . for example , cnn turns in a < u > screenplay that u > at the edges ; it ’ s so clever you want to hate it . dan < c > shows the difference in the edges of the screenplay compared to the original . for rnn , we show the difference between the original and the original in terms of the edges .
2 presents the results of fine - tuning on sst - 2 . the results are shown in table 2 . the average number of words in each sentence is 69 . 0 % compared to 69 . 5 % in the original sentence . the difference in the number of occurrences is due to the amount of words that have been added to the sentence . we also observe that fine - tune has not significantly impacted the quality of the sentences .
3 : sentiment score changes in sst - 2 . the results are shown in table 3 . the negative labels are flipped to positive and vice versa , and indicate that the sentiment increases in positive and negative sentiment .
results are presented in table 1 . the results of the study are summarized in tables 1 and 2 . in general , the results are positive and negative , respectively , while the performance of sst - 2 is positive . as expected , the impact of sift on performance is less pronounced than those of the other two studies .
