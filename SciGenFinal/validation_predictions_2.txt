table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ‘ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the training dataset , the recursive approach outperforms both the iterative and the folding approach on both training and inference .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization .
table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 achieves the best performance on all models . the best performance is achieved with the best representation . we also observe that the best performing representation is the one with the highest number of feature maps .
table 1 : effect of using the shortest dependency path on each relation type . the best f1 ( in 5 - fold ) is shown in table 1 . we find that using the best dependency path improves the performance of the model on all relation types except part_whole .
table 3 shows the performance of the f1 and r - f1 scores . the results are shown in table 3 . we observe that the performance is significantly better than the previous state - of - the - art model . the f1 score is significantly higher than that of the previous model . this is due to the fact that the number of f1 scores is much higher than the average of the other two models .
table 1 shows the performance of the mst - parser on all three of the three categories . the results are shown in table 1 . on all three categories , the performance is comparable to that of the previous state - of - the - art mst parser . in particular , on all of the categories except for the last one , the accuracy is comparable .
table 4 : c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . stagblcc outperforms lstm - parser on both the essay and paragraph level in both cases .
table 2 shows the results of the training and test set . the results are shown in table 2 . we observe that the training set outperforms the test set by a significant margin . our model outperforms all the previous models except for the tgen + model .
table 1 shows the results of the comparison between the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 .
table 2 shows the performance of the sc - lstm model on the test set . the results are shown in table 2 . we observe that the model outperforms both the original and the original tgen + model by a significant margin . on the train set , the model performs better than the original model on all test sets .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the results are shown in table 4 . the total number of errors in the training set is 0 .
table 2 shows the performance of the model on all datasets . the results are shown in table 2 . we observe that the dcgcn ( ensemble ) model outperforms all the other models except for seq2seqk by a significant margin .
table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . seq2seq ( beck et al . , 2018 ) achieves 24 . 5 bleu points . gcnseq ( damonte and cohen , 2019 ) achieves 28 . 5 points . table 2 also shows the performance of the model on amr17 . we observe that the best performance is achieved by using the best of both the single and the ensemble models .
table 1 shows the performance of the models on the seq2seqb dataset . the results are shown in table 1 . we observe that the best performance is achieved with the bow + gcn model , which outperforms all the other models except the cnn model . the best performing model is the cnn + bcgn model .
table 5 shows the effect of the number of layers inside the dc on the performance of the model . the effect of layer number on performance is shown in table 5 . we observe that layer number is the most important factor in the overall performance of our model . we observe a significant improvement in the performance over the previous model with the addition of layers .
table 6 : comparisons with baselines . + rc denotes gcns with residual connections . the results are shown in table 6 . we observe that the performance of dcgcn1 is comparable to that of the baselines on all baselines except for the one that has the highest number of connections .
table 3 shows the performance of the dcgcn model . the results are shown in table 3 . we observe that the best performing model is the one with the best performance . the best - performing model has the best overall performance . this is due to the fact that it has the highest performance of all the models with the least number of false positives .
table 8 : ablation study for density of connections on the dev set of amr15 . the results of the ablation study are shown in table 8 . we observe that removing the dense connections in the i - th block significantly improves the performance of the model . in particular , we observe a significant improvement in the performance for the dcgcn4 model over the previous model .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the decoder and encoder are significantly better than the global node and global node combination .
table 7 : scores for initialization strategies on probing tasks . our paper outperforms all the other models except somo , which outperforms both somo and somo on all the probing tasks except for somo . we observe that somo outperforms somo in all but one of the tasks , with the exception of somo ( table 7 ) .
table 2 shows the performance of the method on the h - cmow dataset . the results are shown in table 2 . we observe that the best performance is achieved by using the best of both the best and the worst of the best . the best performing method is the best on the cmow dataset , which achieves the best result on the wc dataset .
table 2 shows the performance of the hybrid method on the cmow / 784 dataset . the hybrid method outperforms all the other methods on the cbow dataset by a margin of 0 . 2 % and 0 . 6 % respectively . on the other hand , the hybrid model outperforms the other models by 0 . 4 % and 1 . 6 % .
table 3 : scores on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change with respect to hybrid . hybrid outperforms the cmow model on all the tasks except sts13 and sts15 . the difference between hybrid and cmow on sts12 is significant .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot outperforms all the other initialization strategies except for sick - r , which outperforms our paper by a significant margin . we observe that the performance of our model is comparable to that of the other models .
table 6 : scores for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . on the supervised downstream tasks , our method outperforms the cmow - r by a significant margin . we observe that the performance of our method is comparable to that of cmowr .
table 2 shows the performance of the method with respect to the cmow - r . the results are shown in table 2 . we observe that the best performance is achieved by using the best of both the best and the worst of the best . the best performing method is somo , which outperforms the best cmow .
table 3 shows the performance of the cmow - r method on the sick - e model . the cmowr model outperforms all the other models except the mpqa model by a significant margin . it outperforms the other methods by a margin of 2 . 6 points .
table 2 shows the performance of the supervised learning model . the results are shown in table 2 . we observe that the model outperforms all the other models in all cases . in particular , we observe that it outperforms the previous model in all but one case . the performance of supervised learning is comparable to previous models .
table 2 shows the results on the test set under two settings . the results are shown in table 2 . the model with the best f1 score is mil - nd ( model 1 ) , which achieves a score of 69 . 38 ± 1 . 59 . on the other hand , the model that achieves the highest f1 scores is mil ( model 2 ) which achieves 69 . 42 ± 0 . 59 and 69 . 87 ± 0 . 59 , respectively . as expected , the best performance is achieved by using the supervised learning model . in particular , the performance of the model with supervised learning is significantly better than the model without .
table 6 shows that g2s - gat outperforms all the other models in terms of the number of instances in which the model is included in the dataset . the model with the best performance is the one with the highest number of features in the gat dataset , which has the best overall performance . the gat model outperforms the other two models by a significant margin .
table 3 shows the performance of the model on the ldc2015e86 and ldc2017t10 datasets . the g2s - gat model outperforms the s2s model by a significant margin . the results are shown in table 3 . we observe that the model performs better than the model with respect to gat .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model outperforms all the other models on the test set , except for the external bleu model , which outperforms both the external and internal models .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we observe that the bilstm model outperforms the geb model on the development set by a large margin . in particular , the model with the best performance is the one with the largest size .
table 2 shows the performance of the model on the g2s - gat dataset . the model outperforms all the other models on the s2s dataset by 0 . 51 % and 0 . 43 % respectively . on the gat dataset , the model performs better than all the others . the model achieves the best performance on all the datasets .
table 8 : fraction of elements in the output that are not present in the input ( added ) and the fraction of elements that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn outperform both of these models . the model with the best performance is the one with the highest fraction of missing elements .
table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . as expected , the accuracy of pos tagging is significantly better than sem tagging .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag achieves 95 . 06 % and 95 . 11 % pos accuracy , respectively . unsupemb achieves 91 . 95 % pos and 91 . 41 % sem accuracy . on the other hand , the unsupervised embeddings improve the accuracy by 0 . 95 % .
table 4 shows the performance of our model on the pos tagging accuracy and the sem tagging accuracy . the results are shown in table 4 . our model outperforms all the other models on both the pos and sem accuracy . we observe that our model achieves the best performance on both of these metrics .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe a significant improvement in the pos tagging accuracy over the previous state of the art .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the performance on the training set is shown in table 8 . on the adversarial dataset , the performance of the attacker is comparable to that of the corresponding adversary .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . as expected , pan16 outperforms pan16 on all tasks except the one .
table 2 : protected attribute leakage : balanced & unbalanced data splits . the results are shown in table 2 . we observe that the unbalanced and balanced data splits are less likely to leak the protected attribute .
table 3 shows the performance on different datasets with an adversarial training . the results are shown in table 3 . on pan16 , the performance is comparable to that of pan16 on pan16 with the same training set .
table 6 : accuracies of the protected attribute with different encoders . the results are shown in table 6 . with the guarded encoder , the rnn performs better than with the leaky encoder . the rnn outperforms both guarded and leaky with respect to accuracy .
table 2 shows the performance of the model with the best performance on all the params . the best performance is achieved on all params with the exception of # params , where the performance is significantly lower than the previous state - of - the - art model . on the other hand , on the last params , the model achieves the highest performance .
table 2 shows the performance of the model on the 250k test set . the results are shown in table 2 . we observe that the model outperforms all the other models on the test set , except for the lstm model , which outperforms the gru model by a significant margin .
table 2 shows the results of the model and the work on yelppolar time . the results are shown in table 2 . we observe that the model outperforms the work by a significant margin . as shown in zhang et al . ( 2015 ) , the performance of the amapolar err is comparable to that of the work . however , we observe that it is significantly worse than the work , as shown in the table .
table 3 shows the bleu score on the wmt14 english - german translation task . the model outperforms all the other models on the task , except for the case - insensitive tokenized tokenized model , which outperforms the other two models by a significant margin . in particular , it outperforms gnmt by a large margin .
table 4 shows the exact match / f1 - score on the squad dataset . the results are shown in table 4 . the model outperforms all the other models in terms of accuracy and f1 score . as expected , the best model is the lstm model , which outperforms the other two models by a significant margin . however , the performance of the gru model is slightly worse than the lrn model .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru denote the reported results . the model outperforms all the other models on the task , except for the gru model , which outperforms lrn .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . our model outperforms all the other models on both tasks .
table 1 shows the performance of the system on the human and the oracle retrieval test set . the results are shown in table 1 . on the human test set , the system outperforms the system by a significant margin . on the oracle test set by a large margin , it outperforms both the system and the mtr .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . table 4 shows the performance of all the automatic systems on the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the results are shown in table 4 .
table 3 shows the performance of the model on the ted talks dataset . the results are shown in table 3 . we observe that the model outperforms all the other models on the europarl dataset by a significant margin . in particular , the model with the best performance is the one with the highest p < 0 . 05 .
table 3 shows the performance of the model on the ted talks dataset . the results are shown in table 3 . we observe that the model outperforms all the other models on the europarl dataset by a significant margin . in particular , the model with the best performance is the one with the highest p < 0 . 01 .
table 3 shows the performance of the model on the ted talks dataset . the results are shown in table 3 . we observe that the model outperforms the previous model on all three datasets . on the europarl dataset , we observe that it outperforms all the previous models except for the first one . this is due to the fact that it has the best performance on the first dataset .
table 1 shows the performance of the europarl model . we observe that the best performance is on the numberrels dataset . the best performance on the totalterms dataset is achieved by using the hclust embeddings . on the totalroots dataset , we see that the average number of roots per row is 1 . 05 .
table 1 shows the performance of the europarl model . the performance of our model is shown in table 1 . our model outperforms all the other models in terms of numberrels and totalterms . in particular , our model performs better than all the others .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the results are shown in table 1 . as expected , the enhanced version of the model outperforms the baseline model by a significant margin . the enhanced version outperforms both the baseline and the baseline by a large margin . in addition , the performance of the enhanced model is comparable to the baseline .
table 2 : performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the performance of the model with p2 on the validation set is significantly better than that with p1 .
table 5 : comparison on hard and soft alignments . the results are shown in table 5 . hmd - f1 and wmd - unigram outperforms ruse on hard alignments by a significant margin . the performance of hmd - recall + bert is comparable to the performance of ruse .
table 2 shows the results of the bertscore - f1 setting on the sent - mover dataset . the results are shown in table 2 . we observe that bert score is significantly higher than the average on the meteor + + dataset . the average score is 0 . 716 points higher than on the meteor + dataset .
table 2 shows the performance of the bertscore - f1 model on the sent - mover dataset . the results are shown in table 2 . we observe that the best performance is achieved on the smd dataset . the best performance on the sfhotel dataset is achieved by using the bleu - 2 model . on the w2v dataset , the performance is comparable to that on the btlu - 1 model .
the results are shown in table 1 . we observe that the accuracy of the bertscore - recall setting is significantly higher than that of the baselines . the performance of the word - mover setting is also higher than the baseline setting .
table 1 shows the performance of our model on the table . we observe that our model outperforms all the other models in all three domains . our model achieves the best performance on all the domains except for the domain - specific domain .
table 2 shows the performance of the models on the yelp dataset . the results are shown in table 2 . we observe that the model with the best transfer quality is the one with the highest transfer quality . the transfer quality of the yelp model outperforms all the other models with respect to both transfer quality and transfer quality score .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . acc is the only metric for which the human model outperforms the machine model by a significant margin .
table 2 shows the performance of our model on the table 1 . the results are shown in table 2 . we observe that our model outperforms all the other models on the test set by a significant margin . our model achieves the best performance on all the test sets except for the first set .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleus than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . the best model achieves the highest acc at the same level of accuracy as the best classifier . our best model also achieves the best accuracy at the highest level of acc as well .
table 2 : percent of reparandum tokens that were correctly predicted as disfluent . note that nested disfluencies exclude repetition tokens . the average number of disfluency tokens is 0 . 66 compared to 0 . 79 for the overall dataset . we also note that the number of repetition tokens that are correctly predicted to be disfluential is significantly lower than the overall number . this is due to the fact that the reparanda length is much longer than the average .
table 3 : relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . percentages in parentheses show the fraction of tokens belong to each category . the best performing category is content - content , which has the highest percentage of tokens . the worst performing category , however , is function - function , which contains only one content word .
table 2 shows the performance of the model on the test set . the results are shown in table 2 . we observe that the model outperforms the best test set by a significant margin . the performance of our model is comparable to that of the previous model .
table 2 shows the performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms the state of the art on the test dataset by a significant margin . the accuracy of our embeddings is comparable to that of the best - performing rnn - based embedders .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . the best performing method is the ac - gcn model , which outperforms the previous models by a significant margin . on the nyt dataset , the best performing model is the neuraldater model by a large margin .
table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details . the accuracy of the oe - gcn model outperforms the sgcn by a significant margin .
table 2 shows the performance of our model on all three stages . the performance of the model on each stage is shown in table 2 . we observe that our model outperforms all the other models on all stages except the first stage . on the second stage , we observe that the model performs better than all the others .
table 1 presents the results of our test set . the results are shown in table 1 . we observe that our method outperforms all the other methods except for cross - event , which outperforms the other two by a significant margin .
table 2 shows the performance of all the models . the results are shown in table 2 . we observe that the best performing model is the one with the best performance on the english - only and spanish - only languages . this model outperforms all the other models except for the fine - tuned one .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . the fine - tuned model outperforms the cs - only model on both the dev and test set .
table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . fine - tuned - disc outperforms fine - tuned - disc on both test and dev sets .
table 7 shows the performance of the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the performance of type combined gaze features is shown in table 7 . we observe that type combined features improve the precision and recall of the model .
table 5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined model outperforms the baseline model by a significant margin . the performance of the type combined approach is shown in table 5 .
table 1 : results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - extended embeddings outperform lstm - pp by a significant margin . the results are shown in table 1 . syntactic - sg embedding outperforms the original model by a margin of 2 . 7 points . the difference between the two models is due to the fact that syntactic sg embedding is more efficient than the previous model .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd ( full ) and ontolstm - pp ( partial ) outperforms the full uas by a significant margin . on the other hand , the performance of the full system is comparable to that of the original uas ( full ) .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . our model outperforms the full model by a significant margin .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . as expected , the domain - tuned model outperforms the en - de model with a bleu % score of 66 . 7 % and 66 . 0 % respectively .
table 3 shows the performance of the domain - tuned subs1m and domain - tuned subs1ms . the performance of both subs and domains is shown in table 3 . we observe that the performance is comparable to that of the original subs . however , we observe a significant difference in performance between the subs and the domains . in particular , the subs model outperforms the domains model .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . all results with marian amun . as expected , the best automatic captions outperform all the other captions by a significant margin . for example , the multi30k captions have the best performance .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , etc . are shown in table 5 . the enc - gate and dec - gate strategies outperform the en - de and mscoco17 strategies . in particular , the enc - gates outperform both en - fr and mscoco17 in terms of the bleu % score . we observe that the encogates outperforms the decogates by a significant margin .
the results are shown in table 1 . we observe that the subs3m model outperforms the subs6m model by a significant margin . in particular , it outperforms all the other models except the ms - coco model , which outperforms both the subs5m model and the mscoco model by 2 . 5 points . furthermore , the performance of the subs7m model is comparable to that of all the others .
table 2 shows the performance of the two approaches . the results are shown in table 2 . we observe that en - fr - smt - back outperforms both en - es - ht and en - rnn - ht in terms of ttr and mtld . as expected , the performance gap between the two methods is small , but it is still significant . in particular , we observe a significant performance improvement over the previous model .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . for the train and test splits , we use the en – fr and en – es language pairs , respectively .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 .
table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . bleu scores are significantly higher than ter scores . ter scores are slightly lower than bbleu scores , but are comparable to the baseline scores .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one trained on the rsaimage dataset . the results are shown in table 2 . vgs outperforms rsaimage by a significant margin .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled audio2vec - u is the auditory supervised model . the results are shown in table 1 . vgs outperforms all the other models in terms of recall and recall rate .
we report further examples in the appendix . the classifiers are shown in table 1 . the classifier turns in a screenplay of the original on sst - 2 . dan and cnn turn in the screenplay of a screenplay with the edges at the edges . cnn turns on the edges of the screenplay .
table 2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the previous sentence . a score of 0 indicates that the finetuning has not changed the total number of words in the sentence , while a score of 1 indicates that it has increased or decreased the number . the number of instances of each word has also been increased .
table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa .
table 1 summarizes the results of the experiment . the results are shown in table 1 . the results of our experiment are summarized in table 2 . our model outperforms all the other models except pubmed , which outperforms corr .
