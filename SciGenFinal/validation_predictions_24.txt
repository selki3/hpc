table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in table 2 , both approaches give good gains in performance in terms of both inference and training .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . balanced datasets also have lower throughput , indicating that the benefits of high parallelization are less pronounced in a balanced dataset .
the max pooling strategy consistently performs better in all models with different representation configurations as shown in table 2 . the conll08 model achieves the best performance with a f1 score of 75 . 57 on the hyperparametric test set ( table 2 ) . the softplus representation is comparable to the softplus baseline , but does not generalize well compared to softplus ,
table 1 shows that using the shortest dependency path on each relation type bridges the gap between the best f1 and the worst f1 score . as expected , macro - averaged models perform better than the best - performing ones without sdp .
the results are shown in table 1 . in all but one case , our model ( y - 3 ) obtains better results than the best previous state - of - the - art on all metrics . for example , on the f1 and f1 50 % measures , the y - 3 model achieves 100 % and 50 % f1 success on average , respectively , with an absolute improvement of 2 . 59 points over the previous state of the art .
the results are shown in table 1 . for brevity we report only the results for paragraph level and sentence accuracy . the results reconfirm that ame model can easily distinguish between the true response and negative responses . it achieves over a 100 % accuracy on average in all cases , with an absolute boost of 50 % in the exceptional case of mate .
table 4 shows that our system outperforms the lstm - parser and the original embeddings on average . note that the mean performances are lower than the majority performances over the runs given in table 2 .
table 1 shows that the original tgen model can be further improved with the help of a layer of syntactic and semantic cleansers . the results are slightly worse than the results of tgen − but still superior to sc - lstm and ser . adding entity nodes improves the results for all metrics except for meteor .
table 1 shows that our cleaned version of the e2e dataset is comparable to the original one , in terms of number of distinct mrs , as measured by our slot matching script , see section 3 .
table 1 shows that the original tgen model outperforms the original and the best - performing variant , tgen + on all metrics . with respect to error reduction , the results are slightly worse than those of sc - lstm , but still superior to tgen − and ser on three of the four metrics .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as expected , the majority of errors in our tgen model ( 71 % ) are caused by errors caused by adding , missing , and wrong values ( 14 % ) , as shown in table 4 .
table 1 compares our model with previous stateof - the - art on both datasets . the results are shown in bold . our all - model ensemble achieves the best results with an all score of 25 . 2 . relative to all previous models , graphlstm ( song et al . , 2018 ) achieves the highest score with 27 . 4 % and pbmt achieves the lowest score with 22 . 2 % .
table 2 shows the performance of our model on amr17 . our ensemble model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art on par with seq2seq and ggnn2seq models .
table 2 shows that the best performances for english - german and czech are obtained using the bow + gcn model , followed by the seq2seq model . the results are shown in table 2 . as can be seen , the smaller performance gap between the single model and the multi - model approach is less pronounced in english , but still indicates that there is a significant performance gap to be overcome .
table 5 shows that the number of layers inside our dc model is the most important factor in the performance improvement . our model achieves state - of - the - art results with a f1 of 1 . 8 and muli - domain f1 scores of 2 . 2 and 6 . 1 on the test set .
table 6 shows that gcns with residual connections outperform gcns without residual connections . the results are slightly worse than those without , but still superior to the baselines in terms of generalization . adding rc information improves the generalization ability of gcns , improving the bias metric by 3 . 8 points in the standard task formulation and to parity in the gold - two - mention case . gcn with rc information on residual connections improves further relative performance to both gcn and dcgcn .
the results are shown in table 4 . our model outperforms the previous state - of - the - art on all metrics by a noticeable margin . relative to the best previous state of the art , dcgcn achieves 4 . 2 % higher d / c on average compared to the previous best state - ofthe - art .
table 8 shows the ablation study for density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block reduces the number of connections , but does not reduce the overall performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . we report results using the best performing model , dcgcn4 . the results show that after applying our domain - adaptive decoding scheme , the graph attention and domain - aware attention are effectively complemented by the global node and hierarchical combination features .
table 7 shows that initialization strategies generally outperform glorot and subjnum in all aspects ( differences are statistically significant with t - test , p < 0 . 01 ) except for length . our paper improves upon these results by 3 . 8 points in the depth metric and to parity in coordinv . it also improves its score in the tense metric by 3 points .
the results are shown in table 1 . our h - cmow model outperforms the previous state - of - the - art on all metrics by a noticeable margin . it improves upon the strong baselines by 3 . 8 points in the tn and contributes significantly to the growth of the integral part - ofspeech metric .
table 3 presents the results of models trained and tested on the stacked test sets of subj , mpqa , mrpc , trec and sick - r . our model ( cbow ) obtains the best results with a marginal gain of 0 . 6 % over the previous state of the art on all metrics .
table 3 shows that our model outperforms the cbow and cmow approaches on all three downstream tasks . hybrid also shows a relative improvement in performance over the baseline cbow model . with respect to sts12 and sts16 , the results are slightly worse than those of hybrid but still superior to cbow on all the other three tasks .
table 8 presents the performance of our system for initialization and supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except mpqa . it improves upon the glorot model by 3 . 8 points in the last evaluation .
table 6 shows that our approach outperforms the best previous approaches across the four downstream tasks . our cbow - r model improves upon the strong baselines by 3 . 2 points in the unsupervised tasks .
the results are shown in table 1 . our method outperforms the previous state - of - the - art on all metrics by a noticeable margin . our topconst - based model improves upon the strong baselines by 3 . 8 points in the depth metric and gives a performance gain of 2 . 6 points on coordinv . the difference is less pronounced for subjnum , but still represents a significant performance gain .
the results are shown in table 1 . our model outperforms all the base the results reconfirm that the cbow approach is superior to the state - of - the - art on all metrics when trained and tested on the simulated test sets .
the results are shown in table 1 . supervised learning outperforms all supervised and unsupervised learning methods in terms of all metrics on both loc and misc datasets . in particular , it improves the results in the name matching task and the hierarchical clustering task . it achieves state - of - the - art results in all three metrics , and in all subtasks except for the nested loc case . name matching is the most difficult part of the task , and only slightly outperforms the supervised learning method . regularization improves performance in all metrics , with the exception of e + loc .
uncertain in low - supervision settings . in table 2 we report the results of all models trained on the test set with different feature sets . name matching and named entity recognition perform the best , with a gap of 10 . 5 % and 3 . 7 % f1 scores , respectively , compared to the previous state of the art . supervised learning by itself achieves the best f1 score of 43 . 57 % . in model 2 , the improvement is much larger , reaching 83 . 59 % . finally , τmil - nd ( model 2 ) shows the best performance with a f1 of 73 . 26 . these results show that after applying our supervised learning scheme , the model can learn to solve more than simple name matches .
table 6 shows that the g2s models are comparable in terms of performance with the best - performing state - of - the - art models . the results reconfirm that the ability to extract compact regions from supporting documents is a major advantage in the low - supervision settings . the g2sgnn model considerably outperforms the s2s model in both ref and conf ( table 6 ) . as shown in the table , compact regions are important for future research as they improve the generalization ability of the model .
table 3 presents the results of experiments on the hidden test set of bleu and meteor in the distractor and fullwiki setting , respectively . the results are summarized in table 3 . our model improves upon the previous state - of - the - art on all metrics by 3 - 2 . 7 points on the ldc2015e86 and ldc2017t10 scores . by further adding entity nodes , the g2s model improves further , by 3 points on average , to reach a new state - ofthe - art level of 29 . 28 ± 0 . 03 . from this group of results , we can further compare our model with previous state of the art on three of the four datasets . on the other hand , our model performs slightly worse than the other two baselines on only one dataset , the lddc2017 dataset .
table 3 shows the performance of our model on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model improves over the previous state - of - the - art on all metrics by 3 . 23 points .
the results of the ablation study are shown in table 4 . our model significantly outperforms the previous state - of - the - art on both metrics in terms of performance on the ldc2017t10 development set and the meteor dataset .
the results are shown in table 2 . as can be seen , the smaller graph diameter and average sentence length seem to indicate , the g2s model can rely less on superficial cues . nevertheless , it has the advantage of training on a larger corpus , with an average of 25 . 42 frames per sentence .
table 8 shows that gold significantly outperforms the s2s model and the g2s - gat model in the production evaluation set of ldc2017t10 . the token lemmas are used in the comparison . as shown in table 8 , the fraction of elements in the output that are missing in the generated sentence is lower than that in the input , indicating that gold has better generalization ability .
table 4 shows the pos and sem tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is 88 . 6 % and 87 . 2 % , respectively , while sem is 85 . 3 % and 85 . 2 % .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . our embeddings outperform the best previous approaches using unsupervised word embedds . word2tag also achieves a significant improvement in pos tagging accuracy over the baseline .
table 1 shows the system ' s performance on the pos tagging and sem tagging metrics . our model outperforms all the state - of - the - art models on all metrics by a noticeable margin .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model achieves state - of - the - art results , outperforming all the baselines except english , with an absolute improvement of 1 . 9 points over the best previous state of the art .
table 8 shows the attacker ’ s performance on three different datasets . results are on a training set 10 % held - out . as shown in table 8 , the diversity of the protected attributes is high , indicating that the attacker is able to pick out the most interesting ones . gender and race features are the most difficult ones to detect , however , the age and gender features are relatively stable .
table 1 shows the performance of our system for training directly on a single task . as expected , all the cues for this task are gender - neutral , with the exception of age . gender - neutral cues result in significantly higher accuracy . overall , the system performs better than pan16 with a gap of 2 . 8 points from the last published results .
table 2 shows that the balanced and unbalanced task data splits result in significantly better results for pan16 . gender and race features have the least and age have the most significant differences in performance , respectively , compared to gender - parity . sentiment and name tags have the highest percentage of leaks , indicating that they are particularly difficult to protect .
the performance of our system on these datasets is shown in table 3 . as expected , the gender - parity task is the most difficult one to solve , followed by race and sentiment . the effectiveness of our feature - based adversarial training is proved by the significant drop in performance from baseline to current state - of - the - art .
the results in table 6 show that the guards on the leaky embeddings are comparable to those on the guarded ones , indicating that the rnn encoders can handle the increased challenge of redundancy .
table 1 shows that the training set size for our model is 22m , with a gap of 10 . 5m points from the last published results ( yang et al . , 2018 ) and surpassing all stateof - the - art models by 3 points . lrn also achieves competitive or better results than the lstm model by a margin of 3 . 7 points in the finetune test set .
table 1 shows that the training time and the average number of iterations for our model are the most important factors in the success of our model . the results are presented in table 1 . the first group shows that our lstm model can easily exceed the base requirement with a gap of only 0 . 5 acc points . the second set of results show that it is comparable to state - of - the - art gru model in terms of both acc time and time to solve tasks .
table 3 presents the results of baselines trained on tweets from one domain and tested on all tweets from other domains , in the unsupervised setting . we benchmark against the following baselines : amapolar err , yelppolar time and timeunit . the results are presented in table 3 . the first set of results in zhang et al . ( 2015 ) shows that the hierarchical clustering approach based on yelp data can significantly improve the results for both domains with good recall scores . the second set in the table shows that adapting the learning rate for the task at hand , while maintaining high recall , can help the model to improve results in the future .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . the model makes use of the best performing feature set available from the newstest2014 dataset , namely , multi - params decoding speedup . it achieves the best performance with a gain of 2 . 67 points over the previous state - of - the - art on average .
table 4 shows that our model obtains an exact match / f1 - score of 76 . 14 / 83 . 83 on the squad dataset . although the number of parameters in our base model is slightly more than the elmo baseline , we managed to improve upon it with a 2 . 67 / 71 . 41 f1 score .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model obtains the best performance with an f1 - score of 90 . 94 . although the number of parameters in our model is small , it considerably improves over the previous state - of - the - art in terms of performance on ner tasks .
table 7 shows that our model improves the performance on the snli task with the use of base + ln setting and the test perplexity on the ptb task with a base setting .
table 2 presents the results of system and word analogy experiments . we benchmark against the following systems : oracle retrieval ( b - 2 , b - 4 ) and mtr ( r - 2 ) . table 2 shows that both systems are comparable in terms of feature extraction . however , oracle has the advantage of training on a larger corpus . this result along with the intrinsic evaluations show that the system is more than able to learn the task to a high degree . word analogies generated by the system are particularly difficult to reproduce , since the number of words in question is relatively small . retrieving the word embeddings for each sentence is extremely difficult , with an f1 of 1 . 55 on average . our system achieves state - of - the - art results , outperforming both the systems and the human on every metric by a significant margin .
table 4 presents the results of human evaluation . our system outperforms all the automatic systems we base it on in grammatical accuracy ( grammaticality ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that our model can easily distinguish between the true response and negative responses .
the results are shown in table 1 . table 1 shows that for all three domains , our model outperforms the previous state - of - the - art on three out of the four datasets . the difference is most prevalent in the relation to ted talks , where our europarl model ( at 0 . 7 % ) is nearly 5 % better than the previous best performance . on the other hand , for lang , our hclust model performs relatively better , at 1 . 7 % .
the results are shown in table 1 . table 1 shows that for all three domains , our model outperforms the previous state - of - the - art on all metrics by a noticeable margin . the largest performance gap is seen on the ted talks dataset , where our europarl model ( at 0 . 7 % ) is nearly 5 % higher than the previous best state - ofthe - art . on the smaller scale , our hclust model performs slightly better than the other two baselines on all datasets but is still inferior to both parallel and ted talks .
the results are shown in table 1 . table 1 shows that for all three domains , our model outperforms the previous state - of - the - art on three out of the four datasets . the difference is most prevalent in the relation to ted talks , where our europarl model ( at 0 . 7 % ) is nearly 5 % better than the previous best state - ofthe - art . on the other hand , for svm datasets , our hclust model performs relatively better , at 0 . 8 % .
from table 1 , we report the performance of our system on all metrics . our number - based system outperforms all the base the best performing ones by a noticeable margin . europarl has the best overall performance with a gap of 11 . 05 points from the last published results . from left to right , we see : total terms , roots , docsub , df , docsub and slqs .
from table 1 , we can see that the most representative metrics are total terms , averagedepth and docsub . from left to right , we have europarl , docsub , slqs , tf , df and wikisub . more importantly , we also have totalterms , which measures the semantic relations in question , and averagedepth .
in table 1 we compare the performance of our proposed approach against the baseline visdial model on the validation set of visdial v1 . 0 . the enhanced version of our model ( lf ) outperforms the baseline model by a significant margin . it achieves 73 . 42 % ndcg % compared to the previous state - of - the - art on average .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . as shown in table 2 , applying p2 improves the performance for all the models except for the one using the history shortcut . the model that learned the most effective was the coatt model .
table 5 shows that hmd - f1 and wmd - bigram achieve relatively high accuracies on hard and soft alignments , respectively , compared to previous approaches .
the results are shown in table 1 . first , we compare against the baselines on the direct assessment and sent - mover datasets . our system achieves the best results with a f1 score of 0 . 85 . compared to the previous state - of - the - art on both metrics , we see that meteor + + and ruse significantly outperform all the alternatives except bertscore - f1 .
the results are shown in table 1 . the first group shows that bleu and meteor significantly outperform all the base the second group show that bertscore - f1 significantly improves over the baselines when using the sent - mover and w2v feature - values . from the second group of table 1 , we see that the transfer learning method significantly improves the results for all metrics with two tasks .
word - mover achieved the best results with a f1 - score of 0 . 949 on the metric m1 and m2 according to our second set of experiments . the difference is most prevalent in relation to leic , showing that word - mover alone can significantly improve the predictive performance of the model when using only one type of clustering feature , namely , the elmo feature .
in table 1 we report the results of models trained only on shen - 1 and para - trained models . the results are broken down in terms of performance on similarity test set and multi - domain learner . the results of m1 and m2 show that when using only one type of dependency layer , the model performs better than the model without . when using both types of dependency trees , the results are slightly worse but still superior to the results obtained using the other two .
table 3 shows the results for english , spanish , french , dutch , dutch and turkish . our model improves the results in all aspects . the transfer quality and the semantic preservation scores are the most consistent ones , with the exception of transfer quality . fluency is relatively stable , improving only by 0 . 3pp over the best baseline . we observe that yelp significantly improves its transfer quality over the baselines . semantic preservation and fluency are the only areas where the improvement is less pronounced .
table 5 shows the results of human evaluation for validation of these metrics . our system verifies 94 % of the summaries against the spearman ’ s ρ b / w sim and human ratings of semantic preservation . it also verifies 84 % of 200 examples against the pp metric .
the results are shown in table 1 . the results of m1 and m2 show that when only using shen - 1 and para - para features , the model performs better than the best previous state - of - the - art on all metrics . moreover , the results are slightly worse when using 2d features , indicating that the syntactic relations in question are strongly concentrated in the lexical regions of interest .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best model ( yang2018 ) achieves higher acc than previous work on two of the four datasets ( fu - 1 and ng et al . , 2018 ) and also outperforms the best previous work by a margin of 2 . 8 points in acc . the results are shown in table 6 . multi - decoder and domain - aware classifiers achieve the highest accu and the best overall results , but are worse than the simple - transfer model , because the number of transferred sentences is much smaller . from this group of results , we can further see that the transfer baseline achieved by our model is significantly higher than the previous best state - of - the - art .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of tokens in a sentence , and number of repetition tokens , the average number of iterations in the sentence , the length of rephrase tokens and the overall number of disfluencies . as shown in table 2 , nested disfluency tokens are particularly difficult to predict , but can be reduced with a reasonable effort .
table 3 shows that for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , the model predicts the rephrase to be disfluent . however , for those that consist entirely of function - words , the picture is less clear , showing that the majority of tokens belong to each category .
the results are shown in table 2 . in the single - domain case , the text model performs best while the innovations model performs the best . when we add in the text and innovations features , the results are slightly worse than the single model but still superior to the model by a margin of 2 . 2 points . as expected , in the multilingual case , text alone alone does not improve performance , but incorporating the innovations improves results by 3 . 8 points .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model improves upon the previous approaches by 3 . 43 points in the accuracy ratio . it closely matches the performance of the best state - ofthe - art cnn - based embeddings and self - attention sentences . the accuracy drop of 10 . 53 points from the average of the word2vec embedding to the low f1 score of 7 . 54 on the micro f1 test set .
table 2 shows the performance of all the methods that we base our model on on the apw and nyt datasets . our model significantly outperforms all the previous approaches .
table 3 compares the performance of our neuraldater model with and without graph attention . our approach shows that both word attention and graph attention reduce forgetting , improving the performance by 9 % in the word attention task and by 4 % in graph attention .
the results are shown in table 1 . the first group shows that all models perform well on the one - to - n test set , with the exception of cnn . jrnn significantly outperforms all the other models except for argument argument . the argument argument is the most difficult part to solve , since it requires a lot of data and leads to incorrect predictions . fortunately , it is the only part of the model that can be improved with t - test . next , we show the results of stepwise learning on the cnn dataset . stepwise learning improves performance on both datasets with good recall scores . the dmcnn model achieves state - of - the - art results , and even outperforms the jrnn model in terms of all metrics on both test sets .
table 1 shows the results for event identification and event classification . our method dkrn outperforms all the state - of - the - art methods in both event and argument identification . for argument identification , it achieves the best results with a f1 - score of 68 . 7 . from table 1 , we observe that argument identification is the most difficult part , followed by trigger identification .
the results are shown in table 1 . as can be seen , all models give similar results on the test set . the spanish - only model achieves the best results , while the english - only one is slightly better . the fine - tuned model by fine - tuned gives the worst results . finally , all models give the same performance on the dev perp and test wer metrics , which shows the advantage of finetuning the model after training .
concerning training with only subsets of the data , we note that fine - tuning improves the results for both datasets with a gap of 10 . 2 % in the test set and to parity with the best performing model ( lei et al . , 2018 ) on the dev set . cs - only training also improves results , but only by a margin of 3 . 8 % over fine - tuning .
the results in table 5 show that fine - tuning the l2r model reduces error on the dev set and on the test set , both when using only one gold sentence and when using the gold sentence with two gold sentences in the set as a gold sentence . the difference in accuracy between the gold sentences is minimal , however we see significant difference in test set performance .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to the current state - of - the - art is statistically significant ( p < 0 . 01 ) with respect to all metrics , which shows significant improvement in the performance of our model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvement from baseline to type combined gaze features is statistically significant ( p < 0 . 01 ) and r = 0 . 03 , both for type combined and baseline gaze features .
the results on the belinkov2014exploring test set are shown in table 1 . the glove - extended embeddings outperform the ontolstm - pp model , indicating the effectiveness of our method . our approach uses the syntactic skipgram embedding scheme , and improves the generalization ability of the system . we notice that the hpcd model achieves the best generalization performance , with an f1 - score of 89 . 7 on the test set .
table 2 shows the performance of our system with various pp attachment predictors and oracle attachments . our model obtains the best results with a 94 . 97 % ppa acc . on the full uas test set .
table 3 shows that removing sense priors and context sensitivity ( attention ) completely improves the predictive performance of our model .
in table 2 we report the bleu % scores of incorporating subtitle data and domain tuning for image caption translation . the results are slightly worse than the results with marian amun et al . ( 2017 ) in en - de and mscoco17 , but still superior to en - fr and multi30k on flickr17 . adding subtitle data improves the results for both datasets , the improvement is larger when the ensemble is added , as expected , the larger improvement is greater when the domain tuning is used .
table 3 shows that domain - tuning improves the results for both en - de and mscoco17 models . the results are slightly worse than those of en - fr and flickr17 , but still superior to the plain plain model in terms of hoco .
table 4 shows the bleu scores of our model taking only the best 5 captions and adding automatic image captions as well as marian amun ' s multi - task learning method . the results are shown in table 4 . our model improves upon the results of en - de and mscoco17 using the best five captions . however , the improvement is less pronounced for multi30k , which shows that more captions are required to improve performance .
table 5 shows that our encoder and dec - gate strategies outperform the en - de and mscoco17 approaches , indicating that visual information is more easily integrated into the decoder . the results of using multi30k + ms - coco + subs3mlm again indicate that the architectural choice that gives the best performance is the multi - layer approach , i . e . that it can integrate visual information with semantic information .
in en - de and mscoco17 datasets , all fine - tuning schemes give similar results . however , the results are slightly worse than those in en - fr and flickr17 . due to space limitations , we report results in table 3 with respect to multi - lingual features only . subsequently , we also report results using the ensemble - of - 3 model instead of the text - only model , which achieves the best results . the results are shown in bold .
table 1 shows that en - fr - ht and en - es - ht achieve better results on the test set compared to the models using the trans - f1 layer . the results are statistically significant with respect to all metrics , with the exception of ttr . in general terms , all models perform comparably to the best previous state - of - the - art on both mtld and ttr datasets .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model splits the training and development sentences , which means that there are no duplicates in the output for the two sets , i . e . no questions having no correct answer at all .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results show that the models can learn to reason over more than simple verbs with a minimum of 90 % accuracy .
table 5 shows the automatic evaluation scores for the rev systems . our system outperforms the previous state - of - the - art on all metrics by a noticeable margin .
table 2 shows the performance of our model against the best performing rsaimage model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . in terms of recall , the vgs model obtains the best average recall @ 10 and the median rank is 0 . 0 .
the results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the other models are rsaimage and audio2vec - u . in terms of recall , the acoustic embeddings achieve the best performance with a mean of 1 . 414 % and a median rank of 0 . 9 % . from this group of models , the only ones that do not receive significant improvement in recall are audio - 2vec and segmatch .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . the first example shows that dan can turn a screenplay screenplay into a sentence that is easily hate - able . it turns in a screenplay that is at the edges , but at the same time is very clever , because the edges are shaped in a way that makes it easy to hate it . cnn also shows that it is very useful for this task . we report further examples in the appendix . as shown in table 1 , the use of rnn improves the results for the task .
table 2 shows that the number of occurrences in sst - 2 has increased , decreased or stayed the same through fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . it is clear from table 2 that the quality of the rnn model has not changed as a result of the loss of some words . however , the effectiveness of the dan model has increased . the improvement from a 0 . 0 % baseline to a 7 . 5 % baseline is significant .
table 3 shows that sentiment scores in sst - 2 have increased as a result of the flipped labels being flipped from positive to negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence . flipped labels cause the sentiment score to increase in positive and negative sentiment , respectively .
table 2 compares the results of pubmed and sift with other approaches . our approach establishes a new state - of - the - art in the low - supervision setting , outperforming all the other approaches except pubmed by a large margin .
