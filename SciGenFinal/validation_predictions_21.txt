table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results show , once the training and inference tasks are completed , the model performs much better in terms of both feature extraction and feature extraction . the difference in performance between the iterative and recursive approaches is less pronounced for training , but still indicates significant performance improvement .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the results in table 2 show that the max pooling strategy consistently performs better in all models variations . it achieves the best performance with a f1 score of 75 . 83 % on the conll08 model ( i . e . in 5 - fold ) with optimal values given the correct representation . the same tendency is observed for ud v1 . 3 ( 76 . 83 % ) . it is clear from table 2 that the use of softplus improves the hyperparametrization performance for all models . as hard coreference problems are rare in ud , we do not have significant performance improvement .
table 1 shows the results for relation extraction using the shortest dependency path on each relation type . we find that using sdp reduces the error of our model by about 3 . 5 % in relation extraction and gives a significant performance gain in overall results .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of embeddings perform comparably when trained and tested on the same dataset ( y - 3 , r - f1 and f1 ) . in fact , the performance gap between the two is much narrower with the former performing much better than the latter . the results are presented in table 3 . the performance gap is most prevalent between those using y - 3 : y < italicized r - values and those using jim - y - rn , with an absolute improvement of 2 . 59 points over the previous state of the art .
the results of paragraph prediction accuracy are presented in table 1 . we can see that mst - parser achieves an absolute improvement of more than 50 % over the state - of - the - art freelancer - trained system on average on all three metrics . on the essay level , it achieves 50 % improvement over the previous state of the art . it achieves higher accuracy on the paragraph level and on the f1 level , achieving an absolute boost of 10 % on the score prediction accuracy .
the results in table 4 show that our lstm - parser system outperforms the previous state - of - the - art on both essay and paragraph parsing tasks .
the results are presented in table 1 . the results of original and cleanups are shown in bold . after removing the effect of corrupted data , the original tgen model performs much better than the cleaned model . it achieves a new best performance of 43 . 14 % on the bleu metric , which shows significant performance improvement over the previous state of the art . replacing the corrupted data with clean , fresh tgen data results in a significant drop in performance compared to the original . this confirms the importance of redundancy removal .
table 1 shows the comparison of the original e2e data and the cleaned version . the difference in quality between the two sets is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . the difference is much larger in terms of number of distinct mrs , total number of textual references and slot matching instances , as shown in table 1 , our cleaned dataset has 21 % higher ser than the original one .
the results are presented in table 1 . the results of adding correct answers after training and adding wrong answers are shown in bold . tgen + achieves the best results , outperforming original tgen and original sc - lstm . replacing the training data with the add - forward results results in a significant improvement over the performance of tgen − . adding correct answers improves the bleu and nist scores , but does not improve the meteor scores . rouge - l performs slightly better than original , but still inferior to tgen ,
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) as well as the number of instances that had correct values ( add , removed , and cleaned ) are shown in table 4 . the difference in errors between original and cleaned instances is minimal , however we find significant difference in the overall number of errors due to different mix - up in the training data .
table 3 compares the performance of our approach with previous approaches on the single - domain and multi - domain datasets . the results are presented in bold . our approach achieves state - of - the - art results , outperforming all the previous approaches except for tree2str , which obtains a performance improvement of 2 . 5 points over the previous state of the art . graphlstm ( song et al . , 2018 ) achieves a performance gain of 3 . 6 points over previous work , which shows significant performance improvement on datasets with fewer training examples . the gap between our approach and previous work is modest , we observe that the gap between the two approaches is less pronounced for the dcgcn ensemble , as shown in fig . 3 , the size and type of training data are the most important factors in performance . our approach outperforms previous work on both datasets .
the results on amr17 are shown in table 2 . our model achieves a performance improvement of 3 . 5 bleu points over the previous state - of - the - art on both datasets . the difference is most prevalent in terms of performance with svm , where our dcgcn model obtains a performance gain of 2 . 5 points . ggnn2seq ( beck et al . , 2018 ) shows a performance drop of 2 points compared to previous work .
table 3 presents the results for english - german , czech and french , compared to english - czech . the results are broken down in terms of performance on single and multi - word embeddings . for english , we see that the model using bow + gcn achieves the best performance with 43 . 8 % on average compared to the previous state - of - the - art on both languages . on the other hand , ggnn2seq ( beck et al . , 2018 ) achieves the highest performance with 41 . 4 % on both language types , outperforming the previous best single model by a noticeable margin . we see that both languages benefit from more data sharing , as the performance gap between the two is narrower with single - word models .
table 5 shows the effect of the number of layers inside our dc network on performance . we observe that for all but one of the 21 blocks , adding more layers inside the network helps the model to converge . this confirms what klinger et al . ( 2017 ) report : more layers , thereby reducing the performance gap between the top and bottom of the network , results in a better performance .
table 6 shows that the rcn with residual connections outperforms the gcn with no residual connections . the results are slightly worse than those with rc + la connections , but still superior to the results with rc - la without . moreover , the difference is less pronounced with respect to baselines , adding rc reduces the rc gap between the two groups , we find that the performance reach the best when the rc - based model is used with both residual connections and pure connections .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the coreference signal is localized on specific objects and that these objects are in the deep layers of the network ( e . g . , the hidden regions ) . the performance of these models is very similar when using only one type of clustering scheme , namely , dcgcn ( 1 ) , which relies on pre - trained word embeddings . the difference is most prevalent in terms of d - score , which shows that once the model has learned its task , it is unable to learn the task to a high degree .
the results of an ablation study on the dev set of amr15 . 1 are shown in table 8 . the results show that removing the dense connections in the i - th block reduces the density of connections , and consequently the performance of dcgcn4 .
the results of an ablation study of the modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that both approaches yield comparable performance with the original dcgcn4 model . however , under the current setup , the results are significantly worse than the results under the previous approach . we find that the advantage of domain - aware attention is outweighed by the disadvantages of dependency aggregation and coverage mechanism . when using both approaches , we get significantly better results .
table 7 shows the performance of our initialization strategies on various probing tasks . our paper obtains the best results with 35 . 8 % overall improvement over glorot ( 31 . 8 % ) and 29 . 4 % improvement over the previous state of the art on subjnum . the difference is less pronounced for topconst , but still represents a significant performance gain . topconst initialization is crucial for our model to achieve state - of - the - art results . it helps improve the joint attention span of the subjnum and length functions , and the bshift function . it reduces the repetition rate of the given object and gives a significant improvement in generalization performance .
the results are presented in table 1 . we observe that the h - cmow variant outperforms the original cbow and h - cbow in every metric by a noticeable margin . it achieves state - of - the - art results , outperforming both the original and the variant with a gap of 10 . 5 points from the last published results . the difference is most prevalent in the subtasks of depth and length , subjnum and tense are the most difficult to solve , as shown in the second group of tables , combining the two features results in significantly better results .
the results are shown in table 1 . we see that the hybrid approach outperforms the original method by a noticeable margin . it achieves state - of - the - art results on all datasets , outperforming the original cbow / 784 model by a margin of 3 . 6 points . the results show that the use of cbow improves the sub - domain coverage for all datasets except sst2 , and boosts the average score for sst5 by 2 points .
table 3 shows the performance of our models on the four downstream tasks as compared to the previous state of the art approaches . hybrid method outperforms cbow and cmow in all but one of the four cases . the difference is most prevalent on sts13 , where cbow obtains a significant performance gain of 44 . 2 % compared to hybrid . in comparison , cmow achieves a significant improvement of 19 . 6 % over cmp . with respect to sts16 , the difference is less pronounced , but still significant .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . glorot ( 87 . 6 % ) and n ( 0 , 0 . 1 ) achieves outstanding results , outperforming all the stateof - the - art methods except trec by a significant margin . it achieves over 90 % improvement over the previous state of the art on three of the four sub - tasks and outperforms all supervised downstream methods except mpqa by a noticeable margin .
table 6 shows the performance of our method compared to the state - of - the - art cbow - r method on the four downstream tasks . the results show that our approach significantly outperforms the previous state of the art method in terms of all metrics . the difference is most prevalent in sts12 and sts16 , where the cbow approach performs significantly better than the cmow approach .
the results are presented in table 1 . we observe that the compactness and recall of our method outperforms the previous state - of - the - art methods on every metric by a noticeable margin . the difference is most prevalent in the subtasks of depth and tense , where our method obtains the best performance . subjnum and topconst are the only two that perform significantly worse than the others . cmow - r achieves the best results overall , outperforming the other two methods by a significant margin .
the results are presented in table 3 . the first group shows that the cbow - r method outperforms the cmow - based approach , and the second group shows the results of re - scoring after replacing these 10 models with the ones from the previous batch . as can be seen , the difference in performance between the two approaches is minimal , however we see significant performance drop when using only one type of sub - classifier , namely , between sst2 and sst5 . when using both subsets of the training data , the results are slightly better than the previous state of the art .
the results are shown in table 1 . supervised learning underperforms all supervised and unsupervised learning methods except for the case where it obtains a slight improvement over the best previous state - of - the - art model , namely , that of τmil - nd . in all but one case , the difference between the two approaches is less pronounced under supervised learning . the difference is most prevalent in the subtasks of loc and misc , where the supervised learning method generally performs better than the other two methods . name matching under - performs both supervised learning and all other methods except the case of the e + loc model , which shows the diminishing returns from cross - domain training .
results on the test set under two settings are shown in table 2 . name matching and supervised learning achieve the best results , with mil - nd model achieving the highest f1 scores . supervised learning achieves the best overall performance , with a f1 score of 43 . 57 % , a slight improvement over the previous state of the art . we observe that τmil - nd ( model 2 ) is comparable with the best previous state - of - the - art model , in terms of both eq . and rn , and achieves the highest precision on the name matching and f1 tests . it closely matches the performance of the original model with only 0 . 5 % absolute difference . the difference is less pronounced under the noregionalization scheme , where the model gets a performance boost of 2 . 26 % on the f1 test set . with respect to rn and λ , the difference is much smaller under the supervised learning scheme , but still significant , at 3 . 5 % .
table 6 presents the results of model training on the hidden test set of hotpotqa in the distractor and fullwiki setting , compared to the published results . the results are broken down in terms of performance on extractive and abstractive keyphrases , with g2s - gat achieving the best performance . it can be observed that , when using only pure word embeddings , the model performs better than the former state - of - the - art on both datasets . moreover , the difference is much narrower with respect to ref and gen , showing that the training data used for derivation are much more specialized .
table 3 compares the performance of our model with previous state - of - the - art work on the three datasets . the results are broken down in terms of performance on bleu , meteor , and g2s datasets . our model obtains the best results with a gap of 3 . 42 points from the previous state of the art . on the ldc2015e86 dataset , it achieves a final score of 30 . 28 % , which is marginally better than the previous best performance by a margin of 2 . 55 points . on ldc2017e86 , it obtains a provisional 3 . 53 % higher performance . this confirms the value of finetuning word embeddings during training . we observe a drop in performance between the two meta - data sets as a result of fewer training instances , i . e . , fewer iterations , leading to lower performance on some datasets .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . the results show that g2s - ggnn models significantly outperform the previous state - of - the - art on this test set , outperforming both glove - trained and un - trained models . note , however , the large difference in performance between external test set and internal test set due to the high number of training examples in the latter .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state of the art on both datasets with a bleu score of 62 . 42 % and meteor score of 59 . 49 % .
the results are shown in table 1 . we observe that g2s - ggnn model achieves a significant improvement over the previous state - of - the - art on all metrics by 3 . 51 % in terms of sentence length and average graph diameter . it achieves the best performance on three of the four metrics with a gap of 3 . 26 % between the two .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when gold is applied to the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , g2s - gat significantly outperforms the s2s model in terms ofiss ,
table 4 shows the pos and sem accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the pos feature accuracy is close to the state - of - the - art , and sem features are comparable to the best performing 3rd nmt encoders .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag embeddings significantly outperform the original embedding method , showing that the upper bound encoder is more useful for predicting the most frequent tags . it also achieves a significant improvement in pos accuracy compared to using unsupervised word embedds . the difference in sem accuracy is less pronounced , but still significant .
table 4 presents the performance of our system on the four types of tagging accuracy tests . we observe that , let alone a drop in performance , our system outperforms all stateof - the - art methods in all aspects except for pos tagging accuracy . it achieves the best performance with an absolute improvement of 2 . 8 % on average compared to the previous state of the art .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results show that the residual encoder performs best , with an absolute improvement of 2 . 9 % over the strong bias of uni . we observe that the bi - layers approach is comparable in both languages , with a minor improvement under english . as expected , the bi layer performs best in terms of pos .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the difference between the attacker score and the corresponding adversary is minimal , however we see significant difference in performance due to different classifiers being used for different tasks . the classifiers used for this analysis are gender - neutral and age - based .
accuracies when training directly towards a single task are shown in table 1 . the results show that pan16 significantly outperforms pan16 with respect to all the sentiment and gender - neutral tasks . gender - neutral features result in significantly higher accuracy , as do age and classifiers . the classifier trained on pan16 achieves the best results with 83 . 2 % accuracy on the gender and age tasks .
the results in table 2 show that the balanced and unbalanced data splits result in significantly better performance for gender - balanced task prediction . gender - based and racial - based data leaks are less prevalent , but still represent a significant amount of data leakage . the racial disparities are much smaller than in the balanced data split , indicating that gender - based bias is less prevalent in the training data . overall , the results are very similar for all three groups .
the performance on these datasets with an adversarial training set is shown in table 3 . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in leakage due to high correlation between the number of tokens in the training set and the quality of the named entity . in pan16 , we find that the gender - based diversity feature contributes significantly to the task performance , lowering the overall performance . gender - based features contribute significantly less than the other three .
the results presented in table 6 show that the rnn encoders perform comparably to the guard - encoders when the protected attribute is protected . guarded rnns perform better than the original ones when the leaky embeddings are used .
the results in table 3 show that the training set size and the number of parameters used for each sub - step are the most important factors in model performance . the lrn model achieves a performance improvement over the previous state of the art on both datasets with a gap of 10 . 3 % on the ptb and wt2 sub - steps . it closely matches the performance of the best previous work by yang et al . ( 2018 ) in terms of both finetune and tuning using the best performing feature set . we observe that the size and type of parameter sharing contribute similarly to the improvement , with the former yielding higher performance on the smaller scale . as can be seen in the results in the second group of table 3 , both the lrn and the lstm models achieve performance gains over previous work with smaller training sets . however , the difference is less pronounced with respect to finetune feature sharing , showing that it is harder to train and maintain a model with large training set .
the results are presented in table 1 . the results of applying our final model to the training data are shown in bold . the results show that our approach exceeds the previous state - of - the - art on every metric by a significant margin . on the lstm dataset , our approach obtains the best performance with an absolute improvement of 2 . 43 % . the difference is less pronounced with respect to the bert time metric , our approach outperforms previous work on two of the four datasets by a noticeable margin . when using the base acc and time metrics , our proposed approach achieves the best results with a gap of 1 . 43 points from the last published results .
the results of zhang et al . ( 2015 ) on the three domains are shown in table 1 . the results are broken down in terms of err , time extraction and yelppolar err . our model obtains the best results with an absolute improvement of 3 . 55 % on average over the previous state of the art on all three domains . relative error reductions range from 0 . 005 - 0 . 08 % . the difference is most prevalent in amapolar time , which shows significant performance drop . at the same time , our approach results in a significant improvement on yelp time , outperforming previous work by 4 % .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task . it can be observed that the gnmt model significantly outperforms the other methods in terms of both training time and decoder time . the difference is most prevalent in the case of gnmt , which takes a significant amount of time to train and decode compared to other methods . lrn also performs significantly better than olrn and sru . at the same time , its training time is shorter than those of either lrn or sru , which shows the advantage of finetuning word embeddings during training . as shown in the second group of table 3 , when training and decoding , the gru model takes significantly less time to solve a sentence compared to the other method . when using only one sentence per training batch , the time to decode takes significantly longer than that to train .
table 4 : exact match / f1 - score on squad dataset . we report the results of our final model with only elmo as parameter number , and rnet * as the other parameter number . the results show that our model obtains a significant improvement in performance with the help of elmo feature - values . it closely matches the performance of previous work ( wang et al . , 2017 ) with only 0 . 5 % absolute difference . at the same time , it gets a slight improvement with respect to f1 score ,
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . it achieves a result of 90 . 94 % , which is marginally better than the previous state - of - the - art . the number of parameters in our lstm model is slightly higher than other methods , but still comparable with other sophisticated neural models . at the same time , our model obtains the best performance with respect to the number of parameter number , which shows the advantage of finetuning the training set during optimization .
table 7 shows the performance of our model on the snli and ptb tasks with the base + ln setting and the test perplexity using the elrn setting . with the base setting , our model obtains 85 . 26 % accuracy improvement over the previous state - of - the - art model . the difference is less pronounced with the ptb task .
table 3 presents the results on word analogy task . our system outperforms all the state - of - the - art systems in terms of both word analogy and sentence prediction using the current set of features . oracle retrieval achieves the best results with an r - 2 score of 23 . 05 on average . on the other hand , sentence prediction accuracy is only slightly better than system performance at 23 . 08 . sentence prediction using oracle is more accurate than system prediction at 21 . 64 . word analogy task precision is relatively high , both on the system and in the mtr . we observe that sentence selection speedup considerably when using oracle , both for b - 2 and b - 4 tasks .
table 4 shows the human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is significantly better at selecting the correct grammatical relations and its output is more interpretable . the average number of tokens per sentence is slightly higher than the others , but still comparable with the best human evaluation . overall , our system obtains the highest quality score , ranking in the top 2 % of evaluations .
the results are shown in table 1 . we observe that for all three datasets , our approach outperforms the previous state - of - the - art on every metric by a significant margin . on the europarl dataset , we see that p < 0 . 005 and r > 0 . 01 compared to previous state of the art on all datasets except for ted talks . for the ted talks dataset , the gap is narrower but still significant , with an absolute improvement of 2 . 5 points over the previous best state - ofthe - art . on the other hand , for the sub - tables dataset , our model performs slightly better than the previous one on all metrics except for the relation extraction procedure .
the results are presented in table 1 . we observe that for all three datasets , our approach outperforms the previous state - of - the - art on every metric by a noticeable margin . on the europarl dataset , we see that p < 0 . 01 , while on the ted talks dataset we get 0 . 03 . the difference is less pronounced for the dsim dataset , but still significant , with an absolute improvement of 2 . 5 points over the previous best state of the art . when we compare our approach to previous work , we find that our approach yields significantly better results on the datasets with fewer training examples ,
the results are shown in table 1 . we observe that for all three datasets , our approach outperforms the previous state - of - the - art on every metric by a noticeable margin . on the europarl dataset , we see that our approach yields significantly better results than df and tf datasets . the difference is most prevalent in the ted talks dataset , where our approach results in a 17 % improvement on average compared to df . on the other hand , this gap is much narrower on the sub - lexical dataset , showing that our proposed approach yields a comparable performance with those of df , tf and other methods .
the performance of our approach on the europarl dataset is presented in table 1 . we observe that the average depth of our dsim and slqs datasets is significantly lower than the previous state - of - the - art on all metrics except for docsub . the difference is most prevalent in terms of depth cohesion , which shows that our approach relies on superficial features such as word embeddings and lexical features . overall , our approach results in significantly better performance than previous approaches .
the performance of our approach on the europarl dataset is presented in table 1 . we observe that the average depth of our dsim and slqs datasets is significantly higher than the previous state - of - the - art on all metrics except for docsub . the difference is most prevalent in terms of depth cohesion , which shows that our approach relies on superficial cues . overall , our approach results in significantly better performance than previous approaches .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model ( lf ) outperforms the original version ( kutuzov et al . , 2017 ) in terms of both question type and answer score sampling . it achieves a significant improvement over the performance of the baseline model by 3 . 42 % over the strong lemma using the combination of weighted softmax loss , binary sigmoid loss , and generalized ranking loss , respectively . moreover , it achieves a noticeable drop in performance when using the feature - rich learned representation of the hidden dictionary instead of the naive softmax representation . this indicates that the use of selective attention leads to better performance .
the results of ablative studies on the visdial v1 . 0 validation set are shown in table 2 . it can be seen that applying p2 improves the performance for all the models except for the one using the history shortcut . the difference between the performance of applying p1 and p2 indicates that p2 is the most effective one ( i . e . , it leads to a better performance ) .
table 5 shows the performance of our approach compared to previous approaches on the hard and soft alignments . hmd - f1 model significantly outperforms the previous state - of - the - art on both metrics , achieving 4 . 3 % higher precision compared to wmd - bigram and 3 . 4 % higher performance compared to ruse . ruse also achieves a significant performance improvement over wmd - unigram when using bert feature - values ,
the results are shown in table 1 . the first group shows that bert score - f1 is relatively consistent with the baselines while the average is significantly lower than ruse ( * ) and meteor + + . sent - mover the second group shows lower performance than the baseline due to more training data and because the training set size is larger . when we add in the smd + w2v baseline , we get 0 . 8 % improvement on average compared to the baseline .
the results are shown in table 1 . the first group shows that bertscore - f1 consistently achieves the best results among all the three metrics with a gap of 0 . 3 points from the previous state - of - the - art results . next , we see that meteor achieves the highest score with an absolute improvement of 3 points over bleu - 1 . sent - mover performs similarly , achieving an absolute boost of 2 points over the previous best state of the art . we observe that the quality of the transfer learning method is relatively consistent across all metrics , with the exception of if - lex , which is lower than the others .
the results are shown in table 1 . word - mover accuracy on the training set is relatively consistent across all metrics with the exception of leic ( * ) where it is significantly worse than other approaches . the numerical results displayed in the table show that the word - mover using the best performing feature set is the bertscore - recall metric , which shows significant performance improvement when using only one type of word - layer , namely , when using word - weights with both elmo and bert features , as shown in the second group of tables . sentances are consistently better than the wordweights using the other two base - based features . when using both meteor and spice features , the results are slightly worse , but still superior than the results obtained using leic .
the results are shown in table 1 . the results show that the approach chosen by vaswani et al . ( 2017 ) achieves the best results with an absolute improvement of 3 . 81 points over the previous state - of - the - art model on all subtasks . it closely matches the performance of the original l2r model with only 0 . 005 % absolute difference .
table 3 presents the results on the semantic and transfer quality datasets . the results show that yelp significantly outperforms google translate and semanticnet in terms of transfer quality and semantic preservation . semantic preservation results are significantly worse than transfer quality , in particular , there is a significant drop in performance between the transfer quality metric and the semantic preservation metric . this indicates that the semantic features extracted by yelp are quite specialized , and that the model performance obtained using them may not be suitable for production use . we observe a drop in transfer quality between semantic and semantic features as well .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . we use spearman ’ s [ italic ] ρ b / w sim and human ratings of semantic preservation as our metrics for validation . as the table shows , both methods yield high accuracies ( 94 % on average ) and high human ratings ( 84 % ) , confirming the effectiveness of our approach .
the results are shown in table 6 . the results of m1 and m2 show that the approach chosen by vaswani et al . ( 2017 ) results in significantly better performance than the previous state - of - the - art on all subtasks except for sim . it can be observed that the performance reach its best when using only one type of wrapper , namely , when using the combination of lexical and syntactic prefixing , with an absolute improvement of 2 . 5 points over the state of the art in m1 . moreover , the improvement is much larger when using both meta - para and named entity nodes , giving a significant performance boost . in sim , the difference is less pronounced , but still significant .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results in table 6 show that our model achieves higher acc than previous work on similar sentiment transfer using the same training data . however , the difference between transfer with original and untransferred sentences is less pronounced , with the former achieving the highest acc and the latter the lowest . we find that using the multi - decoder approach gives a significant performance gain , which is expected in a single shot framework . it is clear from table 6 that the use of different classifiers in use helps the model to achieve higher acc .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent when we included repetition tokens and nested disfluencies . reparandum length is the average of the number of tokens in a sentence , and the average number of repetition tokens , excluding repetition tokens . as a sanity check , we also include the length of tokens to prevent from an endless repetition of the same sentence leading to incorrect predictions .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . it can be seen in table 3 that the content - disfluency tokens contain a significant fraction of tokens that are of high quality ( around 50 % ) , making them suitable for use in production . however , the remainder belong to the lower - frequency category , which is less productive for production .
the results are shown in table 2 . we observe that the text - rich innovations model outperforms the single - class approach when trained and tested on the same dataset , confirming the importance of word embeddings adaptation . moreover , the results are slightly superior when training and testing on both datasets with different iterations of the training data , in both cases , the average number of iterations per test is significantly higher when using both text and innovations , as the results show , once the innovations are added , the performance reach the best . when using both raw and innovations - based text - based features , the model performs best in the late stages when training on the single dataset . this corroborates our intuition that incorporating all the information available in the data during the training phase helps the model to converge .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves substantial gains in accuracy over the previous state of - the - art on both datasets . it closely matches the performance of rnn - based and self - attention neural networks . it even achieves a slight improvement over the accuracy of the learned word2vec embeddings by a noticeable margin .
table 2 shows the performance of all the methods that we consider for the document dating problems on the apw and nyt datasets . our unified model significantly outperforms all previous models . it achieves 60 % higher accuracy than the previous state - of - the - art model on both datasets , and 62 % higher than the ac - gcn model .
table 3 compares the performance of our approach with and without word attention for this task . our approach shows marked performance improvement . with word attention , our neuraldater model achieves 62 . 6 % higher accuracy compared to the approach by lin et al . ( 2017 ) on the word attention task . with graph attention , our approach achieves 65 . 2 % higher performance .
the results are shown in table 1 . the first group shows that the training set size and the average number of iterations per stage are the most important factors in model performance . the dmcnn model achieves the best results with 75 % on both 1 / 1 and 1 / n tasks , followed by the jrnn model with 75 % . the argument argument is the most difficult part of the model to solve . it takes a considerable amount of data to solve and achieves only average performance under - fitting , which shows the performance of the argument layer . when it comes to training , the only thing that consistently beats the argument stage is the quality of the neural network itself . this is because the training data is significantly less diverse , so there is less variation in performance across the various stages .
table 3 presents the results on event identification , event classification and event recall . the proposed method outperforms dfgn and f1 by a significant margin . it achieves state - of - the - art results on both datasets with respect to all three high - level features . dfgn performs well on both event and event features , with an absolute improvement of 2 . 8 points over dfgn . on the argument feature , it achieves a performance gain of 1 . 7 points over f1 .
consistent with the results in table 1 , all models show lower precision on the dev perp and test wer metrics compared to the previous state - of - the - art . the results are broken down in terms of language adaptation : in english - only case , the model using only the word " lexica " performs best , while in case of spanish - only there is a significant drop in performance . fine - tuned - lm achieves the best results , outperforming all the other methods except the original shuffled - lm . finally , the best performance is obtained by applying the best performing dev acc metric , which shows the extent to which fine - tuning has impacted the model performance .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuning gives a significant performance improvement over cs - only training , improving the bias metric by 4 . 8 % in the standard task formulation and to parity in the gold - two - mention case . the results show that fine - tuning reduces recall , but does not improve performance significantly over naive training .
the results in table 5 show that fine - tuning gives a significant performance improvement over the monolingual approach . the difference in accuracy between the dev and test set is less pronounced for the former , but still significant : fine - tuned - disc gives a 4 . 53 % improvement over fine - tuned - lm .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline to the current state - of - the - art results is statistically significant ( p < 0 . 01 ) and r = 0 . 012 , both compared to the baseline and the previous best state of the art . it can also be observed that the f1 score improved by 1 point over the baseline showing a significant drop in performance .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for using the conll - 2003 dataset as compared to the baseline . our approach shows a significant improvement in performance over the baseline model by 2 . 5 points in terms of precision and r = 0 . 03 .
results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . the results show that our approach outperforms the previous state - of - the - art on both the syntactic and semantic word embeddings . our approach uses the best performing feature set , and achieves a final score of 89 . 8 % , which places it second among all 3 methods . it closely matches the performance of the original hpcd implementation .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . it can be seen in table 2 that the hpcd model outperforms the original lstm - pp model with a large margin . the difference is less pronounced with the oracle attachment , however , still showing significant performance drop .
table 3 shows the results of removing sense priors and context sensitivity ( attention ) from the model . the results show that the ppa accuracy drop significantly as a result of removing both sense and attention features .
in table 2 , we report the bleu % scores of the models using subtitle data and domain tuning for image caption translation . the results show that both domain - tuned and domain - aware subtitle data improve the results for both en - de and multi - domain - aware models . moreover , the improvement is much larger for mscoco17 , showing that once the domain - domain information is added , the model performs better than the original model under both conditions .
the results of domain - tuned h + ms - coco are shown in table 3 . the results show that when using domain - aware features , the performance reach the best when the model is trained and tested on both en - de and mscoco - trained models , with a gap of 10 . 5 % in performance between the two sets . as can be seen , the results are markedly better when the label - aware model is used , as the results show , simply adding the labels helps the model to perform better overall .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results show that , using only the best five captions , automatic captions give a significant performance improvement . moreover , the multi - task approach gives a 2 . 4 % boost in performance compared to using all the captions individually . in en - de and mscoco17 , the model using marian amun ( maurice amun et al . , 2017 ) achieves a final accuracy of 62 . 2 % , which is slightly higher than the previous state of the art .
table 5 shows the results for visual information integration using multi30k + ms - coco + subs3mlm and detectron mask surface , the results show that enc - gate and dec - gate strategies achieve higher bleu % scores than en - de and mscoco17 , indicating that the use of pre - trained word embeddings helps the model to better interpret visual information . however , the improvement is less pronounced when using multilingual encoders , with respect to sub - mlm features , the encoder technique achieves the best performance , achieving 62 . 53 % of the overall score . the same tendency is observed with respect to finetuned encoder and decoder : both methods obtain 62 % and 62 % of overall score , respectively , compared to the previous state of the art .
we observe that the multi - lingual approach by mastro et al . ( 2018 ) achieves the best results , outperforming the text - only approach by a noticeable margin . the results are broken down in terms of visual features , with the sub - category of " visual features " showing the most significant performance drop . sub - categories as adjectives antonyms and performer action have the highest percentage of misspelling terms , so we observe lower performance in these categories for both datasets . overall , the results are slightly better than the results of " text - only " when using only the word " lm " .
table 3 shows the results for english and german on the test set of hotpotqa in the distractor and fullwiki setting , compared to the previous state - of - the - art on both sets . the results are broken down in terms of performance on ttr and mtld , with en - fr - ht achieving the best performance , followed by en - es - ht . in both cases , the model using the trans - f1 layer achieves better results than the original trans - rnn layer .
for brevity we only report the number of parallel sentences in the train , test and development splits for the language pairs we used . the results are shown in table 1 . our en – fr model splits the word embeddings into 10 parallel sentences , which gives a significant performance gain over dfgn .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results show that the training vocabulary is comparable in both languages , with the exception of spanish , where it is slightly more diverse .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems obtain decent performance , but are still significantly worse than the ter - based system , which shows the performance reach the upper limits of the performance range for a single rev system .
the results on flickr8k are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the generated rsaimage . the mean mfcc score of our model is 0 . 7 , which means that our model significantly outperforms the previous state - of - the - art model .
the results on synthetically spoken coco are shown in table 1 . the acoustic embeddings generated by audio2vec - u outperform the others in terms of recall @ 10 and average chance , as expected , the average recall of rsaimage is significantly higher than that of vgs . in addition , the mean mfcc score of our model is slightly higher than the other two methods . we observe that the use of multi - headed attention leads to better recall and lower chance . when using only rsaimage , we do not need to consider the effect of concatenation noise , rather , we focus on the quality of matching instances ,
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that is much easier to hate than the original because it has fewer edges , hence less effort is required to turn it into a screenplay . cnn also uses a lot less effort to turn a screenplay , since it turns on a single thread instead of multiple threads . rnn also uses fewer features , making it more useful for production use . we report the results in table 2 .
table 2 shows the part - of - speech changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . overall the rnn scores show that the number of occurrences have increased , decreased or stayed the same as a result of fine - tuning . however , the difference between the average number of instances for nouns and verbs is much smaller . it is clear from table 2 that there is no significant difference in the overall performance between the two sets .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment .
table 3 presents the results of the second study . results show that our approach outperforms the previous state - of - the - art approach by a noticeable margin . our approach confirms the value of selective attention . specifically , it improves the generalization ability of the sst - 2 and pubmed metrics by 3 - 4 points in the positive direction compared to previous work .
