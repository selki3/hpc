table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , the smaller number of instances in the training set makes it easier for the model to learn new tasks and inference to converge over time .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that the dropout probability dropout function is the most important factor in model performance for both ud v1 . 3 and conll08 . as shown in the second group of table 2 , selecting the correct filter size and the correct activation func . also helps the model to learn faster and accuracy .
table 1 shows that the shortest dependency path has the greatest effect on relation type for both relation types . the model - feature model significantly outperforms the macro - averaged model and the topic model in terms of f1 score . as shown in the table , the model with sdp has a significant advantage over the model without .
the results are shown in table 3 . the first group of results show that the accuracy of the r - f1 and f1 measures is relatively consistent across all models with the exception of the case of " y - 3 " . the second group show that accuracy is still significantly lower than the first group with respect to all models except " y " - 3 .
the results of paragraph accuracy are shown in table 3 . we show that mst - parser achieves an accuracy level of 100 % on average with 50 % of sentences in the correct sentence category and 50 % on the correct paragraph level .
as shown in table 4 , the average c - f1 score for the two indicated systems is 60 . 40 ± 13 . 57 % on the essay and paragraph level , respectively , compared to 56 . 24 ± 2 . 97 % and 62 . 74 ± 1 . 87 % for the paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference is less pronounced for the lstm - parser , which shows that the parser can learn the structure of sentences to a high degree .
the results are shown in table 3 . the original tgen model outperforms the original tgn + model when the training set is cleaned and the cleaned set is tested on the same set . as the results show , the difference in performance between the original and the cleaned tgen + model is less pronounced for the original model when compared to the cleaned model .
as shown in table 1 , the original e2e data and our cleaned version are comparable in terms of both number of distinct mrs and total number of textual references , as measured by our slot matching script , see section 3 . however , the difference in ser percentage between the original and the cleaned version is much larger ( 17 . 5 % vs . 11 . 5 % ) .
the results are shown in table 3 . the original tgen model outperforms both the original sc - lstm and the original tgn + model . as the results show , the difference between the two approaches is less pronounced for tgen + than for original , however , when trained and tested on the same dataset , the results are the same , adding the correct answer to the wrong question always improves the result for both sets . adding correct answers to wrong questions improves the accuracy of both sets , as shown in the second group of table 3 , adding correct answers only improves the results for the original model when compared to the original . replacing the wrong answer with the correct one always improves results for both groups .
table 4 shows the results of manual error analysis of our tgen model on a sample of 100 instances from the original test set . the errors we found in our model are divided into three categories : added , missed , and wrong values . tweaked instances tend to have higher error rates than original instances , as shown in fig . 4 .
the performance of our model compared to previous approaches on the hidden test set is presented in table 2 . graphlstm ( song et al . , 2018 ) achieves the best overall performance with an all score of 25 . 9 % and a joint all - achievement ( all ) of 28 . 2 % compared to the previous state - of - the - art on both the hidden and the external test set . while the joint all - learner achieves the highest overall score , the relative lower performance of the ensemble model is less pronounced than that of the single - headed model .
as shown in table 2 , our model achieves 24 . 5 bleu points on amr17 , a significant improvement over the previous state - of - the - art seq2seq model ( beck et al . , 2018 ) by a margin of 3 . 6 points . similarly , our ensemble model ( dcgcn ) achieves a significant performance improvement of 5 . 5 points over both the previous best state of the - art models .
table 3 presents the results for english - german , czech and slovak for both languages . the results are shown in bold . our model outperforms the previous state - of - the - art models on both languages with two tasks . as the results show , the difference between the performance of the single and multi - task models is mostly due to the smaller size of the training data set , which results in lower precision on the single task .
as shown in table 5 , the number of layers inside a dc network has a significant effect on the performance . for example , when we add 6 layers to our model , we get a performance gain of 2 . 5 bleu over the previous state of the art model .
table 6 shows that the rcn with residual connections outperforms the baselines in terms of bias metric b and c . the difference between rc and rc + la ( 2 ) is less pronounced for gcns without residual connections , however , for those that have connections , the difference is much larger , we find that the difference between bias metrics and rc metrics is much smaller for both gcns . gcns with rc and residual connections perform better than those without . as shown in table 6 , the average rc number of connections per gcn is 22 . 5 , while the average number of rc numbers per residual connection is 21 . 7 .
the results are shown in table 4 . the first group of results show that the small size of the training data does not impact the performance of the model , as the average number of iterations per generation is close to the size of training data , which means that the model is more than able to learn the task well enough to interpret the output .
as shown in table 8 , removing the dense connections in the i - th block of the dev set reduces the overall density of connections , but does not reduce the overall performance .
the results of an ablation study for modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that both approaches yield comparable output : the global node and the linear combination of the nodes have the worse performance , while the global combination has the better performance . as shown in the table , both approaches use coverage mechanisms to improve the decoding performance .
table 7 shows the performance of our initialization strategies on various probing tasks . glorot and topconst receive relatively high scores for initialization and tense metrics , indicating that their compactness and length - invariability give them a competitive advantage over other initialization strategies .
we find that the h - cmow model outperforms the smaller cbow / 400 and h - cbow variants on every metric by a significant margin . on the other hand , the smaller difference between the two sets is less pronounced for the somo model , which shows the advantage of finetuning the concatenation of input and output .
the results of cbow and cmow are shown in table 3 . hybrid method outperforms both cmp and cmp . the results show that cbow obtains a significant performance improvement over the monolingual method when trained and tested on the same dataset , i . e . cbow / 784 achieves 90 . 6 % improvement on average compared to the previous best state - of - the - art model , sick - r .
table 3 shows the relative improvements on the unsupervised downstream tasks that our models achieve with respect to hybrid . the cbow model shows a considerable performance gain over the cmow model over the strong baselines across all three datasets . hybrid also shows a relative performance improvement relative to the original approach . as shown in the second row of table 3 , when cbow is combined with cmow , it achieves a final score of 62 . 2 % on the sts13 and sts16 tasks , which shows that the model can learn the task to a high degree even under the difficult requirement of a low false positive rate .
table 8 shows the performance of our system for initialization and evaluation on the three supervised downstream tasks . glorot ( 86 . 6 % ) and sick - r ( 83 . 4 % ) both receive high scores for initialization , but are significantly lower than our system ( 87 . 6 % ) , indicating that the performance gain comes from a better understanding of the task at hand .
the performance of our method on the unsupervised downstream tasks is shown in table 6 . our cbow - r model significantly outperforms the cmow model in terms of both training objective and overall score .
the results are shown in table 3 . we find that the shorter attention span of the subsets gives a performance advantage over the larger ones . subjnum and topconst consistently outperform the cbow - r in all but one of the metrics , while the difference between the two is less pronounced in the case of the last group .
the results are shown in table 3 . the most striking thing about the cbow - r is that it achieves state - of - the - art results on all subtasks , outperforming the previous methods on only one out of the 10 sub - committees .
the results are shown in table 3 . name matching and named entity recognition have the best performance , while supervised learning has the worst performance . in general terms , the system performs better than the previous state - of - the - art on all loc and misc datasets . however , the difference between the two approaches is less pronounced when we consider the subtasks for loc and all misc , as shown in the second group of table 3 , fine - tuning the loc and the misc improves the performance for both systems over the strong baselines .
results on the test set under two settings are shown in table 2 . name matching and named entity recognition have the highest f1 scores , while supervised learning has the lower f1 score . the results of both settings show that the model with the best overall performance is the one using the best supervised learning scheme . however , the results are slightly less clear when using the τmil - nd ( model 2 ) with the same training set as the original model . it can be observed that the difference in f1 between the two sets is less pronounced when the model using the learned reward function is used in the final setup .
table 6 shows that the g2s - gat model significantly outperforms the previous state - of - the - art models in terms of both accuracy and ease of setup . moreover , the model significantly improves the generalization ability of the model with respect to both ref and concatenation . as shown in the table , the smaller size of the gat dataset ( micro - f1 ) and the smaller number of entries for each ref metric mean that it is easier for the model to converge to a final state .
the results are shown in table 3 . the first group of results show that the g2s model significantly outperforms the previous state - of - the - art models on all datasets except for konstas et al . ( 2015e86 ) . the second group show that , while the gat model performs slightly better than s2s on some of the datasets , it should not be dismissed as inferior because it has been trained on a larger corpus and has been tested on a significantly larger set of data . as the results show , the performance gap between the two sets is narrower with respect to the ldc2017t10 dataset , but still large enough to warrant further study .
results on the test set of ldc2015e86 are shown in table 3 . the models trained with additional gigaword data perform similarly to those trained with pre - g2s - ggnn data . however , the performance drop is less pronounced for the g2s model compared to the previous experiments . when the model is trained with external gigaword data , it achieves the best performance with a bleu score of 32 . 23 .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous state - of - the - art models with a bleu score of 62 . 42 / 59 . 37 and meteor score of 59 . 37 / 57 . 37 on the development set .
we find that the g2s model with the shortest average sentence length ( s2s - gin ) achieves a precision improvement of 1 . 51 % over the previous state - of - the - art model ( ggnn ) over the best previous model with a comparable number of sentences .
table 8 shows that gold significantly outperforms the s2s model and the g2s - gat model in terms of the fraction of elements in the output that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , the smaller size of the output indicates that gold has better interpretability .
table 4 shows the pos and sem accuracy for different target languages trained with different target sentences on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy improves as the number of sentences in the corpus grows , while sem accuracy decreases as the training size grows .
the pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 . word2tag significantly outperforms both mft and unsupemb in terms of most frequent tags and the upper bound of the baselines for sem tags .
table 4 presents the system ' s performance on the four metrics for the task at hand . the system performs well on all metrics with the exception of the pos tagging accuracy where it performs slightly worse than the previous best state - of - the - art .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . as the table shows , the bi - sem feature - values have higher precision than the res - and bi - nmt features for all languages except english , which shows the advantage of the nested structure of the encoder .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the gender and race features have the highest impact on the attacker score , while the age and sentiment have the lowest impact . gender and sentiment are the only two areas in which the attacker performs significantly worse than the corresponding adversary .
accuracies are shown in table 1 . our model outperforms the previous state - of - the - art on every metric by a significant margin . for example , gender and age accuracy are both close to the state - ofthe - art while the accuracy for sentiment accuracy is only slightly higher .
as can be seen in table 2 , there is a noticeable imbalance in the distribution of the data between balanced and unbalanced data splits , with the former having more accurate task accuracy and the latter having more inaccurate task accuracy . the gender and race - based features have the least and the age - based ones have the most accurate task .
the performance of our model on these datasets with an adversarial training set is shown in table 3 . as the table shows , the difference between the attacker score and the corresponding adversary ’ s accuracy is small but significant , indicating that the model can learn the task to a high degree even under the difficult requirement of a low accuracy baseline .
the results are shown in table 6 . the rnn encoders do not encode the protected attribute as well as the leaky one , so the accuracy of the encoded attribute is lower than the guarded one . however , the difference in the interpretability between the two approaches is less pronounced for the rnn .
the results are shown in table 2 . the first group of results show that the lstm has the advantage of training on large datasets with a minimum of training data and a maximum of 22m parameters , while the work of yang et al . ( 2018 ) has more than 50 , 000 parameters and performs considerably worse than the previous state of the art . we find that the size of the training set and the number of parameters used to fine - tune the model are the most important factors in the success of our model . the size and type of parameter sharing vary depending on the training data set , but generally speaking , the performance of the model is the same across all three sets .
the results are shown in table 2 . the first group of results show that the lstm is comparable to previous state - of - the - art models in terms of both time and accuracy . however , the difference in accuracy between the base and the final model is much larger , we find that the difference between the two sets is mostly due to the smaller size of the training set ( i . e . , between 250k and 1 . 36m bleu ) and the number of training instances ( e . g . , between 50 , 000 and 300 , 000 ) . table 2 also shows that the training time increases as a result of more training instances being added to the model during the development phase .
the results of zhang et al . ( 2015 ) are shown in table 2 . the first group of results show that the lstm model significantly outperforms the previous state - of - the - art in terms of both err and time . next , we find that the work performed by this model is comparable to the best previous state of the art on both yelp and amapolar time scales . however , the difference is less pronounced for yelp , as the number of entries in the dataset is much smaller .
table 3 shows the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task . our model obtains a case - in - the - infrared ( c - i - n ) score of 26 . 57 out of a possible 42 . 36 on the training set and a corresponding case - out - of - train score of 34 . 67 out of 50 . 40 on the evaluation set .
table 4 shows the exact match / f1 - score of our model on the squad dataset . the model with the greatest f1 score is rnet * ( 71 . 41 / [ bold ] 79 . 83 % on average compared to the previous best state - of - the - art model , lstm ( 75 . 45 / 78 . 41 % ) with elmo as parameter number 2 . 5 and sru ( 2 . 44 / 71 . 67 % ) as parameter numbers 3 . 5 . as shown in the table , the combination of parameter number of base and elmo significantly improves the f1 - scores of the model with respect to all parameter numbers except for the last parameter , which shows the diminishing returns from incorporating elmo into the model . finally , the number of parameter numbers of base increases with the growth of elmo parameter number , showing that the model can rely less on superficial cues .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . the number of parameter numbers in our model exceeds the number of parameters required to make a correct ner prediction . as shown in the table , the lstm model obtains the best performance with an f1 of 90 . 56 on the three stages of the task .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn model significantly outperforms elrn and glrn in terms of accuracy on both tasks .
table 3 presents the results for b - 2 , b - 4 and b - 5 . our system outperforms the previous state - of - the - art on every metric by a significant margin . for example , oracle retrieval achieves an r - 2 / r - 4 score of 23 . 38 / 71 . 55 and 7 . 97 / 7 . 55 on average , respectively , with an absolute improvement of 1 . 55 / 0 . 38 and 2 . 45 / 1 . 55 points over the previous best state of the art .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the accuracy obtained by our system is relatively high even under the difficult requirement of a low standard deviation . the second best result is achieved by candela ( 30 . 2 % ) on grammatical accuracy ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , which shows the extent to which our system can distinguish between syntactic and semantic information . the third best result by a large margin is obtained by retrieval ( 50 . 6 % ) on content richness , which shows that the retrieval process can learn a great deal about the semantic information contained in a sentence .
the results are shown in table 3 . table 3 shows that for the most part , our model outperforms the previous state - of - the - art on every metric by a significant margin . however , the difference is most prevalent for the ted talks dataset , where our model performs slightly better than both the previous best state - ofthe - art models .
the results are shown in table 1 . table 1 shows that for the most part , our model outperforms the previous state - of - the - art on every metric by a significant margin . for example , the en / pt score on the ted talks dataset ( ted talks ) is significantly higher than the previous best state of the art . on the other hand , the difference is less pronounced for the europarl dataset , which shows that our model performs better on a variety of test sets .
the results are shown in table 1 . table 1 shows that , for the most part , our model outperforms the previous state - of - the - art on every metric by a significant margin . however , the difference is most prevalent for the ted talks dataset , where our model performs slightly better than both the previous best state - ofthe - art models .
as shown in table 3 , the average depth of the roots for each relation is significantly lower than the average length of the relation with respect to both europarl and slqs . the difference between the average and maxdepth of roots is less pronounced for europarl , but still significant . as the table shows , the relative depth of roots and relation is the most important factor in the derivational performance of a relation extraction system . according to the table , the number of roots per relation is relatively consistent across all relation types , with the exception of docsub , which has a lower average depth .
as shown in table 1 , the average depth and the average number of roots for each relation are the most important metrics for the success of our system . europarl has the best overall performance with a weighted average of 9 . 29 % on both metrics . the difference between the average and maxdepth metrics is less pronounced for europarl , but larger for slqs and df . according to the table , the difference between maxdepth and average depth is more pronounced for df , which shows the difficulty of mixing relation types .
in table 1 , we compare the performance of our proposed approach against the baseline and enhanced version of visdial v1 . 0 . the enhanced version ( l lf ) shows a considerable performance improvement over the baseline model ( lf + p1 ) by 3 . 63 ndcg % in terms of question type , answer score sampling , and hidden dictionary learning , while the enhanced version shows a significant performance drop of 3 . 36 ndcg % . as the table 1 shows , the advantage of the p1 factor accruing over the weighted softmax loss is clear from the large improvement in the r0 , r1 , r2 , r3 scores of our approach over the strong baselines .
as shown in table 2 , the model with the best performance is the one that applies p2 ( i . e . , hidden dictionary learning ) the most effective . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the effectiveness of p2 indicates that the learned representations are more interpretable and interpretable than those without .
as shown in table 5 , the hmd - recall and hmd - f1 models perform similarly to the pre - trained models on both hard and soft alignments . however , the accuracy of the recall and hard alignments is higher than that of the soft alignment , indicating that the bert models are better at selecting the correct recall locations and the correct alignments for each event .
the results are shown in table 3 . the average score of bert score f1 and bertscore - f1 scores for each setting is reported in bold . compared to the meteor + + baseline , the sent - mover significantly outperforms the baselines and ruse ( * ) has the advantage of training on a larger corpus of data . as shown in the second group of table 3 , the set selection quality is also impacted by the type of setting used for the setting selection . for example , if the setting is de - en , then the average score for the given entity is likely to be lower than that for ru - en .
the results of bertscore - f1 are shown in table 2 . the system achieves the best results with an f1 score of 1 . 078 on the four out of the five metrics and a bleu - 2 score of 0 . 176 on the other metric . sent - mover outperforms the baselines on all metrics except for the inf / inf metric when using the w2v embeddings . when using the meteor - based baselines , the system achieves a lower f1 than the bert score but higher score than the other two methods .
the results are shown in table 3 . word - mover accuracy on the standard task set is set at 0 . 866 while the average number of correct answers is 0 . 939 . the difference in accuracy between the standard and the non - standard setting is less pronounced for the word - mover set , but still shows significant performance drop when using the additional features of elmo and bertscore - recall .
the results are shown in table 3 . para - para - lang improves the general performance for all models except for those using shen - 1 . the most interesting thing about this is that the average number of points per lexical category increased when the model was trained with both lexical and syntactic prefixes , i . e . m0 [ ital ] + cyc + para = 0 . 7 times as many points as when trained with lexical prefixes alone . in general terms , this means that more points are added for each lexical entity than are subtracted for syntactic ones .
the results are shown in table 2 . the transfer quality and transfer quality of the models a and b closely match the best state - of - the - art results of experiment 1 . semantic preservation and semantic preservation are the only two aspects of the model that are significantly worse than the others . we find that the semantic preservation results are markedly better than the transfer quality , indicating that semantic preservation is more important than semantic preservation for the task at hand . finally , the difference in transfer quality between yelp and the other two models is less pronounced than that between semantic and syntactic preservation . as shown in the second group of table 2 , semantic preservation and precision are the most important aspects of model performance .
table 5 shows the results of human sentence - level validation for each metric for each dataset . our approach verifies the accuracy of the summaries with 94 % accuracy and 84 % accuracy on average .
the results are shown in table 3 . para - para models outperform the model without it , the first group of results show that the syntactic base of the model can be further improved with the help of syntactic and syntactic distance augmentation . the second group show that it is possible to further improve the model with syntactic or semantic augmentation using the lexical features of the pre - trained model , but this does not improve the general performance .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc with the same number of tokens , and their accuracy is higher than the accuracy achieved by our simple transfer model . however , our best model ( yang2018 ) achieves higher acc than the best previous work with similar numbers of tokens and human references . the difference in acc between transfer and untransferred sentences is less pronounced for our model , but the accuracy is still high .
as shown in table 2 , the number of reparandum tokens that were correctly predicted as disfluent decreases as the repetition length increases , which implies that the disfluency generated by the repetition tokens is less pronounced than those generated by rephrase tokens .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . as shown in table 3 , there is a significant imbalance in the distribution of tokens that belong to each category , with more than 50 % of tokens in each category predicted to belong to the disfluency category . however , for those that are in the repair category , the picture is less clear .
the results are shown in table 2 . the first group of results show that the innovations - rich text model perform best while the text - rich innovations model performs best on the test set . as the results show , the combination of innovations and text improves the generalization ability of the model over the single - sentiment approach . however , the biggest gains are obtained during the late stages of development when the innovations are added to the model .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embeddings achieve high accuracy ( 83 . 43 % on average ) and surpass the performance of self - attention sentences ( 63 . 63 % ) in terms of accuracy on the discuss and unrelated tasks . rnn - based sentences achieve accuracy ( 71 . 43 % ) and accuracy ( 69 . 53 % ) on the agree and disagree tasks , respectively , which shows that the learned representations are more interpretable and therefore easier to reason about . the accuracy of unrelated tasks is higher than the learned ones , indicating that there is a need to design more complicated sentences and to refine the model for future research .
as shown in table 2 , the unified model significantly outperforms all previous methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the accuracy of our model significantly improves over the previous state - of - the - art methods .
as shown in table 3 , both word attention and graph attention have high accuracy ( with and without attention ) for this task , making them comparable in the accuracy to the effectiveness of word attention alone . however , the accuracy drop of neuraldater when using graph attention is less pronounced than that for word attention .
the results are shown in table 3 . embedding + t improves the generalization ability of all models , but does not improve the performance on the 1 / 1 and 1 / n tasks . the argument argument reduces performance on both cnn and jrnn , but helps the model to improve on the task on the larger scale . trigger and argument are the only two that do not improve significantly over the previous state of the art . as shown in the second group of table 3 , the argument is the most difficult part to solve , since it requires a lot of data and leads to incorrect answers .
table 3 presents the results for event identification and event classification . the system performs well across all aspects with the exception of event identification . as shown in the table , the method identification and the role - role identification are the most important factors in event classification performance . in general , all methods show lower error on event identification than the previous state of the art . however , the difference is less pronounced for argument identification and role identification .
the results are shown in table 3 . the first group shows that fine - tuned models perform better than the original ones on both dev perp and test wer when compared to both english - only and spanish - only language modes . next , the results show that , when using both lexical and syntactic word embeddings , the accuracy of the final model decreases as compared to the original model . as the results of tuning the model decreases over time , performance of all models decreases as well .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the original cs - trained model with a large margin . fine - tuning reduces overfitting and upsampling of the training data to improve the generalization ability .
as shown in table 5 , fine - tuned - disc improves the accuracy for both the dev set and the test set , and for both sets when the gold sentences are in the gold sentence set . fine - tuned - disc also shows a drop in accuracy compared to the original setup , but the difference is narrower .
we show the precision ( p ) and recall ( r ) on the conll - 2003 dataset for using type - aggregated gaze features for training on all three eye - tracking datasets , and test the model on the corresponding data for evaluation in table 7 . the improvement in precision and recall over the baseline model is statistically significant ( t - test , p < 0 . 01 ) with a f1 - score of 1 . 61 , which indicates a significant improvement in the model ' s performance .
we show the precision ( p ) and recall ( r ) for using type - aggregated gaze features for the conll - 2003 dataset as well as the f1 - score ( f1 ) in table 5 . the improvements in precision and recall are statistically significant ( t - test , p < 0 . 01 ) and r = 0 . 03 , both for the baseline and the type combined set , respectively .
the results on belinkov2014exploring ’ s test set are shown in table 1 . the hpcd and glove - extended embeddings obtain by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 outperform the ontolstm - pp model with a large margin . however , the difference between the two approaches is less pronounced for the lstm model , which shows that the semantic information injected into the pre - trained wordnet can be further improved with a reasonable amount of effort .
results shown in table 2 show that the hpcd - based uas outperforms the ontolstm - pp model with features coming from various pp attachment predictors and oracle attachments . further , the accuracy of the uas increases as the dependency parser learns to predict more pp features .
as shown in table 3 , the effect of removing sense priors and context sensitivity ( attention ) have a significant ( p < 0 . 01 ) effect on the accuracy of our model .
as can be seen in table 2 , incorporating subtitle data and domain tuning improves the bleu % scores for both en - de and multi30k models , and improves the overall results for en - fr and mscoco17 . as marian amun ( 2017 ) and co - workers ( 2018 ) note , the smaller size of the dataset also affects the results , since the multi - domain tuning reduces the translation noise caused by noisy noisy noisy echoes .
the results of domain - tuned h + ms - coco are shown in table 3 . as can be seen , the model with the best overall performance is the one with the most domain - aware features , namely , the h + hoco model in en - de and en - fr .
table 4 shows the bleu scores of en - de , en - fr and mscoco17 models for both languages for the task at hand . adding automatic image captions improves the general performance for all models except for those using marian amun ( cf . table 4 ) . the smaller size of the dataset ( micro - f1 ) and the number of frames in multi - task setups ( micro30k ) does not improve performance for any model .
the results in table 5 show that enc - gate and dec - gate strategies achieve comparable or better bleu % scores than en - de and mscoco17 ( + dec - gate ) . encoding visual information with the gate - disambiguation technique achieves the best overall performance , while dec - gating visual information achieves the worst performance . the en - fr and en - dian strategies achieve the best bbleu % and w - score , respectively , with an absolute improvement of 2 . 38 points over the previous state - of - the - art .
we find that the multi - lingual approach outperforms the text - only approach by a large margin . for example , the ensemble - of - 3 approach by subs3m ( gill2048 ) and subs6m ( mscoco17 ) achieves a final accuracy of 69 . 45 / 71 . 45 and 62 . 71 / 59 . 86 respectively , while the combined approach by en - de and en - fr ( fancellu et al . , 2017 ) achieves an accuracy of 68 . 36 / 62 . 71 and 58 . 59 / 55 . 45 respectively .
the results are shown in table 3 . as can be seen , en - fr - ht and en - es - ht both receive significantly better performance than the tf - based models on the yule ' s i and ii test sets . however , the difference between the two approaches is less pronounced for the mtld test set , as the results show , the transition quality between tf and tf based models can vary considerably depending on the translation context . the tf - backed model performs best on the i / i test set and the ttr test set . it is clear from the results that the semantic information injected into the model during the translation process has a significant impact on the final output .
as shown in table 1 , the number of parallel sentences in the train , test and development splits for the language pairs we used is 1 , 467 , 489 , 499 , 487 and 7 , 723 , respectively , with a total of 5 , 734 sentences per language pair .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the average number of words per training vocabulary is 113 , 132 for english , and 131 , 104 for french , while the average number is 169 , 195 for spanish .
automatic evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - smt - rev systems receive significantly better evaluation scores than the tf - based systems . as the table 5 shows , the rephrase quality of the original sentences is relatively the same across all systems , but the performance on the transformers is significantly worse .
results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the audiovisual model from the previous study ( chan et al . , 2017 ) . the mean mfcc rank of the vgs model is 0 . 2 , which means that the model has a low recall @ 10 and a median rank of 17 . 0 .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . it achieves the highest recall @ 10 and median rank , while the row labeled u denotes the acoustic supervised model . the acoustic embeddings based on audio2vec - u show lower recall , but higher chance to make a correct prediction . as shown in the second group of table 1 , the relative importance of recall drops as the recall increases for both acoustic and synthetically generated sentences .
we report further examples in the appendix . the most interesting ones are shown in table 1 . as the table shows , the rnn turns in a screenplay that is easier to hate than the original because the edges are sharper and the shapes are more pronounced , making it easier to pick out the parts of the screenplay that the user wants to hate . however , the cnn classifier is still more accurate , since it turns on a on ( the edges ) and the part of the screen with the worst performance is at the edges . it can also be seen that the dan classifier has better performance than rnn on sst - 2 .
table 2 shows that the number of occurrences have increased , decreased or stayed the same through fine - tuning of the original sentence in sst - 2 . as the table shows , the most interesting part - of - speech changes are those for nouns and verbs , which have the highest percentage of occurrences . however , for adjectives and verbs the picture is less clear , tweaking these two parts of the speech has little effect on the overall performance . the rnn model outperforms both cnn and dan with a large margin .
the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . as the table 3 shows , the flipped labels have a significant effect on sentiment as well as rnn score .
the results of the second study are shown in table 2 . as can be seen , the difference in accuracy between pubmed and sst - 2 is less pronounced for positive and negative evaluations , with the former achieving 98 % accuracy and the latter achieving 99 % .
