< extra_id_0 > training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c >
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representation .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] best f1 ( in 5 - fold ] best f1 ( in 5 - fold ] best f1 ( in 5 - fold ] best f1 ( in 5 - fold
< extra_id_0 > c - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > 67 . 69 c > 67 . 69 c > 67 . 69 c > 67 . 69 c > 67 . 69 c > y
< extra_id_0 > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > essay level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > lstm - parser c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > lstm - parser c > ( 100 % ) in % for the two indicated systems ; essay
< extra_id_0 > bleu c > [ bold ] train c > [ bold ] test c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] train c > [ bold ] add c >
< extra_id_0 > mrs c > [ bold ] part c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c > [ bold ] refs c
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > c > [ bleu c > [ bleu c > [ bleu c > [ bleu ] c > [ bleu c > [ bleu ] c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) .
< extra_id_0 > 23 . 3 . graphlstm ( song et al . , 2018 ) has a higher all score than snrg ( song et al . , 2018 ) and tree2seqk ( song et al . , 2018 ) with a higher all score than snrg ( song et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) with a higher all score than snrg ( song et al . , 2018 ) with a higher all score is higher than tsp ( song et al . , 2018 ) and gcnseq ( song et al . , 2018 ) with a higher all score of 24 . 4 points , respectively .
< extra_id_0 > the model size in terms of parameters . gcnseq achieves 24 . 5 bleu points . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points .
< extra_id_0 > english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ b
< extra_id_0 > 2 c > 1 c > 2 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 51 . 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c > 3 c
< extra_id_0 > + rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc .
< extra_id_0 > 20 . 9m ) and dcgcn ( 2 ) c > 180 c > 10 . 9m ) and dcgcn ( 3 ) c > 180 c > 11 . 3m ) and dcgcn ( 4 ) c > 180 c > 11 . 3m ) and dcgcn ( 3 ) c > 180 c > 11 . 3m ) and dcgcn ( 3 ) c > 180 c > 11 . 3m , respectively , and dcgcn ( 4 ) c > 11 . 4m , respectively . however , dcgcn ( 4 ) c > 12 . 8m ( bold ) respectively .
< extra_id_0 > table 8 : ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections .
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder ( table 9 : ablation study for modules used in the graph encoder and the lstm decoder ) compared to the bold model ( table 9 : ablation study for modules used in the graph encoder and the lstm decoder ) .
< extra_id_0 > c > initialization c > depth c > objnum c > length c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > subjnum c > tense c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c >
< extra_id_0 > c > sts14 c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > cmp . c > cmp . cmp . cmp . cmp . cmp . cmp .
< extra_id_0 > mr , mpqa , mpqa and mrpc c > initialization c > sick - e c > sick - b c > sick - r c > sick - r c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 61 . 7 c > [ bold ] 61 . 7 c > [ bold ] 61 . 7 c > [ bold ] 61 . 7 c > [ bold ] 61 . 7 c > c > c > c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > 81 . 1 c > tense c > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > wc c > topconst
< extra_id_0 > mrpc c > mpqa c > mrpc c > mrpc c > sick - e c > sick - b c > sick - r c > sick - r c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ b
< extra_id_0 > all loc c > all org c > all misc c > in [ italic ] e + loc c > all org c > all misc c > in [ italic ] e + org c > all misc c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + r c > in [ italic ] e + f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > bold > ent / bold > c > ref gen con / bold > neu / bold > c > ref gen con / bold > neu / bold > c > c > s2s c > 73 . 47 c > 76 . 46 c > 76 . 46 c > 76 . 46 c > 76 . 46 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold >
< extra_id_0 > bold > model / bold > bold > bleu / bold > c > song et al . ( 2018 ) c > 200k c > 31 . 60 c > guo et al . ( 2019 ) c > 200k c > 31 . 60 c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > meteor / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bils
< extra_id_0 > 7 - 13
< extra_id_0 > bold > added / bold > bold > miss / bold > c > c > c > c > c > c > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . bi and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders is shown in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > gender c > 9 . 7 cap > attacker ’ s performance on different datasets . attacker ’ s performance on different datasets is shown in table 8 .
< extra_id_0 > c > task c > accuracy c > sentiment c > 67 . 4 c > [ empty ] c > [ italic ] gender c > 67 . 4 c > [ empty ] c > [ italic ] gender c > 67 . 4 c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ]
< extra_id_0 > table 2 : protected attribute leakage : balanced & unbalanced data splits . in pan16 , we see a significant difference between a balanced task acc and unbalanced task acc .
< extra_id_0 > task acc c > protected attribute c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > guarded c > 59 . 3 c > leaky c >
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) . yang et al . ( 2018
< extra_id_0 > base acc c > + bert acc c > + bert time c > + ln acc c > + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c >
< extra_id_0 > amapolar time c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > bleu score on wmt14 english - german translation task . train : time in seconds per training batch measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence measured on newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of elmo . “ # params ” : the parameter number of base . rnet * : results published by wang et al . ( 2017 ) .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in ner task . “ # params ” : the parameter number in ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test perplexity on snli task with base + ln setting . ptb task with base + ln setting and snli task with base + ln setting .
< extra_id_0 > retrieval [ bold ] r - 2 and r - 2 are the best performing mtr compared to the system retrieval [ bold ] r - 2 compared to the system retrieval [ bold ] r - 2 compared to the system retrieval [ bold ] r - 2 compared to the system retrieval [ bold ] r - 2 compared to the system retrieval [ bold ] r - 2 compared to the system retrieval [ bold ] mtr compared to human c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the best results among automatic systems are highlighted in bold .
< extra_id_0 > slqs and dfqs . hclust and dfqs perform better than dfqs and dfqs and dfqs . hclust performs better than dfqs and docsub .
< extra_id_0 > slqs c > docsub c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > slqs c > docsub c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > dsim and slqs . dsim and slqs perform better than hlqs and hlqs , respectively . hlqs and hlqs perform better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs . hlqs performs better than hlqs and hlqs .
< extra_id_0 > dsim and slqs , respectively . dsim and slqs perform better than hlqs and hlqs , respectively . hlqs and hlqs perform better than hlqs and hlqs , respectively .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 is shown in table 1 .
< extra_id_0 > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c >
< extra_id_0 > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > lv - en c > bold > direct assessment / bold > fi - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > bold > direct assessment / bold > baselines c > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold
< extra_id_0 > inf . bold > nat / bold > bold > qual / bold > bold > nat / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > bold > nat / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > /
< extra_id_0 > m2 and m1 compared to m1 and m2 compared to baselines based on spice and bertscore - recall ( * ) compared to baselines based on spice and bertscore - recall ( * ) compared to baselines based on spice and bertscore - recall ( * ) compared to baselines based on spice and bertscore - recall ( * ) compared to baselines based on spice and bertscore - recall ( * ) compared to baselines based on baselines based on spice ( * ) .
< extra_id_0 > 22 . 3 c > 8 . 81 c > gm c > acc c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m2 : m0 c >
< extra_id_0 > transfer quality a > b c > semantic preservation b > a c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c >
< extra_id_0 > c > metric c > acc c > % of machine and human judgments that match c > 94 c > 84 c > acc c > % of machine and human judgments that match c > 0 . 79 c > 0 . 67 c > spearman ’ s [ italic ] and spearman ’ s [ italic ] b / w negative pp and human ratings of fluency c >
< extra_id_0 > - < extra_id_1 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . bleu is between 1000 transferred sentences and human references , and bleu is between 1000 transferred sentences and human references . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 6 - 8 c > overall c > 0 . 39 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > nested disfluent rephrase c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 . 69 c > 0 .
< extra_id_0 > reparandum length [ bold ] 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > content - content c > 0 . 61 ( 30 % ) c > 0 . 58 ( 52 % ) c > 0 . 80 ( 32 % ) c
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c >
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding c > 12 . 43 c > 01 . 30 c > 53 . 24 c > 79 . 53 c > 81 . 72 c > rnn -
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empt
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] classification ( % ) c > [ italic ] classification ( % ) c > [ italic ] classification ( % ) c > [ italic ] classification ( % ) c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c
< extra_id_0 > wer
< extra_id_0 > 50 % train dev c > 50 % train test c > 50 % train dev c > 50 % train test c > 75 % train dev c > 75 % train test c > cs - only c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > cs - only c > c >
< extra_id_0 > table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . on the dev set and on the test set , the accuracy on the dev set and on the test set is similar to that on the test set .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > table 5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset ( p , recall , f1 - score , f1 - score , f1 - score , f1 - score , f1 - score , f1 - score , p , recall , recall , recall , recall , recall , recall , recall , recall , recall , recall , recall , recall , recall , recall and f1 - score for conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . syntactic skipgram performs better than glove - retro and glove - extended , respectively .
< extra_id_0 > c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > rbg + hpcd ( full ) c > 94 . 17 c > 88 . 51 c > rbg + ontolstm - pp c > 94 . 60 c > 98 . 97 c > rbg + hpcd ( full
< extra_id_0 > c > [ bold ] ppa acc . c > [ bold ] ppa acc . c > [ bold ] ppa acc . c > - sense priors and context sensitivity ( attention ) from the model .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > mscoco17 and en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c >
< extra_id_0 > autocap ( dual attn . ) and autocap 1 - 5 ( concat ) compared to en - fr and mscoco17 , respectively . adding automatic image captions with autocap ( dual attn . ) and autocap ( dual attn . ) compared to en - fr and mscoco17 and mscoco17 .
< extra_id_0 > mscoco17 c > flickr16 c > flickr17 c > mscoco17 c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > dec - gate
< extra_id_0 > mscoco17 c > en - fr c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > < extra_id_1 > > yule ’ s i c > ttr c > mtld c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c >
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > table 2 shows the training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the bleu and ter scores ( en - fr - rnn - rev and en - fr - trans - rev ) are shown in table 5 .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > vgs c > 15 c >
< extra_id_0 > recall @ 10 ( % ) c > median rank c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate hate . we report further examples
< extra_id_0 > 3 cnn / bold > 3 cnn / bold > 3 cnn / bold > 3 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 3 cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > c
< extra_id_0 > bold > rnn / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold >
< extra_id_0 > bold > sst - 2 / bold > positive bold > sst - 2 / bold > negative bold > pubmed / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi /
