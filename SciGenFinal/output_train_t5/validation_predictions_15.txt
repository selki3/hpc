< extra_id_0 > training c > throughput ( instances / s ) inference c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c >
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] c > c > c > [ bold c > [ bold c > [ bold c > [ bold ] with sdp c > [ bold ] with sdp c > [ bold c > [ bold ] with sdp c > c > [ bold ] with sdp c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c - f1 100 % c - f1 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c
< extra_id_0 > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > lstm - parser c > lstm - parser c > essay c > 64 . 741 . 97 c > 56 . 242 . 87 cap > table 4 : c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< extra_id_0 > bleu c > bleu c > bleu c > bleu c > bleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c
< extra_id_0 > cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . cap > table 1 : data statistics comparison for the original data and our cleaned version .
< extra_id_0 > bleu c > nist c > [ bold ] system c > [ bold ] train c > [ bleu c > [ bleu ] c > [ bold ] train c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > [ bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > 23 . 3 r > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) c > 22 . 4 r > graphlstm ( song et al . , 2018 ) c > 24 . 4 r > gcnseq ( damonte and cohen , 2019 ) c > 24 . 4 r > gcnseq ( song et al . , 2016 ) c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . ggnn2seq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 .
< extra_id_0 > english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] c > [ bold c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic n c >
< extra_id_0 > gcn + rc ( 2 ) has a b 16 . 8 c > c 48 . 1 c > [ bold ] gcn + rc ( 2 ) has a b 16 . 8 c > c 48 . 1 c > [ bold ] gcn + rc + la ( 2 ) has a b 18 . 3 c > c 47 . 9 c > [ bold ] 52 . 9 c > [ bold ] 52 . 9 c > [ bold ] 52 . 9 c > + rc ( 9 ) and + rc
< extra_id_0 > 52 . 0m compared to dcgcn ( 1 ) and dcgcn ( 2 ) . compared to dcgcn ( 3 ) and dcgcn ( 4 ) , the model performs better than dcgcn ( 4 ) and dcgcn ( 4 ) . the model performs better than dcgcn ( 4 ) and dcgcn ( 4 ) .
< extra_id_0 > [ bold ] model c > b c > c r > c > - 4 dense blocks c > 25 . 5 c > 55 . 4 c > - 3 , 4 dense blocks c > 23 . 8 c > 53 . 1 c > - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm decoder performs better than the encoder and the lstm encoder ( table 9 ) .
< extra_id_0 > topconst c > initialization c > depth c > objnum c > length c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > mrpc and mrpc have the best results . mrpc and mrpc have the best results . mrpc and mrpc have the best results . mrpc and mrpc have the best results .
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > cmp . c > cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp .
< extra_id_0 > mr , mpqa , mpqa , mpqa , mpqa , mpqa , mrpc , mrpc and glorot . our paper c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ cbow - c ] c > c > c > [ bold ] c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [
< extra_id_0 > 78 . 7 c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mrpc c > mpqa c > mrpc c > sick - e c > sick - b c > sick - r c > sick - e c > sick - e c > sick - b c > sick - r c > sick - r c > sick - e c > sick - e c > sick - r c > sick - r
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > all org c > all misc c > all misc c > all loc c > all org c > all org c > all misc c > all misc c > 96 . 26 c > 89 . 48 c > all org c >
< extra_id_0 > all p c > 35 . 87 c > 35 . 87 c > 35 . 87 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > gen ref gen con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bol
< extra_id_0 > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > bold > model / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > bold > bleu / bold > c > bold > size / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > bold > added / bold > and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . bi and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > mention c > gender c > 9 . 7 cap > attacker ’ s performance on different datasets . results are on a training set 10 % held - out .
< extra_id_0 > c > task c > accuracy c > sentiment c > 67 . 4 c > [ empty ] c > mention c > 67 . 4 c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 2 : protected attribute leakage : balanced & unbalanced data splits . mention c > 67 . 4 c > 64 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 8 c > 59 . 4 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c >
< extra_id_0 > task acc c > leakage c > 5 . 0 on different datasets with an adversarial training . is the difference between the attacker score and the corresponding adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ cap > table 6 : accuracies of the protected attribute with different encoders
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) .
< extra_id_0 > + bert acc c > + ln acc c > + bert time c > + ln time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > amapolar time c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > decode c > decode c > decode c > decode c > decode c >
< extra_id_0 > c > base c > + elmo c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > -
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” : the parameter number in conll - 2003 english ner task .
< extra_id_0 > c > snli with base + ln setting and test perplexity on ptb task with base + ln setting . c > elrn c > 83 . 56 c > 169 . 81 cap > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting .
< extra_id_0 > r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] w / system retrieval [ bold ] c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > r - 2 w / system retrieval [ bold ] w / system retrieval [ bold ] w / system retrieval [ bold ] # sent c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > r - 2 w / r - 2 w / r - 2 w / r - 2 w / r - 2 w / r - 2 w / r - 2 w / r - 2 c > 66 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > 22 c > r - 2 c > r - 2 c > r - 2 c > r - 2
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the best result among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > slqs and docsub . hclust and dlqs perform better than lang c > dsim c > dsim c > docsub c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dsim c > dlqs c >
< extra_id_0 > slqs and docsub . hclust and dlqs perform better than lang c > and patt c > and tlqs c > and tlqs c > and tlqs c > .
< extra_id_0 > tf c > df c > docsub c > hlqs c > hlqs c > hlqs c > hlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs
< extra_id_0 > slqs and patt dsim perform better than hlqs and hlqs . hlqs performs better than hlqs and hlqs , but europarl performs better than hlqs and hlqs . hlqs performs better than hlqs and hlqs .
< extra_id_0 > slqs and dlqs , respectively . hlqs and dlqs have the highest avgdepth , while europarl has the lowest avgdepth . hlqs has the lowest avgdepth , whereas europarl has the lowest avgdepth .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . qt is the baseline , answer score sampling , and hidden dictionary learning , respectively . lf is the enhanced version as we mentioned in table 1 .
< extra_id_0 > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > fi - en c > ruse c > 0 . 750 c > 0 . 750 c > 0 . 750 c >
< extra_id_0 > lv - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 652 c > 0 . 646 c > c > c > c > c > c > c > c > c > c > c > c > / bold > c > c > c > / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > and sfhotel bold > qual / bold > bold > qual / bold > c > sfhotel bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . bertscore - recall ( * ) and spice ( * ) have m1 and m2 respectively . bertscore - recall ( * ) has m1 and m2 respectively . bertscore - recall has m1 and m2 respectively .
< extra_id_0 > outperforms sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c >
< extra_id_0 > transfer quality a > b c > semantic preservation b > a c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c >
< extra_id_0 > c > acc c > % of machine and human judgments that match c > 94 c > 84 c > human sentence - level validation of metrics ; 150 examples for sim and pp ; 150 for spearman ’ s [ italic ] b / w negative pp and human ratings of fluency c > 0 . 67 c > 0 . 67 ; see text for validation of gm .
< extra_id_0 > 37 . 3 c > 10 . 0 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . bleu is between 1000 transferred sentences and human references , and bleu is between 1000 transferred sentences and human references . our best models achieve higher bleu than previous work at similar levels of acc .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model c > 28 . 53 c > 65 . 43 c > 65 . 43 c > 83 . 54 c > [ bold ] c > [ bold ] c >
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details .
< extra_id_0 > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] all c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ em
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ italic ] p c > [ italic ] p c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [
< extra_id_0 > wer
< extra_id_0 > train dev c > 50 % train test c > 50 % train test c > 75 % train dev c > 75 % train test c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > c > c >
< extra_id_0 > dev cs vs . monolingual ( mono ) c > cs - only - disc c > 71 . 33 c > fine - tuned - disc vs . cs - only - disc c > 74 . 40 c > cs - only - disc vs . monolingual ( mono ) c > 62 . 80 c > cs - only - disc vs . cs - only - disc vs .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > conll - 2003 c > [ bold ] p , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) .
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . glove is a glove vector retrofitted by faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended refers to the embeddings obtained by running autoextension rothe and schütze ( 2015 ) on glove .
< extra_id_0 > c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > rbg + hpcd ( full ) c > 94 . 17 c > 89 . 59 c > rbg + oracle pp c >
< extra_id_0 > c > [ bold ] ppa acc . c > full c > 89 . 7 c > - sense priors c > 88 . 4 c > - attention c > 87 . 5 cap > effect of removing sense priors and context sensitivity ( attention ) from the model .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) have significantly higher performance than autocap 1 - 5 ( concat ) and autocap 1 - 5 ( dual attn . ) compared to en - fr and mscoco17 , respectively . adding automatic image captions with autocap 1 - 5 ( concat ) and autocap 1 - 5 ( dual attn . ) is significantly better than autocap ( dual attn . ) and autocap ( dual attn .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > 68 . 30 c > 61 . 38 c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > c > c > c > c > c > c > c > c
< extra_id_0 > mscoco17 and en - fr c > en - fr c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > < extra_id_1 > > yule ’ s i c > ttr c > mtld c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > en - fr - trans - ff c > en -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the train , test and development splits show the number of parallel sentences in the train , test and development splits .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . src c > 113 , 132 c > 131 , 104 c > trg c > 168 , 195 cap > training vocabularies for the english , french and spanish data used for our models .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems ( en - fr - rnn - rev and en - fr - smt - rev , respectively ) .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > median rank c > 0 . 0 c > rsaimage c > 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0
< extra_id_0 > c > [ empty ] c > recall @ 10 ( % ) c > rsaimage c > 0 . 5 c > chance c > 0 c > 3 , 955 c > 0 . 0 c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > turns in a u > screenplay that u > at the edges edges edges curves so clever easy want hate hate hate hate hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > 3 cnn / bold > and 3 cnn / bold > . cnn / bold > and 3 cnn / bold > . cnn / bold > and 3 cnn / bold > . cnn / bold > and 3 cnn / bold > . cnn / bold > . cnn / bold > cnn / bold >
< extra_id_0 > bold > rnn / bold > c > bold > dan / bold > c > c > positive c > + 20 . 2 c > + 24 . 9 c > + 28 . 6 c > + 19 . 3 c > c > c > c > flipped to negative c > + 31 . 5 c > + 29 . 3 c > c > cnn / bold >
< extra_id_0 > better than sst - 2 and pubmed / bold > . compared to sst - 2 and pubmed / bold > , it is better than sst - 2 and pubmed / bold > . compared to sst - 2 and pubmed / bold > , it is better than sst - 2 and sst - 2 . compared to pmi and pubmed / bold > , it is better compared to sst - 2 and sst - 2 . compared to pmi , it is better acc .
