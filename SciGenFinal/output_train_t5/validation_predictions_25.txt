< extra_id_0 > table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > activation func . c > [ bold ] hyper parameters l2 reg . c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ] hyper parameters learning rate c > [ bold ]
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) with sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp .
< extra_id_0 > r - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > y - 3 : y c > 67 . 89 c > 67 . 89 c > 67 . 89 c >
< extra_id_0 > c > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > [ parser ] c > [ empty ] c > [ parser ] c > 100 % in c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< extra_id_0 > bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu
< extra_id_0 > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu ] c > [ bleu c > [ bleu ] c > [ bleu c > [ bleu c > [ bleu ] c > [ bold ] c > [ bold ser c > [ bold ser c > [ bold ser c > [ bold ser c > [ bold ] [ bold ] [ bold ] [ bold ] [ bold ] [ bold [ bold [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold [ bold [ bold ] [ bold [ bold ] [ bold [ bold ] [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ bold ] [ cider [ b
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > 23 . 3 and gcnseq 24 . 4 respectively . graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) have the best performance . snrg has the best performance , but only slightly better performance than snrg ( song et al . , 2017 ) and gcnseq ( damonte and cohen , 2019 ) have the best performance . snrg has the best performance , but only slightly better performance than snrg ( pourdamghani et al . , 2016 ) .
< extra_id_0 > amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . ggnn2seq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . ggnn2seq ( damonte and cohen , 2019 ) achieves 24 . 5 points .
< extra_id_0 > english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > b c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > gcn + rc ( 2 ) has a b 16 . 8 and c 48 . 1 respectively . gcn + rc + la ( 2 ) has a b 16 . 8 and c 48 . 1 respectively . compared to baselines , gcn + rc ( 4 ) has a b 16 . 8 and c 47 . 9 respectively . compared to baselines , gcn + rc ( 4 ) and + rc + la ( 4 ) has a b 16 . 8 and c 47 . 9 respectively . compared to baselines , gcn + rc ( 4 ) has a better overall performance than dc
< extra_id_0 > d c > b c > c c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d >
< extra_id_0 > table 8 : ablation study for dense connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm encoder and the graph encoder are shown in table 9 . the lstm encoder and the encoder are shown in table 9 . the encoder and decoder are shown in table 9 .
< extra_id_0 > initialization c > depth c > objnum c > length c > coordinv c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > subjnum c > tense c > objnum c > topconst c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mpqa c > mpqa c > mrpc c > sick - e c > sick - r c > hybrid c > sick - r c > sick - r c > sick - r c > 79 . 2 c > 79 . 6 c > 79 . 6 c > 79 . 6 c > [ bold ] c > sick - r
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cmp .
< extra_id_0 > mpqa c > mpqa c > mpqa c > mpqa c > trec c > sick - e c > sst5 c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c >
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > c > [ bold - r c > [ bold - r c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > depth c > tense c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc
< extra_id_0 > mpqa c > mrpc c > trec c > sick - e c > sst5 c > sick - r c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cmow - r c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all org c > all misc c > mil - nd c > 57 . 15 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > in [ italic ] e + p c > all r c > all f1 c > all p c > all p c > all r c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > ref gen bold > ent / bold > c > gen ref gen bold > neu / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > > bold > meteor / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 .
< extra_id_0 > table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . konstas et al . ( 2017 ) c > 200k c > 27 . 40 c > song et al . ( 2018 ) c > 31 . 60 c > guo et al . ( 2018 ) c > 200k c > 31 . 60 c > c > c > g2s - ggnn c >
< extra_id_0 > bold > meteor / bold > c > bold > size / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > 0 - 7
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer is shown in table 4 .
< extra_id_0 > table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > zh c > zh c > zh c > en c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > c > c > c > c > c > c > pos tagging accuracy c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging c > pos tagging
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . uni / bidirectional / residual nmt encoders show the best pos and sem tagging accuracy .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > mention c > gender c > 9 . 7 on a training set 10 % held - out . is the difference between the attacker ’ s score and the corresponding
< extra_id_0 > data c > task c > accuracy c > sentiment c > 67 . 4 c > [ italic ] gender c > 67 . 4 c > [ empty ] c > [ italic ] gender c > 67 . 4 c > [ italic ] c > [ empty ] c > [ italic ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ italic ] c > [ italic ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > [ cap ] c > c > c > c > c > c > c > c > c > c > c > accuracies when training directly towards a single task c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mention c > sentiment c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > task acc c > 67 . 4 c > 64 . 5 c > 73 . 5 c > unbalanced leakage c > 73 . 8 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 7 c >
< extra_id_0 > data c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > gender c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 6 : accuracies of the protected attribute with different encoders . leaky achieves 64 . 5 c > 67 . 8 c > 67 . 8 c > 67 . 8 c > 67 . 8 c > 59 . 3 c > 54 . 8 c > 59 . 3 c > 54 . 8 c > 59 . 3 c > 54 . 8 .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm c > lstm
< extra_id_0 > + bert acc c > + ln acc c > + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > gru c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > amapolar time c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > zhang et al . ( 2015 ) c >
< extra_id_0 > model c > # params c > train c > decode c > train c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > decode c > decode c > decode c > decode c > train c > decode c > decode c > train c > decode c > decode c > decode c > decode c > decode c > decode c > bleu c > decode c > bleu c > c > c > bleu c > c > c > c > c > c > c > c > c > c > bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 4 : exact match / f1 - score on squad dataset . “ # params ” : the parameter number of base . “ rnet * ” : the results published by wang et al . ( 2017 ) .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . “ # params ” : the parameter number in ner task . lstm * denotes the reported f1 score .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test accuracy on snli task with ptb task with base + ln setting and test accuracy on snli task with ptb task with base + ln setting . elrn performed better than glrn and elrn , respectively .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > top - 1 / 2 : % of evaluations a system being ranked as a best . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test ) .
< extra_id_0 > tf c > dsim c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > c > c > c > c > clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust
< extra_id_0 > slqs c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > dflust c > hclust c > df c > df c > df c > df c > df c > hclust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > clust clust clust clust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > tf c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > clust clust clust clust clust clust c > c > c > c > c > c > c > c > c > c > c > c > clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust
< extra_id_0 > dsim c > hlqs c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > hclust c > europarl c > maxdepth : c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > dsim cluster clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust c
< extra_id_0 > dsim c > tlqs c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > hclust c > europarl c > maxdepth : c > 980 c > 1 , 000 c > 1 , 000 c > 1 , 000 c > 1 , 000 c > hlqs c > docsub c > docsub clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as mentioned in table 1 . the qt , s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . lf and p1 denote regressive loss , weighted softmax loss , and generalized ranking loss .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 .
< extra_id_0 > fi - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > de - en c > bold > direct assessment / bold > fi - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 672 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > c > c > c > c > c > c > / bold > c > c > c > c > c > bertscore - f1 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c > 0 . 646 c
< extra_id_0 > > nat / bold > c > sfhotel bold > qual / bold > c > sfhotel bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . < extra_id_1 > c > m1 c > m2 c > m2 c > m2 c > m1 c > m2 c > m2 c > m1 c > m2 c > m2 c > 0 . 809 c > 0 . 784 c > 0 . 784 c > 0 . 784 c > 0 . 789 c > 0 . 784 c >
< extra_id_0 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > 0 . 702 c > 49 . 9 c > 12 . 8 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > gm c > sim c > sim c > gm c > gm c > m5 : m0 [ italic ] + cyc + para c > 0 . 754 c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > transfer quality a > b and transfer quality tie c > semantic preservation a > b and transfer quality tie c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > table 5 summarizes the validation of yelp , spearman ’ s and spearman ’ s ( see text for validation of gm ) . acc is the percentage of machine and human judgments that match , with 94 % of machine and human judgments that match . spearman ’ s ( see text for validation of gm ) and spearman ’ s ( see text for validation of pp ) . spearman ’ s and spearman ’ s ( see text for validation of gm ) .
< extra_id_0 > pp c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > 0 . 818 c > 26 . 3 c > 19 . 2 r > gm c > m5 : m0 [ italic ] + cyc + para c > 19 . 2 r > c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than our best models ( see table 6 ) .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > rephrase c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 1 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ single c > text + raw + innovations c > 86 . 46 c > 86 . 47 c > 87 . 00 c > – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding c > 12 . 43 c > 01 . 30 c > 53 . 24 c > 81 . 72 c > [ bold ] 83 . 54 c > [ bold ] 83 . 54 c > [ bold ] 83 . 54 c > [ bold ] c >
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ bold ] embedding + t c > 69 . 8 c > 69 . 8 c > [ empty ] c > embedding + t c > 69 . 8 c > [ empty ] c > embedding + t c > 69 . 8 c > [ empty ] c > [ bold ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [
< extra_id_0 > trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ) c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold c > [ bold
< extra_id_0 > dev wer
< extra_id_0 > 50 % train dev c > 50 % train test c > full train test c > 75 % train dev c > cs - only c > cs - only c > cs - only c > 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c
< extra_id_0 > table 5 summarizes the results on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . on the dev set and on the test set , the accuracy of the fine - tuned - disc is higher than on the fine - tuned - disc .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > table 5 summarizes the performance of type - aggregated gaze features on the conll - 2003 dataset ( p , r , f1 - score , f1 - score , f1 - score , p , f1 - score , p , r , f1 - score , f1 - score , p , r , f1 - score , f1 - score , p , r , f1 - score , p , f1 - score , p , r , f1 scores for conll - 2003 dataset . type - aggregated gaze features on conll - 2003 dataset ( p , f , p , f , p , f , p , p , f , p , p , p , p , p , f , p , f , p , f , p , f , p , f , p , f , p , f , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , co
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . the glove - retro and glove - extended vectors achieve 88 . 7 on the syntactic - sg and lstm - pp datasets . ontolstm - pp embeddings achieve 89 . 7 on the wordnet and verbnet datasets .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . rbg + hpcd ( full ) achieves 94 . 17 and 88 . 51 points , respectively . ontolstm - pp achieves 98 . 97 points , respectively .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the ppa acc . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > subs1m [ italic ] [ italic ] lm + ms - coco c > 66 . 9 c > + labels c > c > c > c > c > en - de c > en - tuned c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - de c > multi30k c > 61 . 4 c >
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate
< extra_id_0 > mscoco17 and en - fr c > en - fr c > mscoco17 c > mscoco17 c > subs3m [ italic ] [ italic ] lm detectron c > 68 . 30 c > 62 . 45 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > 53 . 04 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > ttr c > mtld c > yule ’ s i c > mtld c > mtld c > en - fr - trans - ff c > en - fr - smt - back c > 1 . 0925 c > 1 . 0925 c > 121 . 5801 c > en - fr - trans - back c > 1 . 0925 c > en - fr - trans -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the train , test and development splits show the number of parallel sentences in the train , test and development splits for the language pairs we used .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev system has the highest bleu and ter scores ( en - fr - trans - rev has the highest bleu and ter scores ) .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > vgs c > 15 c > 0 . 2 c > mfcc c > 0 c > 0 . 0 c > mean mfcc c > 0 c > 711 c > 0 . 0 c > 0 . 0 c > vgs c > 0 c > 0 . 0 c > vgs c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > she turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > bold > rnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > c
< extra_id_0 > bold > rnn / bold > bold > dan / bold > table 3 shows the changes in sentiment score with respect to the original sentence and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence and vice versa .
< extra_id_0 > better than pubmed / bold > and bold > sst - 2 / bold > positive and bold > negative c > to evaluate c > conclusions r > r > r > sift / bold > and bold > sst - 2 / bold > and bold > sst - 2 / bold > r > r > r > acc c > c > c > c > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold
