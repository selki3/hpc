< extra_id_0 > training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c >
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > c > + 27 . 82 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > c - f1 100 % c > r - f1 50 % c > r - f1 100 % c > r - f1 50 % c > r - f1 100 % c > f1 50 % c > f1 50 % c > f1 50 % c > y - 3 c > 49 . 59 c > 34 . 35 c > 47 . 69 c > 47 . 69 c > 47 . 69 c > y - 3 c
< extra_id_0 > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > lstm - parser c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ parser ] c > [ empty ] c > [ empty ] c > [ empt
< extra_id_0 > bleu cider nist cider rouge - l cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider
< extra_id_0 > cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) .
< extra_id_0 > bleu c > nist c > [ bold ] system c > [ bold ] train c > [ bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bold ] original c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > 23 . 3 r > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) have a better performance than snrg ( song et al . , 2017 ) and gcnseq ( damonte and cohen , 2019 ) have a better performance than snrg ( song et al . , 2018 ) and gcnseq ( song et al . , 2018 ) have a better performance . graphlstm ( song et al . , 2018 ) have a better performance .
< extra_id_0 > gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . ggnn2seq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 .
< extra_id_0 > english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] c > [ italic ] n c >
< extra_id_0 > . < extra_id_1 > c > b 16 . 8 c > c 48 . 1 c > [ bold ] gcn + rc ( 2 ) c > b 16 . 8 c > [ bold ] + rc + la ( 2 ) c > c 47 . 9 c > [ bold ] 21 . 2 c > + rc + la ( 3 ) c > 21 . 2 c > + rc + la ( 3 ) c > 21 . 2 c > + rc + la ( 3 ) c > 21 . 2 c >
< extra_id_0 > 52 . 0m compared to dcgcn ( 1 ) and dcgcn ( 2 ) . compared to dcgcn ( 3 ) and dcgcn ( 4 ) , dcgcn ( 3 ) performs better than dcgcn ( 4 ) and dcgcn ( 4 ) , respectively .
< extra_id_0 > [ bold ] model c > b c > c r > c > - 4 dense blocks c > 25 . 5 c > 55 . 4 c > - 3 dense blocks c > 23 . 8 c > 53 . 1 c > - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i -
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm encoder and the dcgcn4 encoder perform better than the dcgcn4 encoder and the lstm decoder respectively .
< extra_id_0 > initialization c > depth c > objnum c > tense c > coordinv c > topconst c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > objnum c > tense c > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > 79 . 2 compared to mrpc and mrpc , respectively . sick - e and sick - b perform better than sick - e and sick - r , respectively . sick - r performs better than sick - e and sick - r .
< extra_id_0 > c > sts14 c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cmp .
< extra_id_0 > 6 % compared to mrpc and mrpc , respectively . mrpc and mpqa perform better than glorot and sick - e and sick - r , respectively . mrpc and mrpc perform better than glorot and sick - r , respectively .
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 7 c > [ bold ] 52 . 2 c > [ bold ] 53 . 2 c > [ bold ] 52 . 2 c > [ bold ] 53 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > c > c > [ bold - c > [ bold - c > [ bold - c > [ bold - c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold
< extra_id_0 > 78 . 7 c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > mrpc and mpqa mrpc mrpc mrpc mrpc mrpc mrpc c > sick - e c > sick - r c > sick - e c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > all org c > all misc c > all org c > all misc c > all misc c > 96 . 26 c > 89 . 46 c > [ bold ] 89 . 46 c > [ bold ] 89 . 46 c > [ bold ] c >
< extra_id_0 > in [ italic ] e + p c > all r c > all f1 c > all p c > 35 . 87 c > 35 . 87 c > 69 . 38 0 . 68 c > 69 . 38 c > 69 . 38 0 . 68 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 13 c >
< extra_id_0 > gen bold > ent / bold > c > gen ref gen bold > neu / bold > c > gen ref gen bold > neu / bold > c > s2s c > 73 . 45 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > 13 . 46 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > , bold > bleu / bold > , bold > meteor / bold > , bold > bleu / bold > , bold > bleu / bold > , bold > g2s / bold > , bold > g2s / bold > , bold > g2s / bold > , bold > 25 . 70 , bold > 29 . 90 , bold >
< extra_id_0 > bold > model / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > bold > meteor / bold > bold > size / bold > bold > size / bold > bold > size / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > bold > added / bold > and the fraction of elements in the input graph that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . table 4 : sem and pos tagging accuracy using features extracted from the nmt encoding layer .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > zh c > zh c > zh c > zh c > zh c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c >
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . table 5 shows that bi and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages , is significantly higher than uni and bi .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > mention c > gender c > 9 . 7 cap > attacker ’ s performance on different datasets . results are on a training set 10 % held - out . cap > attacker ’ s performance on different datasets .
< extra_id_0 > task c > accuracy c > 67 . 4 r > c > mention c > 67 . 4 r > [ empty ] c > mention c > 67 . 4 r > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > data c > dial c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment
< extra_id_0 > data c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ rnn ] c > guarded
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018
< extra_id_0 > + bert acc c > + ln time c > + bert time c > + ln time c > + bert time c > + ln time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yahoo err c > yahoo time c > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > train c > decode c > train c > train c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > train c > decode c > decode c > decode c > train c >
< extra_id_0 > model c > + elmo c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - -
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number in ner task . lstm * denotes the reported result .
< extra_id_0 > table 7 : test accuracy on snli and ptb task with base + ln setting and test perplexity on ptb task with base + ln setting . elrn and glrn performed better than glrn and elrn , respectively .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 sent c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > 66 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > top - 1 / 2 : % of evaluations a system being a grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) on a scale of 1 to 5 ( best ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the best result among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 .
< extra_id_0 > 0443 and 0 . 0761 respectively . hclust and dlqs perform better than lang c > corpus c > docsub c > docsub c > hlqs c > hlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > c >
< extra_id_0 > slqs and docsub . hclust and docsub perform better than lang c > dsim c > df c > docsub c > hlqs c > hlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlqs c > tlq
< extra_id_0 > 0490 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661
< extra_id_0 > slqs and docsub perform better than the corpus tlqs and docsub tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tlqs tl
< extra_id_0 > slqs and docsub perform better than the corpus c > dsim c > hlqs c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c >
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . lf is the baseline version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set .
< extra_id_0 > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > cs - en c > cs - en c > cs - en c > lv - en c > lv - en c > c > c > c > c > c > c >
< extra_id_0 > de - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 552 c > 0 . 538 c > 0 . 646 c > 0 . 646 c > 0 . 646
< extra_id_0 > . < extra_id_1 > > nat / bold > c > sfhotel bold > qual / bold > c > sfhotel bold > qual / bold > c > sfhotel bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . < extra_id_1 > compared to m1 and m2 and m2 compared to m1 and m2 with leic ( * ) and spice ( * ) achieving 0 . 939 0 . 949 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 759 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 0 . 749 .
< extra_id_0 > 22 . 3 c > 8 . 81 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c >
< extra_id_0 > transfer quality a > b c > transfer quality tie c > semantic preservation a > b c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c >
< extra_id_0 > c > acc c > % of machine and human judgments that match c > 94 c > 84 c > human sentence - level validation of metrics ; 150 examples for sim and pp ; 150 for spearman ’ s [ italic ] b / w negative pp and human ratings of fluency c > 0 . 67 ; see text for validation of gm .
< extra_id_0 > 37 . 3 c > 10 . 0 r > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + para 0 . 7
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . bleu is between 1000 transferred sentences and human references , and bleu is between 1000 transferred sentences and human references . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > reparandum length [ bold ] 3 - 5 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ bold ] dev mean c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > early c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model c > 24 . 53 c > 05 . 06 c > 79 . 53 c > 81 . 72 c > [ bold ] 83 . 54 c >
< extra_id_0 > burstysimdater c > 38 . 5 c > 42 . 5 c > accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] c > [ empty ] c > [ empty ] c > [ bold ] c > [ bold ] c > [ empty ] c > [ bold ] c > [ bold ] c > [ empty ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ empty ] c > [ empty ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ italic ] p c > [ italic ] p c > [ italic ] f1 c > [ italic ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ) c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold
< extra_id_0 > perp test wer
< extra_id_0 > train dev c > 50 % train test c > 50 % train test c > 75 % train dev c > full train test c > 75 % train dev c > 75 % train test c > cs - only c > 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] c >
< extra_id_0 > dev cs vs . monolingual ( mono ) set , according to the type of gold sentence in the set : code - switched ( cs ) vs . fine - tuned - disc ( lm ) . on the dev set and on the test set , according to the type of gold sentence in the set : cs - only ( lm ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > table 5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset ( p , r , f1 - score , f1 - score , f1 - score , p , f1 - score , f1 - score , p , r , f1 - score , f1 - score , p , f1 - score , p , f1 - score , f1 - score , p , f1 - score , p , f1 - score , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , con
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . glove is a glove vector retrofitted by faruqui et al . ( 2015 ) to wordnet 3 . 1 , and glove - extended refers to the embeddings obtained by autoextension rothe and schütze ( 2015 ) on glove .
< extra_id_0 > c > [ bold ] full uas c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] full uas c > [ bold ] ppa acc . c > rbg + hpcd ( full ) c > 94 . 17 c > 88 . 51 c > c > c > rbg + hpcd ( full
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the ppa acc . the effect of removing sense priors and context sensitivity ( attention ) from the ppa acc . is shown in table 3 .
< extra_id_0 > c > en - de c > flickr16 c > mscoco17 c > mscoco17 c > en - fr c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - de c >
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) c > 32 . 2 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > enc - gate c > dec - gate
< extra_id_0 > mscoco17 and en - fr c > en - fr c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > < extra_id_1 > > yule ’ s i c > ttr c > mtld c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > en - fr - trans - ff c > 0 . 7107 c > 1 . 0925 c > 118 . 5801 c > en - fr - trans - back c > en -
< extra_id_0 > table 1 summarizes the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . src c > 113 , 132 c > 131 , 104 c > trg c > 168 , 195 cap > training vocabularies for the english , french and spanish data used for our models .
< extra_id_0 > table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev system has a higher bleu score than the en - fr - smt - rev system .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > [ empty ] c > [ empty ] c > [ empty ] c > vgs c > 15 c > 17 c > 0 . 0 c > mfcc c > 0 c > 711 c > 0 . 0 c > 0 . 0 c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 . 5 c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate hate . we report further examples in the appendix . we report further examples in table 1 .
< extra_id_0 > bold > rnn / bold > , bold > dan / bold > , bold > dan / bold > , bold > dan / bold > , bold > dan / bold > , bold > , bold > , bold > , bold > , bold > , bold > , bold > , bold >
< extra_id_0 > bold > rnn / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > r >
< extra_id_0 > bold > sst - 2 / bold > positive bold > pubmed / bold > negative bold > sst - 2 / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > bold > pmi / bold > c > better c >
