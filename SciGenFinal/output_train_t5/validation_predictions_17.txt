< extra_id_0 > training c > throughput ( instances / s ) inference c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c >
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representation .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > + 27 . 82 c > + 27 . 82 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > diff .
< extra_id_0 > c - f1 100 % c - f1 50 % c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c
< extra_id_0 > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > lstm - parser c > 100 % in c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the mean performance is lower than the majority performance over the two indicated systems .
< extra_id_0 > bleu cider nist cider rouge - l cider rouge - l cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider nist cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider
< extra_id_0 > cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . cap > table 1 : data statistics comparison for the original e2e data .
< extra_id_0 > bleu c > nist c > [ bold ] system c > [ bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cider cide
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) perform better than snrg ( song et al . , 2018 ) and gcnseq ( song et al . , 2018 ) in terms of performance . graphlstm ( song et al . , 2018 ) performs better than snrg ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) performs better than pbmt ( song et al . , 2018 ) perform better than pbmt ( song et al . , 2018 ) .
< extra_id_0 > the model size in terms of parameters . gcnseq achieves 24 . 5 bleu points . gcnseq achieves 24 . 5 bleu points . gcnseq achieves 24 . 5 bleu points . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points .
< extra_id_0 > english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - german b c > [ bold ] english - german b c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic n c >
< extra_id_0 > gcn + rc ( 2 ) c > b 16 . 8 c > c 48 . 1 c > [ bold ] gcn + rc ( 2 ) c > b 16 . 8 c > [ bold ] + rc + la ( 2 ) c > c 47 . 9 c > [ bold ] c > [ bold ] 21 . 2 c > + rc + la ( 3 ) c > 21 . 2 c > + rc + la ( 3 ) c > + rc + la ( 3 ) c >
< extra_id_0 > 52 . 0m compared to dcgcn ( 1 ) and dcgcn ( 2 ) , respectively . compared to dcgcn ( 3 ) and dcgcn ( 4 ) , dcgcn ( 3 ) and dcgcn ( 4 ) have significantly higher performance . however , dcgcn ( 3 ) and dcgcn ( 4 ) have significantly lower performance .
< extra_id_0 > table 8 summarizes the ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing dense connections in the i - th block . - i dense block denotes removing dense connections in the i - th block . - i dense block denotes removing dense connections in the i - th block .
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm encoder and the dcgcn4 encoder perform better than the dcgcn4 encoder and the lstm decoder respectively .
< extra_id_0 > initialization c > depth c > objnum c > length c > coordinv c > initialization c > tense c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > subjnum c > tense c > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > mpqa and mrpc have the best performance . mrpc and mrpc have the best performance . mrpc and mrpc have the best performance . mrpc and mrpc have the best performance .
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > < extra_id_1 >
< extra_id_0 > mrpc , mpqa , mpqa , mpqa , mpqa , mrpc , mrpc , trec , glorot , mrpc , sick - e , sick - r , sick - r , sick - r , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , sick - r , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc , mrpc
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ bold ] 61 . 2 c > [ cbow - c ] c > [ bold - c ] c > [ bold - c ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r ] c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold - r = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold ] = c > [ bold = c > [ bold = c > [ bold = c > [ bold = c > [ bold = c > [ bold = c > [ bold = c > [ bold = c > [ bold
< extra_id_0 > 78 . 7 c > tense c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mrpc c > mpqa c > mrpc c > sick - e c > sick - r c > sick - e c > sick - b c > sick - r c > sick - e c > sick - e c > sick - r c > sick - r c > sick - e c > sick - r c > sick - r
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > in [ italic ] e + loc c > all org c > all per c > all misc c > all misc c > mil - nd c > 96 . 26 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > 69 . 38 c > 69 . 38 0 . 68 c > supervised learning c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > c >
< extra_id_0 > gen con / bold > neu / bold > ref gen con / bold > neu / bold > gen ref gen con / bold > neu / bold > gen ref gen con / bold > neu / bold >
< extra_id_0 > , bold > meteor / bold > , bold > bleu / bold > , bold > bleu / bold > , bold > bleu / bold > , bold > bleu / bold > , bold > bleu / bold > , bold > g2s / bold > , bold > , bold > , bold > , bold > , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et al . ( 2018 ) , cao et
< extra_id_0 > bold > model / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
< extra_id_0 > bold > meteor / bold > bold > size / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 7 - 13
< extra_id_0 > bold > added / bold > and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the gold refers to the reference sentences .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . table 4 : sem and pos tagging accuracy using features extracted from the nmt encoding layer .
< extra_id_0 > table 2 shows mft , unsupemb and word2tag tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c
< extra_id_0 > 2 91 . 7 c > 3 91 . 8 c > 4 91 . 9 c > uni c > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages , in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > mention c > gender c > 9 . 7 cap > attacker ’ s performance on different datasets . results are on a training set 10 % held - out .
< extra_id_0 > data c > task c > accuracy c > task c > accuracy c > sentiment c > 67 . 4 c > [ empty ] c > mention c > 67 . 4 c > [ italic ] c > [ empty ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > data c > dial c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment
< extra_id_0 > task acc c > leakage c > 5 . 0 on different datasets with an adversarial training . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ cap > table 6 : accuracies of the protected attribute with different encoders .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) and yang et al . ( 2018 ) compared the results of yang et al . ( 2018 ) compared the results of yang et al .
< extra_id_0 > + bert acc c > + ln time c > + bert time c > + ln time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yahoo err c > yahoo time c > yelppolar err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > model c > # params c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > decode c > decode c > decode c > decode c > decode c > decode c > decode
< extra_id_0 > model c > + elmo c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c >
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the f1 score on conll - 2003 english ner task . lstm * denotes the parameter number .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting . elrn performed better than glrn and elrn on snli task with base setting .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > the highest standard deviation among automatic systems is 1 . 0 . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > 0443 and 0 . 0761 respectively . lang c > corpus c > docsub c > docsub c > hclust c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > hclust c >
< extra_id_0 > slqs and docsub . hclust outperforms df and docsub in terms of p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c >
< extra_id_0 > 0490 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c > 0 . 0661 c >
< extra_id_0 > slqs and docsub are based on df and docsub and hclust , respectively . europarl and hclust are based on avgdepth and avgdepth , respectively . europarl and hclust are based on avgdepth and avgdepth .
< extra_id_0 > slqs and dlqs , respectively . hlqs and dlqs perform better than hlqs and dlqs , respectively . hlqs and dlqs perform better than hlqs and dlqs , respectively .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . lf is the baseline version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set is shown in table 1 .
< extra_id_0 > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c >
< extra_id_0 > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > cs - en c > bold > direct assessment / bold > fi - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 652 c > 0 . 646 c > c >
< extra_id_0 > sfhotel bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > / bold > nat / bold > nat / bold > f1 > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . < extra_id_1 > and m2 differ significantly from m1 compared to m2 and m2 respectively . the results show that spice and bertscore - recall perform better than spice and bertscore - recall , respectively . the results show that spice and bertscore - recall perform better than spice and bertscore - recall , respectively . the results show that spice and bertscore - recall perform better than spice and bertscore - recall .
< extra_id_0 > shen - 1 c > 0 . 728 c > 83 . 2 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1cyc + para
< extra_id_0 > transfer quality a > b c > transfer quality tie c > semantic preservation a > b c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > c > metric c > method of validation c > yelp c > lit . c > human sentence - level validation of metrics ; 150 examples for sim and pp ; 150 for spearman ’ s [ italic ] and 150 for spearman ’ s [ italic ] and 150 for spearman ’ s [ italic ] b / w negative pp and human ratings of fluency ; see text for validation of gm .
< extra_id_0 > shen - 1 c > 37 . 3 c > 10 . 0 c > gm c > m1 : m0 [ italic ] + cyc + para + lang c > 0 . 818 c > 27 . 3 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > 21 . 6 c > c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . bleu is between 1000 transferred sentences and human references , and bleu is between 1000 transferred sentences and human references . our best models achieve higher bleu than previous work at similar levels of bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > reparandum length [ bold ] 3 - 5 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > [ bold ] test mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > early c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model performs better than the state - of - art algorithms on the fnc - 1 test dataset .
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets ( higher is better ) . the unified model significantly outperforms all previous models on the document dating problem ( higher is better ) .
< extra_id_0 > table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of word attention and graph attention for this task . please see section 6 . 2 for more details .
< extra_id_0 > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] 1 / n c > [ bold ] all c > [ empty ] c > embedding + t c > 68 . 1 c > 36 . 6 c > 59 . 8 c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ]
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ italic ] p c > [ italic ] p c > [ italic ] f1 c > [ italic ] f1 c > [ italic ] identification ( % ) c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > wer
< extra_id_0 > train dev c > 50 % train test c > 50 % train test c > 75 % train dev c > full train test c > 75 % train dev c > 75 % train test c > cs - only c > 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] 73 . 0 c > [ bold ] c > cs - only c > cs - only c > cs - only c > cs - only c > c > c > [ bold ] c > [ bold ] c > [ bold ] c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > dev cs vs . monolingual ( mono ) sets , according to the type of the gold sentence in the set : code - switched ( cs ) vs . fine - tuned - disc ( lm ) sets .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . using type - aggregated gaze features trained on all three eye - tracking datasets is shown in table 7 .
< extra_id_0 > table 5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset ( p , r , f1 - score , f1 - score , f1 - score , p , f1 - score , f1 - score , f1 - score , p , f1 - score , p , f1 - score , f1 - score , f1 - score , p , f1 - score , f1 - score , p , f1 - score , f1 scores for type - aggregated gaze features on conll - 2003 dataset ( p , p , f , p , p , f , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , co
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . lstm - pp and glove - retro perform better than glove - extended and glove - extended , respectively .
< extra_id_0 > ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > rbg + hpcd ( full ) c > 94 . 17 c > 89 . 59 c > rbg + hpcd ( full
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the ppa acc . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) .
< extra_id_0 > and mscoco17 . the subs1m [ italic ] lm + ms - coco outperforms the subs1m [ italic ] subs1m [ italic ] subs1m [ lm + ms - coco ] subs1m [ lm + ms - coco ] subs1m [ lm + ms - coco ] subs1m [ italic ] subs1m [ lm + ms - coco ] subs1m [ lm + ms - coco ] subs1m [ italic ] subs1m [ italic ] subs1m [ italic ] subs1m [ italic ] subs1m [ ms - cococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococococo coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco
< extra_id_0 > autocap ( dual attn . ) and autocap 1 - 5 ( concat ) are significantly better than autocap ( dual attn . ) and autocap ( dual attn . ) compared to multi30k and mscoco17 , respectively . adding automatic image captions with automatic image captions is significantly better than autocap ( dual attn . ) and autocap 1 - 5 ( concat ) compared to autocap ( dual attn . ) and autocap ( dual
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate
< extra_id_0 > mscoco17 and en - fr c > en - fr16 c > mscoco17 c > mscoco17 c > en - fr16 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 8656 and 109 . 4506 respectively . mtld and en - fr - trans - ff perform better than en - fr - trans - back and en - fr - trans - back , respectively . mtld performs better than en - fr - trans - back and en - fr - trans - back , respectively . mtld performs better than en - fr - trans - back and en - fr - trans - back , respectively .
< extra_id_0 > table 1 summarizes the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . src c > 113 , 132 c > 131 , 104 c > training vocabularies for english , french and spanish data used for our models .
< extra_id_0 > en - fr - trans - rev and en - fr - trans - rev have significantly higher bleu and ter scores than en - fr - trans - rev and en - fr - trans - rev , respectively .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > vgs c > 15 c > 0 . 2 c > mfcc c > 711 c > 0 . 0 cap > table 2 : results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations .
< extra_id_0 > recall @ 10 ( % ) c > median rank c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > turns in a u > screenplay that u > at the edges edges edges curves so clever easy want hate hate hate hate hate hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > bold > rnn / bold > 3 cnn / bold > 3 cnn / bold > 3 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold >
< extra_id_0 > bold > rnn / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > sentiment score changes in sst - 2 . the numbers indicate that the score increases in positive and negative sentiment .
< extra_id_0 > better than sst - 2 and pubmed / bold > . compared with pmi and pubmed / bold > , the best / bold > and pmi / bold > , the better / bold > , the better / bold > , the better / bold > , the better / bold > , the better / bold > , the better / bold > , the better / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold >
