< extra_id_0 > training c > throughput ( instances / s ) inference c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > [ bold ] hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representation .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) with sdp . using the shortest dependency path on each relation type is shown in table 1 . using the shortest dependency path is shown in table 1 .
< extra_id_0 > c - f1 100 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c - f1 50 % c
< extra_id_0 > c > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > essay level f1 c > essay level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > lstm - parser c > essay vs . paragraph level ; c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level ; note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the mean performance is lower than the majority performance over the two indicated systems .
< extra_id_0 > train c > test c > train c > train c > test c > train c > train c > train c > test c > train c > train c > train c > test c > train c > train c > train c > test c > train c > train c > test c > train c > test c > train c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test c > test bleu c > test bleu c > test bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu
< extra_id_0 > cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . cap > table 1 : data statistics comparison for the original and our cleaned version .
< extra_id_0 > nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select sel
< extra_id_0 > table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > 23 . 3 c > gcnseq ( song et al . , 2018 ) and tree2str ( song et al . , 2018 ) have a higher all score than snrg ( song et al . , 2017 ) and gcnseq ( damonte and cohen , 2019 ) have a higher all score than snrg ( song et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) have higher all score than 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4 2 . 4
< extra_id_0 > the model size in terms of parameters . gcnseq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points on amr17 . ggnn2seq achieves 24 . 5 bleu points on amr17 . gcnseq achieves 24 . 5 bleu points . table 2 shows the model size in terms of parameters .
< extra_id_0 > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c > 1 c > 2 c > 1 c > 1 c > 2 c > 50 . 3 c > c > 51 . 3 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > + rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denotes rc denoted in table 6 .
< extra_id_0 > b c > d c > b c > b c > b c > b c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c >
< extra_id_0 > [ bold ] model c > b c > c r > 25 . 5 c > 55 . 4 c > - 4 , 4 dense blocks c > 23 . 8 c > 53 . 1 c > - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing dense blocks
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . c > - linear combination c > 22 . 9 c > 52 . 4 c > 52 . 4 c > - coverage mechanism c > 22 . 9 c > 52 . 4 c > 52 . 4 c > - graph attention c > 24 . 9 c > 52 . 4 c > - graph attention c > 24 . 9 c >
< extra_id_0 > initialization c > depth c > objnum c > tense c > coordinv c > topconst c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > method c > subjnum c > tense c > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mpqa c > mpqa c > mpqa c > mrpc c > sick - e c > sick - r c > hybrid c > sick - r c > 79 . 2 c > 79 . 6 c > 87 . 6 c > 87 . 6 c > [ bold ] c > [ bold ] c > [ bold ] c > sick - r
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cmp .
< extra_id_0 > mr , mpqa , mpqa , mpqa , mpqa , mrpc , glorot , and glorot . our paper c > n ( 0 , 0 . 1 ) c > 88 . 4 c > 69 . 6 c > 69 . 6 c > 69 . 6 c > 69 . 6 c > 69 . 6 c > 69 . 6 c > 69 . 6 c > [ bold ] c > [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c > sts14 c > sts15 c > sts16 r > c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 5 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] 52 . 2 c > [ bold ] c > c > c > c > c > c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > 78 . 7 c > tense c > objnum c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mpqa c > mpqa c > mrpc c > sick - e c > sick - b c > sick - r c > cmow - c c > 79 . 3 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c >
< extra_id_0 > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all per c > all misc c > all misc c > mil - nd c > 57 . 15 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > system c > all p c > all r c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c > 29 . 13 c >
< extra_id_0 > ref gen bold > ent / bold > c > ref gen bold > neu / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > bold > bleu / bold > bold > meteor / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 16 0 . 16 0 . 16
< extra_id_0 > bold > model / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / d > / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al . / et al .
< extra_id_0 > bold > meteor / bold > c > 57 . 6m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > table 8 shows the fraction of elements in the output that are missing in the generated sentence ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound are shown in table 2 .
< extra_id_0 > zh c > fr c > zh c > en c > en c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders are shown in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > mention c > gender c > 9 . 7 on a training set 10 % held - out . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > data c > task c > accuracy c > 67 . 4 c > mention c > 67 . 4 c > sentiment c > 67 . 4 c > pan16 c > mention c > 67 . 4 c > [ italic ] gender c > 67 . 7 c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ]
< extra_id_0 > data c > dial c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > sentiment
< extra_id_0 > task acc c > leakage c > 5 . 0 on different datasets with an adversarial training . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > table 6 : accuracies of the protected attribute with different encoders . embedding leaky c > 64 . 5 c > 67 . 8 c > guarded c > 54 . 8 .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) compared to yang et al . ( 2018 ) . yang et al . ( 2018 ) compared to yang et al . ( 2018 ) .
< extra_id_0 > base acc c > + bert acc c > + bert time c > + ln acc c > + bert time c > + ln time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > amapolar time c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > zhang et al . ( 2015 ) c >
< extra_id_0 > model c > # params c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > decode c > train c > decode c > decode c > decode c > decode c > train c > decode c > train c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > c > base c > elmo c > rnet * c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > - c > -
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . “ # params ” denotes the parameter number in ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and sent , respectively . c > [ italic ] w / system retrieval [ bold ] r - 2 and sent [ italic ] w / system retrieval [ bold ] r - 2 and sent [ sent ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold . the best result among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 .
< extra_id_0 > tf c > dsim c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df cluster cluster cluster clust cluster cluster clust cluster cluster clust cluster cluster clust cluster cluster clust cluster cluster clust cluster cluster cluster cluster cluster clust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster c
< extra_id_0 > dsim c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cluster clust cluster clust cluster clust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust clust clust clust clust clust clust clust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster
< extra_id_0 > tf c > dsim c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > dflust c > dflust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster clust cluster cluster cluster cluster cluster cluster cluster cluster clust cluster clust cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster
< extra_id_0 > dsim c > dsim c > docsub c > docsub c > docsub c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c > hclust c >
< extra_id_0 > dsim c > tlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs cluster cluster clust cluster clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust c
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . lf is the baseline version as we mentioned in table 1 . lf is the enhanced version as we mentioned in table 1 .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . compared to the baseline , p2 shows the best performance ( i . e . , hidden dictionary learning ) .
< extra_id_0 > cs - en c > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > , < extra_id_1 > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 552 c > 0 . 538 c > 0 . 646 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 610 c > 0 . 552 c > 0 . 720 cs - en cs - en cs - en cs - en cs - en cs - en cs - en cs - en cs - en cs - en cs - en cs - en cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - f1 cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore - cscore -
< extra_id_0 > inf . c > sfhotel bold > qual / bold > c > sfhotel bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . < extra_id_1 > c > m1 c > m2 c > m2 c > m2 c > m1 c > m2 c > m2 c > m2 c > bertscore - recall c > 0 . 709 c > 0 . 749 c > 0 . 749 c > 0 . 749 c > 0 . 749 c > 0 . 749 c > c >
< extra_id_0 > shen - 1 c > 0 . 728 c > 63 . 2 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para 0 . 7
< extra_id_0 > transfer quality a > b c > transfer quality tie c > semantic preservation a > b c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c >
< extra_id_0 > c > metric c > acc c > % of machine and human judgments that match c > 94 c > 84 c > human sentence - level validation of metrics ; 150 examples for sim and pp ; 150 for spearman ’ s [ italic ] and 150 for pp ; see text for validation of gm .
< extra_id_0 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para + lang c > 0 . 818 c > 26 . 3 c > 18 . 8 c > gm c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu than prior work at similar levels of bleu . our best models achieve higher bleu than prior work at similar levels of bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > 3 - 5 c > 3 - 5 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 1 c > 1 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( content - function ) , or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model performs better than the state - of - art algorithms .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > stage c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ bold ] all c > [ bold ] embedding + t c > 68 . 1 c > 25 . 5 c > 59 . 8 c > [ empty ] c > embedding + t c > 75 . 2 c > embedding + t c >
< extra_id_0 > trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold c > [ bold ] c
< extra_id_0 > dev wer
< extra_id_0 > train dev c > 50 % train test c > 50 % train test c > 75 % train dev c > 75 % train test c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > cs - only c > c > c > c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > cs - only c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 5 summarizes the results on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset is shown in table 7 .
< extra_id_0 > conll - 2003 c > [ bold ] c > baseline c > 93 . 89 c > 94 . 16 c > f1 - score ( f1 ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . using type - aggregated gaze features on the conll - 2003 dataset is shown in table 5 .
< extra_id_0 > table 1 summarizes the results on belinkov2014exploring ’ s ppa test set . lstm - pp and lstm - pp perform better than glove - retro and ontolstm - pp , respectively .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachment predictors . the rbg dependency parser with features coming from hpcd ( full ) and hpcd ( full ) achieves 94 . 17 and 88 . 51 respectively .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the ppa acc . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > c > en - de c > mscoco17 c > en - fr c > en - fr c > en - de c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - de c > en - fr c > en -
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > lm + ms - coco c > + domain - tuned c >
< extra_id_0 > autocap 1 - 5 ( concat ) and autocap 1 - 5 ( concat ) are significantly better than autocap ( dual attn . ) and autocap ( dual attn . ) are significantly better than autocap ( dual attn . ) is significantly better than autocap ( dual attn . ) is significantly better than autocap ( dual attn . ) is significantly better than autocap ( dual attn . ) is significantly better than autocap ( concat ) is significantly better than autocap 1 - 5 ( concat ) is significantly better than autocap ( dual attn .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > dec - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c > enc - gate c >
< extra_id_0 > mscoco17 c > en - fr c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > ttr c > mtld c > yule ’ s i c > mtld c > mtld c > en - fr - trans - ff c > 9 . 2793 c > en - fr - trans - ff c > 1 . 0172 c > 121 . 5801 c > en - fr - trans - back c > 9 . 2793 c > 1 . 0172 c > en - fr
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the train , test and development splits show the number of parallel sentences in the train , test and development splits for the language pairs we used .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 summarizes the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev system has a higher bleu score than the en - fr - trans - rev system , with a higher bleu score than the en - fr - trans - rev system .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > median rank c > 0 . 0 c > rsaimage c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > c >
< extra_id_0 > c > recall @ 10 ( % ) c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c
< extra_id_0 > she turns in a u > screenplay screenplay that ’ s so clever you want hate hate hate hate hate . we report further examples in the appendix . we report further examples in table 1 .
< extra_id_0 > bold > rnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold >
< extra_id_0 > bold > rnn / bold > bold > dan / bold > table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence .
< extra_id_0 > bold > sst - 2 / bold > negative c > bold > pubmed / bold > conclusion c > bold > sst - 2 / bold > positive c > bold > sst - 2 / bold > negative c > better c > n ’ t c > evaluate c > conclude r > c > better c > c > c >
