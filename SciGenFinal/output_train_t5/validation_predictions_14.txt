< extra_id_0 > training c > throughput ( instances / s ) inference c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c >
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) without sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] best f1 ( in 5 - fold ) with sdp c > [ bold ] with sdp c > [ bold ] with sdp c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > f1 50 % c > 67 . 69 c > 67 . 69 c > y - 3 : yitalic > c / italic > - 1 c > 67 . 69 c >
< extra_id_0 > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > stagblcc c > lstm - parser c > lstm - parser c > essay c > 64 . 741 . 97 c > 56 . 242 . 87 c > paragraph c > 56 . 242 . 87 c > 56 . 242 . 87 c > lstm - parser c > 56 . 242 . 87 c
< extra_id_0 > bleu c > bleu c > bleu c > bleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c
< extra_id_0 > cap > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . cap > table 1 : data statistics comparison for the original and our cleaned version .
< extra_id_0 > bleu c > bleu c > bleu c > bleu c > bleu c > bleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c > tleu c
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > 23 . 3 c > graphlstm ( song et al . , 2018 ) and tree2str ( song et al . , 2018 ) c > all c > 24 . 4 c > gcnseq ( damonte and cohen , 2019 ) c > 24 . 4 c > gcnseq ( damonte and cohen , 2019 ) c > 24 . 4 c > gcnseq ( song et al . , 2017 ) c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > 24 . 4 c > 24 . 4 c > 24 . 4 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > the model size in terms of parameters . gcnseq achieves 24 . 5 bleu points . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points .
< extra_id_0 > english - czech b c > [ bold ] english - german # p c > [ bold ] english - german b c > [ bold ] english - czech # p c > [ bold ] english - german b c > [ bold ] english - german b c > [ bold ] english - german b c > [ bold ] english - german b c > [ bold ] english - german b c > [ beck et al .
< extra_id_0 > 2 c > 1 c > 2 c > 2 c > 2 c > 2 c > 2 c > 50 . 3 r > c > c > 2 c > 3 c > 20 . 0 c > 51 . 9 c > 51 . 9 c > 53 . 3 c > c > c > c > c > c > 1 c > 2 c > 3 c > 53 . 3 c > 53 . 3 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc .
< extra_id_0 > 52 . 0m compared to dcgcn ( 1 ) and dcgcn ( 2 ) . compared to dcgcn ( 3 ) and dcgcn ( 4 ) , we find that dcgcn ( 3 ) performs better than dcgcn ( 4 ) and dcgcn ( 4 ) , respectively .
< extra_id_0 > c > b c > c r > c > [ bold ] model c > b c > c r > c > - 4 dense blocks c > 25 . 5 c > 55 . 4 c > - 3 dense blocks c > 23 . 8 c > 53 . 1 c > - i dense block denotes removing the dense connections in the i - th block . c >
< extra_id_0 > table 9 : ablation study for modules used in the dcgcn4 encoder and the lstm decoder . the lstm decoder is based on the dcgcn4 encoder and the lstm decoder , respectively .
< extra_id_0 > table 7 : initialization scores on probing tasks . glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > objnum c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst
< extra_id_0 > mpqa and mrpc . mrpc and mrpc perform better than sick - e and sick - r . sick - r performs better than sick - e and sick - r , respectively . sick - r performs better than sick - e and sick - r .
< extra_id_0 > c > sts14 c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > cmp . c > cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp . cmp .
< extra_id_0 > mr , mpqa and mrpc . mrpc and mrpc perform better than glorot and sick - e and sick - r , respectively . our paper performs better than glorot and sick - r and sick - r .
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > cmow - c c > [ bold ] 31 . 9 c > [ bold ] 43 . 5 c > [ bold ] 61 . 0 c > [ bold ] 61 . 0 c > [ bold ] 61 . 0 c > [ bold ] 61 . 0 c > [ bold ] 61 . 0 c > [ bold ] 61 . 0 c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ b
< extra_id_0 > 78 . 7 c > tense c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc >
< extra_id_0 > mrpc and mpqa mrpc mrpc mrpc mrpc mrpc mrpc mrpc c > sick - e c > sick - b c > sick - r c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c >
< extra_id_0 > all loc c > all org c > all misc c > in [ italic ] e + loc c > all org c > all misc c > all org c > all misc c > all org c > all misc c > all misc c > mil - nd c > 57 . 15 c > 89 . 46 c > 89 . 46 c >
< extra_id_0 > all p c > all r c > in [ italic ] e + p c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 13 c > 69 . 13 c > 69 . 13 c > 69 . 13 c > 69 . 13 c > 69 . 13 c > c >
< extra_id_0 > gen ref gen con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bold > con / bol
< extra_id_0 > bleu / bold > c > bleu / bold > c > bleu / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > c > bold > model / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > /
< extra_id_0 > bold > meteor / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > bold > size / bold > c > 57 . 6m c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > bold > added / bold > bold > miss / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound . pos tagging accuracy with baselines and an upper bound .
< extra_id_0 > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh
< extra_id_0 > pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . bi and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual encoders is shown in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > gender c > 9 . 7 cap > attacker ’ s performance on different datasets . attacker ’ s performance on different datasets is shown in table 8 .
< extra_id_0 > c > task c > accuracy c > sentiment c > 67 . 4 c > [ empty ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 2 : protected attribute leakage : balanced & unbalanced data splits . mention c > 67 . 4 c > 64 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 5 c > 73 . 5 c > 59 . 4 c > 59 . 7 c > 59 . 7 c > 59 . 7 c > 59 . 4
< extra_id_0 > data c > task acc c > leakage c > [ empty ] c > mention c > gender c > 72 . 5 c > 57 . 3 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c > 8 . 0 c >
< extra_id_0 > c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > guarded c > 59 . 3 c > 54 . 8 cap > table 6 : accuracies of the protected attribute with different encoders
< extra_id_0 > ptb + finetune c > ptb + dynamic c > wt2 base c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) c > yang et al . ( 2018 ) c > yang et al . ( 2018 ) c > yang et al . ( 2018 ) c > yang et al . ( 2018 ) c >
< extra_id_0 > base acc c > + bert time c > + bert time c > + bert time c > + bert time c > + bert time c > + bert time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yahoo err c > yahoo time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c >
< extra_id_0 > bleu score on wmt14 english - german translation task . train : time in seconds per training batch measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence on newstest2014 dataset .
< extra_id_0 > “ # params ” : the parameter number of base . “ rnet * ” : the results published by wang et al . ( 2017 ) . “ # params ” : the parameter number of elmo .
< extra_id_0 > “ # params ” : the parameter number in conll - 2003 english ner task . “ # params ” denotes the parameter number in ner task .
< extra_id_0 > table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting . ptb task with base + ln setting and test perplexity on snli task with base + ln setting .
< extra_id_0 > r - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent c > [ sent c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ] c > [ sent ]
< extra_id_0 > the highest standard deviation among automatic systems is highlighted in bold . the highest standard deviation among automatic systems is 1 . 0 , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is highlighted in bold .
< extra_id_0 > slqs and dfqs . hclust and dfqs perform better than dfqs and dfqs and dfqs , respectively .
< extra_id_0 > slqs and dlqs . hclust and dlqs perform better than dlqs and dlqs . however , dlqs performs better than dlqs and dlqs , respectively .
< extra_id_0 > slqs and dlqs , respectively . hlqs and dlqs perform better than dlqs and dlqs , respectively . hlqs and dlqs perform better than dlqs and dlqs .
< extra_id_0 > dsim and slqs . dsim and slqs perform better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs , respectively .
< extra_id_0 > dsim and slqs . dsim and slqs perform better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs , respectively . hlqs performs better than hlqs and hlqs .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . ndcg % comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 is shown in table 1 .
< extra_id_0 > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > coatt c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > fi - en c > fi - en c > fi - en c > fi - en c > lv - en c > lv - en c > lv - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > cs - en and tr - en c > bold > direct assessment / bold > lv - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > zh - en c > bold > direct assessment / bold > zh - en c > bertscore - f1 c > 0 . 6
< extra_id_0 > . sfhotel bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > qual / bold > bold > bold > qual
< extra_id_0 > m2 and m1 respectively . bertscore - recall c > 0 . 809 c > 0 . 750 r > bertscore - recall c > 0 . 809 c > 0 . 750 r > bertscore - recall c > 0 . 750 r > bertscore - recall c > 0 . 750 r > bertscore - recall c > 0 . 750 r > c > 0 . 750 r >
< extra_id_0 > 22 . 3 c > 8 . 81 c > gm c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 c >
< extra_id_0 > transfer quality a > b c > semantic preservation b > a c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c >
< extra_id_0 > c > metric c > spearman ’ s [ italic ] b / w negative pp and human ratings of fluency c > 0 . 79 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67 c > 0 . 67
< extra_id_0 > 37 . 3 c < extra_id_1 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ bold
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc , but untransferred sentences achieve the highest bleu than previous work at similar levels of bleu . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] reparandum length [ bold ] 3 - 5 c > 0 c > 0 c >
< extra_id_0 > 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] type c > [ bold ] reparandum length [ bold ] 1 - 2 c > [ bold ] reparandum length [ bold ] 3 - 5 c > [ bold ] type c > content - content c > 0 . 61 ( 30 % ) c > 0 . 58 ( 52 % ) c > 0 . 80 ( 32 % ) c >
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ empty ] c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ empty ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] c > – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding c > 12 . 43 c > 01 . 30 c > 53 . 24 c > 79 . 53 c > 81 . 72 c > [ bold ]
< extra_id_0 > the unified model significantly outperforms all previous models on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . please see section 6 . 2 for more details .
< extra_id_0 > 1 / n c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ bold ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ]
< extra_id_0 > ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] f1 c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] classification ( % ) c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ]
< extra_id_0 > wer
< extra_id_0 > train dev c > 50 % train test c > 50 % train test c > 75 % train dev c > 75 % train test c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > cs - only c > 73 . 0 c > [ bold ] c > [ bold ]
< extra_id_0 > dev cs vs . monolingual ( mono ) c > test mono c > fine - tuned - lm c > 71 . 33 c > cs - only - disc c > 74 . 40 c > cs - only - disc vs . monolingual ( mono ) c > 71 . 33 c > fine - tuned - disc vs . cs - only - disc vs . monolingual c >
< extra_id_0 > table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) for all three eye - tracking datasets and tested on the conll - 2003 dataset .
< extra_id_0 > conll - 2003 c > [ bold ] p , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . conll - 2003 c > baseline c > 93 . 89 c > 94 . 16 c > 94 . 03 * .
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . syntactic - sg and lstm - pp perform better than glove - retro and glove - extended .
< extra_id_0 > c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > [ bold ] full uas c > [ bold ] ppa acc . c > rbg + hpcd ( full ) c > 94 . 17 c > 89 . 59 c > rbg + oracle pp c >
< extra_id_0 > c > [ bold ] ppa acc . c > full c > 89 . 7 c > - sense priors c > 88 . 4 c > - attention c > 87 . 5 cap > effect of removing sense priors and context sensitivity from the model .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . adding subtitle data and domain tuning for image caption translation ( bleu % scores ) .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c >
< extra_id_0 > autocap ( dual attn . ) and autocap 1 - 5 ( concat ) c > 32 . 2 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > 27 . 0 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > mscoco17 c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > flickr16 c > flickr17 c > mscoco17 c > mscoco17 c > enc - gate c > 68 . 30 c > 61 . 38 c > dec - gate c > 61 . 38 c > dec - gate
< extra_id_0 > mscoco17 and en - fr c > en - fr c > mscoco17 c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > < extra_id_1 > > yule ’ s i c > ttr c > mtld c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > en - fr - trans - ff c > en -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of parallel sentences in the train , test and development splits for the language pairs we used is shown in table 1 .
< extra_id_0 > r > c > src c > trg r > c > training vocabularies for the english , french and spanish data used for our models . training vocabularies for the english , french and spanish data used for our models .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - trans - rev and en - es - rnn - rev systems have the highest automatic evaluation scores ( bleu and ter ) for the rev systems .
< extra_id_0 > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > [ empty ] c > [ empty ] c > [ empty ] c > recall @ 10 ( % ) c > median rank c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 711 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c
< extra_id_0 > c > [ empty ] c > recall @ 10 ( % ) c > [ empty ] c > recall @ 10 ( % ) c > [ bold ] c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c
< extra_id_0 > she turns in a u > screenplay screenplay screenplay that u > at the edges ; it ’ s so clever you want hate hate hate hate hate . cap > table 1 shows examples of the different classifiers compared to the original on sst - 2 .
< extra_id_0 > 3 cnn / bold > 3 cnn / bold > 3 cnn / bold > 3 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > 4 cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > c
< extra_id_0 > bold > rnn / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > bold > dan / bold > r >
< extra_id_0 > better than sst - 2 and pubmed . compared to sst - 2 and pubmed , it is better than sst - 2 and pubmed . compared to sst - 2 and pubmed , it is better than sst - 2 and pubmed . compared to sst - 2 and pubmed , it is better than sst - 2 and pubmed . compared to sst - 2 and pubmed , it is better than sst - 2 . compared to pmi , it is better c >
