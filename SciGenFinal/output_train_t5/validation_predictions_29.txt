< extra_id_0 > training c > throughput ( instances / s ) inference c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput ( instances / s ) training c > throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . fold performs the best on inference with efficient parallel
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp and [ bold ] best f1 ( in 5 - fold ) with sdp . using the shortest dependency path on each relation type is shown in table 1 . using the shortest dependency path is shown in table 1 . using the shortest dependency path is shown in table 1 .
< extra_id_0 > c - f1 100 % c > r - f1 50 % c > r - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > f1 50 % c > f1 50 % c > y - 3 significantly outperforms y - 3 and y - 3 . c >
< extra_id_0 > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > essay vs . paragraph level ; c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level ; note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the mean performance is lower than the majority performance over the runs given in table 2 .
< extra_id_0 > nist c > bleu c > [ bold ] nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bold ] ser c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bold ] bleu c > [ bold bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu
< extra_id_0 > the clean version of the e2e data shows that the number of distinct mrs , total number of textual references , and ser as measured by our slot matching script is significantly higher than the original e2e data ( see section 3 ) . our clean version shows that the number of distinct mrs , total number of textual references , and ser are significantly higher than the original version .
< extra_id_0 > nist c > [ bold ] nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bold ] ser c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bleu c > [ bold ] bleu c > [ bold bleu c > [ bold bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) achieves a bold score of 22 . 8 and 25 . 6 respectively . graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) achieve a bold score of 25 . 6 and 25 . 6 respectively . graphlstm ( song et al . , 2018 ) achieves a score of 24 . 4 and 24 . 4 respectively .
< extra_id_0 > amr17 . gcnseq ( damonte and cohen , 2019 ) achieves 24 . 5 bleu points . table 2 shows the model size in terms of parameters in terms of amr17 . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . dcgcn achieves 24 . 5 bleu points .
< extra_id_0 > english - czech b c > [ bold ] english - german p c > [ bold ] english - czech b c > [ bold ] english - german p c > [ bold ] english - czech b c > [ bold ] english - german p c > [ bold ] english - german b c > [ bold ] english - german p c > [ bold ] english - german b c > [ beck et al . , 2017 ] english - german c c > [ beck et al . , 2017 ] english - german b c > [ beck et al . , 2017 ] english - german p c > [ beck et al . , 2017 ] english - german p c > [ beck et al . , 2017 ] english - german p c > [ beck et al . , 2018 ] english - german c > [ beck et al . , 2018 ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > b c > [ italic ] b c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines . gcn + rc + la ( 2 ) compared to baselines .
< extra_id_0 > d c > # p c > d c > # p c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > [ bold ] d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c
< extra_id_0 > [ bold ] model c > b c > c c > - 4 dense blocks c > 25 . 5 c > 55 . 4 c > - 3 , 4 dense blocks c > 23 . 8 c > 53 . 1 c > - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > table 9 shows the ablation study for the dcgcn4 encoder and the lstm decoder . the lstm encoder and lstm decoder have significantly better performance than the encoder and lstm decoder , respectively .
< extra_id_0 > initialization c > depth c > objnum c > length c > coordinv c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > method c > tense c > objnum c > topconst c > subjnum c > depth c > depth c > depth c > depth c > depth c > depth c > depth c > depth c > depth c > objnum c > topconst c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mrpc c > trec c > sick - e c > sst5 c > sick - r c > hybrid c > sick - r c > 90 . 0 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) 79 . 8 c > [ bold ) 79 . 8 c > [ bold ) 79 . 8 c > [ bold ) 79 . 8 c > [ bold ) 79 . 8 c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 . 4 c > 79 .
< extra_id_0 > c > sts14 , c > sts15 , c > sts16 , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , c > hybrid , cmp .
< extra_id_0 > mrpc and mpqa . mrpc and mpqa perform better than sick - e and sick - r , respectively . our paper performs better than glorot and sick - r in terms of initialization .
< extra_id_0 > c > sts15 c > sts16 c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c >
< extra_id_0 > tense c > objnum c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mrpc c > trec c > sick - e c > sst5 c > sick - r c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ cbow - r c > [ cbow - r c > [ cbow - r c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > 79 . 4 c > 79 . 4 c > 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [ bold ] 79 . 4 c > [
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all per c > all misc c > all loc c > all misc c > 89 . 46 c > 89 . 46 c > all misc c >
< extra_id_0 > all p c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > all p c > all p c > all f1 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c > 69 . 38 c >
< extra_id_0 > ref gen bold > ent / bold > c > gen ref gen bold > neu / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > bold > bleu / bold > bold > meteor / bold > c > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14
< extra_id_0 > table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . konstas et al . ( 2017 ) showed that models trained with additional bleu data perform better than those trained with bleu data .
< extra_id_0 > bold > meteor / bold > c > c > c > c > c > ablation study on the ldc2017t10 development set . c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > table 8 shows the fraction of elements in the output that are not present in the input ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound is shown in table 2 .
< extra_id_0 > ru c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > zh c > z
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the uni / bidirectional / residual encoders perform better than uni / bidirectional / residual encoders in terms of accuracy .
< extra_id_0 > task c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > sentiment c > mention c > gender c > 9 . 7 on a training set 10 % held - out , attacker ’ s performance on different datasets is shown in table 8 . the difference between the attacker ’ s score and the adversary ’ s accuracy is shown in table 8 .
< extra_id_0 > task c > accuracy c > sentiment c > 67 . 4 c > [ empty ] c > [ italic ] gender c > 67 . 4 c > [ empty ] c > [ italic ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > c > c > c > c > c > c > c > c > c > accuracies when training directly towards a single task c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mention c > sentiment c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > data c > mention c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > task acc c > leakage c > mention c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > gender c > 72 . 5 c > 57 . 3 c > 6 . 9 cap > performances on different datasets with an adversarial training are shown in table 3 . the difference between the attacker score and the adversary ’ s accuracy is shown in table 3 .
< extra_id_0 > embedding guarded and leaky with different encoders . table 6 shows the accuracies of the protected attribute with different encoders . rnn achieves 59 . 3 and 54 . 8 % accuracy .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared yang et al . ( 2018 ) compared to yang et al . ( 2018 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 )
< extra_id_0 > base time c > + bert acc c > + bert time c > + ln acc c > + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yahoo time c > amafull err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > this c >
< extra_id_0 > bleu score on wmt14 english - german translation task . train and decode : time in seconds measured from 0 . 2k training steps on tesla p100 . decode : time in milliseconds used to decode one sentence on newstest2014 dataset .
< extra_id_0 > table 4 : exact match / f1 - score on squad dataset . “ # params ” : the parameter number of base . rnet * : the results published by wang et al . ( 2017 ) .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . “ # params ” : the parameter number in ner task . lstm * denotes the reported result .
< extra_id_0 > table 7 : test accuracy on snli and ptb task with base + ln setting and test perplexity on ptb task with base + ln setting . elrn outperforms glrn and glrn in accuracy and perplexity tests .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] r - 2 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > r - 2 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > top - 1 / 2 : % of evaluations a system being ranked as a best . top - 1 / 2 : % of evaluations a system being ranked as a best . human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among all automatic systems is 1 . 0 . the highest standard deviation among automatic systems is 1 . 0 .
< extra_id_0 > 0443 c > 0 . 0761 c > 0 . 0553 c > 0 . 0553 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > 0 . 0761 c > pt
< extra_id_0 > 0326 and tf c > 0 . 0356 respectively . hclust and docsub perform better than lang c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > 0 . 0326 and 0 . 0326 respectively .
< extra_id_0 > 0761 and tf c > 0 . 1038 and 0 . 0641 respectively . hclust and docsub have the best p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p
< extra_id_0 > slqs , docsub and hclust are all corpus based on df and docsub and hclust . europarl has the highest avgdepth and maxdepth , respectively , and the highest avgdepth . europarl has the highest avgdepth and maxdepth , respectively .
< extra_id_0 > slqs , df and docsub are all corpus based on df and docsub and hclust . europarl has the highest avgdepth and maxdepth , respectively , and the highest avgdepth . europarl has the lowest avgdepth and maxdepth .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as mentioned in table 1 . the qt , s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . r0 , r1 , r2 , r3 denote regressive loss , weighted softmax loss , and generalized ranking loss , respectively .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . using coatt , coatt and coatt , we can see that p2 achieves the best performance ( i . e . hidden dictionary learning ) on different visdial validation sets .
< extra_id_0 > cs - en c > fi - en c > fi - en c > lv - en c > ruse c > 0 . 750 c > 0 . 750 c > 0 . 750 c > 0 . 750 c > 0 . 750 c > 0 . 750 c > 0 . 750 c > 0 . 712 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > cs - en and fi - en and zh - en , respectively , and bertscore - f1 and ruse ( * ) achieve the best performance . bertscore - f1 achieves the best performance on all metrics except for zh - en and zh - en , respectively .
< extra_id_0 > and inf , respectively . sfhotel and bertscore - f1 achieve significantly better performance than bleu - 1 and bleu - 2 . sfhotel achieves significantly better performance than bertscore - f1 and sfhotel , respectively . sfhotel achieves significantly better performance than bertscore - f1 .
< extra_id_0 > leic ( * ) and spice ( * ) have m1 and m2 scores , respectively , while spice and bertscore - recall have m1 and m2 scores , respectively , and bertscore - recall have m1 and m2 scores , respectively , and m1 and m2 scores are significantly lower than the baselines . however , bertscore - recall scores are significantly lower than those of spice and bertscore - recall , respectively , and m1 and m2 scores are significantly higher than m1 and m2 scores are significantly lower than those of word - mover , respectively , respectively .
< extra_id_0 > gm and sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m0 : m0 : m0 : m0 : m0 : m0 : m4 : m0 : m4 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m
< extra_id_0 > transfer quality a > b and transfer quality tie c > semantic preservation b > a and semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > c > metric c > acc c > % of machine and human judgments that match c > 94 c > 84 c > human sentence - level validation of metrics ; 150 examples for sim and pp ; see text for validation of gm ; see text for validation of acc ; see text for validation of pp .
< extra_id_0 > we found that < extra_id_1 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > 0 . 818 c > 26 . 4 c > 19 . 2 c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work at similar levels of acc . our best models achieve higher bleu than previous work at similar levels of bleu . however , untransferred sentences achieve the highest bleu than previous work at similar levels of bleu .
< extra_id_0 > reparandum length [ bold ] 3 - 5 . reparandum length [ bold ] 3 - 5 . reparandum length [ bold ] 6 - 8 . reparandum length [ bold ] 8 + . reparandum length [ bold ] – c > – c > – c > – c > – c > – c > 0 . 62 .
< extra_id_0 > reparandum length [ bold ] 3 - 5 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither .
< extra_id_0 > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > early c > text + innovations c > [ italic ] – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding compared with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves a better performance than the state - of - art ones .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > 1 / 1 c > [ bold ] 1 / n c > [ bold ] all c > [ bold ] embedding + t c > 75 . 3 c > 69 . 8 c > 69 . 8 c > [ bold ] embedding + t c > 75 . 3 c > 69 . 8 c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > [ bold ) c > embedding + t c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > embedding + t c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( bold ) c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold ] c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold ] c > [ bold c > [ bold c > [ bold
< extra_id_0 > wer c > dev perp c > dev perp c > test wer c > test wer c > test perp c > test wer c > all : cs - only + vocab - lm c > 68 . 61 c > 68 . 61 c > 57 . 9 c > 13 . 89 c > 13 . 89 c > 13 . 89 c > all : cs - only
< extra_id_0 > train test and full train test set using discriminative training with only subsets of the code - switched data . table 4 summarizes the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set are shown in table 4 .
< extra_id_0 > table 5 summarizes the results on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . on the dev set and on the test set , the accuracy on the dev set and on the test set is higher than on the test set .
< extra_id_0 > table 7 summarizes the performance of type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 . type - aggregated gaze features are shown in table 7 .
< extra_id_0 > c > [ bold ] p , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . using type - aggregated gaze features on the conll - 2003 dataset is shown in table 5 .
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . lstm - pp and glove embeddings show the best performance . ontolstm - pp and glove - extended embeddings show the best performance . ontolstm - pp embeddings show the best performance .
< extra_id_0 > table 2 summarizes the results of rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results show that the rbg dependency parser outperforms the hpcd ( full ) system in terms of ppa accuracies .
< extra_id_0 > [ bold ] ppa acc . c > - sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) is shown in table 3 .
< extra_id_0 > adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > and mscoco17 . we observe that subs1m ( lm + ms - coco ) outperforms subs1m ( lm + ms - coco ) and subs1m ( lm + ms - coco ) . we observe that subs1m ( lm + ms - coco ) outperforms subs1m ( lm + ms - coco ) and subs1m ( lm + ms - coco ) . we observe that subs1m ( lm + ms - coco ) outperforms ( mscoco17 ) .
< extra_id_0 > mscoco17 and en - de c > flickr16 and en - de c > flickr17 c > mscoco17 c > mscoco17 c > en - de c > multi30k c > 61 . 4 c > 32 . 0 c > 32 . 0 c > + autocap 1 - 5 ( concat ) c > 44 . 1 c > [ bold ] c > c >
< extra_id_0 > and mscoco17 . enc - gate and dec - gate perform better than enc - gate and enc - gate , respectively . enc - gate and dec - gate perform better than enc - gate and enc - gate , respectively . enc - gate and dec - gate perform better than enc - gate and enc - gate , respectively .
< extra_id_0 > and mscoco17 , respectively . subs3m and subs6m perform better than en - fr , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , mscoco17 , respectively , respectively .
< extra_id_0 > < extra_id_1 > c > yule ’ s i c > ttr c > mtld c > mtld c > en - fr - trans - ff en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – es has a total of 1 , 472 , 203 parallel sentences in the train , test and development splits , and en – es has 459 , 633 parallel sentences .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems ( en - fr - rnn - rev and en - fr - trans - rev ) . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > recall @ 10 ( % ) and median rank ( 0 . 0 ) on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the row labeled vgs is the visually supervised model from flickr8k .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 . 0 c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > she turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > bold > rnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnncnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold >
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence and vice versa . the numbers indicate that the score increases in positive and negative sentiment with respect to the original sentence .
< extra_id_0 > not evaluate c > evaluate c > evaluate c > evaluate c > conclude r > c > best c > love c > bad c > evaluate c > evaluate c > conclude r > sift / bold > bold > sst - 2 / bold > positive c > n ’ t c > evaluate c > evaluate c > conclude r > acc c >
