< extra_id_0 > table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all models with different representation .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) with sdp . using the shortest dependency path on each relation type is shown in table 1 . using the shortest dependency path is shown in table 1 .
< extra_id_0 > > r - f1 100 % c > r - f1 50 % c > r - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > y - 3 c > 67 . 89 c > 67 . 89 c > 67 . 89 c >
< extra_id_0 > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > essay vs . paragraph level ; c - f1 ( 100 % ) for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . note that the mean performance is lower than the majority performance over the runs given in table 2 .
< extra_id_0 > nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bl
< extra_id_0 > ser ( % ) compared to the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script ) .
< extra_id_0 > nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select select
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) have the best performance . snrg and gcnseq ( damonte and cohen , 2019 ) have the best performance . snrg and gcnseq have the best performance . snrg and snrg have the best performance .
< extra_id_0 > the model size in terms of parameters . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . dcgcn achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points .
< extra_id_0 > english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - german # p c > [ bold ] english - czech # p c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > b c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + la ( table 6 ) .
< extra_id_0 > d c > b c > c c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c > d c >
< extra_id_0 > c > b c > c r > c > - 4 dense blocks on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm encoder and the graph encoder perform better than the encoder and the lstm decoder in terms of attention and coverage mechanisms .
< extra_id_0 > initialization c > depth c > objnum c > length c > coordinv c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > method c > bshift c > tense c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > topconst c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mpqa c > mpqa c > mrpc c > sick - e c > sick - r c > hybrid c > sick - r c > sick - r c > hybrid c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > mpqa c > mpqa c > mpqa c > mpqa c > trec c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc c > mrpc
< extra_id_0 > c > sts14 c > sts15 c > sts16 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > objnum c > tense c > objnum c > topconst c > somo c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc
< extra_id_0 > mpqa c > mrpc c > trec c > sick - e c > sst5 c > sick - r c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ cmow - c c > [ bold ] c > [ cbow - r c > [ cbow - r c > [ bold ] c > [ bold ] c > [ cbow - r c > [ cbow - r c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ b
< extra_id_0 > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all loc c > all misc c > all misc c > mil - nd c > 57 . 15 c > all org c >
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > in [ italic ] e + p c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > in [ italic ] nd ( model 2 ) c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ italic , c > in [ bold , c > in [ bold , c > in [ bold , c > in [ bold , c > in [ bold ] in [ bold , c > in [ bold , c > c > c > c > c > c > c > c > c > c > c > supervised learning c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > gen bold > ent / bold > c > gen ref gen bold > neu / bold > c > gen ref gen bold > neu / bold > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > > bold > meteor / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 14 0 . 14 1 . 14 1 . 14 1 . 14 1 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 .
< extra_id_0 > c > bold > model / bold > song et al . ( 2018 ) c > 200k c > 31 . 60 c > g2s - ggnn c > 200k c > 31 . 60 c > g2s - ggnn c > 200k c > 31 . 60 c > g2s - ggnn c > 200k c > 31 . 60 c > c >
< extra_id_0 > bold > meteor / bold > bold > size / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > bilstm c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > 0 - 7
< extra_id_0 > table 8 shows the fraction of elements in the output that are missing in the generated sentence ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) for the test set of ldc2017t10 . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer is shown in table 4 .
< extra_id_0 > table 2 shows mft and unsupemb tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound is shown in table 2 .
< extra_id_0 > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual encoders are shown in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > mention c > sentiment c > mention c > mention c > gender c > 9 . 7 on a training set 10 % held - out . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > data c > task c > accuracy r > c > sentiment c > 67 . 4 r > c > [ empty ] c > [ italic ] gender c > 67 . 7 r > c > [ empty ] c > [ italic ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > [ captcha ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > accuracy c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > task acc c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c > unbalanced leakage c >
< extra_id_0 > task acc c > leakage c > 5 . 0 on different datasets with an adversarial training . is the difference between the attacker score and the adversary ’ s accuracy . is the difference between the attacker score and the adversary ’ s accuracy .
< extra_id_0 > embedding guarded and leaky with different encoders . embedding guarded and leaky with different encoders achieves 67 . 8 accuracies in table 6 .
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > wt2 + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 )
< extra_id_0 > + bert acc c > + ln acc c > + bert time c > + ln + bert time c > + ln + bert time c > + ln + bert time c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > amapolar time c > yahoo err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > lstm c >
< extra_id_0 > model c > # params c > train c > decode c > train c > train c > train c > decode c > train c > train c > decode c > train c > train c > decode c > train c > decode c > train c > decode c > train c > decode c > train c > decode c > decode bleu c > decode bleu c > decode c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > decode bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > bleu c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > model c > + elmo c > rnet * achieves the best match / f1 score on squad dataset . rnet * achieves the best match / f1 score on lstm dataset .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . “ # params ” : the parameter number in ner task . lstm * denotes the reported result .
< extra_id_0 > c > snli c > ptb model with base + ln setting and test accuracy on snli task with base + ln setting and test perplexity on ptb task with base + ln setting . c > snli model with base + ln setting and test accuracy on snli task with base + ln setting and test accuracy on snli task with ptb setting .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > top - 1 / 2 : % of evaluations a system being ranked as a best . top - 1 / 2 : % of evaluations a system being ranked as a best . human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among automatic systems is 1 . 0 . the highest standard deviation among automatic systems is 1 . 0 .
< extra_id_0 > tf c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > slqs and docsub . hclust outperforms df and docsub in terms of p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p c > p
< extra_id_0 > tf c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > dsim c > hlqs c > docsub c > hclust c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hllust c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > dsim c > hlqs c > docsub c > hclust c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hlqs c > hllust c > hlqs c > hlqs c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as we mentioned in table 1 . the qt and answer score sampling performed better than lf on the validation set of visdial v1 . 0 . using regressive loss , regressive loss , and generalized ranking loss , respectively .
< extra_id_0 > table 2 summarizes the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . using coatt , coatt and coatt , we can see that p2 indicates the most effective one ( i . e . hidden dictionary learning ) on different visdial models .
< extra_id_0 > fi - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > de - en and zh - en , respectively , and bertscore - f1 and ruse ( * ) achieve the best performance . bertscore - f1 achieves the best performance on all metrics except for fi - en and zh - en , respectively .
< extra_id_0 > > nat / bold > c > sfhotel bold > qual / bold > c > sfhotel bold > qual / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > and m2 respectively . < extra_id_1 > and m2 mean that leic ( * ) and spice ( * ) achieve the best results . however , bertscore - recall ( * ) achieves a lower performance than spice ( * ) and bertscore - recall ( * ) achieves a higher performance than spice ( * ) and bertscore - recall ( * ) achieves a higher performance than spice ( * ) and a higher performance than spice ( * ) and bertscore - recall ( * ) achieves a higher performance ( * ) achieves a lower performance ( * ) performance .
< extra_id_0 > gm and sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m0 : m0 : m0 : m0 : m0 : m0 : m4 : m0 : m4 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m
< extra_id_0 > transfer quality a > b c > semantic preservation b > a c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > c > metric ; yelp c > lit . c > human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 examples for sim and pp ; see text for validation of gm ; see text for validation of acc ; see text for validation of pp .
< extra_id_0 > gm and sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > 0 . 818 c > m6 : m0 [ italic ] c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than prior work at similar levels of acc . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than our best models ( see table 6 ) .
< extra_id_0 > reparandum length [ bold ] 3 - 5 c > reparandum length [ bold ] 3 - 5 c > rephrase c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 1 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > reparandum length [ bold ] 3 - 5 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither of these categories .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] test best c > [ italic ] c > [ bold ] dev mean c > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > early c > text + raw + innovations c > 86 . 53 c > [ italic ] c > [ italic ] c > [ italic ] c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ bold ] c > [ bold c > [ bold c > [ bold ] c > [ bold c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c ] [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c > [ single c ] [ single c ] [ single
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > average of word2vec embedding c > 12 . 43 c > 01 . 30 c > 81 . 72 c > [ bold ] 83 . 54 c > [ bold ] 83 . 54 c > [ bold ] 83 . 54 c > [ bold ] c > self - attention sentence embedding c >
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task .
< extra_id_0 > model c > [ bold ] 1 / 1 c > [ bold ] 1 / n c > [ bold ] all r > c > [ empty ] c > embedding + t c > 69 . 8 c > 69 . 8 c > embedding + t c > 69 . 8 c > [ empty ] c > embedding + t c > 69 . 8 c > [ empty ] c > [ bold ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > [ empty ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > embedding + t c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > classification ( % ) c > [ bold ] trigger [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster f1 cluster cluster cluster cluster cluster role cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster f1 cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster bold cluster bold cluster cluster bold cluster
< extra_id_0 > dev wer c > test perp c > test wer c > test perp c > test wer c > test perp c > test wer c > test perp c > test perp c > test wer c > spanish - only - lm c > 329 . 68 c > 25 . 1 c > english - only - lm c > all :
< extra_id_0 > train dev and full train test set using discriminative training with only subsets of the code - switched data . the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data .
< extra_id_0 > table 5 summarizes the results on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . on the dev set and on the test set , the accuracy on the dev set and on the test set is significantly higher than on the test set .
< extra_id_0 > c > [ bold ] conll - 2003 c > [ bold ] p c > [ bold ] recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . c > [ bold ] baseline c > [ bold ] 63 . 92 * f1 - score ( f1 ) for using
< extra_id_0 > conll - 2003 shows that type - aggregated gaze features outperform type - aggregated gaze features on the conll - 2003 dataset ( p , r , f1 - score , f1 - score , f1 - score , p , r , f1 - score , f1 - score , p , r , f1 - score , f1 - score , p , r , f1 - score , p , r , f1 - score , p , r , f1 for conll - 2003 dataset ( p ) on conll - 2003 dataset ( p , r ) on conll - 2003 dataset ( p , r , f1 , f1 - score , p , r , p , r , p , r , p , r , f1 - score , p , p , r , p , r , f1 - score , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , p , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , conll - 2003 , co
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . lstm - pp and glove - extended embeddings perform better than lstm - pp and lstm - pp embeddings .
< extra_id_0 > table 2 summarizes the results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . rbg + hpcd ( full ) achieves 94 . 17 and 88 . 51 points , respectively . ontolstm - pp achieves 89 . 59 points , respectively .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores with marian amun ) . all results with marian amun are shown in table 2 .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > en - de c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > en - de c > flickr16 c > flickr17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > en - fr c > multi30k c > 61 . 4 c > + autocap 1 - 5 ( concat ) c > 32 . 2 c > + autocap 1 - 5 ( concat ) c > 32 . 2 c > + autocap 1 - 5 ( concat ) c > 44 . 1 c >
< extra_id_0 > and mscoco17 . enc - gate and dec - gate perform better than enc - gate and enc - gate , respectively . mscoco17 and enc - gate perform better than enc - gate and dec - gate , respectively . mscoco17 and enc - gate perform better than enc - gate and dec - gate , respectively .
< extra_id_0 > mscoco17 and subs3m en - fr c > en - fr c > mscoco17 c > mscoco17 c > en - fr c > en - fr c > en - fr c > en - fr c > mscoco17 c > mscoco17 c > mscoco17 c > subs3m c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > ttr c > mtld c > yule ’ s i c > mtld c > mtld c > en - fr - trans - ff c > 0 . 7107 c > 1 . 0925 c > en - fr - trans - back c > 1 . 0925 c > 121 . 5801 c > en - fr - trans - back c > 1 . 0925 c > en - fr
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . for the train , test and development splits , the number of parallel sentences in the train , test and development splits is shown in table 1 .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 summarizes the automatic evaluation scores ( bleu and ter ) for the rev systems ( en - fr - rnn - rev and en - fr - smt - rev ) .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > median recall @ 10 ( % ) c > vgs c > 15 c > 0 . 2 c > mfcc c > 0 c > 0 . 0 c > rsaimage c > 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > 0 . 0 c > vgs c > 0 . 0 c > 0 . 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c >
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 . 0 c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > she turns in a u > screenplay that u > at the edges edges edges curves so clever easy want hate hate hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > bold > rnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > c
< extra_id_0 > bold > rnn / bold > bold > dan / bold > table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence .
< extra_id_0 > bold > sst - 2 / bold > positive c > bold > pubmed / bold > negative c > bold > sst - 2 / bold > conclusion c > better c > better c > n ’ t c > evaluate c > conclude r > r > c > bold > sift / bold > and bold > sift / bold > / bold > c > / bold > c > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold
