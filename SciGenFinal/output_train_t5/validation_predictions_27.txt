< extra_id_0 > table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ’ s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation .
< extra_id_0 > table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 .
< extra_id_0 > the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations with different representation . the max pooling strategy consistently performs better in all model variations . the max pooling strategy consistently performs better in all model variations .
< extra_id_0 > [ bold ] best f1 ( in 5 - fold ) without sdp compared with [ bold ] best f1 ( in 5 - fold ) without sdp compared with [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 ( in 5 - fold ) without sdp compared to [ bold ] best f1 without sdp .
< extra_id_0 > > r - f1 100 % c > r - f1 50 % c > r - f1 100 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > r - f1 50 % c > y - 3 c > 51 . 59 c > 37 . 38 c > 37 . 92 c > 47 . 92 c > y - 3
< extra_id_0 > and essay level acc . c > paragraph level acc . c > paragraph level acc . c > paragraph level c - f1 c > paragraph level r - f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c > paragraph level f1 c >
< extra_id_0 > c > [ empty ] c > lstm - parser c > [ empty ] c > lstm - parser c > essay vs . paragraph level ; c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 .
< extra_id_0 > bleu c > nist c > [ bold ] system c > [ bold ] train c > [ bleu c > [ bleu c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bold ] train c > [ bleu ] c > [ bleu ser c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu bleu
< extra_id_0 > table 1 : data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the original e2e data contains 4 , 862 distinct mrs , 42 , 061 textual references , and ser ( % ) .
< extra_id_0 > nist c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ] c > [ bleu ser c > [ bleu c > [ bleu ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser ser tgen tgen tgen tgen tgen tgen tgen
< extra_id_0 > table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies , slight disfluencies ) .
< extra_id_0 > graphlstm ( song et al . , 2018 ) and snrg ( song et al . , 2018 ) achieves a bold score of 22 . 8 . snrg achieves a bold score of 24 . 4 . graphlstm achieves a bold score of 24 . 4 . snrg achieves a bold score of 25 . 6 .
< extra_id_0 > amr17 . gcnseq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points . ggnn2seq achieves 24 . 5 bleu points .
< extra_id_0 > german b c > [ bold ] english - czech # p c > [ bold ] english - czech b c > [ bold ] english - german # p c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - czech b c > [ bold ] english - german b c > [ bold ] english - german b c > c > [ bold ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2017 ] c > [ beck et al . , 2018 ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > b c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > [ italic ] n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c > n c >
< extra_id_0 > . + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + rc denotes + la denotes + rc denotes + rc .
< extra_id_0 > dcgcn ( 2 ) has 22 . 2m and dcgcn ( 3 ) has 22 . 9m and dcgcn ( 4 ) has 22 . 9m and dcgcn ( 4 ) has 22 . 9m and dcgcn ( 4 ) has 22 . 9m and dcgcn ( 4 ) has 12 . 3m . the model has a better performance than the dcgcn ( 1 ) and dcgcn ( 3 ) , which have a better performance than the dcgcn ( 1 ) and dcgcn ( 3 ) . the model has a better performance than the dcgcn ( 3 ) and dcgcn ( 4 ) , which have a better performance .
< extra_id_0 > table 8 shows the ablation study for density of connections on the dev set of amr15 . - i dense block denotes removing the dense connections in the i - th block . - i dense block denotes removing the dense connections in the i - th block .
< extra_id_0 > table 9 : ablation study for modules used in the graph encoder and the lstm decoder . the lstm encoder and the graph encoder are shown in table 9 . the lstm encoder and the encoder are shown in table 9 . the encoder and decoder are shown in table 9 .
< extra_id_0 > initialization c > depth c > objnum c > length c > coordinv c > glorot c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > method c > depth c > tense c > objnum c > topconst c > topconst c > topconst c > topconst c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc wc
< extra_id_0 > mpqa c > mpqa c > mpqa c > mrpc c > sick - e c > sick - r c > hybrid c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > 90 . 0 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > sick - r c > sick - r c > sick - r c > sick - e c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 79 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ] 73 . 8 c > [ bold ]
< extra_id_0 > sts15 and sts16 are compared to the hybrid model . hybrid model achieves a higher score on unsupervised downstream tasks than hybrid model . hybrid model achieves a higher score on unsupervised downstream tasks than hybrid .
< extra_id_0 > mrpc , mpqa , mpqa and trec . our paper c > outperforms glorot c > sick - e c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > sick - r c > 89 . 8
< extra_id_0 > c > sts15 c > sts16 c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ cbow - r c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ b
< extra_id_0 > topconst c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > cmow - c c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c > wc c >
< extra_id_0 > mpqa c > mrpc c > mrpc c > sick - e c > sst5 c > sick - r c > cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > c > [ cmow - c c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] c > [ bold ] .
< extra_id_0 > all org c > all misc c > all loc c > all org c > all misc c > all misc c > all loc c > all org c > all misc c > all org c > all misc c > all loc c > all misc c > mil - nd c > 57 . 15 c > all org c >
< extra_id_0 > all p c > all r c > all f1 c > in [ italic ] e + p c > in [ italic ] e + r c > in [ italic ] e + f1 c > in mil - nd ( model 2 ) c > in [ bold ] 37 . 91 c > in [ bold ] 37 . 91 c > in [ bold ] 37 . 38 0 . 38 c > in [ bold ] 37 . 13 c > in [ italic ] c > in [ italic ] c > in [ italic ] c > in [ bold c > in [ bold ] 37 . 19 c > in [ bold ] 37 . 19 c > in [ bold ] 37 . 19 c > in [ bold ] 37 . 19 c > in [ bold nd ( model 2 ) in [ bold ] 37 . 19 c > 69 . 19 c > 69 . 19 c > 69 . 19 c > supervised learning c > 69 . 19 c > 69 . 19 c > 69 . 19 c > 69 . 19 c > 69 . 19 c > 69 . 19 c > 69 . 19 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 38 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68 0 . 68
< extra_id_0 > ref gen bold > ent / bold > c > gen ref gen bold > neu / bold > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > g2s - gin c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > > meteor / bold > bold > bleu / bold > bold > bleu / bold > bold > g2s - gin c > 22 . 55 0 . 17 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 16 0 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 1 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14 0 . 14
< extra_id_0 > table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . konstas et al . ( 2017 ) achieve 27 . 40 on the 200k test set . song et al . ( 2018 ) achieve 31 . 60 on the 200k test set .
< extra_id_0 > bold > meteor / bold > bold > size / bold > c > 57 . 6m c > c > c > ablation study on the ldc2017t10 development set . c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0 - 7
< extra_id_0 > table 8 shows the fraction of elements in the output that are missing in the generated sentence ( added ) and the fraction of elements in the input that are missing in the generated sentence ( miss ) . the token lemmas are used in the comparison .
< extra_id_0 > table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer .
< extra_id_0 > table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . pos and sem tagging accuracy with baselines and an upper bound is shown in table 2 .
< extra_id_0 > zh c > fr c > zh c > en c > en c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c > pos tagging accuracy c >
< extra_id_0 > table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders are shown in table 5 .
< extra_id_0 > task c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > mention c > sentiment c > mention c > mention c > gender c > 9 . 7 on a training set 10 % held - out . is the difference between the attacker ’ s score and the corresponding adversary ’ s accuracy .
< extra_id_0 > data c > task c > accuracy c > task c > accuracy c > sentiment c > 67 . 4 c > mention c > 67 . 4 c > sentiment c > 67 . 4 c > mention c > 67 . 4 c > age c > 64 . 8 c > accuracy c > mention c > 83 . 9 c >
< extra_id_0 > task acc and unbalanced leakage are presented in table 2 . mention & sentiment & sentiment splits are shown in table 2 . the unbalanced task acc and unbalanced data splits are shown in table 2 . the unbalanced task acc and unbalanced data split are shown in table 2 .
< extra_id_0 > data c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > mention c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > sentiment c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > table 6 : accuracies of the protected attribute with different encoders . leaky achieves 64 . 5 c > 67 . 8 c > 67 . 8 c > 67 . 8 c > 67 . 8 c > 59 . 3 c > 54 . 8 c > 59 . 3 c > 54 . 8 c > 59 . 3 c > 59 . 3 c > 59 . 3 c >
< extra_id_0 > ptb + finetune c > wt2 + dynamic c > ptb + finetune c > wt2 + dynamic c > yang et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2018 ) et al . ( 2017 ) et al . ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 ) ( 2017 )
< extra_id_0 > + bert time c > + ln acc c > + bert time c > + ln time c > + bert time c > + ln time c > + bert time c > + ln time c > + bert time c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > yahoo err c > yahoo time c > amafull err c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > yelppolar time c > lstm c > 74k c > 0 . 947 c > 1 . 362 c >
< extra_id_0 > bleu score on wmt14 english - german translation task . train and decode : time in seconds measured from 0 . 2k training steps on tesla p100 . train and decode : time in milliseconds used to train and decode one sentence .
< extra_id_0 > table 4 : exact match / f1 - score on squad dataset . rnet * achieves the best match / f1 - score . lstm achieves the best match / f1 - score , whereas lstm achieves the best f1 score .
< extra_id_0 > table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the f1 score on conll - 2003 english ner task .
< extra_id_0 > table 7 : test accuracy on snli and ptb task with base + ln setting and test perplexity on ptb task with base + ln setting . elrn outperforms glrn , elrn and glrn in accuracy and perplexity .
< extra_id_0 > b - 2 and r - 2 , respectively . c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] # word c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > [ italic ] w / system retrieval [ bold ] c > [ italic ] c > [ italic ] c > [ italic ] c > [ italic ] w / system retrieval [ bold ] r - 2 c > [ italic ] w / system retrieval [ bold ] # sent c > [ italic ] w / system retrieval [ bold ] c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > top - 1 / 2 : % of evaluations a system being ranked as a best . top - 1 / 2 : % of evaluations a system being ranked as a best . human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) is highlighted in bold , with statistical significance marked with ( approximation randomization test , p0 . 0005 ) . the highest standard deviation among all automatic systems is 1 . 0 . retrieval c >
< extra_id_0 > 0443 and 0 . 0761 respectively . df c > df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > 0 . 3330 c >
< extra_id_0 > slqs and docsub , respectively , and tf c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > clust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > 0491 and 0 . 0661 respectively . df c > docsub c > hclust c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > hclust c > df c > df c > df c > df c > df c > df c > df c > df clust c > c > c > c > c > c > c > c > c > c > c > clust clust clust clust c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > dsim c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > docsub c > hclust c > europarl c > maxdepth : c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > 11 . 82 c > dsim c > docsub clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust clust .
< extra_id_0 > slqs , docsub and hclust perform well on all corpus except for dlqs and docsub . europarl performs well on all corpus except for dlqs and docsub . europarl performs well on all corpus except for dlqs and docsub .
< extra_id_0 > table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . lf is the enhanced version as mentioned in table 1 . qt , s and d denote question type , answer score sampling , and hidden dictionary learning , respectively . lf + p1 denotes regressive loss , weighted softmax loss , and generalized ranking loss .
< extra_id_0 > table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . using coatt and coatt , we can see that p2 indicates the most effective one ( i . e . , hidden dictionary learning ) .
< extra_id_0 > fi - en c > lv - en c > lv - en c > cs - en c > cs - en c > cs - en c > cs - en c > lv - en c > lv - en c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c
< extra_id_0 > de - en and fi - en , respectively , and zh - en and zh - en have similar results . bertscore - f1 and ruse - f1 have the best performance , respectively . bertscore - f1 and ruse - f1 have the best performance .
< extra_id_0 > nat , and bagel bold > qual / bold > . sfhotel bold > qual / bold > achieves a better performance than bertscore - f1 and bleu - 2 , respectively . sfhotel achieves a better performance than bertscore - f1 and bleu - 2 .
< extra_id_0 > m1 and m2 respectively , and leic ( * ) c > 0 . 949 / bold > 0 . 750 / bold > 0 . 750 / bold > 0 . 750 / bold > 0 . 750 / bold > bertscore - recall c > 0 . 750 / bold > 0 . 750 / bold > 0 . 709 / bold > 0 . 750 / bold > 0 . 749 / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > / bold > bertscore - recall / bold > bertscore - recall / bold > bertscore - recall / bold > bertscore - recall / bold > bertscore - recall / bold > bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore - recall bertscore
< extra_id_0 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m4 : m0 : m0 : m0 : m0 : cyc + para : m4 : m0 : m4 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : cyc + para : m0 : m0 : m0 : cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 + cyc + para : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0 : m0
< extra_id_0 > transfer quality a > b and transfer quality tie c > semantic preservation a > b and transfer quality tie c > semantic preservation tie sim c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim c > semantic preservation tie sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim sim
< extra_id_0 > c > acc c > % of machine and human judgments that match c > 94 c > 84 c > human sentence - level validation of metrics ; 150 examples for each dataset for sim and pp ; see text for validation of gm ; see text for validation of acc ; see text for validation of gm .
< extra_id_0 > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > sim c > m1 : m0 [ italic ] + cyc + para c > 0 . 818 c > 0 . 778 c > gm c > m6 : m0 [ italic ] + cyc + para c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > gm c > m1 : m0 [ italic ] + cyc + para c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > cyc + para c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 cyc + para 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 0 . 818 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c > 27 . 3 c >
< extra_id_0 > bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than prior work at similar levels of acc . our best models achieve higher bleu than previous work , but untransferred sentences achieve the highest bleu than our best models ( right table ) .
< extra_id_0 > reparandum length [ bold ] 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 3 - 5 , rephrase 1 - 2 , rephrase 1 - 2 , rephrase 1 - 2 , rephrase 1 - 2 , rephrase 1 - 2 , rephrase 3 - 5 , restart 3 - 5 , rephrase 3 - 5 , et al . , et al . , et al . , et al . , et al . , et al . , table 2 .
< extra_id_0 > table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either in the reparandum or repair ( content - function ) , or in neither . the proportion of tokens belong to each category is shown in table 3 .
< extra_id_0 > c > [ bold ] dev mean c > [ bold ] test best c > [ italic ] c > [ bold ] dev mean c > [ bold ] dev best c > [ bold ] dev mean c > [ bold ] dev best c > [ italic ] c > early c > text + raw + innovations c > 86 . 46 c > 86 . 47 c > [ italic ] c > [ italic ] – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
< extra_id_0 > accuracy ( % ) agree c > accuracy ( % ) disagree c > accuracy ( % ) unrelated c > performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model achieves a higher accuracy than the state - of - art ones .
< extra_id_0 > table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models .
< extra_id_0 > table 3 shows the accuracy ( % ) comparisons of component models with and without attention . the results show the effectiveness of word attention and graph attention for this task . please see section 6 . 2 for more details .
< extra_id_0 > model c > [ bold ] 1 / 1 c > [ bold ] all c > [ bold ] all c > [ bold ] all c > [ bold ] embedding + t c > 75 . 3 c > 59 . 8 c > embedding + t c > 75 . 3 c > 59 . 8 c > embedding + t c > 75 . 3 c > embedding + t cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn cnn
< extra_id_0 > trigger [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ) c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold ] classification ( % ) c > [ bold ] identification ( % ) c > [ bold ] identification ( % ) c > [ bold [ bold [ bold [ bold ] identification ( % ) c > [ bold [ bold [ bold [ bold ] identification ( % ) c > [ bold [ bold [ bold ] identification ( % ) c > [ bold [ bold [ bold [ bold ] identification ( % ) c > [ bold [ bold ] classification ( % ) c > [ bold [ bold [ bold
< extra_id_0 > wer c > dev perp c > test perp c > test wer c > test perp c > test wer c > spanish - only - lm c > 329 . 68 c > 25 . 1 c > 31 . 4 c > 13 . 89 c > 13 . 89 c > cs - only + vocab - lm c > 43 . 61 c >
< extra_id_0 > train dev and on the test set using discriminative training with only subsets of the code - switched data . table 4 summarizes the results of discriminative training with only subsets of the code - switched data using discriminative training .
< extra_id_0 > table 5 summarizes the results on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) . fine - tuned - disc achieves a higher accuracy than fine - tuned - disc ( 71 . 33 ) .
< extra_id_0 > table 7 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset are shown in table 7 .
< extra_id_0 > table 5 summarizes the performance of type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . type - aggregated gaze features achieve better recall , recall and f1 - score ( f1 - score ) for the conll - 2003 dataset .
< extra_id_0 > table 1 shows the results on belinkov2014exploring ’ s ppa test set . lstm - pp and glove embeddings perform better than lstm - pp and lstm - pp embeddings .
< extra_id_0 > table 2 summarizes results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the rbg dependency parser outperforms the hpcd ( full ) and hpcd ( full ) .
< extra_id_0 > table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the effect of removing sense priors and context sensitivity ( attention ) from the model is shown in table 3 .
< extra_id_0 > table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun are shown in table 2 .
< extra_id_0 > and mscoco17 . the results show that subs1m ( lm + ms - coco ) outperforms subs1m ( lm + ms - coco ) and subs1m ( lm + ms - coco ) with domain - tuned labels . however , subs1m ( lm + ms - coco ) also outperforms subs1m ( lm + ms - coco ) by a significant margin .
< extra_id_0 > mscoco17 and en - de c > flickr16 c > flickr17 c > mscoco17 c > multi30k c > 61 . 4 c > + autocap 1 - 5 ( concat ) c > 32 . 2 c > + autocap 1 - 5 ( concat ) c > 32 . 2 c > + autocap 1 - 5 ( concat ) c > 32 . 2 c > + autocap 1 - 5 ( concat
< extra_id_0 > and mscoco17 . enc - gate and dec - gate achieve better performance than enc - gate and dec - gate , respectively . enc - gate and dec - gate achieve better performance than enc - gate and dec - gate , respectively . enc - gate and dec - gate achieve better performance than enc - gate and enc - gate , respectively .
< extra_id_0 > and mscoco17 . en - fr c > en - fr c > flickr16 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > mscoco17 c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c > c >
< extra_id_0 > ttr c > mtld c > yule ’ s i c > mtld c > en - fr - trans - ff en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back en - fr - trans - back c > mtld c > mtld c > en - fr - trans - back en - fr - trans -
< extra_id_0 > table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – es has 1 , 472 , 203 parallel sentences in the train , test and development splits , and en – es has 4 , 99 , 487 parallel sentences .
< extra_id_0 > table 2 : training vocabularies for the english , french and spanish data used for our models . the training vocabularies for the english , french and spanish data used for our models are shown in table 2 .
< extra_id_0 > table 5 : automatic evaluation scores ( bleu and ter ) for the rev systems ( en - fr - rnn - rev and en - fr - trans - rev ) . the bleu and ter scores for the rev systems are shown in table 5 .
< extra_id_0 > recall @ 10 ( % ) and median rank ( 0 . 0 ) on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the mean mfcc is 0 and the mean mfcc is 0 . 0 .
< extra_id_0 > recall @ 10 ( % ) c > rsaimage c > 0 . 4 c > chance c > 0 c > 0 . 0 c > chance c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0 c > 0
< extra_id_0 > she turns in a u > screenplay that u > at the edges edges edges curves so clever you want hate hate hate hate hate hate . we report further examples in the appendix .
< extra_id_0 > bold > rnn / bold > bold > dan / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold > cnn / bold >
< extra_id_0 > the numbers indicate the changes in percentage points with respect to the original sentence . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate that the score increases in positive and negative sentiment .
< extra_id_0 > better than pubmed . bold > sst - 2 / bold > positive c > better than bold > sst - 2 / bold > negative c > better than bold > sift / bold > to evaluate c > concluded r > r > r > r > r > r > r > r > r > r > r > r > r > / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold > pmi / bold >
