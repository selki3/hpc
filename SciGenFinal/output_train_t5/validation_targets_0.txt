table 2 compares the throughput of performing inference and training on the treelstm model using our implementation , the iterative approach , and the folding technique . the amount of resources is sufficient for executing forward computations , and therefore our framework outperforms the folding technique for the inference task with up to 4 . 93x faster throughput . as a result , the folding technique performs better than the recursive approach for the training task .
table 1 shows the throughput of training the treernn model using these three datasets . for all batch sizes , the training throughput on the balanced dataset is the highest , while the throughput on the linear dataset is the lowest . as a result , our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees . another interesting fact in table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset , as the batch size increases . on the contrary , for the linear dataset , the recursive implementation fails to efficiently make use of cpu resources and thus the performance gain provided by increasing the batch size is relatively high .
table 2 presents the optimal values for each configuration using different dependency representations . we see that the optimized parameter settings vary for the different representations , showing the importance of tuning for these types of comparisons . the results furthermore show that the sdps based on the stanford basic ( sb ) representation provide the best performance , followed by the conll08 representation . we observe that the results for the ud representation are quite a bit lower than the two others .
we find that the effect of syntactic structure varies between the different relation types . however , the sdp information has a clear positive impact on all the relation types ( table 1 ) .
accordingly , as table 3 shows for the essay level ( paragraph level omitted for space reasons ) , results are generally weaker : as in eq . ( 1 ) — the overall result is worst . we find that when we train stagbl with only its main task — with label set in y contrast , when we include the ' natural subtasks ' " c " ( label performance increases typically by a few percentage points .
we train and test all parsers on the paragraph level , mate is slightly better kiperwasser performs decently on the approximate match level , but not on exact level . the best parser by far is the lstm - parser . it is over 100 % better than kiperwasser on exact spans and still several percentage points on approximate spans . : performance of dependency parsers , stagblcc , lstm - er and ilp ( from top to bottom ) . on the other hand , our results in table 2 indicate that the neural taggers blcc and blc ( in the lstmer model ) are much better at such exact identification than either the ilp model or the neural parsers .
these results detail that the taggers have lower standard deviations than the parsers . the difference is particularly striking on the essay level where the parsers often completely fail to learn , that is , their performance scores are close to 0 % .
the results for testing on cleaned data ( table 3 , top half ) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging ( cf . section 3 ) , as reflected in the lower woms . however , the improved results for training and testing on clean data ( i . e . seeing equally challenging examples at training and test time ) , suggest the increase in performance can be attributed to data accuracy rather than diversity . looking at the detailed results for the number of added , missing , and wrong - valued slots ( add , miss , wrong ) , we observe more deletions than insertions , i . e . the models more often fail to realise part of the mr , rather than hallucinating additional information . however , the results in bottom halves of tables 2 and 3 do not support our hypothesis : we observe the main effect on ser from cleaning the missed slots , reducing both insertions and deletions . again , one possible explanation is that cleaning the missing slots provided more complex training examples .
this resulted in 20 % reduction for train and ca . 8 % reduction for dev in terms of references ( see table 1 ) . on the other hand , the number of distinct mrs rose sharply after reannotation ; the mrs also have more variance in the number of attributes . this means that the cleaned dataset is more complex overall , with fewer references per mr and more diverse mrs .
the results in table 2 ( top half ) for the original setup confirm that the ranking mechanism for tgen is effective for both woms and ser , whereas the sc - lstm seems to have trouble scaling to the e2e dataset . we hypothesise that this is mainly due to the amount of delexicalisation required . however , the main improvement of ser comes from training on cleaned data with up to 97 % error reduction with the ranker and 94 % without . 11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism , such as the reranker ( 0 . 97 % vs . 4 . 27 % ser ) . woms are slightly lower for tgen trained on the cleaned data , except for nist , which gives more importance to matching less frequent n - grams . this suggests better preservation of content at the expense of slightly lower fluency . in other words , however , the results in bottom halves of tables 2 and 3 do not support our hypothesis : we observe the main effect on ser from cleaning the missed slots , reducing both insertions and deletions . again , one possible explanation is that cleaning the missing slots provided more complex training examples .
the results in table 4 confirm the findings of the automatic metrics : systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance , with the fully - cleaned one showing a few more slight disfluencies than the other . the systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency . all fluency problems we found were very slight and no added or wrong - valued slots were found , so missed slots are the main problem .
moreover , we compare our dcgcn ( single ) and dcgcn ( ensemble ) model with the state - of - the - art semi - supervised models on the amr15 test set ( table 3 ) , including non - neural methods such as tsp ( song et al . , 2016 ) , pbmt ( pourdamghani et al . , 2016 ) , tree2str ( flanigan et al . , 2016 ) and snrg ( song et al . , 2017 ) . all these non - neural models train language models on the whole gigaword corpus . our ensemble model gives 28 . 2 bleu points without external data , which is better than them . following konstas et al . ( 2017 ) ; song et al . ( 2018 ) , we also evaluate our model using external gigaword sentences as training data . we first use the additional data to pretrain the model , then finetune it on the gold data . using additional 0 . 1m data , the single dcgcn model achieves a bleu score of 29 . 0 , which is higher than seq2seqk ( konstas et al . , 2017 ) and graphlstm ( song et al . , 2018 ) trained with 0 . 2m additional data . when using the same amount of 0 . 2m data , the performance of dcgcn is 4 . 2 and 3 . 4 bleu points higher than seq2seqk and graphlstm . dcgcn model is able to achieve a competitive bleu points ( 33 . 2 ) by using 0 . 3m external data , while graphlstm achieves a score of 33 . 6 by using 2m data and seq2seqk
table 2 shows the results on amr17 . our single model achieves 27 . 6 bleu points , which is the new state - of - the - art result for single models . in particular , our single dcgcn model consistently outperforms seq2seq models by a significant margin when trained without external resources . for example , the single dcgcn model gains 5 . 9 more bleu points than the single models of seq2seqb on amr17 . these results demonstrate the importance of explicitly capturing the graph structure in the encoder . in addition , our single dcgcn model obtains better results than previous ensemble models . for example , on amr17 , the single dcgcn model is 1 bleu point higher than the ensemble model of seq2seqb . our model requires substantially fewer parameters , e . g . , the parameter size is only 3 / 5 and 1 / 9 of those in ggnn2seq and seq2seqb , respectively . the ensemble approach based on combining five dcgcn models initialized with different random seeds achieves a bleu score of 30 . 4 and a chrf + + score of 59 . 6 . under the same setting , our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms . for ggnn2seq , our single model is 3 . 3 and 0 . 1 bleu points higher than their single and ensemble models , respectively . we also have similar observations in term of chrf + + scores for sentence - level evaluations . dcgcn also outperforms graphlstm by 2 . 0 bleu points in the fully supervised setting as shown in table 3 . note that graphlstm uses char -
table 4 shows the results for the englishgerman ( en - de ) and english - czech ( en - cs ) translation tasks . bow + gcn , cnn + gcn and birnn + gcn refer to employing the following encoders with a gcn layer on top respectively : 1 ) a bag - of - words encoder , 2 ) a one - layer cnn , 3 ) a bidirectional rnn . pb - smt is the phrase - based statistical machine translation model using moses ( koehn et al . , 2007 ) . our single model dcgcn ( single ) achieves 19 . 0 and 12 . 1 bleu points on the en - de and encs tasks , respectively , significantly outperforming all the single models . for example , compared to the best gcn - based model ( birnn + gcn ) , our single dcgcn model surpasses it by 2 . 7 and 2 . 5 bleu points on the en - de and en - cs tasks , respectively . our models dcgcn ( single ) and dcgcn ( ensemble ) consist of full gcn layers , removing the burden of employing a recurrent encoder to extract non - local contextual information in the bottom layers . compared to non - gcn models , our single dcgcn model is 2 . 2 and 1 . 9 bleu points higher than the current state - of - theart single model ( ggnn2seq ) on the en - de and en - cs translation tasks , respectively . in addition , our single model is comparable to the ensemble results of seq2seqb and ggnn2seq , while the number of parameters of our models is only about 1 / 6 of theirs . additionally , the ensemble dc
layers in the sub - block . table 5 shows the effect of the number of layers of each sub - block on the amr15 development set . densenets ( huang et al . , 2017 ) use two kinds of convolution filters : 1 1 and 3 3 . similar to densenets , we choose the values of n and m for layers from [ 1 , 2 , 3 , 6 ] . we choose this value range by considering the scale of non - local nodes , the abstract information at different level and the calculation efficiency . for brevity , we only show representative configurations . we first investigate dcgcn with one block . in general , the performance increases when we gradually enlarge n and m . for example , when n = 1 and m = 1 , the bleu score is 17 . 6 ; when n = 6 and m = 6 , the bleu score becomes 22 . 0 . we observe that the three settings ( n = 6 , m = 3 ) , ( n = 3 , m = 6 ) and ( n = 6 , m = 6 ) give similar results for both 1 dcgcn block and 2 dcgcn blocks . since the first two settings contain less parameters than the third setting , it is reasonable to choose either ( n = 6 , m = 3 ) or ( n = 3 , m = 6 ) . for later experiments , we use ( n = 6 , m = 3 ) .
the first block in table 6 shows the performance of our two baseline models : multi - layer gcns with residual connections ( gcn + rc ) and multi - layer gcns with both residual connections and layer aggregations ( gcn + rc + la ) . in general , increasing the number of gcn layers from 2 to 9 boosts the model performance . however , when the layer number exceeds 10 , the performance of both baseline models start to drop . for example , gcn + rc + la ( 10 ) achieves a bleu score of 21 . 2 , which is worse than gcn + rc + la ( 9 ) . in preliminary experiments , we cannot manage to train very deep gcn + rc and gcn + rc + la models . in contrast , our dcgcn models can be trained using a large number of layers . for example , dcgcn4 contains 36 layers . when we increase the dcgcn blocks from 1 to 4 , the model performance continues increasing on amr15 development set . we therefore choose dcgcn4 for the amr experiments . using a similar method , dcgcn2 is selected for the nmt tasks . when the layer numbers are 9 , dcgcn1 is better than gcn + rc in term of b / c scores ( 21 . 7 / 51 . 5 v . s . 21 . 1 / 50 . 5 ) . gcn + rc + la ( 9 ) is sightly better than dcgcn1 . however , when we set the number to 18 , gcn + rc + la achieves a bleu score of 19 . 4 , which is significantly worse than the bleu score obtained by dcgcn2 ( 23 . 3 ) . we also try gcn + rc + la ( 27 ) , but it does not converge . in conclusion , these results above can show the robustness and effectiveness of our dcgcn models .
