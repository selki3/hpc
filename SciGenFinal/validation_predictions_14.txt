table 2 shows the throughput and inference performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iteration approach is more appealing for production use , as it allows more instances to be iterated over a single dataset , thereby reducing the number of instances for each iteration .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that selecting the maximum pooling size and the dropout probability are the most important factors in model performance improvement . the softplus representation also improves over the default representation ,
table 1 shows the effect of using the shortest dependency path on each relation type . it can be observed that macro - averaged models perform better than the best f1 models without sdp , because their dependency paths are shorter .
the results of r - f1 and f1 are shown in table 1 . the results are summarized in bold . for brevity we only report results for those using the " y " - 3 embeddings with 50 % f1 coverage . for " y - 3 " we only show results for models using the full complement of f1 features .
the results of paragraph prediction accuracy are shown in table 1 . the results are presented in terms of paragraph level acc . and fullword accuracy . for brevity we only report results for the full paragraph accuracy . the results show that mst - parser achieves an accuracy of 100 % on average , with 50 % on fullword .
the performance of the parser for the two indicated systems is shown in table 4 . the average c - f1 score for each system is reported in parentheses . the difference in performance between the two indicates that the parser performs better at the paragraph level .
the results are shown in table 1 . the results show that the original tgen model can be further improved with the help of a simple layer - based cleaning program . the results of tgen + and tgen − are slightly worse than the original model , but still superior than the cleaned model . finally , the results of sc - lstm are slightly better than tgen , indicating that more data is available for further study .
table 1 compares the performance of the original e2e dataset with the cleaned version . the difference in mr statistics is minimal ( 0 . 5pt / 2pt ) but significant ( 17 . 69 % ) , with the difference in ser percentage being much larger ( 71 . 42 % ) . the difference between the original and the cleaned dataset is less pronounced for the test dataset , but larger for the train dataset , indicating that the training data is more interpretable .
the results are shown in table 1 . the results of tgen + and tgen − models are presented in bold . as expected , the results are slightly worse than those of original tgen models . however , the difference is less pronounced for sc - lstm , which obtains a performance improvement of 2 . 83 points over the original model .
results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) are shown in table 4 . the difference in errors between the original and the cleaned set is minimal , however we found a significant difference in the number of errors in the training set ( around 5 % ) , which shows the extent to which tgen can be improved with additional training data .
for completeness , here we also compare our model against other state - of - the - art approaches . the first set of results in table 1 shows that the hierarchical clustering approach by itself achieves the best results with an all score of 25 . 2 out of a possible 25 . 9 . the second set shows that graphlstm outperforms all the base models with a gap of 2 . 6 points from the last published results . the third set in the table shows the results of multi - headed clustering .
table 2 shows the performance of our model with respect to amr17 . our model achieves 24 . 5 bleu points . by comparison , the best performance by any ensemble model is achieved by seq2seqb ( beck et al . , 2018 ) , which achieves a performance of 52 . 6 points . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves a final score of 59 . 4 points .
table 3 presents the results for english - german , czech and slovakian . the results are presented in table 3 . the performance of each model is presented in terms of performance on the test set in english , german and czech . as the results show , the performance gap between the single and the multi - class models is slim , with the former achieving a performance gap of 2 . 8 % on average in english and 3 . 6 % on the czech .
the effect of the number of layers inside each dc is shown in table 5 . the first group shows that more layers inside the network helps the model to improve interpretability . the second group shows a drop in performance as a result .
comparisons with baselines are shown in table 6 . rcn with residual connections outperforms rcn without residual connections . gcn with rc ( 2 ) and rc + la ( 6 ) achieves the best results , outperforming the baselines in terms of bias metric by a noticeable margin .
the results are shown in table 4 . the first group shows that the model with the best performance is the dcgcn ( 1 ) , while the second group shows the worst performance . the difference in performance between the two groups is mostly due to the smaller size of the training set and the number of iterations required to encode each sentence .
table 8 shows the ablation study results for the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly decreases the overall density of connections .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results of " - global node " and " - linear combination " modes result in significantly worse performance than " - direction aggregation " and " coverage mechanism " modes .
table 7 shows the performance of our initialization strategies for different probing tasks . glorot obtains high scores for initialization and tense metrics , indicating that it is well - equipped to handle the task at hand . subjnum achieves state - of - the - art results with a score of 82 . 8 out of a possible 100 .
the results are shown in table 3 . the h - cbow achieves state - of - the - art results . it obtains the best generalization performance with a gap of 3 . 6 bshift points from the previous best state - ofthe - art model . subjnum is the most productive classifier , achieving a joint feature - level score of 87 . 6 out of a possible 100 .
the results are shown in table 5 . hybrid outperforms the original method and cbow achieves the best results . the results reconfirm that the cbow / 784 embedding method is indeed superior to the original cmow model in terms of test set quality . it achieves state - of - the - art results on three of the four test sets ( sst2 , sst5 , sts - b , and sick - r ) .
table 3 shows the performance of our models on the unsupervised downstream tasks . the cbow and cmow methods show considerable improvements in performance over the best state - of - the - art approaches . hybrid outperforms the original method , showing a considerable margin of performance improvement . cbow shows a margin of more than 10 % improvement over the strong monolingual approach . with respect to sts13 and sts16 , the difference is less pronounced , but still significant .
table 8 presents the performance of our system for initialization and supervised downstream tasks . our system achieves state - of - the - art results , outperforming glorot ( 86 . 6 % vs . 86 . 6 % ) , sick - e ( 71 . 4 % ) and trec ( 73 . 7 % ) by a significant margin .
scores for different training objectives on the unsupervised downstream tasks are shown in table 6 . the cbow - r achieves the best performance with a score of 43 . 2 % on the sts13 and sts16 tasks . it outperforms the cmow - based method with a gap of 3 . 9 % on sts12 and 6 . 6 % .
the results are shown in table 1 . the best results are obtained by the subjnum method , which achieves state - of - the - art results in terms of concatenation . it obtains a joint feature - vec and topconst score of 8 . 6 / 10 . 0 and 7 . 9 / 7 . 8 respectively , with a gap of 2 . 5 / 4 . 5 points from the previous best state - ofthe - art method .
the results are shown in table 1 . the best results are obtained by the method of cbow - r . it achieves state - of - the - art results with a score of 90 . 6 % on subj and 87 . 9 % on sst5 .
the results are shown in table 1 . name matching and supervised learning achieve state - of - the - art results . in general terms , they outperform the best state - ofthe - art systems on every metric by a significant margin . for example , name matching achieves a score of 96 . 03 % on e + loc and 93 . 57 % on per , with a gap of 2 . 36 % on misc . supervised learning achieves a performance gap of 3 . 48 % on loc and 6 . 57 % . the performance gap is narrower on τmil - nd , but still suggests that supervised learning is more effective than random clustering . finally , the performance gap between the two systems is narrower under λmisc , but not much narrower than that under per .
results on the test set under two settings are shown in table 2 . name matching and supervised learning achieve the best results , with τmil - nd achieving the best f1 scores . supervised learning also achieves the highest f1 score , with a result of 71 . 38 % in the case of model 1 and 69 . 59 % in model 2 . these results show that the model with the best named entity is already well - equipped to perform this task in the production setting . finally , the effectiveness of our model is proved by the large difference in f1 scored between the two sets under the best and worst case scenarios .
the results in table 6 show that the g2s - gat model is comparable to the best previous state - of - the - art models in terms of generalization . however , the performance gap is narrower than the gap between the best state - ofthe - art model and the best generalized model . the difference is mostly due to the smaller size of the dataset ( s2s ) and the number of instances in which the model can be trained on a single dataset ( ggnn ) .
table 3 presents the results of experiments on the ldc2015e86 and ldc2017t10 datasets . the results are summarized in table 3 . the g2s - gat model outperforms the s2s model in terms of bleu score and meteor score , while the model by song et al . ( 2018 ) achieves a performance improvement of 3 . 42 points over the previous best state - of - the - art model ( g2sggnn ) . the model by konstas et al . , ( 2017 ) obtains the best performance on ldc2016e86 , but is slightly worse than the previous state of the art model on the other two datasets .
results on ldc2015e86 test set when models are trained with additional gigaword data are shown in table 3 . the g2s - ggnn model achieves state - of - the - art results with a bleu score of 32 . 23 / 71 . 40 and 28 . 20 / 28 . 23 on the test set .
results of the ablation study on the ldc2017t10 development set . table 4 shows that the model with the best performance is the geb model with 27 . 6 % improvement in bilstm performance .
the results are shown in table 1 . the first group shows that g2s - gat has the best performance with a f1 score of 3 . 51 % and a graph diameter diameter of 7 . 28 × 10 . 28 .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . the model with the best performance is the g2s - gat model , with a f1 score of 50 . 06 and a miss score of 28 . 35 . it outperforms the other models with a gap of 3 . 67 points .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy improves as the number of target languages grows , but does not improve significantly over the size of the corpus .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . the results show that the classifier using unsupervised word embeddings is more accurate than the word2tag encoder with a baseline of 91 . 41 % pos and 91 . 55 % sem .
table 4 presents the system ' s performance on the three types of tagging accuracy metrics . the system achieves state - of - the - art results on all metrics with a minimum of a gap of 10 . 8 % on average compared to previous state - ofthe - art systems .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the bi - based approach improves the pos tagging accuracy over the residual approach , since it eliminates the reliance on repetition . however , the difference in overall accuracy between bi - and res - based approaches is less pronounced for english , indicating that there is still a need to design more sophisticated approaches to improve interpretability .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary is small but significant ( p = 0 . 01 ) .
accuracies are shown in table 1 . our model outperforms the state - of - the - art in terms of accuracy for a single task . for example , it achieves an accuracy of 83 . 2 % on the dial task and 77 . 9 % for the mention task . the gender - based classifiers outperform the previous state of the art on both tasks .
table 2 shows the results for balanced and unbalanced data splits . the results show that the racial disparities in the task accuracy are less pronounced for the unbalanced dataset , but still indicate that there is a significant imbalance in the distribution of the information that gets leaked .
performances on different datasets with an adversarial training set are shown in table 3 . the results are summarized in terms of the difference in the attacker score and the corresponding adversary ’ s accuracy . δ is the difference between the amount of time taken to train the task and the amount taken to leak the assigned attribute .
the results are shown in table 6 . the rnn encoders do not seem to be able to distinguish between the protected and non - protected attributes well , however , their performance is comparable .
the performance of our model with finetune on the base and dynamic modes is presented in table 1 . the results are presented in bold . our model obtains the best results with a f1 score of 85 . 97 out of a possible 100 .
the results presented in table 1 show that the lstm achieves state - of - the - art results with a gap of 3 . 5 bert acc points from the last published results ( rocktäschel et al . , 2016 ) . the difference in performance between the base and the best performing model is less pronounced when using the base time - averaged approach , but still suggests significant performance improvement . table 1 compares the performance of different approaches with each other in terms of acc and time taken to compute the required parameters . the results reconfirm that the advantage of using the baselines alone is considerable when using a multi - headed attention architecture ( ln ) and named entity recognition time ( bert ) .
the results of zhang et al . ( 2015 ) are shown in table 1 . the summaries presented in the table summarize the results of experiments performed on the coreference dataset of amapolar , yahoo time and yelppolar . the results are summarized in terms of the number of parameters and the average number of responses per second for each error generation algorithm . for example , this model obtains an err of 4 . 86 and a time - error - free f1 score of 3 . 86 . these results show that the model performs well on both datasets with different difficulty levels .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task . our model obtains the best performance with a score of 42 . 67 / 71 . 86 . the performance gap between gnmt and olrn is narrower than the gap between gru and sru , but still represents a significant performance gap .
table 4 shows the exact match / f1 - score of our model on the squad dataset . our model obtains the best performance with a f1 score of 76 . 14 / [ bold ] and a match rate of 7 . 83 / [ cao et al . , 2017 ] . the difference in match rate between the base and the best - performing model ( + elmo ) is less pronounced , but still shows that our model has the best generalization ability .
table 6 shows the f1 score of our model ( lstm * ) on the conll - 2003 english ner task . the model obtains the best performance with a score of 89 . 56 out of a possible 89 . 94 .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the model with the best performance is the glrn model with a base - biased ln - based model .
table 1 compares the performance of word embeddings with the best state - of - the - art systems . we benchmark against the following systems : word2vec , oracle , system retrieval , multi - tranlink ( mtr ) , and human . the results are presented in table 1 . the performance gap between human and system is much narrower than that between mtr and oracle . retrieving the same word twice in the same sentence results in significantly better results for both systems . oracle outperforms human in terms of average number of words per sentence .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . the average number of errors per evaluation is less than 0 . 5 , with seq2seq achieving a lower standard deviation than candela and h & w hua and wang ( 2018 ) . overall , the quality of our system is slightly better than the best automatic system .
the performance of these models on the test set is presented in table 5 . the results are summarized in terms of p < 0 . 001 . for example , for the " ted talks " dataset , we see that it achieves the best performance with a p - score of 0 . 86 . for the " europarl " dataset we see an overall improvement of 1 . 36 . for " docsub " and " slqs " we see a gap of 2 . 36 and 1 . 37 points , respectively .
the results are shown in table 1 . table 1 presents the results for english , spanish , french , dutch , russian and spanish . the results of the best performing classifiers are presented in bold . for english , we see that our model outperforms the best previous state - of - the - art classifiers on every metric by a significant margin . on the other hand , for spanish , our model performs slightly better than the previous best state - ofthe - art model .
the performance of these models on the test set is presented in table 3 . the results are summarized in terms of p < 0 . 001 . for the " ted talks " dataset , we see that the model performance obtained by parallel outperforms the model by a significant margin . for " europarl " and " docsub " we see a gap of more than 2 . 5 points in performance .
the performance of each metric is presented in table 1 . the most interesting ones are the averagedepth and maxdepth metrics . for europarl , the average depth metric is set at 11 . 05 , which means that the number of tokens per row is 1 . 05 times the average of the max - pooled ones . according to the table , there is a 1 . 46 / 1 . 78 correlation between average and max - depth metrics , with a gap of 3 . 86 / 3 . 46 points from the last published results .
the performance of each metric is presented in table 1 . the most interesting ones are the averagedepth and maxdepth metrics . europarl has the best performance with a maxdepth of 9 . 43 / 9 . 29 and a avgdepth of 2 . 29 / 2 . 29 , respectively . according to the table , the maxdepth metric is the most important metric for semantic metrics . it helps the parser to select the best terms for each row and label them with the correct semantic entity .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model outperforms the baseline model in terms of both qt and d scores . the difference is less pronounced with respect to r0 and r3 scores , but still shows significant performance improvement . we conjecture that the enhanced model has better generalization ability .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . note that only applying p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . using p2 also improves the rva score for baseline and coatt models , but does not improve the performance for visdial .
the results are shown in table 5 . the hmd - f1 model outperforms the previous state - of - the - art approaches on both hard and soft alignments . it achieves the best results with a precision score of 0 . 823 / 0 . 866 on the " hard alignments " compared to the " soft alignments " .
the results of baselines are shown in table 1 . the average score of the baselines for each setting is reported in bold . for example , bertscore - f1 has the highest average score , while ruse ( * ) is the second - highest .
the results are shown in table 1 . the first set of results show that the baselines set by bertscore - f1 achieves the best results with a f1 score of 0 . 176 . the second set shows the performance of bleu - 2 with the highest score . next , we show the results of baselines using the sent - mover and w2v classification schemes . the results again show that these methods significantly improve the generalization ability of bagel and sfhotel .
the results are shown in table 1 . the first set of results show that the word - mover task can be further improved with a stepwise improvement in the recall metric . the second set shows that the feature - rich clustering approach further boosts performance for the classification task .
the results of " para - para " model are shown in table 6 . the results are summarized in terms of acc and gm scores . in general terms , the results are as follows : " m0 [ ital ] + cyc " achieves the best results with a f1 score of 0 . 81 , and " m6 " gets the best score of 1 . 07 . para - based models outperform " n " - based models with a gap of 3 . 8 points .
table 3 presents the transfer quality and semantic preservation results . the results are presented in tables 3 and 4 . the transfer quality results are summarized in terms of a - b and b - a metrics . they show that yelp has the best transfer quality . semantic preservation results show that the semantic preservation performed by yelp is comparable to the best state - of - the - art state - ofthe - art transfer quality model . finally , the variation of the fluency test set is statistically significant with respect to transfer quality as well as semantic preservation .
table 5 shows the results of human sentence - level validation for each metric . the summaries generated by our system match the human ratings of semantic preservation and syntactic fluency , but do not match the accuracy of generalization ( see § 2 ) .
the results are shown in table 6 . the results of m1 and m2 show that the model with the best performance is m0 [ ital ] , while m3 [ ital ] is slightly worse than m6 [ ital ] .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results in table 6 show that our model achieves higher acc than prior work with similar training data and the same number of tokens , indicating that our classifiers are better at this task . however , the difference between transfer and untransferred sentences is less pronounced , with our model achieving the highest acc .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average number of repetition tokens in a sentence , followed by the number of iterations for each repetition token . for nested disfluencies , we only report the length of the repetition tokens that are in the same sentence , i . e . , those that are 1 - 2 .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . table 3 shows the percentage of tokens in each category that belong to each category and the average length of the repair tokens . percentages in parentheses show the fraction of tokens per category whose content word is in the repair ( or repair ( function - function ) sentence .
the results are shown in table 1 . the results show that the text - rich innovations model outperforms the single - class approach when trained and tested on the test set with the best performing feature set . moreover , the performance gap between the best and average feature - rich model is narrower when using both text and innovations . in particular , the model trained on the best test set by itself achieves the best performance with 86 . 53 % on average .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model achieves the best performance with an accuracy of 83 . 43 % on the test dataset . the accuracy increase from the average of the original embeddings to the best performing model is almost entirely due to a reduction in false positive responses .
accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . attention - aware neuraldater outperforms the attentive neuraldater and ac - gcn , while maxent - joint achieves the best performance . the joint model outperforms both the adversarial and attentive neuraldater methods on both datasets .
table 3 compares the performance of our neuraldater model with and without word attention . our model obtains 62 . 6 % accuracy on the word attention task . the accuracy is slightly higher than the performance with s - gcn , indicating the effectiveness of graph attention .
the results are shown in table 1 . the first group shows that the argument argument is the most important part of the model , followed by the embedding stage . the second group shows the performance of all the stages combined . embedding + t achieves state - of - the - art results , outperforming both cnn and dmcnn . finally , jrnn achieves the best performance with a gap of 3 . 6 points from the previous best state - ofthe - art model .
table 1 presents the results on event identification and event classification . the system performs well on both datasets with a gap of 10 . 8 % and 6 . 9 % on the error rate , respectively , compared to previous state - of - the - art systems . on the argument dataset , the system performs better than previous state of the - art models on both event and role - based validation . on the event dataset , it achieves a performance gap of 2 . 7 % and 4 . 9 % , respectively , with respect to f1 and f1 scores .
for english - only and spanish - only learners , fine - tuned and shuffled - lm outperform the original models , but do not exceed the performance of the best - performing single - word learner model . the results are shown in table 3 . regularization reduces performance for both languages , but does not improve performance for spanish - language learners . as expected , the performance gap between the best and worst - performing model is narrower when using the lexical features of the original language .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the original cs - trained model with a gap of 3 . 5 bleu from the full train test set .
the results are shown in table 5 . the fine - tuned - disc model shows marked performance improvement over the monolingual approach . it achieves 75 . 40 % accuracy on the test set compared to the original cs - only model ( 71 . 60 % vs . 71 . 40 % ) , and 75 . 33 % accuracy vs . 75 . 80 % on the dev set .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( p < 0 . 01 ) with a f1 score of 1 . 61 , which indicates significant improvement in the interpretability .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvements in precision and recall are statistically significant ( p < 0 . 01 ) with a f1 score of 0 . 03 , which indicates significant improvement in recall without a drop in performance .
results on belinkov2014exploring ’ s test set are shown in table 1 . the hpcd ( full ) and glove - extended embeddings outperform the ontolstm - pp model with a gap of 3 . 7 points from the original ( farrequi et al . , 2015 ) . the difference is narrower than the gap between syntactic - sg and wordnet , which shows the performance of the augmented reality decoding method . syntactic sg embedding achieves the best results with a match of 88 . 8 % on the test set , outperforming the original lstm by a margin of 2 . 3 points .
results shown in table 2 show that the hpcd - based uas outperforms the ontolstm - pp - based system with features coming from various pp attachment predictors and oracle attachments . the performance gap is narrower with the use of oracle pp as an attachment predictor , however it is still significant .
the results are shown in table 3 . the effect of removing sense priors and context sensitivity ( attention ) from the model shows that the model performs better with fewer instances requiring context sensitivity .
the results in table 2 show that using subtitle data and domain tuning improves the bleu % score for image caption translation . the multi30k dataset outperforms the en - de and mscoco17 sets , but does not exceed the performance of en - fr and flickr17 , indicating that the domain - tuned model can handle the additional subtitle data better . finally , the performance improvement is much larger when the ensemble - of - 3 model is used , showing that domain tuning does improve the translation performance .
the results of domain - tuned h + ms - coco are shown in table 3 . the results are summarized in terms of a and t . the h + model achieves state - of - the - art results with an f1 score of 66 . 2 / 60 . 7 and t - score of 67 . 3 / 59 . 7 , respectively , with an absolute improvement of 2 . 8 / 3 . 0 and 6 . 4 / 4 . 9 points over the strong baselines .
table 4 shows the bleu scores for en - de , en - fr , and mscoco17 models . adding automatic image captions improves performance for both sets . the results show that the multi - task approach improves the general performance for all models except for the case of flickr17 . it also improves the multi30k model , since it requires fewer instances of each caption to appear in the same sentence .
the results in table 5 show that both enc - gate and dec - gate strategies achieve high bleu % scores ( 68 . 86 % and 62 . 40 % ) , respectively , compared to the performance of en - de and mscoco17 using the original embeddings ( 62 . 58 % ) and the multi30k + ms - coco + subs3mlm model ( 52 . 38 % ) . the difference in performance between the methods is mostly due to the smaller size of the encoder / decoder set , however , it does not represent a significant performance drop . encoding and decoding achieve remarkably similar results , with the former achieving an overall improvement of 3 . 38 % and the latter a gain of 2 . 86 % .
the results of " multi - lingual " and " visual features " approaches are shown in table 1 . the results are summarized in bold . sub - 3m and subs6m lm detectrons achieve the best results with scores of 62 . 72 % and 62 . 86 % , respectively , on the combined test set , while " gn2048 " achieves the best performance with 67 . 43 % . the ensemble - of - 3 approach by itself achieves a lower performance than the text - only approach , however , it achieves a better generalization result than the monolingual approach .
the results are shown in table 1 . the performance of each approach is reported in terms of ttr and mtld scores . for example , en - fr - ht achieves the best results with a ttr score of 9 . 01 out of 10 . 86 and a mtld score of 6 . 43 . however , the performance gap with en - es - ht is much narrower , with an f1 score of 1 . 43 out of 100 .
the number of parallel sentences in the training , test and development splits for the language pairs we used . table 1 shows that en – fr has 1 , 472 , 203 sentences in total , which is 7 . 7 % more than en – es .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to the src and trg measures , which shows that the model can learn the most common words for each language .
the automatic evaluation scores ( bleu and ter ) for the rev systems are shown in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems receive relatively high evaluation scores , both for the original embeddings and the re - envisioned ones . however , the performance gap between the two systems is narrower than expected by bleu , indicating that the performance gain comes from a better model design .
results on flickr8k are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the model from the second author ' s ( chan et al . , 2017 ) second submission . the mean mfcc rank of the vgs model is 0 . 0 , which means that it has the best recall .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual supervised model . rsaimage is supervised using pascal - vocal2vec embeddings and multi - headed attention . the average recall @ 10 is 0 . 7 , which means that rsaimage has 27 . 7 % chance to match the true image in question . the mean mfcc score is 1 . 414 , or 0 . 0 % better than the chance of chance for any other classifier .
we report further examples in the appendix . the examples in table 1 show that the rnn classifiers turn in sentences with significantly better performance than the original on sst - 2 . the difference is most prevalent in the edges , where the dan classifier turns in a sentence with a sharper edge . it is also more difficult for cnn to turn a screenplay into a sentence , since the edges are narrower .
table 2 shows the pos changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . since fine - tuning has increased the number of occurrences in the sentence , the overall number of words in the generated sentences has increased . however , the percentage of instances for some words has decreased or stayed the same . this indicates that the quality of the interpretability obtained by rnn has not improved .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels result in a significant increase in sentiment score .
table 1 presents the results of pubmed and sst - 2 . the results are summarized in table 1 . the results reconfirm that the ability to distinguish between good and bad responses is a relatively simple task . however , it is difficult to confirm whether a given response is positive or negative , as the results are statistically significant only with respect to positive responses .
