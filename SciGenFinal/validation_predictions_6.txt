the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . we observe that the iterative approach performs better on both training and inference than the recursive approach , and is comparable to tensorflow ’ s iterative model in terms of performance .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 shows the performance of the treernn model with varying tree balancedness . we observe that the balanced dataset has the highest throughput , but the linear one has the smallest performance improvement .
the max pooling strategy consistently performs better in all model variations . hyperparameter optimization results for each model with different representation are shown in table 2 . conll08 and ud v1 . 3 consistently outperforms sb and softplus in terms of f1 score ( i . e . , f1 . in 5 - fold ) with optimal values in all but one case . sigmoid and softplus models consistently perform better than softplus and sb with the same number of iterations . we observe that the dropout probability of dropout drops significantly when the model is trained with the best representation .
the results of using the shortest dependency path on each relation type are shown in table 1 . we observe that the macro - averaged approach achieves the best f1 score , and the model - feature approach obtains the most consistent results . the results show that macro - adaptive approaches have the advantage of using shorter dependency paths .
the results are shown in table 3 . we observe that the best performing model is the y - 3 model , which achieves the best f1 and r - f1 scores . it achieves a f1 score of 50 . 59 % , a 50 % improvement over the previous state - of - the - art model ( y - 3 : y - 1 ) .
we observe that mst - parser achieves the best results on all three test sets . the results are shown in table 3 . as expected , it achieves the highest percentage of completions on all test sets , surpassing all the other approaches except for mate , which achieves the lowest percentage . it is clear from the results that it is able to distinguish between the best and the worst performing approaches .
the results are shown in table 4 . the average c - f1 score for the two indicated systems is 60 . 40 ± 13 . 57 % compared to 56 . 24 ± 2 . 87 % for lstm - parser . note that the mean performances are lower than the majority performances over the runs given in table 2 .
the results are shown in table 3 . we observe that when the original model is cleaned , it performs better than both the original and the cleaned model . when the model is added to the training set , it achieves the best results , surpassing the original by 3 . 5 points . the results show that the cleanliness of the model improves over the performance of the original when it is combined with the clean - up step .
table 1 compares the original e2e dataset with the cleaned version of our slot matching script , as measured by the number of distinct mrs , total number of textual references , ser and slot matching percentage . we observe that the original dataset contains more than twice as many textual references as the cleaned one , which indicates that the slot matching technique is more effective in deterministic tasks .
the results are shown in table 3 . we observe that the original tgen + model outperforms the tgen − model by a significant margin . the difference in performance between the two sets of models is statistically significant , with the average score of 63 . 83 % vs . 63 . 27 % for the original model . it can also be observed that the difference between the original and the improved tgen model is less pronounced in the case of sc - lstm , which shows that the training data are more representative of the training state of the art .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the number of errors for each error is shown in parentheses . we found that the most common types of errors are those caused by missing or incorrect values , which indicate that the tgen model has a high degree of disfluency . as shown in table 4 , we found that adding missing and incorrect values to the training data leads to significantly higher error rates .
the results are shown in table 3 . we observe that the best performing model is graphlstm ( song et al . , 2018 ) , followed by seq2seqk , which achieves an average score of 25 . 9 % higher than the previous state - of - the - art on all three metrics .
the results are shown in table 2 . our model achieves 24 . 5 bleu points , which is more than double the performance of seq2seqb ( beck et al . , 2018 ) and surpasses both the best ensemble and single - task models by a significant margin . we observe that the size of our model is comparable to those of the best single task models , but smaller than those of ggnn2seq . as a result , we believe that our model can be used as a baseline for future work .
the results are shown in table 1 . we observe that the best models for english - german and czech are bow + gcn and seq2seqb , while the best single model is birnn + . we also see that the performance gap between these two models is less pronounced for bow - based models , but it is still significant .
the effect of the number of layers inside the model is shown in table 5 . we observe that when the model has more than 6 layers , the performance of the model can be significantly improved . when the model only has 3 layers , we observe that the performance drop is less pronounced , but still noticeable .
comparisons with baselines are shown in table 6 . rc denotes the number of connections with residual connections in a given gcn and the average number of rc + la connections . + rc denotes the percentage of connections in the final gcn that are derived from residual connections . we observe that the gcn with the highest number of residual connections is the one with the most significant improvement in performance .
the results are shown in table 3 . we observe that the best performing model is dcgcn ( 1 ) , which achieves a cagr of 10 . 4 % and a bleu score of 12 . 2 % . the average number of iterations of the model is 10 . 2 % higher than the average of the best previous state - of - the - art models , indicating that the model has more interpretability .
ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . we observe that removing the dense connections in the i - th and i - nth blocks significantly reduces the number of connections , and the overall ablation study results are comparable to those of dcgcn4 . however , the ablation results are less pronounced for dense connections , as shown in fig . 8 .
ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . we observe that the coverage mechanism used by the decoder achieves the best results with a precision of 23 . 6 % compared to 22 . 7 % for the encoder . the results of the ablation study also show that the direction aggregation and linear combination strategies have the most significant impact on decoding performance .
scores for initialization strategies on probing tasks are shown in table 7 . glorot achieves the best results , with a score of 31 . 8 % higher than our paper on all three tasks . we observe that the best initialization strategies are somo , subjnum and topconst , which achieves a joint score of 81 . 3 % and 81 . 8 % , respectively . the best performance is achieved by somo with a joint score of 82 . 3 % , which is slightly higher than the previous best performance of 79 . 9 % . we note that somo achieves the highest joint score on all the three tasks , with an absolute improvement of 3 . 5 % over our previous best score . finally , we observe that our system obtains the highest joint score on the three probing tasks .
the results are shown in table 3 . we observe that the h - cmow model outperforms both the cmow and the cbow model in terms of both depth and length . the h - cbow model achieves the best results on both datasets . it achieves the highest score on the two datasets with an absolute improvement of 3 . 6 points over the previous state - of - the - art cmow model . on the other hand , it achieves the worst performance on the third dataset with a score of 2 . 2 points .
the results are shown in table 3 . hybrid method outperforms all the other methods except for cbow , which shows a slight drop in performance compared to cmow in terms of sick - r score . we observe that the difference between cbow and cmow is less pronounced than in hybrid , but still shows a significant improvement over cmow . as the results show , the performance gap between cmow and hybrid is small , but noticeable .
table 3 shows the results of our models on unsupervised downstream tasks attained by our models . the results show that cbow and cmow have the most significant improvements over hybrid , with cbow achieving a cmp . score of 62 . 5 % and 62 . 6 % , respectively , compared to hybrid ' s 39 . 0 % . hybrid also shows a significant improvement over the cmow score of 39 . 2 % , with a gain of 26 . 6 % over the cbow score . finally , we observe that the performance gap between hybrid and cbow is less pronounced with respect to sts13 and sts14 .
scores for initialization strategies on supervised downstream tasks are shown in table 8 . we observe that glorot achieves the best results with an absolute improvement of 3 . 6 points over the previous state - of - the - art on the supervised downstream task , and a 4 . 6 point improvement over the best state of the art on the single step initialization task .
scores for different training objectives on the unsupervised downstream tasks are shown in table 6 . we observe that the cmow - r method outperforms the cbow - c method on all the tasks except for sts13 and sts14 , where the latter achieves the best performance . the performance gap between cmow and cbow is less pronounced on sts16 , where cbow achieves the highest performance .
the results are presented in table 3 . we observe that our method outperforms the previous state - of - the - art cmow - r and cbow - c models on all three metrics . our method achieves the best results on all metrics , outperforming both the previous best and the best - performing cmow models on three of the four metrics . it also achieves the highest precision on all the metrics .
the results are shown in table 3 . we observe that the cmow - r model outperforms the cbow - c model in terms of test set quality . the cmow model achieves a cie score of 90 . 6 % on average compared to 90 . 2 % on the previous best state - of - the - art model .
the results are shown in table 3 . we observe that the supervised learning approach outperforms both the best state - of - the - art supervised and unsupervised approaches in terms of both loc and misc scores . supervised learning achieves the best results on all three metrics , outperforming all the baselines except for τmil - nd , which achieves the highest misc score . it is clear from the results that supervised learning is superior to the state of the art when it comes to the task of name matching .
results on the test set under two settings . the results are shown in table 2 . name matching and supervised learning achieve the highest f1 scores , followed by τmil - nd , which achieves the best overall f1 score . supervised learning achieves the highest overall score , with a f1 of 73 . 38 ± 1 . 59 and a τmil - nd score of 38 . 42 ± 0 . 38 . we observe that the model with the best f1 is mil - nd ( model 1 ) , which achieves an absolute improvement of 2 . 38 points over the previous state - of - the - art model , and a 3 . 38 point improvement over the best supervised learning model , mil - 1 .
the results are shown in table 6 . we observe that g2s - gat outperforms all the other approaches on average in terms of both the number of iterations and the average number of entries per model . it is clear from the results that the model with the best performance is the one with the highest number of instances per iteration , i . e . , the one that has the most features per iteration .
the results are presented in table 3 . we observe that g2s - gin outperforms all the other models except for song et al . ( 2018 ) in terms of bleu and meteor scores . it also outperforms s2s by a significant margin . the results show that the gin model is more suitable for the task at hand , but is less suitable for large datasets . finally , we observe that the performance gap between the best and worst models is less pronounced with respect to the ldc2015e86 and ldc2017e86 datasets .
results on ldc2015e86 test set when models are trained with additional gigaword data . we observe that the g2s - ggnn model outperforms all the other models in terms of bleu scores . the results are shown in table 3 . it can be observed that the model trained on the gigaword test set outperforms both the external and the internal test set .
results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . we observe that the model with the best ablation performance is the one with the highest bleu score and meteor score , i . e . , it obtains the best overall ablation result . it is clear that the combination of the best bilstm and the best get improves the model performance .
the results are shown in table 3 . we observe that g2s - gin outperforms all the other models except gat and gat - ggnn in terms of average number of sentences and average length of sentences . it is clear from the results that the gin model is more suitable for shorter sentences , as it has a larger sample size . however , it does not perform as well for longer ones , as shown in fig . 3 .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added ) as well as the fraction ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . we observe that gold outperforms s2s and g2s - ggnn in terms of fraction of missing elements , indicating that gold has a better generalization ability . as shown in table 8 , the model with the highest percentage of missing tokens is the one with the best generalization performance .
as shown in table 4 , the pos tagging accuracy improves with each nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . we observe that pos tags are significantly better than sem tags .
in table 2 , we show the pos and sem tagging accuracy with baselines and an upper bound . word2tag classifier outperforms both the baselines in terms of most frequent tags and upper bound encoder - decoder accuracy . unsupemb classifier achieves the best results on both baselines .
the results are shown in table 3 . we observe that the best performing models are those with the highest accuracy on both the pos and sem datasets . the best performing model is the one with the best accuracy on the pos dataset , which shows that the model with the lowest accuracy is able to achieve the best results on both datasets .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the uni encoder has the best overall performance , with the res encoder having the highest precision . the bi encoder , on the other hand , has the worst performance . as the results show , the best performance is obtained with a single layer of four layers .
table 8 shows the performance of the attacker on different datasets . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . results are on a training set 10 % held - out . we observe that the attacker performs slightly better than the adversary on all three datasets .
accuracies when training directly towards a single task . the results are shown in table 1 . our model outperforms the previous state - of - the - art approach in terms of both accuracy and sentiment scores . we observe that the gender - specific features of our model outperform all the other models except age and race .
the results are shown in tables 2 and 3 . we observe that the best performance is obtained with balanced and unbalanced data splits . as expected , gender and age - based features are the most important in terms of data leakage , followed by sentiment and task accuracy .
the results are shown in table 3 . we observe that gender - specific features have a significant impact on the performance of our model , with the gender - neutral features contributing the most to the model ’ s overall performance . as expected , gender - based features have the greatest effect on the model ' s performance , with a drop of 3 . 5 % in performance compared to the baseline .
the results are shown in table 6 . rnn and guarded encoders significantly outperform rnn with respect to the protected attribute . the rnn encoder significantly outperforms the guarded encoder with a margin of 3 . 8 points . guarded embeddings also outperform the leaky encoder .
the results are presented in table 3 . we observe that the work performed by lstm and gru is comparable to the work by yang et al . ( 2018 ) in terms of both finetuning and performance . however , the performance gap between these two models is much larger . the work by the authors shows that the finetune - based approach outperforms both the baselines and the finetuned approach by a large margin . in particular , the model performed best on the ptb and wt2 baselines with a f1 score of 69 . 36 % and 69 . 59 % , respectively , compared to 62 . 59 % and 62 . 97 % by the work of the authors of the previous work .
the results of rocktäschel et al . ( 2016 ) are presented in table 3 . we observe that the work performed by gru and sru is comparable to that of lstm and atr in terms of the number of parameters and the average time taken to solve the task . however , the difference in performance between the two models is less pronounced for gru , which shows that the model is more suited to the task at hand . it can also be observed that gru performs better than the work of sru and at the same time takes less time to solve a task , which indicates that it is better at the task in hand .
the results of zhang et al . ( 2015 ) are presented in table 3 . the results are summarized in bold . we observe that the lstm model outperforms all the other models except atr and gru in terms of both err and time . it also outperforms the work of zhang et al . , ( 2015 ) . as shown in the table , the model with the highest err is the one with the best time - to - error ratio , which indicates that it has the best generalization ability . finally , we see that the work performed by the gru model is comparable to the work by zhang et al . ( 15 ) and zhang et . ( 9 ) but is less than the performance of atr .
as shown in table 3 , gnmt has the highest bleu score on the wmt14 english - german translation task compared to all the other models except olrn ( 25 . 67 % vs . 26 . 40 % ) and gru ( 26 . 34 % ) on the case - insensitive tokenized translation task . lrn , on the other hand , has the worst performance ( 27 . 67 % ) compared to gnmt ( 26 % . we observe that the difference between gnmt and lrn is mostly due to the smaller number of tokens required to tokenize a sentence , which indicates that the model is more sensitive to tokenized sentences . finally , we observe that when the tokenized tokens are tokenized , the model performs better on the translation task , as shown in fig . 3 .
table 4 shows the exact match / f1 score of our model on squad dataset . we observe that our lstm model outperforms all the state - of - the - art models in terms of parameter number and f1 score . moreover , we observe that the number of parameters used by our model is significantly larger than those used by any other model , indicating that our approach is more generalizable . finally , we see that our model performs better than all the other approaches except for rnet , which is comparable to the results published by wang et al . ( 2017 ) . we notice that the parameter number of base . 5 is larger than that of elmo , which indicates that the model is able to handle more complex tasks .
table 6 shows the f1 score of our model on conll - 2003 english ner task . we observe that our lstm model significantly outperforms all the state - of - the - art models in terms of f1 scores . the model achieves the best result with an f1 of 90 . 56 points . as shown in table 6 , the number of parameter number is the most important factor in the model ' s performance .
the results are shown in table 7 . lrn outperforms elrn and glrn on snli task with base + ln setting and perplexity on ptb task with base setting . as the results show , lrn is more accurate on both snli and ptb tasks .
the results are presented in table 1 . we observe that the average number of words per sentence is significantly higher for human and for oracle than for system , indicating that the system embedding the word embedding feature is more beneficial for the human . our system outperforms both the human and the system in terms of word embeddings . the results also show that the embedding features of the system are beneficial for both human and system , and that the performance gap between the two is less pronounced for system .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the accuracy of our system is comparable to that of the best human evaluation . the average number of evaluations per human evaluation is 44 . 2 , and the average percentage of evaluations a system gets in the top 1 or 2 for overall quality is 50 . 0 . seq2seq outperforms both human and automatic systems in terms of accuracy .
the results are shown in table 3 . we observe that the model outperforms all the baselines except for ted talks , which shows that it has the advantage of having a larger sample size . the model also outperforms both the best - performing baselines in terms of p < 0 . 01 and r > 0 . 05 , indicating that it is more suitable for the task at hand .
the results are shown in table 3 . we observe that the model outperforms all the other models on both datasets except for the one where we observe that it has the worst performance . in particular , the model with the highest p - value is the one with the best multi - task performance .
the results are shown in table 3 . we observe that the model outperforms all the other models except for ted talks , which shows that it is easier to train the model with a larger sample size . the model with the largest sample size outperforms both the others in terms of p < 0 . 001 and r > 0 . 01 . it also outperforms the models with the smallest sample size , as shown in fig . 3 .
the results are presented in table 1 . we observe that the average depth of our system is 11 . 05 μm , which is slightly higher than the average of the previous state - of - the - art baselines . the average number of roots per row is 3 . 46 times larger than that of other baselines , which indicates that our system has a high degree of cohesion with the lexical roots .
the results are presented in table 1 . we observe that the average depth of our dataset is slightly higher than the max depth of other baselines , and the average number of roots is higher than that of most baselines . also , the number of numberrels is higher , but the average length is lower .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of lf achieves the best results , with an ndcg % of 73 . 42 % compared to 57 . 65 % for the baseline model . in addition , the enhanced version achieves the highest ncdcg % on question type , answer score sampling , and hidden dictionary learning , respectively . finally , the weighted softmax loss is the only loss that does not have a significant effect on the model performance .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 is shown in table 2 . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) , while p1 indicates the least effective one . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . we observe that the model with the best performance is the one with the shortest history shortcut ( p2 ) .
comparison on hard and soft alignments . the results are shown in table 5 . we observe that hmd - recall + bert significantly outperforms wmd - unigram and wmd - bigram in terms of recall , and that hmd - f1 + bert achieves the best recall on both soft and hard alignments , with a gap of 0 . 7 points between the former and the latter . finally , we observe that wmd ' s bigram - based approach outperforms hmd ' s pre - trained approach in both cases .
the results are shown in table 3 . we observe that the baselines used in this study are comparable to those used in meteor + + and bertscore - f1 , with the exception of ruse ( * ) being slightly better . the average bert score is 0 . 686 , which indicates that the method is comparable to the best state - of - the - art baselines .
the results are shown in table 3 . we observe that the bertscore - f1 scores are significantly higher than those of meteor and bleu - 1 , indicating that the baselines are more suitable for the task at hand . the baselines also show that the sfhotel model can be trained on a larger corpus of data , which indicates that the training data are easier to interpret . finally , we observe that bert scores are comparable to those of meteor .
the results are shown in table 3 . word - mover and recall are the most important factors in the performance of our model , followed by spice and meteor . we observe that our model outperforms all the baselines except leic and spice by a significant margin . our model achieves a recall score of 0 . 939 on m1 and 0 . 949 on m2 , a significant improvement over the previous state - of - the - art .
the results are shown in table 1 . we observe that the average score of our model is slightly higher than that of the best previous state - of - the - art models with the same number of iterations . our model outperforms all the other models with a significant margin . para - based models outperform all the others with a margin of 0 . 81 points over the nearest competitor .
the results are shown in table 3 . we observe that the transfer quality and semantic preservation scores of our models are comparable to those of the best state - of - the - art models . however , we observe that our transfer quality scores are slightly lower than those of other models , indicating that our model is more suited to semantic preservation tasks . finally , we see that the model with the highest transfer quality is the one with the best overall semantic preservation score .
the results of human sentence - level validation are shown in table 5 . we observe that the accuracy of our model is comparable to that of the best human evaluators ( i . e . , spearman ’ s ρ b / w sim and human ratings of semantic preservation < 0 . 75 ) .
the results are shown in table 3 . we observe that the model with the best generalization is the one with the highest accuracy . the model using para - para and lexical features outperforms the model using both lexical and syntactic features with an absolute improvement of 3 . 5 points over the previous state - of - the - art .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc , but untransferred sentences achieve the highest bleu . acc ∗ : the definition of acc varies by row because of different classifiers in use . for example , multi - decoder model achieves the highest acc with 22 . 6 % higher accuracy compared to the best unsupervised model ( 22 . 4 % ) and 22 . 4 % higher acc with the best supervised model ( 13 . 4 % ) . we note that the classifiers used in our model are slightly worse than those used in the previous work , but still perform better than the best trained models . we also note that our model achieves higher acc than both the best and the worst trained models with the same number of transferred sentences .
as shown in table 2 , the average number of reparandum tokens that are correctly predicted as disfluent decreases with repetition , but increases with the number of disfluencies , indicating that the disfluency reduction is due to the smaller number of repetition tokens , not to the larger size of the disfuncions .
we show the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair as well as the function word in the repair ( content - content ) . for disfluency that contains a function word , the number of tokens in the content word is less than the fraction of tokens that belong to each category . we also show the percentage of tokens containing function - function words that are predicted to be disfluential . these tokens belong to three different categories : repair ( rephrased , repaired , and disfluenced ) , reparandandum ( disfluential , disfluidated , and non - disfluenced ) and content - function . as shown in table 3 , content - content tokens are the most frequently predicted to disfluidate , followed by function - word tokens , which are the least frequently predicted .
the results are shown in table 3 . we observe that the best model for each test set is the one with the highest average number of iterations , followed by those with the best dev and best best test set . as expected , the model with the most iterations is more likely to have the best results . it is clear from the results that there is a significant difference in the performance between the best and worst models when using innovations and single word embeddings . in particular , we see that the model using innovations is more accurate than the one using single words .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embedding outperforms all the state of the art embeddings except self - attention and rnn in terms of accuracy . we observe that the average accuracy of our model is slightly higher than those of rnn and cnn , indicating that our embedding method is more suitable for the task at hand . our model also achieves higher accuracy on the topic - specific f1 test set , as shown in table 2 . however , it is comparable to rnn - based embedding , as the accuracy is higher on topics that are unrelated to the topic .
accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . as shown in table 2 , attentive neuraldater is more accurate than all the other approaches except for ac - gcn , and outperforms maxent - joint in terms of overall accuracy . in addition , the joint model outperforms the other methods on both datasets by a large margin .
as shown in table 3 , both word attention and graph attention significantly improve the accuracy of the neuraldater component models with and without attention . the accuracy increases with graph attention , and the t - gcn decreases with word attention .
the results are shown in table 3 . we observe that the model with the best performance is the one with the most embeddings , which is jrnn , which achieves 75 . 3 % on average across all three stages . it is clear from the results that embedding helps the model to achieve the best results across all the three stages , but it does not improve upon the performance of the original model .
the results are presented in table 1 . we observe that cross - event event detection has the most significant impact on the f1 score of argument identification and classification . as the results show , cross - event event detection leads to significantly higher f1 scores for argument identification , classification and role than for trigger identification . the results also show that argument identification is the most important factor in the success of cross event detection , followed by classification .
the results are shown in table 3 . we observe that the best performing variant is the one with the highest average dev perp and test acc , followed by the worst performing variant with the lowest average test acc and the highest number of test wer . it can also be seen that the fine - tuned variant of cs - only - lm outperforms all the others in terms of both dev and test scores .
results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . fine - tuned models outperform cs - only models on both sets . the results are shown in table 4 . the fine - tuned model outperforms the cs model in terms of both test set and dev set scores .
the results are shown in table 5 . the fine - tuned version of fine - tuned - disc achieves the highest accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) and gold sentence - based ( lm ) . it achieves the best overall performance .
as shown in table 7 , the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the precision and recall scores obtained by type combined gaze features are significantly higher than those obtained by baseline .
we observe that type - aggregated gaze features significantly improve the recall and precision of our model compared to the baseline model . the precision and recall improvements are statistically significant ( p < 0 . 01 ) and the f1 score ( f1 ) is significantly higher ( p > 0 . 05 ) compared to baseline , indicating that the model is more accurate .
results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings outperform glove - extended and ontolstm - pp in terms of both type and length of tokens . the results are shown in table 1 . we observe that the hpcd model outperforms the original lstm model by a large margin . as the results show , the syntactic embedding quality of syntacticsg is comparable to that of wordnet , verbnet , and rothe and schütze ( 2015 ) on the same test set , but is less than that of the original model .
results are shown in table 2 . we observe that hpcd and ontolstm - pp outperform all the other systems in terms of pp prediction accuracy . as expected , the accuracy of lstm is significantly higher than that of ontolpstm , indicating that the dependency parser is better at predicting pp attachment predictors .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . we observe that the ppa acc . scores for full and attention - based models are significantly higher than those for full - task models .
as shown in table 2 , adding subtitle data and domain tuning for image caption translation improves the bleu % scores for both en - de and multi30k models , but does not improve the multilingual scores for en - fr and mscoco17 models . adding domain tuning also improves the multi - domain scores , as it improves the translation quality of the subtitle and domain - tuned models .
we observe that domain - tuned h + ms - coco achieves the best results for both en - de and en - fr , and mscoco17 achieves the highest overall performance . domain - tuning improves the performance of both sets of models , the results are shown in table 3 . as expected , domain - domain tuning improves the overall performance of all models , and the results are comparable across all three sets .
as shown in table 4 , adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . multi - tasked models outperform en - de and en - fr models in terms of both accuracy and recall . as expected , the multi - task model with the best captions is more accurate , but still performs worse than en - fran models with the same number of captions . in addition , it achieves higher recall , but is less accurate . finally , the multilingual model with marian amun ( mscoco17 ) outperforms en - dde models with only the best 5 captions , but performs better overall .
comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . we observe that enc - gate and dec - gate strategies significantly outperform en - de and en - fr strategies in terms of bleu % score . en - de strategies are significantly more effective at integrating the visual information , as shown in fig . 5 , as the average number of frames per second of encoder and decoder is significantly higher . in addition , encoder + decoder strategies significantly improve the overall performance of the model , as seen in table 6 . mscoco - based encoder - decoder combination achieves the best results , as it achieves the highest percentage of frames .
the results for en - de and en - fr are shown in table 3 . we observe that the multi - lingual approach outperforms all the approaches except ms - coco in terms of the number of lexical features , and the average number of features in the detectron , indicating that the multilingual approach is superior to the visual features approach . however , the results are slightly less clear in en - fran , where we observe that it is harder to detect multilingual features than visual features .
the results are presented in table 3 . we observe that en - fr - ht and en - es - ht are comparable in terms of translation quality to the best state - of - the - art models . however , the difference in performance between the two approaches is less pronounced for enfr - smt - back , which shows that it is easier for the model to learn new features . finally , we observe that the average number of errors per translation is less than those for en - rnn - ff , which indicates that the model is more likely to learn features from the source corpus .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . we found that en – fr and en – es had the most parallel sentences , with 1 , 472 , 203 and 459 , 633 respectively .
training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . we observe that the models are comparable in terms of performance across all three languages , with the exception of spanish , where the model performs slightly worse .
automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . we observe that the system evaluation scores of en - fr - rnn - rev and en - es - smt - rev are comparable in terms of bleu , ter and overall evaluation score . however , the performance gap between the two systems is less pronounced in ter . as a result , we note that the evaluation scores obtained in ter are comparable to those obtained by en - fr - trans - rev .
results on flickr are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one supervised by chrupala et al . ( 2017 ) with the highest recall ( 15 . 2 % ) and highest average rank ( 17 . 0 % ) compared to segmatch .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled u is the one supervised with the audio2vec - u embeddings . we observe that the vgs model significantly outperforms the rsaimage model in terms of recall and recall percentage .
we report further examples in the appendix . the classifiers are shown in table 1 . we observe that the rnn turns in a screenplay that is easier to hate than the original on sst - 2 , but harder to hate on cnn . it can also be observed that the dan classifier is more difficult to hate , as it turns on a lot more edges . finally , the cnn classifier can be seen to be more difficult for hate speech , as the average number of edges turns in the screenplay is larger than those of the original , and the number of curves is larger .
table 2 shows the percentage of occurrences for each part - of - speech that have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the number of occurrences of each word in the final sentence . a score of 0 indicates that finetuning has not changed the overall number of words , and a score of 1 indicates the number has increased or decreased . the symbols are purely analytic without any notion of goodness .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 .
the results are shown in table 1 . we observe that sift outperforms both pubmed and corr ( p < 0 . 001 ) and pubmed ( p = 0 . 00089 ) in terms of accuracy . the results of pubmed are slightly worse than those of corr , but still better than corr .
