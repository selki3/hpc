table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as can be seen in the table , both approaches yield strong baselines comparable to the best stateof - the - art on the training dataset . however , in the case of the movie review dataset , fold performs better both on inference and training , indicating that it is more suitable for production use . since the size and type of training data are important , we also include the number of instances in the loop as a metric for scalability as the table shows , the smaller loop size and the higher number of iterations in the training set , the more likely it is to converge .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . balanced datasets also have lower throughput than linear ones , because the number of instances in the balanced dataset is smaller .
the max pooling strategy consistently performs better in all model variations . hyper parameter optimization results for each model with different representation are shown in table 2 . the conll08 model achieves the best results with a f1 score of 1 . 83 . as hard coreference problems are rare in ud v1 . 3 , we do not have significant performance improvement . however , the dropout probability is low , meaning that the method does not overstate the importance of the feature extraction function . we find that the activation func . is the most important part of the model , and when combined with the learning rate , it achieves the highest performance . finally , we find that softplus also performs better than softplus ,
table 1 shows the effect of using the shortest dependency path on each relation type . as the results show , macro - averaged models perform better than those without sdp , indicating that the importance of dependency trees is somewhat overstated . however , when we add sdp as dependency trees , the effect is slightly less pronounced .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the three types of models perform comparably to each other when the true response is added . however , in the more realistic second case , when the error reduction is only computed on f1 and r - f1 , performance on the y - 3 model drops significantly .
we show the results for english and german . for english , we show results in table 1 . the best results are obtained by mst - parser , which achieves over a 100 % accuracy on average . for german , we see that it achieves more than 50 % .
we note that the average true response percentage for each paragraph is slightly higher than the baseline for the two indicated systems , which shows that the parser performs better at the paragraph level . also , the mean performances are lower than the majority performances for both systems , showing that the ability to distinguish between paragraph and sentence level is a challenge .
the results are presented in table 3 . we show that the original tgen model performed well when trained and tested , but when the model is cleaned , it performs substantially worse than tgen − . as can be seen , tgen is signifi cantly better at removing errors when the training data is removed than when it is added . table 3 shows that once the cleaning data is applied to the model , it achieves the best results .
table 1 shows that the difference in quality between the original e2e data and the cleaned version is minimal ( 0 . 5pt / 2pt ) , however we see significant difference in ser as measured by our slot matching script , see section 3 . also , the number of distinct mrs in our dataset is significantly higher than in the original ( 17 . 69 vs . 11 . 42 ) .
the results are presented in table 3 . we observe that tgen is signifi cantly better than tgen − when trained and tested on the original dataset , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement in the performance . additionally , the results are slightly better than those of tgen + when tested on a standalone dataset . again , this is consistent with what klinger et al . ( 2018 ) report : " adding entity nodes to a training set achieves a significant improvement , but only when it is done in the context of a single entity , typically leading to a drop of more than 2 % .
the results of manual error analysis of tgen are shown in table 4 . as can be seen , the majority of errors in our system are caused by errors caused by adding , missing , and wrong values ( see x4 ) . however , a few cases are rare ( disfluencies ) where the correct values are added but the wrong ones remain .
the performance of these models is presented in table 3 . all models show significant performance drop when trained and tested on the single - domain dataset . the only exception is the graphlstm model , which achieves the best results with a gap of 3 . 2 points .
as can be seen in table 2 , both ensemble and single - model setups achieve high bleu scores ( see x4 ) . the best performance is achieved by the dcgcn ensemble , which achieves 24 . 5 bleu points . by contrast , the seq2seqb ensemble achieves only 27 . 6 bleuu points . though the number of parameters in the ensemble is small , it achieves a significant improvement over the previous state of the art on amr17 .
we compare our model against previous models on the test set in english - german , czech and french , both for the original and the debiased embeddings ( for each language we show the results of the best performing model ) . the results are presented in table 1 . as a baseline , we also compare against bow + gcn ( bastings et al . , 2017 ) , which achieves state - of - the - art results in both languages with a gap of 10 . 5 % on average compared to the previous state of the art . we observe that the seq2seqb model significantly outperforms other models with different feature sets . for example , the best performance is obtained by seq1seq with a single model , which achieves 41 . 8 % on the average .
the effect of the number of layers inside the network is shown in table 5 . we observe that for each layer , there is a significant drop in performance relative to the previous state of the art . the largest drop is seen in the bias metric , which shows that as many as 6 layers inside a network can significantly improve performance .
table 6 compares the results of rc and rc - based models with baselines . the first group shows that both cues yield strong baselines comparable to the strongest gcns ( i . e . , those with residual connections ) , while the second group shows lower performance . adding rc information improves both gcn performance and the baselines in general ( see x4 ) . the difference is less pronounced for dcgcn2 , but still suggests some reliance on residual connections .
the performance of these models is presented in table 4 . we observe that , let alone a reduction in performance , the models achieve state - of - the - art results . the largest gains are obtained on the computer science research abstracts of gcn ( hochreiter and schmidhuber , 1997 ) where dcgcn ( 1 ) reaches a new state - ofthe - art in terms of performance on all metrics and its variants .
we show the ablation study results on the dev set of amr15 . in table 8 we show that removing the dense connections from the i - th block significantly decreases the density of connections . the model performs better than the dcgcn4 model when the number of connections in the dense blocks is removed .
we also performed an ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . we report the results of " - global node & linear combination " and " - direction aggregation " for both models . as these ablation studies show , when the global node is combined with a domain - aware attention mechanism , both the graph attention and the coverage mechanisms are stronger . however , when we switch to a more realistic approach , the results are still not significant ( p < 0 . 01 ) . we note that the combination feature achieved with " - linear combination " achieves the best results .
we present the scores for initialization strategies in table 7 . it can be seen that glorot and topconst receive high scores for both depth and length , indicating that their initialization strategies are effective in probing the hidden regions of the network ( e . g . , depth < 0 . 5 ) . however , when topconst is used in combination with bshift and coordinv , it achieves only marginal improvement ( t - test , p < 0 . 01 ) .
we observe that the h - cbow model achieves state - of - the - art results , outperforming both the original cbow and h - cmow embeddings . it achieves the best results with respect to depth and length , and achieves the highest score on concatenated keyphrases .
the results are presented in table 1 . we observe that the best performances are obtained by our method , which verifies the effectiveness of our model . our model achieves state - of - the - art results across the five datasets , outperforming all the other methods except for sick - e .
table 3 : scores on unsupervised downstream tasks attained by our models . we show that cbow and cmow achieve gains relative to cmp . when trained and tested on the identical subset of sts datasets ( see x4 ) . hybrid also shows a relative improvement in performance relative to cbow , as expected , when trained only on sts13 and 14 , the cbow model suffers from predicting events in the background and performing them in the foreground ( i . e . predicting events to occur in the past ) . when trained on the sts15 dataset , the effect of cbow is less pronounced than for hybrid . this indicates that the model perform better in the context of both the original and the debiased embeddings . as hard coreference problems are rare in the unstudied datasets , we do not have significant performance improvement .
table 8 presents the performance of our system on these three supervised downstream tasks . it can be seen that glorot ( 86 . 6 % ) and n ( 0 , 0 . 1 ) achieves extremely high performance on the three datasets , outperforming all the other methods apart from trec .
scores for different training objectives on the unsupervised downstream tasks are shown in table 6 . the best performing method is the cbow - r method ( gillick et al . ( 2018 ) . it achieves high precision on all the downstream tasks , and achieves over 90 % on sts12 and sts13 . it closely matches the performance of the best supervised method .
the results are presented in table 3 . we observe that the three methods converge on a single goal : depth < tense , coordinv , length and topconst . the difference in accuracy between the three is minimal , but significant with respect to concatenation . subjnum is the most difficult category to solve , since it has the highest correlation with depth . topconst is the only one that is consistently better than cbow - r . while the difference is slim , it is significant enough to warrant further study .
the best performances are obtained by our method , which verifies the effectiveness of our model . we observe that , when combined with effective sub - jurisdiction clustering , the model achieves state - of - the - art results across the five datasets .
the results are shown in table 1 . in all but one case , the system performs better than the best supervised model . supervised learning is signifi cantly better than all the alternatives except name matching . name matching is the most difficult part of the task for both systems . it takes a considerable amount of data to train and fine - tune and , consequently , is only effective when trained and tested on a single location ( e . g . , in all combinations ) . in the absence of this data , automatic metrics results are only slightly better than those of supervised learning . further , when trained on all loc and misc datasets , the performance gap between supervised and unsupervised learning drops significantly . this indicates that supervised learning is strictly superior to supervised learning in most cases .
uncertain in low - supervision settings . results on the test set are shown in table 2 . in all settings , except for name matching , the system achieves high f1 scores . supervised learning ( tmtmil - nd ) achieves the best results with a f1 score of 43 . 57 . however , when trained and tested on a larger dataset , the results are slightly lower ( t - test , p < 0 . 01 ) . name matching is the only weak point in the model that is predicted to drop significantly under supervised learning . we find that the difference between the confidence intervals of f1 and 0 . 03 are less pronounced under automatic learning , but still indicate that there are significant differences in performance between supervised and unsupervised learning .
we show the results for both systems on the hidden test set of hotpotqa in table 6 . for the former , we show that g2s - gat is comparable to the model from ( chan et al . 2008 ) except for the fact that it is trained on golbeck - kutuzov distance instead of ref . moreover , the model is more stable and therefore requires less data to reproduce , which results in a lower precision . the results of the latter are less clear , but still show a considerable performance drop when compared to the former .
we compare our model against previous stateof - the - art on the ldc datasets . the results are presented in table 3 . g2s achieves relatively high performance on all datasets , with the exception of meteor . it achieves the best results on ldc2015e86 and ldc2017t10 . note , however , that it is trained on a significantly larger corpus .
the results on the test set of ldc2015e86 are shown in table 3 . as can be seen , both gigaword and bleu datasets are large enough to support multiple models trained on the same test set ( e . g . , 200 , 000 examples ) . as expected , g2s - ggnn significantly outperforms the other models with fewer examples ,
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model significantly outperforms the previous stateof - the - art on both metrics with a gap of 3 . 6m terms .
we find that g2s - gin significantly outperforms other models with different feature sets in terms of sentence length and average number of frames , from the above table , we observe that , of the 34 . 42 % increase in average sentence length for g2s between 0 - 7 and 20 - 20 δ , the largest gains are obtained on the graph diameter and length of sentences .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . g2s - gat outperforms s2s in terms of both accuracy andiss , as can be seen in table 8 , the smaller size of the output is a result of more than simply a reduction in the mixing of the input and the generated sentences . also , the model performs better when trained and tested on a larger corpus , as shown in fig . 3 .
we investigate the effect of these features on the model ' s semantic performance in table 4 . we observe that for each target language , the semantic features extracted from the 4th nmt encoding layer have high correlation with the pos tagging accuracy . in fact , the pos features have the highest correlation with semantic accuracy .
pos and sem tagging accuracy with baselines and an upper bound . we compare against unsupervised word embeddings using the best performing classifier ( word2tag ) . the results are shown in table 2 . word2tag significantly outperforms mft and unsupemb in both accuracy measures . note that the upper bound on pos is set at 91 . 55 % , which implies that the encoder is well - equipped to handle the extremely high frequency of tags .
the performance of our system on the three metrics is presented in table 4 . we show that it achieves state - of - the - art results on all metrics with a gap of 10 . 5 % on average .
we investigate pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer shows marked improvement in accuracy . the second layer shows a drop of more than 10 % in performance .
the attacker ’ s performance on these datasets is shown in table 8 . results are on a training set 10 % held - out . as the table shows , the difference between the attacker score and the corresponding adversary is minimal , however it is significant . sentiment and race features are the most difficult ones for the attacker to remove . gender and age are the two groups that are particularly difficult for him to remove because of the high correlation with human judgement .
accuracies are presented in table 1 . it can be seen that the feature - rich training set is well - equipped to handle the diverse nature of the task , and the accuracy is high even when training only on a single task . gender bias also contributes significantly to the accuracy , as it affects the recall scores for both genders .
we show the results for tasks that are balanced and unbalanced , and those that are unbalanced . the results are presented in tables 2 and 3 . as can be seen , there is a significant imbalance in the distribution of the data that gets leaked , with respect to gender and race , both for task and sentiment . note that for gender - neutral tasks , the average task accuracy is still relatively high , while for sentiment - free tasks , it is low .
the performance of our system on these datasets is shown in table 3 . as can be seen , all the features mentioned in the previous section cause a significant drop in performance when the attacker is trained with an adversarial dataset . however , for gender - neutral features , the performance is still relatively high , reaching 94 % on pan16 . the same tendency is observed on the wino dataset , with a drop of 2 . 2 % on average . finally , on the sentiment dataset , the difference between the attacker score and the corresponding adversary ’ s accuracy is less pronounced .
accuracies of the protected attribute with different encoders are shown in table 6 . rnn is significantly more accurate than rnn when it is guarded . with respect to the leaky attribute , rnn shows severe overfitting since the size and type of leak are very different . guarded rnn also exhibits overfitting , as shown in fig . 6 .
we compare our model with previous works on the training set of ptb and wt2 . in yang et al . ( 2018 ) , we benchmark against the following models : " ptb base " , " wt2 " and " finetune " . the results , summarized in table 1 , are broken down in terms of feature sets . " dynamic " refers to features added after training the model with finetune and other language features . it can be observed that the number of features considered by each model is relatively small , but when considered in combination with other factors , the difference is significant . " extracting all the features from one dataset leads to a significant overfitting across the other two . this is evident from the large difference in performance between the base and finetune - based models . we observe that for example , the size and type of dependency trees used in our model are very different , with the former yielding significantly more features than the latter . additionally , the model performs significantly worse when trained with dynamic features .
the best results for the second variation of our model are reported in table 2 . we show that the combination time and baseline acc are the most important factors in model performance . the difference between the two is less pronounced for rocktäschel et al . ( 2016 ) and sru ( 2017 ) . as can be seen in the table , when we add in both the base acc and the time to compute the baseline time , the model performs significantly worse than either lstm or gru .
we compare our proposed approach against published work on three datasets . the first set of results are presented in zhang et al . ( 2015 ) . it can be seen that the proposed approach significantly outperforms both published and unpublished work on all metrics by a significant margin . the results are summarized in table 1 . the most striking thing about the drop in performance between amapolar err and yelp time is that it is almost entirely due to small size of the dataset ( micro - f1 ) compared to other approaches . this indicates that the model is unable to learn the task to a high degree . also , note that the number of entries for each sub - domain is significantly less than the previous state of the art . table 1 summarizes these results .
we present the case - insensitive tokenized bleu score of our model on the wmt14 english - german translation task in table 3 . the model makes use of the best performing feature set available on the newstest2014 dataset , namely , the multi - params architecture . as the table 3 depicts , all the features the model has are useful for improving the generalization ability of the model . however , the biggest improvement is obtained when the model is trained on tesla p100 . this is mostly due to large variation in the training set size and the number of tokens used to tokenize the sentences , which results in a significant drop in performance . rather than using tokens , we use time as a metric for training and decoder . this allows us to more accurately compare our model with other methods of tokenizing sentences . we find that , for example , olrn is more than 4x faster than gnmt at decoding one sentence ,
we also include the exact match / f1 - score of our model on the squad dataset in table 4 . it can be seen that all the parameter numbers we considered had exactly one error according to wang et al . ( 2017 ) . however , when we added elmo as a parameter , our model obtains a significant improvement in match rate and f1 score . perhaps the most striking thing about this is that the model only works with one parameter number of base . apart of elmo , other parameters have a significant effect on the model performance as well . table 4 shows that the most interesting ones are the gru and lrn features . though the atr feature is small , it contributes significantly to the model ' s performance . we also observe that it is comparable with other sophisticated neural networks like lstm and sru in terms of match rate .
we present the f1 score of our model on the conll - 2003 english ner task in table 6 . as the table shows , all the parameter numbers we consider have low correlation with the human judgement . however , when we add sru and gru as inputs , their f1 scores improve significantly ( see x4 ) . table 6 shows that although lrn has the highest correlation with human judgement , it is still inferior to other methods . we note that the presence of the gru and sru in the same sentence hurts the model , and thereby leads to incorrect computation of the ner score .
the performance of our model on the snli task with the base + ln setting is shown in table 7 . with the base setting , our model exhibits marked performance improvement . however , it is still significantly worse than the other methods .
table 3 presents the results on paragraph selection . we benchmark against the following systems : oracle retrieval ( b - 2 , b - 4 ) and mtr ( r - 2 ) . for brevity we only report results on systems with automatic system detection . for window - weights , we include only those that are already trained on the system . we maintain performance at the level of the state - ofthe - art , with a minimum of 0 . 5 bleu improvement over previous work .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that seq2seq is more than 1 . 5 times as good as human in terms of grammatical quality . the second best result is obtained by candela ( 30 . 2 % ) on the content richness scale , which shows the extent to which the semantic features captured by the model can be improved with a reasonable selection of the lexical resource from which the statement was derived . seqseq also achieves high quality on syntactic and semantic features , showing that it is well - equipped to improve the interpretability of its output . retrieval is the only automatic system that achieves lower quality than human on all three metrics . although it obtains higher accuracy on two of the four metrics , it is still inferior on the third .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , the model performs better on three of the four datasets when trained and tested on the same dataset . the largest gains are obtained on the " ted talks " dataset , which shows that the semantic features captured by the model are strongly concentrated in the text - similarity rich semantic regions . additionally , for the " sqs " and " docsub " datasets , our model achieves the best performance with a gap of 0 . 5pp over the previous state - of - the - art .
the performance of these models on the test set is presented in table 3 . we observe that , let alone a reduction in performance , the model performs better on three of the four domains when trained and tested on the same dataset . the largest gains are obtained on the " ted talks " dataset , which shows that the semantic features captured by the model are strongly concentrated in the text - similarity rich semantic regions . additionally , for " domain specific references " we observe a drop of more than 2 points in performance for all models except for " slqs " .
the performance of these models on the test set is presented in table 3 . we observe that , let alone a reduction in performance , the model performs well on all datasets except for those linked to " ted talks " . as can be seen , all the models only slightly outperform the baseline on three of the four datasets , when we add " docsub " and " slqs " to the list of datasets , we get 0 . 5 % improvement on average . table 3 shows that for all the three datasets , our model performs better than the other two .
we benchmark against five baselines - dsim , tf , docsub , df , hclust and slqs . the results are presented in table 1 . the first set of results show that all the metrics we consider have low correlation with the average depth of the original embeddings . europarl , in particular , has the highest correlation with depth . while the difference is slim , it is statistically significant , with a gap of 1 . 46 points from the average .
the system performs well on both datasets with different feature sets . europarl achieves the best results with a weighted average depth of 9 . 43 % , which means that more than half of the words in the dataset are actually describing depth . adding the feature set of docsub and slqs improves the average depth to 9 . 73 % , but does not improve the total terms . from this group of metrics , we can further calculate that the number of roots in each dataset is the most important .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 are shown in table 1 . the enhanced version of our model ( lf ) shows marked performance improvement over the baseline model . although the improvement is slim , it is encouraging to continue researching into how to improve the feature extraction procedure for future work . we note that the additional loss function of p1 has a high impact on the performance of the model , and that it should be considered when evaluating future work in this direction .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set is shown in table 2 . the best performing model is coatt . it achieves 73 . 63 % on average compared to the baseline , which indicates that it is more effective in applying the history shortcut . note that only applying p2 is implemented by the implementations in section 5 with the current set of features .
we find that the hmd - f1 model significantly outperforms the wmd - bigram model when trained and tested on the hard and soft alignments . the results are summarized in table 5 . as hard alignments are rare in nature , we find that hmd - recall and bert significantly outperform the wmd - unigram model ,
the results are shown in table 1 . the first group shows that all the metrics we consider have low correlation with the human judgement . the average score of bertscore - f1 is 0 . 57 , while the average of smd and w2v are both 0 . 59 . when we add in additional metrics , such as ruse ( * ) and meteor + + , we get 0 . 63 and 0 . 68 scores , respectively , which are both lower than the baseline . further , we see that the sent - mover is only effective when trained and tested on a single dataset , meaning that it cannot be used across multiple datasets .
the system performs well on both datasets with different feature sets . for example , bertscore - f1 achieves 0 . 85 on the sent - mover and w2v datasets , while meteor achieves a higher score than bleu - 1 . we notice that the feature set derived from smd is particularly useful for sentiment analysis as it eliminates the effect of noisy clustering . when we add the feature - rich sentiment analysis baseline to the baseline , we get 0 . 87 % improvement on average .
word - mover with different feature sets achieves different performance on the training set when we add the feature set of bertscore - recall and word2vec . the results are shown in table 1 . sent - movers with various feature sets achieve higher recall scores than those without . word - mover with the best recall is obtained using the combination of meteor and spice . when we add elmo feature - values to the baseline , we get 0 . 939 % improvement on average .
the results are presented in table 6 . we observe that when we add the features of syntactic and semantic analogy , the model performs better than it does with any other combination of features excluding meta - para . adding all the features together improves the results for sim and gm .
the semantic preservation results are presented in table 3 . we observe that the transfer quality and transfer quality are the most important factors in semantic preservation . " transfer quality " refers to the quality with which a model can transfer information from one domain to another without losing its semantic features . semantic preservation results show that , for example , when a model is trained and tested on a single domain , it can maintain semantic features for both semantic and syntactic embeddings . " fluent features " are the group of features most important for transfer quality . they include transfer quality a , transfer quality b and a tie score . " semantic preservation " is the part of the improvement that is most striking . it shows that the semantic features captured by the model are strongly concentrated in the semantic regions of the speech bubble , which indicates that the model is well - equipped to handle the semantic information that is contained within .
table 5 presents the results of human evaluation for validation of the three metrics . it can be seen that both machine and human judgments that match the acc metric are accurate when they are trained and tested on the same dataset ( see x4 ) . however , when trained on yelp , the accuracy is only slightly higher than the human score . this indicates that human evaluation is more accurate than simply using machine metrics .
we observe that the classifiers trained on simnet outperform the models trained on non - simnet with only one type of classifier pre - trained : the combination of " cyc + para " with " 2d " and " para - lang " .
results on yelp sentiment transfer are shown in table 6 . our best models achieve higher bleu than prior work at similar levels of acc than when the transfer is restricted to 1000 sentences . note that the definition of acc varies by row because of different classifiers in use . as hard coreference problems are rare in unsupervised settings , we do not include results from simple transfer as these have been shown to be worse than in supervised settings . however , when transfer is done with human references , the results are slightly better . we include only results from the second variation of the transfer scheme , namely , those from yang2018 , as those consist of 1000 transferred sentences and human references . the results of simple - transfer are not included as they are worse than the results from any other variation of transfer scheme .
we include the percentage of reparandum tokens that were correctly predicted as disfluencies as a percentage of the overall score in table 2 . reparandum length is the average of the number of tokens in a sentence , and is the length of the repetition tokens . as this table shows , repetition tokens have a high correlation with disfluency , indicating that they are a significant part of the problem for winocoreference . however , for all other tokens , the correlation is low .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . table 3 shows that for all three categories , our model predicts the majority of tokens to belong to each category correctly . however , for the repair category , the accuracy is only slightly higher .
the results are presented in table 2 . we observe that text and innovations are the most important components of the model , and that combining them improves the results for both datasets . however , for innovations , the improvement is less pronounced than for text . selective attention mechanisms like the text - aggregation and the innovations are crucial for future work ; however , they have a high impact on performance in the short - term ( i . e . , test set f1 > 0 . 2 ) . we find that adapting the feature set for the task at hand , particularly the innovations layer , is beneficial in the longer term ( e . g . , up to 3 . 5 years ) . selecting only innovations and combining them with text improves performance for the shorter - term task , but does not improve the long - term .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is presented in table 2 . our model shows marked performance improvement over the previous approaches . it closely matches the performance of the best - performing automatic neural network ( rnn - based embeddings ) and self - attention sentences .
table 2 shows the performance of all the methods that we consider for the document dating problem . the unified model significantly outperforms all previous models .
the accuracy ( % ) comparisons of both word attention and graph attention are shown in table 3 . with respect to neuraldater , the oe - gcn model shows marked performance improvement . however , ac - gcn shows a drop of more than 2 % in accuracy compared to neuraldater . graph attention is less effective than word attention for this task .
the performance of all models is presented in table 1 . embedding + t achieves the best results , surpassing both cnn and dmcnn , with a gap of 3 . 5 points . while the gap is modest , it is significant and should not be dismissed , as it shows the model performs better when trained and tested on a larger corpus . trigger and argument are the most difficult parts of the model to solve , as their performance is in range of couple of seconds . finally , when trained on a smaller corpus , the ability to reason over semantic distance increases , as expected .
the results are presented in table 1 . first , we present the results on event identification and event classification . all methods show significant improvements in both accuracy and recall . cross - event features significantly improve both for argument and event identification . the best results are obtained with the f1 method , which shows marked improvement on both event and entity identification .
the results are shown in table 3 . we observe that all models give similar results on the test set , with the exception of spanish - only - lm , which gives considerably better results . though fine - tuned - lm achieves lower precision on some benchmarks , it is still comparable with the original model . as can be seen , the two models use completely different vocabulary for each sub - domain , with different features contributing differently to the overall performance . the difference is most prevalent in english , where fine - tuned - lm gives a performance drop of 2 . 36 points on average compared to cs - only . additionally , the difference is less pronounced for french - only , which shows the advantage of language adaptation .
results on the dev set and the test set are shown in table 4 . fine - tuned models perform well over both subsets of the code - switched data , showing that the training set can be further improved with a reasonable selection of the correct subset of the training data . however , fine - tuned models do not generalize well compared to cs - only , indicating that there are some limitations to the approach .
fine - tuned - disc improves the results for both monolingual ( cs - only - disc ) and code - switched ( mono ) . as can be seen in table 5 , the difference in accuracy between the two sets is less pronounced for the case of the gold sentences , which shows that fine - tuned - disc is more useful for both languages . however , for the mono - sentence subset , the improvement is much larger .
we show precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . note that the improvement from baseline to current state - of - the - art is statistically significant ( p < 0 . 01 ) .
we show precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset as well as the baseline dataset in table 5 . the improvement from baseline to type combined is statistically significant ( p = 0 . 03 ) and r > r = 94 . 35 , which shows that the ability to combine gaze features with semantic information is a significant improvement .
results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . the first set of results show that glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 , and the second set shows that it achieves the best results with a combined score of 89 . 7 % on the ppa test set . while the combined score is slightly higher than the previous best result , it should be noted that it uses syntactic skipgram instead of prefixing with prefixes .
results are shown in table 2 . the system achieves the best results with three out of the four approaches . hpcd even outperforms rbg with a large margin . ontolstm - pp also achieves the highest acc . on the uas ,
we observe that the effect of removing sense priors and context sensitivity ( attention ) is also not significant ( p = 0 . 0088 ) . however , it does improve the precision of the model .
we find that incorporating subtitle data and domain tuning improves the bleu % scores for both en - de and multi30k models by 1 . 2pp over " subsfull " . however , for en - fr , the improvement is less pronounced than for mscoco17 . as can be seen in table 2 , the bigger difference between ensemble - of - 3 and sub - sfull is due to larger variation of the referred objects in the image caption translation corpus .
we find that the domain - tuned model outperforms the plain h + ms - coco model when we add all the features described in section 3 . 3 . the results are presented in table 3 . as can be seen , the better results are obtained with the label - neutral model , adding all the labels to the model improves results for both en - de and en - fr ( see x4 ) . however , for mscoco17 , the results are slightly worse than those of lemmatization .
we show bleu scores in % for both languages for the models as well as marian amun . adding automatic image captions improves both the general performance of the models and the sub - projection quality . the results are shown in table 4 . as hard coreference problems are rare in the multi - domain setting , we do not have significant performance improvement . however , the improvement is less pronounced for en - de , which shows the advantage of finetuned automatic captions .
the results in table 5 show that enc - gate and dec - gate achieve better results than en - de and mscoco17 on flickr16 and 17 . while the former achieves a lower bleu % than the latter , it is still comparable with the former on flickr17 . encryption achieves the best results with a gap of 3 . 86 points from the wasserstein - keller distance . further , when using multi30k + ms - coco + subs3mlm , detectron mask surface , the improvement is only 2 . 53 points . we conjecture that this is due to the larger variation in the number of frames in question and the difficulty of extracting visual information from these multiple frames .
we observe that the multi - lingual approach further boosts performance for both datasets when we add in the visual features of " - visual features " and " - ensemble - of - 3 " . the results are slightly better than those of " text - only " . as can be seen in table 3 , the combination feature - values that add visual features to the model achieves the most consistent performance . however , it should be noted that these features are only considered when they are considered in combination with other features .
we compared these models on the test set of hotpotqa in the distractor and treehugger setting , respectively . the results are presented in table 1 . first , we observe that en - fr - ht achieves the best performance with a gap of 2 . 27 points from the last published results ( hochreiter et al . ( 2017 ) on both datasets . in addition , the average number of frames per second for each translation is slightly higher than the previous best state of the art .
for brevity we only report the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of sentences for each language pair is reported in table 1 .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table shows , the difference in performance between the two languages is minimal , however it is significant for spanish , which is more than twice as diverse .
automatic evaluation scores ( bleu and ter ) for the rev systems are presented in table 5 . the en - fr - rnn - rev and en - es - trans - rev systems achieve high performance , both when trained and tested on the real - world dataset ( see x4 ) . however , when trained on the synthetic dataset , the improvement is less pronounced , about 2 . 5 points on average . this indicates that the challenge of semantic transformation is in how to extract relevant information from supporting documents and synthesize these multiple facts to derive a coherent sentence .
results on flickr8k are shown in table 2 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the one supervised by us . our model obtains the highest recall @ 10 and the median rank is 0 . 2 .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the text - supervised model from kutuzov et al . ( 2017 ) . the acoustic features extracted from the rsaimage files are used to improve the recall @ 10 and mean mfcc scores . while the average recall of the two models is slightly higher than the chance for both rsaimage and audio2vec - u , the difference is less pronounced for vgs . as a sanity check , we also compare against other approaches that rely on word embeddings . for example , if a model has two entities in the same sentence , it can match either one of them . in this case , we compare against segmatch and random match . the difference is minimal , however it is statistically significant .
we report the example sentences of the different classifiers compared to the original on sst - 2 . originally , all the classifiers used for this study were described in table 1 . however , since the dan model only works on edges , it has to be used on a screenplay that is at the edges ( hence , there are no examples in the appendix ) . as the table shows , once again , the use of the edges improves the results for both rnn and cnn . as expected , for dan , the average number of edges in a screenplay is significantly less than for rnn .
part - of - speech changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . as the table 2 depicts , the importance of the word " goodness " is only considered in relation to the original sentence . the symbols are purely analytic without any notion of goodness . they have no effect on the performance of the model .
the sentiment score changes in sst - 2 are shown in table 3 . and indicate that the score increases in positive and negative sentiment . however , in the case where negative sentiment is flipped to positive , the sentiment score decreases .
the results are presented in table 6 . as can be seen , when we add up the positive and negative responses from pubmed and sst - 2 , we get 98 % and 99 % on average . however , the difference is much larger for pubmed , which shows that it is more interpretable .
