table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as seen in the table , both the folding and the iterative approaches give good gains on training and inference , respectively . since the recursive approach uses more data , it also requires less data and time to train , which results in a lower overall performance .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the fact that the balanced dataset only contains 10 instances , the small improvement in performance between the balanced and linear datasets is less striking , but still suggests that there is a small amount of room for further performance improvement even under the low threshold of 1 .
results in table 2 show that the max pooling strategy consistently performs better in all models with different representation configurations , and that the dropout probability function also achieves better performance . we note particularly the large difference in f1 score between ud v1 . 3 and conll08 ( 7 . 66e + 00 vs 7 . 57e - 03 ) . as hard coreference problems are rare in ud , we do not have significant performance improvement . however , when using the best performing domain - adaptive feature - pooling scheme , we get a 0 . 83 / 3 . 49 overall improvement over the sigmoid baseline on average .
table 1 shows the results of using the shortest dependency path on each relation type . as the results show , the macro - averaged model does not rely on dependency trees as much as the modelled entity , and consequently has better f1 scores . further , when dependency trees are used as prefixing agents , the effect of sdp is less pronounced , but still significant .
the results are presented in table 3 . we observe that the three types of embeddings perform comparably to each other when the true response is added . however , in the more realistic second case , when recall is only computed on examples with a f1 score of 50 % , performance on the y - 3 subset drops significantly . this indicates that the model is still unable to learn the task to a high degree .
the results are presented in table 1 . we show the results for english and german . for english , we see 21 . 17 % improvement on average compared to the previous state of the art . on the other hand , mst - parser achieves a gap of 10 . 69 % on average with respect to accuracy .
the results reported in table 4 show that the lstm - parser parser performs significantly better on the essay and paragraph level compared to the original embeddings . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference between the average and average c - f1 scores for the two indicated systems is less pronounced for the paragraph level .
the results are shown in table 1 . the results of the best performing models are presented in bold . our tgen model ( hochreiter and schmidhuber , 1997 ) obtains the best results with a gap of 10 . 31 % bleu and 6 . 85 % rouge - l score . next , we compare our model with the best previous state - of - the - art models . we observe that tgen + is significantly better than the original model and tgen − is better than sc - lstm . finally , we see that our model is comparable with both the original and the best - performing previous models .
table 1 presents the data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the cleaned version also exhibits a significant drop in performance compared to the original one .
table 3 presents the results of experiments on the bleu , meteor , rouge - l and ser systems . the results are presented in bold . the original and original embeddings achieved by tgen + are significantly better than those by sc - lstm ( hence , the difference between the two sets is less pronounced ) . when trained with only one type of error detector , the model achieves the best result with a gap of 2 . 27 % between the original and the original score . adding entity nodes improves the results for both sets . the average number of correct answers for each add - value addition is higher than the original , indicating that more data are available to train the model and refine the model for further performance improvement .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . we found that the majority of errors in our tgen instances were caused by errors caused by adding , missing , and wrong values ( see table 4 ) .
table 3 compares the performance of our model with previous stateof - the - art models on the hidden test set . the results show that , let alone a reduction in performance , our model obtains the best general performance on all metrics with a gap of 10 . 2 % from the previous state of the art .
table 2 : main results on amr17 . our ensemble model achieves 24 . 5 bleu points , which marginally outperforms the previous state - of - the - art results . also , we notice a drop in performance between the ensemble and single - parametrized models , as seen in table 2 , the smaller size of the ensemble model does not impact performance , indicating that all the parameters in the ensemble are important .
table 3 presents the results on the english - german and czech datasets . our model ( beck et al . , 2018 ) outperforms the previous state - of - the - art models on both datasets with two tasks . the results show that , compared to the single model , the ggnn2seq model performs better on both languages with a gap of 10 . 8 % in english - czech and 5 . 6 % in german . the gap between the two sets is modest but significant with respect to bias metric , as seen in the results presented in table 3 , the smaller performance gap between mono - and bi - rnn models indicates that the performance gap brought by these models can be overcome with a reasonable selection of the training data from which the model was derived .
table 5 shows the effect of the number of layers inside our dkr - based network on the performance . the first group shows that more layers inside the network helps the model achieve state - of - the - art results . the second group show that it is even more effective with fewer layers .
table 6 compares the performance of our model with previous stateof - the - art gcns with residual connections . gcn with rc as well as residual connections outperforms other models with no connections at all . the results show that the combination feature of rc and la improves the general performance of both gcns . moreover , the model with the most residual connections ( dcgcn ) is closer to the baselines than the other two models . with respect to baselines , our model dcgcn1 ( 9 ) and 2 ( 18 ) is nearly 5 % better than both the other baselines . however , the difference between the two baselines is less pronounced for our model , with a gap of 2 . 5 % with respect to rc .
the performance of these models on the simulated test set is presented in table 4 . the first group shows that , let alone a reduction in performance , the dcgcn model is well - equipped to perform this task with a minimum of 80 % false positive rate . furthermore , it achieves state - of - the - art results , outperforming previous models with a gap of 10 . 5 % on average . next , we notice small improvements relative to the previous state of the art showing that the gcn model can rely less on superficial cues .
table 8 shows the results of an ablation study performed on the dev set of amr15 . the results of removing the dense connections from the i - th and iii - th blocks shows that the dcgcn4 model exhibits a significant drop in the density of connections .
the results of an ablation study for modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that , under the " - global node & linear combination " and " - direction aggregation " models , the global node and the hierarchical clustering of the attention nodes achieves better performance , while the global attention layer achieves the best performance . also , we see that under " - coverage mechanism " the effect of the domain - aware attention layer is less pronounced , but still significant . when using " - graph attention " as a delexcalization module , we get a 0 . 9 / 3 . 4 bleu improvement over the strong lemma baseline .
table 7 presents the performance of our initialization strategies on various probing tasks . our framework establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except the one where it is tested on single - domain data . the results show that the initialization strategies significantly improve the generalization ability of the model , and that the resulting compactness and recall scores are comparable to those of glorot ( see x4 ) .
the results are shown in table 1 . we observe that the h - cmow model achieves outstanding results , outperforms the previous state - of - the - art methods with a gap of 10 . 2 points from the last published results ( h - cbow ) , while surpassing our strong lemma by 3 points . the relative lower bshift and length scores indicate that the concatenation of subjnum and length is the most important part of the model , followed by the bshift metric , which gives a significant performance boost . finally , the topconst and topconst metrics give a performance boost of 2 . 6 points and 1 . 8 points respectively over previous state of the art models .
the results are shown in table 5 . the first group shows that cmp . ow and cmow achieve outstanding results , outperforming the best previous approaches across all three sub - categories . hybrid also achieves competitive or better results than cmp . 6 % and 7 . 2 % better results overall compared to the previous state - of - the - art .
table 3 shows the performance of our models on the four downstream tasks as well as the results of unsupervised mode learning on the sts datasets . as the results show , the cbow and cmow models have seen considerable performance improvement since the last published results ( hochreiter et al . , 2016 ) . hybrid also shows a relative improvement . with respect to sts13 and sts16 , the cmow model achieves a new best performance of 62 . 2 % and 62 . 4 % on average compared to the previous state of the art . when we switch from cbow to hybrid , we see a further 4 . 5 % increase in performance and a decrease of 1 . 8 % in performance compared to cmp .
table 8 presents the performance of our system with respect to initialization and supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level domains , and on all subtasks except mpqa . it significantly boosts performance for both sub - and macro - level tasks , and its average number of iterations is close to those of glorot ( 86 . 6 % vs . 77 . 2 % ) , though the gap is narrower . on the two lower - level datasets , it achieves gains of 3 . 5 % and 6 . 2 % over glorot ( 76 . 4 % and 75 . 7 % ) . these results show that our system can rely less on superficial cues and more on interpretability .
table 6 shows the performance of our method compared to the state of the art cmow - r model on the four downstream tasks . the results show that , compared to cmow , our method performs better on all four tasks . the difference is most prevalent in the sts12 and sts16 tasks , where our cbow model obtains the best performance .
the results are shown in table 1 . we observe that the three approaches give similar performance on the hidden test set . the smaller difference in bshift and tense score between the best performing method and the other two is less pronounced for cbow , but still suggests that there is a significant difference in performance between the two methods . subjnum and topconst are the only two groups of data that are consistently better than the others under tense , indicating that the semantic information injected into the subjnum by the cbow - r is significant enough to result in a measurable improvement . finally , the presence of concatenated keyphrases on the training data helps the model to improve its performance . it achieves state - of - the - art results , outperforming both the best previous methods .
the results are presented in table 5 . the first group shows that cbow - r and cmow achieve outstanding results , outperforming all the base lines with a gap of 10 . 5 % and 6 . 2 % points from the previous state of the art .
table 3 presents the results of models trained on all loc , all org and all misc datasets . the results are presented in bold . our model ( tmtmil - nd ) obtains the best results with an absolute improvement over the previous state of the art on all three datasets . it closely matches the performance of the best state - of - the - art models with only 0 . 40 % absolute difference . supervised learning also achieves competitive or better results than the best previous models with a gap of 10 . 5 % on average from the last published results ( gillick et al . , 2016 ) . to test the contribution of domain - adaptive learning , we compare our model to other models using the three types of data augmentation . we observe that , in all but one case , when using only the org dataset , our model performs better than the other two baselines . in particular , the improvement is much larger under the misc dataset than that under the loc dataset . this validates our hypothesis that combining all loc and misc data helps the model to learn the task to a high degree .
results on the test set are shown in table 2 . the first set shows that after applying our domain - adaptive delexcalization and domain - aware learning methods , the model 1 and model 2 achieve outstanding results . the second set shows the results under the best performing model , τmil - nd . with these settings , both the epm feature - values and f1 scores reach the best performances , while the roc features - values are close to the expected level . finally , we notice a slight drop in performance between the two sets when using the supervised and unsupervised learning methods .
table 6 presents the results of models trained on the hidden test set of g2s - gat . the results are broken down in terms of performance on entity extraction . our model obtains the best results with an absolute improvement of 3 . 86 % on ref and 2 . 45 % on gen . while the model performs slightly better on both datasets , it should be noted that it is trained on a significantly larger corpus . on the other hand , the performance gap between the two sets is less pronounced with respect to ref . with respect to con and neu , our model achieves the best result with a gap of 2 . 43 % on average .
table 3 presents the results on the hidden test set of bleu , meteor and s2s . the proposed g2s outperforms all the base lines with a gap of 10 . 32 ± 0 . 18 points from the last published results ( konstas et al . , 2015b , c ; konstas and gurevych , 2015a ) . the model achieves state - of - the - art results , outperforming both published and unpublished work on every metric by a significant margin . note that the model achieved its best performance on the ldc2015e86 and ldc2017e86 datasets when trained and tested on the same dataset , which shows the advantage of finetuning word embeddings during training . the performance gap between the best and worst performing models is modest but significant with respect to both datasets , with the g & l model achieving 3 . 72 % and 6 . 45 % overall improvement over the previous state of the art on both datasets .
table 3 : results on the ldc2015e86 test set when models are trained with additional gigaword data . the results show that g2s - ggnn models significantly outperform the best previous models with a gap of 10 . 60 % on the external test set and 3 . 40 % over the strong lemma baseline . the results also show a significant drop in performance between the baseline and the current state of the art .
the results of the ablation study on the ldc2017t10 development set are in table 4 . our model significantly outperforms the previous state - of - the - art models with a gap of 10 . 6 % on the bleu metric and 3 . 5 % on meteor metric .
the results are shown in table 5 . as can be seen , the smaller diameter and length diameter increases , the g2s model shows a significant drop in performance . from 7 . 2 % to 25 . 7 % overall improvement on average , and from 25 . 9 % to 29 . 8 % increase on average for s2s .
table 8 shows the comparison of the output and the input for the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , the smaller miss fraction indicates , the g2s model is more effective in generating the correct output when the required tokens are present in the input , while the size and type of missing elements in the output are the most important factors in the generated sentence . also , we notice a drop in performance between s2s ( 47 . 34 % ) and gold ( 50 . 06 % ) compared to the previous state of the art .
table 4 shows the pos and sem tagging accuracy on a smaller parallel corpus containing 200k sentences . the results show that both features significantly improve the pos tagging performance and the sem accuracy . pos even outperforms the original embeddings with a gap of 10 . 2 % from the last state - of - the - art results .
table 2 compares the pos and sem tagging accuracy with unsupervised word embeddings using the baselines and an upper bound encoder - decoder . word2tag also achieves competitive or better results than unsupemb with a baseline of 81 . 41 % on pos and 87 . 55 % on sem . the results show that the encoder decoding ability is further improved with the upper bound being set at 87 . 41 % .
the results are presented in table 4 . after applying the current state - of - the - art tagging accuracy and precision measures , we observe that : 1 model obtains the best performance . 2 model significantly outperforms all the other models apart from the case of the eq . 10 case . 3 model achieves the highest accuracy . 4 model significantly improves over the previous state of the art on all metrics . 5 model significantly boosts accuracy for pos and sem .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results show that the first layer of our model achieves the best performance with respect to all target languages with a gap of 10 . 5 % on average . the second layer is close to the top of the performance range , but still performs substantially worse than the first .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . as shown in table 8 , the attacker significantly improves his performance with the addition of gender and race features . however , he does not improve significantly over the baseline showing that gender features alone do not improve the performance . sentiment features also have a significant effect on the performance , however it is less significant .
the performance of our model on the training data is shown in table 1 . as this table indicates , all the data we trained on had high accuracy on the dial task , indicating that the training set was well - equipped to handle the diverse nature of the task . further , the accuracy reached an all - time high on the sentiment task , showing the effectiveness of the classifier . gender bias also contributed significantly to the model ' s performance . we observed that it was unable to distinguish between gender and age based on the average number of tokens in the dialog , which indicates that gender bias was a significant part of the gender bias .
table 2 shows the results for tasks that are balanced and unbalanced , compared to those that are unbalanced . as expected , the results show , when the task is balanced , the accuracy is better while the unbalanced data is lower . sentiment and race features are the most difficult ones to solve , as the result show in table 2 . however , gender and age feature - values are the only ones that are consistently better than gender , showing the gender bias in our model is overcome by the age and classifiers .
as shown in table 3 , the attackers perform similarly to the corresponding adversaries on all datasets with an adversarial training set . the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in the leakage metric . sentiment and race features are particularly difficult to detect , as the human judgement is biased towards gender bias . we find that the presence of gender - specific features in the protected attributes helps the model to distinguish between the true response and the negative response . finally , age and sentiment features are the most difficult ones to detect . the team found that not utilizing gender features , especially in the case of race , contributed significantly to the model ' s performance .
the results reported in table 6 show that the rnn encoder is more effective at decoding the protected attribute than the leaky embeddings . the rnn also exhibits a significant performance drop when encodified with different encoders .
we show the results of models trained on the hidden test set of ptb and wt2 . the results are presented in table 1 . the first set shows that the model performs well with the finetune and tuning features . the second set shows the results with the best performance . this group of models significantly outperforms the previous state - of - the - art on both datasets with a gap of 10 . 5 % on average . with respect to tuning , we see that the lstm model performs best with a combined finetune / finetune score of 43 . 57 / 71 . 03 and 35 . 63 / 59 . 86 on the wt2 and ptb bases , respectively , with an absolute improvement of 2 . 36 / 0 . 38 and 6 . 45 / 4 . 55 points over the previous best state of the art .
the results of different approaches are shown in table 1 . the first set of results show that all the combinations we considered had good performance on the training set . however , the results do not exceed the upper boundary of what the models could achieve with a reasonable number of parameters , in which again demonstrating the competitiveness of parameter sharing . this group of models outperforms all the other models that did not use parameter sharing : the lstm ( 8 . 36m ) and sru ( 5 . 28m ) are the best performing models . the gru is the worst almost in every metric ( 7 . 43 % ) and atr is the only one that performs much worse . table 1 compares the performance of these models with the previous state of the art on both training and test set . with respect to data augmentation , we also consider the results of rocktäschel et al . ( 2016 ) and ( 2016 ) . in both cases , we see that the model can be further improved with an increase of accuracy and a decrease of time .
table 3 presents the results on the training set of amapolar , yahoo time and yelppolar . we benchmark against the best performing models from the previous literature on all three datasets . the results are presented in bold . the proposed method outperforms both published and unpublished work on every metric by a significant margin . for example , amapolar err and yelp time are both 1 . 8 times better than the previous best state - of - the - art models on both datasets . gru also achieves competitive or better results than lstm and atr , while sru is 1 . 7 times better on yahoo time . table 3 compares the performance of these models on different training sets . as can be seen in the table , the smaller performance gap between atr and gru indicates that the gru has learned to rely less on superficial cues . also , we notice a drop in performance between the best and worst performing model when using only one parameter set .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . as shown in the table , the gnmt model significantly outperforms other methods with a gap of 10 . 67 % from the last published results ( gnmt ) , while olrn is only 1 . 43 % better than atr . with a training time of 1 . 2k seconds , the model takes significantly less time to train and decode compared to other methods . gru also exhibits a significant performance drop compared to atr , as seen in table 3 , the gru model is more than 4x faster at decoding one sentence per training batch compared to the atr model . though sru has seen more training data , it is still less than 5 % faster at decoder . we conjecture that this is due to the smaller training data size and the high number of training instances in the newstest2014 dataset . table 3 also highlights the scalability of using multi - headed attention . our model obtains a significant improvement over the previous state - of - the - art models with a 0 . 9 / 3 . 26 improvement on average compared to glove et al . ( 2017 ) with a 2 . 67 / 10 . 40 overall improvement .
table 4 shows the exact match / f1 - score on the squad dataset of wang et al . ( 2017 ) . as the results show , all the parameter numbers we considered had a significant impact on the model ' s performance . the gru even outperforms the lstm and atr with a f1 score of 1 . 83 / 76 . 76 and 1 . 59 / 78 . 83 , respectively . although the atr and gru have better overall performance , the sru is still comparable with the best performing atr model . finally , rnet * shows a drop of more than 5 % in performance when we add elmo as a parameter to the base .
table 6 shows the f1 score of our model ( lstm ) on the conll - 2003 english ner task . the model obtains the best performance with an f1 - score of 90 . 56 . by comparison , the gru model ( at 89 . 61 % ) and sru ( at 88 . 46 % ) obtain the second and third best f1 scores . although the number of parameters in our lstm model is small , it is comparable with other sophisticated neural models which receive a lot of parameter information about human - generated ner data . as shown in table 6 , the three types of neural models perform comparably to each other . however , lrn is more than 5 % better than other models . with this result , we decided to compare our model against other models using only one parameter number . we observe that lrn significantly outperforms other models with different parameter numbers .
table 7 shows the performance of our model on the snli and ptb tasks with the base + ln setting and the test perplexity setting . the results show that our glrn model significantly improves over the previous state - of - the - art model with the help of the base - and - ln set .
table 2 presents the results on paragraph selection . our system outperforms all the other methods with a gap of 10 . 08 % on average . retrieving the word embeddings from the supporting documents yields significantly better results . the system achieves state - of - the - art results with r - 2 and mtr scores of 1 . 38 and 3 . 97 on average compared to the previous state of the art . sentence prediction using our system is more accurate than that using oracle . we observe that the number of instances for which a human could type a sentence is less than the number for which an entity could type it is more than twice as often as for an entity . our proposed system improves the results for both systems with good recall and accuracy .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 % , which indicates that the seq2seq system is indeed very good . the second best result is achieved by candela ( 30 . 2 % ) on the content richness and appropriateness evaluation , which shows the high quality of his output . the third best result by a large margin is obtained by retrieval ( 50 . 2 % ) . this shows the extent to which the data augmentation technique can improve the interpretability of the output . finally , if a system can extract relevant information from supporting documents , it can further improve its performance .
table 3 presents the results of models trained on the dewey decoder and the ted talks datasets . the results are broken down in terms of performance on extractive and abstractive keyphrases . for extractive topics , we see that the proposed hclust outperforms both the published and unpublished work on every metric by a significant margin . for abstractive data , we observe that the model trained on ted talks performs better than the other two baselines on all datasets except for the case of the dsim dataset . on the other hand , for abstractive datasets , the gap between the en and pt scores is much smaller .
table 3 presents the results of models trained on the dewey decoder and the ted talks datasets . the results are broken down in terms of performance on extractive and abstractive keyphrases . eq . results show that , let alone a reduction in performance , the model performs better on all datasets with two tasks compared to the previous state - of - the - art . on the other hand , if a model has access to lexically less diverse training data , it performs worse than the other two models . in particular , we see that on datasets with fewer training examples , the performance gap between en and pt is much larger .
table 3 presents the results of models trained on the dewey decoder and the ted talks datasets . the results are broken down in terms of performance on extractive and abstractive keyphrases . eq . results show that , let alone a reduction in performance , the model performs better on all datasets with two tasks compared to the previous state - of - the - art . on the other hand , if a model is trained on a single dataset , it performs worse than on all the other datasets except tf .
the performance of our system on the selected metrics is presented in table 1 . we show the results for dsim , tf , df , docsub and tf . from left to right each row displays the averagedepth , averageroots and totalterms . the averagedepth and maxdepth numbers show that our system can distinguish between the true response and negative responses . according to the table , the numberrels with the highest averagedepth are the ones that are considered to be of high quality . europarl has the best performance with respect to all metrics .
from table 1 , we compare the performance of our method with the previous state - of - the - art methods on various datasets . our system outperforms all the base lines with a gap of 10 . 29 % on average . among all metrics , the average depth number of roots is the most important , followed by totalterms and docsub . our hclust model significantly boosts the general performance of all metrics .
table 1 compares the performance of our proposed method with the baseline and the enhanced version of our method , lf , on the validation set of visdial v1 . 0 . in dfgn , both the enhanced and original models perform better than the baseline model . the difference is most prevalent in the second case , where the enhanced model ( lf + p1 ) obtains 73 . 63 % ndcg % compared to the baseline ( 57 . 21 % ) and 62 . 88 % compared with 62 . 97 % in the first case .
the performance ( ndcg % ) of the ablative studies on different models that apply p2 indicates that p2 is the most effective one ( i . e . , hidden dictionary learning ) shown in table 2 . the results also indicate that applying p2 further improves the rva score over the baseline model by 1 . 63 points .
table 5 compares the performance of our hmd - f1 model with previous state - of - the - art models on the hard and soft alignments . we see that , let alone a reduction in performance , our hmd model significantly outperforms all the other models apart from the case of the soft alignment .
the results are shown in table 1 . the first group shows the results on the direct assessment and w2v tasks . our proposed method outperforms the previous state - of - the - art methods across all three domains . we observe that our proposed method significantly boosts the generalization ability of bert score and ruse by about 2 % .
the experimental results on the hidden test set of hotpotqa are shown in table 1 . the proposed hgn outperforms all the base lines with a gap of 10 . 5 % on average . specifically , hgn beats the previous state - of - the - art models on all metrics by a significant margin . on the other hand , it performs substantially worse than the best previous hgn model on two out of the four scenarios .
the semantic threshold for bertscore - recall is set at 0 . 7 while for meteor is set as 0 . 8 . we also consider the numerical threshold for word2vec , as this approach uses word embeddings with elmo as well as bert scores . the results are shown in table 5 . sent - mover achieves the best performance with an f1 score of 0 . 939 on m1 and m2 while word2score is the best f2 score . elmo - based word - weights give a significant performance boost which is expected in a single shot framework . we notice that the relative accuracy drop between word - and sentence - weights as measured by the leic ( * ) and spice ( 0 . 594 ) metrics is significant , which shows that the semantic features extracted by the elmo layer are quite useful .
the results are shown in table 1 . the results of the best performing models with different classifiers are reported in bold . the most representative ones are shen - 1 , sim - 1 and gm . essentially , all models only slightly outperform the baseline model with only one exception : m0 . 7 . with the exception of sim , all the other models perform better when using both meta - and max - classifiers .
table 3 presents the results on the transfer quality and semantic preservation tasks . our model obtains the best results with a gap of 2 . 5 % in f1 score from the previous state - of - the - art results . semantic preservation and fluency are the only two areas where we see significant performance gap . the difference between the two sets is less pronounced for yelp , but still suggests some reliance on superficial cues . finally , we see that the semantic preservation task is very useful , improving upon the strong baseline performance of ame model by 4 . 6 % on average .
table 5 shows the results of human sentence - level validation on the three datasets for validation of acc and pp . the results show that both the method ( which relies on word embeddings simulator , and human ratings ) verifies the accuracy of the summaries , and the human ratings of the semantic preservation . it also shows the percentage of summaries that match the human evaluation criterion that match , as measured by spearman ’ s ρ b / w statisticians .
the results are presented in table 7 . the results of the best performing models with different classifiers are reported in bold . essentially , all models show that when only using one type of classifier , the performance reach the best when using all the features available from the other classifiers . with the exception of sim , shen - 1 is the only classifier that performs significantly worse when using both meta - and max - classifiers .
results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . as shown in table 6 , the transfer results achieved by our best model ( which also applies to fu - 1 ) is significantly better than prior work on sentiment transfer . however , the results obtained by our model do not exceed the upper boundary of acc , because the definition of acc varies by row because of different classifiers in use . the results also show that the style embedding method is worse than the delete / retrieve method , and also requires a lot more data to train , since the training set size is small . we do not include results from simple - transfer as these are not used in this work and are not considered to be comparable to the results of other methods . finally , we include results on yang2018 as a comparison as this is the only case where the classifiers used in the setup are trained with human references .
the performance of our model on the nested disfluency dataset is shown in table 2 . as a baseline , we also consider the results of rephrase experiments , where we only consider tokens that are already in the training set , in case of repetition , rephrase and restart . we find that for all three scenarios , our model can significantly improve the performance with a drop of less than 0 . 5 % in the overall performance .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , is shown in table 3 . as the fraction of tokens in each category grows , the prediction accuracy decreases , indicating that the content word embedding ability of the repair method is beneficial . however , for non - disfluencies , the picture is less clear , showing that more tokens belong to each category as well .
the results are shown in table 2 . the results of the best models show that the text - rich innovations model is more appealing than the plain text model . moreover , the performance gap between the two is less pronounced when the text is combined with innovations , showing the advantage of finetuning word embeddings during training . as the results of applying the best performing feature - rich reward learning method ( itrl - nsp ) shows ( table 2 ) , incorporating all the features boosts the performance of the model , and the average number of iterations taken to solve a given case is less than the time taken to encode the previous iteration . in addition , the model performs better when trained with innovations as well .
the performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model obtains substantial gains in accuracy over the previous state of - art methods . it closely matches the performance of rnn - based and self - attention embeddings with only 0 . 43 % absolute difference . similarly , it achieves a significant improvement in the accuracy percentage with respect to discuss , with a gap of 3 . 59 % from the previous best performance . we show that our model can significantly improve the interpretability without a drop in performance .
table 2 shows the performance of all the methods that we consider for the document dating problem . the unified model significantly outperforms all previous models with an accuracy drop of 2 . 2 % compared to the previous state - of - the - art .
table 3 compares the performance of our method with and without word attention . our approach shows that both word attention and graph attention have a significant impact on the performance , improving the t - gcn by 3 . 6 % in the word attention task and giving a 0 . 9 % boost in the accuracy with respect to the graph attention task .
the results are shown in table 1 . the first group shows that after applying our domain - adaptive delexcalization and domain - aware belief modeling , the model performs well on both datasets with a gap of 10 . 5 % from the previous state of the art . next , we see that dmcnn and jmee achieve state - of - the - art results , respectively , with an absolute improvement of 3 . 2 % and 6 . 4 % over the previous best state of - the - art model . with respect to tn and tn , the jrnn model achieves the best overall performance with 6 . 6 % and 3 . 4 % ) improvement over the strong dmcn baseline . while argument argument is the most difficult part of the model to solve , it is the only part that is consistently better than argument , showing the non - triviality of argument argument removal . we see that the same tendency is observed in the eq . 10 . 4 and 11 . 4 year old tweets are particularly difficult to solve for both groups .
table 1 presents the results on event identification and event classification . our proposed method establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except event identification . it significantly improves the results for both event and argument identification . the proposed method further improves upon the f1 and f1 scores by 1 . 7 points and 0 . 8 points , respectively , with the argument identification and role modeling performance improving by 2 points and 1 point , respectively .
the results are shown in table 5 . perhaps the most striking thing about the shuffled - lm model is that it achieves a better generalization performance than does fine - tuned - lm . fine - tuning - lm reduces repetition , and gives a 0 . 9 / 3 . 4 bleu improvement over the baseline model . the relative lower precision scores of english - only , spanish - only and french - only compared to the original embeddings shows that these languages are already well - equipped to perform this task " out - of - the - box " . when trained on a single domain , the accuracy drops significantly for both languages . as shown in the second group of table 5 , we further compare our model with other methods of leveraging syntactic or semantic information . we find that , in general , all the methods give a comparable performance , but the difference is most prevalent in english , where our model ( cs - only - lm ) is nearly 5 % better than both the other methods . also , we notice a drop in performance between fine - tuned - lm and cs - only + vocab - lm when trained on both domains .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model improves upon the strong lemma baseline by 3 . 8 points in the standard task formulation and by 4 . 2 points in fme .
fine - tuned - disc improves the performance for both gold sentences and monolingual sentences . the results are shown in table 5 . fine - tuned - disc reduces the error of the gold sentences compared to fine - tuned - lm , and gives a 0 . 9 / 3 . 53 overall improvement over the strong lemma baseline . similarly , the performance increase is significant for both mono - and code - switched sentences , which shows the advantage of finetuning word embeddings .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline model to the current state - of - the - art is statistically significant ( p < 0 . 01 ) . as can be seen in table 7 , the type combined gaze features significantly improve the recall and precision of the model , and the f1 score also improves .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for evaluating the recall task on the conll - 2003 dataset . as the results show , the type combined approach shows a significant improvement in precision and recall , and also improves the f1 score by 1 . 35 points , which shows that the ability to select the correct gaze features induces the model to perform better .
results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . the hpcd system establishes a new state - of - the - art on the word embeddings , and the ontolstm - pp model achieves the best performance with an f1 score of 89 . 8 . glove - extended refers to the results obtained by applying autoextend rothe and schütze ( 2015 ) to wordnet 3 . 1 , and to the synset of tokens obtained by faruqui et al . ( 2015 ) . in the original paper , we use syntactic - sg as the type of embedding , and wordnet , verbnet and skipgram as the suffixes . as the results show , both the type and the number of tokens used for the embedding increases as a result of the additional training data . further , the effectiveness of our model decreases as the dependency distance increases .
results shown in table 2 show that the hpcd model significantly outperforms the original lstm - pp model with features coming from various pp attachment predictors and oracle attachments . the hpd also achieves competitive oracle accuracy better than the original ontolstm model . as seen in the results table , the hpd model further boosts the precision of the uas by 1 . 59 points when using all the features available from the pp attachment trainset .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results show that the full model achieves a new state - of - the - art result , significantly improving over the previous state of the art .
subtitle data and domain tuning improve the results for both models . the results reported in table 2 show that the ensemble - of - 3 model achieves better results with a bleu % improvement of 1 . 7 % over the strong baselines . the results also show that domain tuning reduces noise in the output image caption translation as well .
table 3 shows the results for en - de , en - fr and mscoco17 . the results show that the domain - tuned h + ms - coco model achieves competitive or better results than the model with the best overall performance . the models with the most pronounced difference in performance are those using domain - aware features . the results reconfirm that incorporating all the features boosts the generalization ability of the model , since the model performs better in both languages .
table 4 shows the bleu scores of the models using different combinations of automatic image captions . the results show that , when only using the best 5 captions , the model performs better than the model using only one or all 5 . however , when using both autocap and multi - attn . , the model achieves the best performance . with marian amun as guinea pig , we also observe a drop in performance between en - de and en - fr ( which shows the advantage of using both the best and average captions ) .
the results in table 5 show that the encoder and dec - gate strategies achieve better results overall when compared with the en - de and mscoco baseline embeddings . while the former achieves a lower bleu % overall , the latter achieves a higher w - score . encoding visual information with domain - aware decoding achieves a 4 . 45 % overall improvement over the strong lemma baseline on the three datasets . with respect to sub - mlm , the dec - gated model achieves a 3 . 53 % gain over the ensemble model with a gap of 10 . 59 % from the last published results ( gillick et al . , 2016 ) on the two datasets .
the results of " - visual features " compared to " + ensemble - of - 3 " show that the visual features - free model performs better overall , while the multi - lingual model performs worse . the results also show that , when only using the word embeddings with ellipsis , the semantic features alone are beneficial , improving the generalization ability of the model , but doing not improve the performance significantly compared to the combination feature - rich model . finally , we see that the effect of ms - coco on the model is less pronounced when using only visual features , improving performance by 3 . 7 % over the strong lemma baseline .
table 3 presents the results on the hidden test set of hotpotqa in the distractor and fullwiki setting . the proposed hgn outperforms the previous stateof - the - art models on all metrics by a significant margin . for example , en - fr - rnn - back achieves the best performance with a f1 score of 1 . 8776 while en - es - ht - ff gets the second best score . also , the transformer performs better than the other models on both sets with a gap of 2 . 5 % and 1 . 7 % from the last published results .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our proposed system splits the training and development set , and applies the best performing feature set to each language .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results show that the models perform well on both datasets with a gap of 10 . 5 % between the best and worst performances .
table 5 presents the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems obtain decent performance , but are still significantly worse than the best state - of - the - art systems . as seen in table 5 , the transition smoothness is only good when the system is tested on a large number of instances , which means that more than half of the models that perform poorly on a single rev instance are actually better than those that perform well on multiple rev instances .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the other row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . as seen in table 2 , both the average recall @ 10 and median rank of the vgs model are significantly higher than the mean mfcc rank , indicating that the model is more effective in selecting the relevant objects and its recall is high enough to distinguish between realistic and false positives .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the other models are the unsupervised models from ng et al . ( 2017 ) . as seen in the table , the average recall @ 10 percentage of rsaimage is significantly higher than the mean of chance , indicating that the rsaimage embedding model can significantly improve the interpretability without a drop in performance . the same tendency is observed in the case of chance : ame performs similarly to rsaimage , with a drop of 0 . 5 % compared to chrapala2017 . when trained on the generated rsaimages , the ame model achieves the best interpretability with a f1 score of 1 . 0 .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as seen in the table , all the classifiers turn in a screenplay that is similar in some sense , except for dan . with respect to embeddings , rnn shows much worse performance than dan and cnn . want to hate it ? as shown in the second example , the rnn also shows severe performance drop when using a on - off switch . when a rnn is trained on a new screenplay , it learns to distinguish between the edges and the curves , and to pick out the most interesting ones .
table 2 shows the pos changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . as the table indicates , the number of occurrences have increased , decreased or stayed the same as the original sentences . however , the percentage of instances in the current sentence has increased , which indicates that the quality of the output has not changed . the rnn model shows a drop of about 5 % in performance as a result of tuning . rnp , on the other hand , shows a significant improvement of 6 . 5 % over the baseline model .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results show that the sentiment score increases as a result of the flipped labels being flipped from negative to positive .
table 1 presents the results of the second study . our approach outperforms all the base lines with a gap of 10 . 5 % from the last published results ( pubmed , p < 0 . 001 ) . the results reconfirm that the ability to pick out the good and bad moments from the bad ones is a major part of the success of the model , and that selecting only the good ones leads to a better generalization .
