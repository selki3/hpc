table 2 shows the training and inference performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 also highlights the advantage of using a smaller number of instances to train a model , as compared to the size of training instances in our iterative framework . the smaller training instances also result in faster inference performance , as the model requires less data to train and more instances to inference . when training with a larger dataset , the performance gap between the recursive and the iterative approaches is less pronounced , as shown in table 2 .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
the max pooling strategy consistently performs better in all model variations . it is clear from table 2 that selecting the best filtering size and the maximum pooling rate are the most important factors in model performance . additionally , selecting the dropout probability and the activation func . are the only two scenarios that do not improve significantly over the best baseline representation .
table 1 shows the effect of using the shortest dependency path on each relation type . as the results show , macro - averaged models perform better than the best ones without sdp , indicating that the advantage of the shorter dependency path is more than offset by the cost of adding sdp .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that the smaller performance gap between r - f1 and y - 3 shows that predictive performance is less important for models using only one type of clustering scheme , namely , that the model using only the " y " - classifier performs better on all metrics except f1 . the results are broken down in terms of coverage on three types of datasets , with the exception of " f1 " .
the results are presented in table 1 . as the results show , precision on paragraph prediction is relatively high with mst - parser achieving 50 % and 50 % accuracy on average compared to other approaches . precision on sentence prediction is higher than other approaches as well , confirming the effectiveness of paragraph prediction . the performance gap between the best and worst performing approaches is modest but significant with muli - domain parsers achieving high precision on both error and prediction accuracy . on the other hand , for completeness , we also include the precision numbers for paragraph prediction using only f1 and f1 level as well as r - f1 to show trend on accuracy .
we observe lower performance for the two indicated systems on the essay and paragraph level compared to the lstm - parser . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between the average and the lower - performing systems is less pronounced for paragraph level , indicating that the parser performs better on essay .
table 3 presents the results of models trained and tested on the hidden test set of hotpotqa on the distractor and nist datasets . the results are presented in tables 3 and 4 . as expected , the performance gap between the original and the cleaned model is narrower with the former performing better on datasets with fewer training instances . replacing the training instances with the clean ones results in a larger performance gap with the original model , as shown in table 3 .
table 1 compares the original e2e data with the cleaned version . the difference in statistics between the two sets is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . the difference between the original and the cleaned data is less pronounced for the train dataset , but larger for the test dataset , as shown in table 1 .
table 3 presents the results on error reduction using the best performing models . we observe that original and original models are comparable , with the exception of the case of sc - lstm , where tgen is superior to original and tgen + is inferior to original . the results are statistically significant with respect to training and test set both on the " original " and " wrong " datasets ( table 3 ) . as the results of adding items to the training data after removing the wrong ones show , precision is relatively high with both the original and the original model performing better on both datasets .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as expected , the majority of errors ( 62 . 4 % ) are caused by errors caused by missing training data or incorrect values ( 14 . 6 % ) and about 5 % by slight disfluencies ( 5 . 6 % ) .
table 3 presents the performance of our model on the hidden test set of gcnseq and graphlstm . as the results of clustering experiments ( table 3 ) indicate , the advantage of our approach is clear : our model obtains competitive or better results than all the alternatives except tree2str and pbmt .
table 2 presents the performance of our model on amr17 . our model achieves 24 . 5 bleu points , a slight improvement over the previous stateof - the - art on seq2seqb ( beck et al . , 2018 ) and surpasses ggnn2seq by 3 . 5 points .
table 1 presents the results for english , german and czech . our model outperforms all the base lines with a gap of 10 . 8 % on average compared to the previous state of the art on both languages . we observe that the performance gap between the single and multi - language models is narrower with the former performing better on english , while the latter performs better on czech . table 1 compares our model with the best performing models across all languages .
table 5 shows the effect of the number of layers inside each dc layer on model performance . the first group of layers significantly boosts performance , the effect is most prevalent in the " block a " group , as shown in table 5 , the more layers inside a dc layer , the better the model performs . however , the effect is less pronounced for " b " and " c " groups ,
comparisons with baselines are shown in table 6 . rcn with residual connections outperforms rcn without residual connections , confirming the value of rcn in low - supervision settings . we observe that gcn with rc and la connections outperform both the gcn baseline and the dcgcn2 baseline , indicating that rcn models are better at selecting connections with low residual connections . moreover , the performance gap between rcn and the baselines is narrower with the help of lexical features such as rc - lstm and rc - voc ( table 6 ) .
table 4 presents the results of models trained and tested on the hidden test set of pascal - voc in the distractor and fullwiki setting , respectively . the results are presented in tables 1 and 2 . the performance gap between dcgcn ( 1 ) and other models is modest but significant with bias scores consistently high ( p < 0 . 05 ) and large ( cid : 27 ) consistently showing that the model performing best on hidden test sets is due to better model design .
table 8 shows the ablation study results for the density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly reduces the number of connections in a single block .
table 9 shows the results of an ablation study for modules used in the graph encoder and the lstm decoder . we used the best performing model , dcgcn4 , as our reference . the results show that both the language modeling objective and the coverage mechanism are well - balanced , with the exception of the domain - aware aspect .
table 7 presents the performance of our initialization strategies on a variety of probing tasks . glorot outperforms our paper in terms of all metrics except tense , indicating the advantage of initialization speedup . subjnum initialization is the most stable among all the initialization strategies , achieving a score of 79 . 8 % on average compared to our paper ' s 35 . 9 % . as the results show , precision is relatively high when precision is high , confirming the importance of initialization performance .
we observe that the h - cmow model outperforms both the cbow and the cmow on every metric by a significant margin . the difference is most prevalent in the subtasks of depth and length , with the difference being most prevalent for subjnum . subjnum and length are the most important metrics for concatenation performance , followed by bshift and coordinv . the results of concatenating these metrics with the best performing method are presented in table 3 . as a baseline , let ' s also consider the performance of our h - cbow model compared to the previous stateof - the - art approach . our model obtains the best results on both subtasks with a gap of 10 . 6 points from the previous best performance .
table 3 presents the results of models trained and tested on the same dataset . hybrid models outperform both monolingual and multi - lingual models . the results are statistically significant , with cbow achieving gains of 0 . 6 % and 0 . 7 % over the strong baselines on average compared to cmow and sick - r . we observe that the smaller size of the training data leads to fewer models performing optimally on some of the test sets , as shown in the results table .
table 3 shows the relative improvements on the four downstream tasks as a result of our model design . hybrid outperforms the original method , showing a considerable margin over the performance of cbow and cmow . as the results show , the advantage of using cbow over cmow is less pronounced when the model is trained on a larger dataset , indicating that the model performs better in the unsupervised setting . however , when trained on the larger datasets , the performance gap with the original model becomes less pronounced , showing that cbow still leads to superior performance .
table 8 presents the performance of our system on the initialization and the supervised downstream tasks . our glorot - based system outperforms all the stateof - the - art methods except for sick - r , indicating the advantage of a better initialization baseline .
table 6 shows the performance of our method compared to the best stateof - the - art method on the four downstream tasks . the difference in performance between the methods is most prevalent on the sts12 and sts16 tasks , where our method obtains the highest performance . our proposed cbow - r outperforms the cmow method on all the four tasks .
table 3 presents the results on concatenated attention . our approach outperforms all the stateof - the - art methods on every metric by a noticeable margin .
the results are presented in table 1 . the best performances are achieved by our method , which outperforms all the alternatives except sick - r . we observe that our approach yields superior results on subj , cr , mrpc and trec datasets , confirming the value of data augmentation .
table 3 presents the results on loc / loc and misc for english , german , french , dutch , russian and turkish . our system outperforms all the alternatives with a gap of 10 . 45 points over the best previous state - of - the - art model . the results are presented in tables 3 and 4 .
table 2 presents the results on the test set under two settings . name matching and named entity recognition are the most important aspects of the model performance , followed by f1 scores . the results are presented in table 2 as a result of using only one type of training data type , namely , e - e - f1 and r - r . supervised learning improves performance on both the name matching and entity recognition tasks , however , the biggest gains are obtained on the f1 metric , which shows the diminishing returns from mixing training data with random ones . moreover , τmil - nd ( model 2 ) shows a significant drop in performance compared to the original model ( i . e . , from 43 . 38 to 37 . 45 f1 ) in terms of both e - and r scores , indicating that the model is no longer well - equipped to handle the task at hand . we conjecture that this is due to the large variation in the training data size between the training set and the final test set , which explains the low performance of both methods .
table 6 presents the results of model alignment on the hidden test set of hotpotqa in the distractor and fullwiki setting . the results are presented in tables 6 and 7 . the performance gap between " g2s " and " ggnn " is modest but significant , with the g2s model performing better on both datasets in terms of both the ref and the precision metrics .
table 3 presents the results on the hidden test set of bleu and meteor using the best performing models from the previous literature . our model outperforms all the other models with a gap of 10 . 32 ± 0 . 53 points from the last published results on three of the four datasets .
table 3 shows the performance improvement on the ldc2015e86 test set when models are trained with additional gigaword data . the g2s model outperforms both the best previous models with a gap of 10 . 60 k bleu points from the last published results .
table 4 shows the results of an ablation study of our model on the ldc2017t10 development set . our model outperforms all the stateof - the - art models with a large margin .
we observe that g2s - gin outperforms all the alternatives with a margin of 3 . 51 % and 6 . 43 % over the best baseline model , respectively .
table 8 shows the results for the test set of ldc2017t10 . our model outperforms the best previous approaches with a large margin . the difference is most prevalent in the output , which shows that our g2s model can easily distinguish between the true and negative states of the output .
table 4 shows the pos and sem accuracy using different target languages trained with different parallel corpus on a smaller parallel corpus ( 200k sentences ) . as expected , the pos tagging accuracy is higher than sem accuracy , indicating that the features extracted from the 4th nmt encoding layer are more useful for target languages . additionally , the sem accuracy is slightly higher than pos accuracy , showing the advantage of redundancy removal .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . our model outperforms the best previous approaches using unsupervised word embeddings using the best performing encoder and decoder . word2tag also exhibits a significant improvement in pos accuracy over the unsupemb model
table 4 presents the system ' s performance on the four types of tokens . we observe that the system performs well on all metrics with the exception of pos tagging accuracy . the performance gap between 0 and 1 indicates that the model performs better on some of the subtasks when training with only one model .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer performs best , followed by bi and res , with a gap of 10 . 5 points from the average .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as expected , the attacker significantly outperforms the adversary in all three tasks . gender and race are the most difficult classifiers to predict , followed by age and sentiment . finally , the presence of a racial epithet in the training data hurts the ability to predict complaints .
accuracies when training directly towards a single task are shown in table 1 . our model outperforms all the stateof - the - art methods with a large margin . gender bias and age bias result in lower accuracy , but higher accuracy overall .
table 2 presents the results for tasks with balanced and unbalanced data splits . as expected , the gender and racial disparities are the most pronounced in terms of data leakage , followed by age and task prediction accuracy . gender and race are the only two groups that are consistently worse than gender for both tasks . overall , the results are slightly better than the results of pan16 for gender and race , indicating that gender bias is less prevalent in the training data . we observe that the presence of gender bias in the prediction data is less pronounced for tasks that are gender - neutral .
as shown in table 3 , the adversarial training set performs similarly to the naive test set . the difference between the attacker score and the corresponding adversary ’ s accuracy is less pronounced for pan16 , indicating that the training set is more difficult to predict . as expected , the gender and race features are the most difficult ones to predict , followed by age and sentiment . finally , the leakage metric is the smallest when training on pan16 .
as shown in table 6 , the rnn encoders perform similarly to the guards when the protected attribute is only slightly leaky . however , the difference in accuracy is less pronounced when the guard is used .
table 3 presents the results on the hidden test set of ptb and wt2 . we benchmark our model against the best performing baselines and performs well on both datasets with a gap of 10 . 45 points from the last published results ( yang et al . , 2018 ) . the results are presented in table 3 . the performance gap between our model and the best previous work is modest but significant , with our model performing better on three out of the four scenarios ( ptb , wt2 and lrn ) .
table 3 presents the results of experiments using the base acc and time metrics on the lstm and gru datasets . the results are presented in table 3 . as a baseline , we also consider the performance of " this " model compared to " lstm " and " sru " . as shown in the table , the performance gap between the two models is narrower when using only the base acc metric , indicating that the training time and the number of iterations used to compute the model is less important . however , when using both the base and the time metric , the results are slightly better . this model performs better than both the previous stateof - the - art models on both datasets .
the results of zhang et al . ( 2015 ) are summarized in table 1 . table 1 presents the results of models trained on the amapolar err , yelppolar and yahoo time datasets . the results are broken down in terms of training time and precision , with the former showing lower precision , while the latter shows higher precision . specifically , the results show that this model performs better than the lstm model on all datasets , indicating that it has better generalization ability . note , however , that the performance drop is most prevalent on twitter , where it appears to be harder to train than on the other datasets .
table 3 presents the case - insensitive tokenized bleu score on the wmt14 english - german translation task . lrn outperforms other methods with a large margin . as shown in the table , lrn is comparable to gnmt in terms of both training and decoding time . however , the difference is less pronounced with respect to the decoder speed with lrn clas table 3 shows that lrn has superior performance on the training and the decode tasks . gnmt also outperforms lrn and sru in both subtasks . at the same time , it has the advantage of training on a larger dataset , with a training time of 1 . 2k training steps on tesla p100 . when training on the newstest2014 dataset , the time to decode one sentence is shorter than that to train , indicating the scalability of our approach .
table 4 presents the exact match / f1 - score of our model on the squad dataset . the model performs well with the parameter number of " elmo " and " params " as inputs , indicating that the model is well - equipped to handle the diverse input - specific tasks . however , the model performs slightly worse when using only elmo as a parameter , indicating the effectiveness of parameter sharing . we observed that rnet * outperforms lrn and lstm in terms of match rate and f1 score . table 4 shows the results of using only elmo as parameter for the base and adding the other parameters to improve the model performance . finally , we observe the results published by wang et al . ( 2017 ) on the validation set of quad dataset , where rnet is the only model that performs significantly worse than lrn .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model outperforms all the other methods with a large margin . the number of parameters used to train our model is the most important factor in the model performance . our model obtains a 90 . 56 f1 overall score , which marginally outperforms the previous stateof - the - art model .
table 7 shows the test accuracy on snli and ptb tasks with base + ln setting and test perplexity on the standard training set . lrn model outperforms elrn and glrn with a large margin . with the base setting , the model exhibits severe overfitting since the training set size is small .
table 1 shows the system performance on word analogy task for english . retrieving sentences from word analogy tasks is easier than both human and system . word analogy tasks are easier than sentence analogy tasks for both systems . oracle retrieval is more accurate than human on both datasets with a gap of 10 . 8 % on b - 2 and 4 . 9 % on oracle . the system performance gain is larger when using mtr instead of r - 2 , indicating the advantage of redundancy removal . sentence quality is relatively high with both systems using the mtr and oracle features . human outperforms both systems with a large margin .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is well - equipped to perform in the low - resource settings . the seq2seqaug model outperforms candela and h & w hua and wang ( 2018 ) in syntactic and content richness ( appr ) , and recalls accuracy ( k 500 ) . retrieval is the second best performing automatic system , followed by human . human evaluation shows a lower standard deviation , but higher accuracy overall , confirming the value of human judgement . we observe that the quality of the output is relatively consistent across all metrics , with a system being ranked in top 1 or 2 for overall quality .
table 3 presents the results for english and spanish on the test set of " ted talks " and " europarl " . as expected , the results are significantly worse than those for " lang " . table 3 shows that the performance gap between en and europarl is greater than that between ted talks and " sqs " . however , the gap is narrower than that with " docsub " .
table 3 presents the results for english and spanish on the " ted talks " dataset . we observe that the performance gap between en and pt is narrower than that between ted talks and europarl , indicating that training on the text - similarity based dataset leads to better generalization . additionally , the gap is narrower with respect to " docsub " and " slqs " . table 3 compares the performance of different clustering schemes on the ted talks dataset compared to " europarl " . the results are presented in table 3 . as the results of clustering approaches get worse with each passing generation of data , our model outperforms all the other baselines except " sqs " by a noticeable margin .
table 3 presents the results for english and spanish on the test set of " ted talks " and " europarl " . as expected , the results are significantly worse than those for " lang " . table 3 shows that the performance gap between en and europarl is greater than that between ted talks and " sqs " . however , the gap is narrower than that with " docsub " .
according to the table , the average depth and the number of roots per row are the most important metrics for dsim and slqs performance . europarl features the best overall performance with a gap of 11 . 05 points from the last published results . as a result , we consider only maxdepth and averagedepth when evaluating the features of a dataset , excluding the effect of mixing lexical entities . the results are presented in table 1 . we observe that the quality of the dataset is relatively high when we consider both the average and maxdepth of the roots , as expected , this results in significantly better performance .
according to the table , the average depth and the average number of roots per row are the most important metrics for dsim and slqs performance . europarl ' s maxdepth and averagedepth metrics are the only ones that consistently show significant difference in performance between the original and the best - performing dsim model . as expected , the difference between the average and maxdepth metrics is most prevalent for datasets using hclust clustering .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is presented in table 1 . the enhanced version of our model ( lf ) outperforms the baseline model ( p1 ) by a noticeable margin . as the results show , the advantage of using softmax loss over traditional ranking loss is also reflected in higher performance . however , the biggest performance gain is obtained with the use of our more sophisticated hidden dictionary learning algorithm . this validates our hypothesis that the human judgement is biased towards selecting correct answers based only on the correct question type and answer score .
performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set is shown in table 2 . note that only applying p2 indicates the most effective one ( i . e . , the hidden dictionary learning method ) , indicating that p2 is more effective than applying only p1 . moreover , the model performing best when using p2 with the history shortcut is the one that gets the best performance .
table 5 compares the performance of our hmd - f1 model with other approaches on hard and soft alignments . the results are presented in table 5 . our hmd model significantly outperforms all the alternatives except wmd - unigram , indicating the advantage of redundancy removal .
the results are presented in table 1 . first , we report the average score of bertscore - f1 and ruse ( * ) on the four domains and the average number of responses for each setting as a metric for completeness . as the results show , the baselines are relatively consistent with each other , with the exception of ru - en , which is slightly worse than de - en and zh - en .
the experimental results are presented in table 1 . the first set of results show that bertscore - f1 scores significantly boosts the generalization ability of boundary - based models , indicating that the training data quality is high when using only bleu and meteor . next , we observe that the performance gain comes from a better model design using the three baselines and the w2v classification scheme . when using the sfhotel clustering feature , the results are slightly worse than those without . however , the improvement comes mostly from the higher quality of the clustering performance on the boundary features .
the performance on the word analogy test set is presented in table 1 . word analogy accuracy is relatively consistent across all metrics , with the exception of leic ( * ) where it is significantly worse . sent - mover performance is slightly better than word analogy accuracy on both m1 and m2 metrics , indicating that the semantic information injected into the model by the additional cost term is beneficial . moreover , the recall scores of bertscore - recall and meteor are both statistically significant ( p < 0 . 001 ) with respect to word analogy , indicating the effectiveness of the feature extraction procedure .
the results are presented in table 6 . we observe that the advantage of language modeling over intuition is most prevalent in the lower - frequency settings , adding the lexical functions of " cyc " and " para " improves the results for all models except for those using " shen - 1 " . adding " 2d " improves performance for all but m6 ,
table 3 presents the transfer quality and semantic preservation results . our model outperforms all the other models with a large margin . the results are statistically significant with respect to transfer quality , semantic preservation and fluency . our model obtains the best results on all metrics with a gap of 2 . 5 points from the previous state of the art on average .
table 5 presents the results of human sentence - level validation for each metric . our approach verifies the accuracy of our summaries with a minimum of 94 % on both metrics . the results are statistically significant ( p < 0 . 01 ) with respect to both sim and pp metrics , and to human ratings of semantic preservation .
the results are presented in table 6 . we observe that the models performing best with the " para - para " model are those with the best generalization ability . adding the lexical features of " cyc " and " lang " improves performance for all models except for those using " shen - 1 " .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc with the exception of the " delete / retrieve " row , where our model achieves 31 . 4 % higher acc compared to the best previous work . however , the difference between transfer and untransferred sentences is less pronounced , indicating that the quality of our model is less sensitive to human references . we notice that the classifiers in use achieve higher acc scores when training with only 1000 tokens instead of the 1000 required for a simple transfer .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent as well as the overall number of disfluencies for each type as a percentage of the overall prediction score . reparandum length is the average of the number of tokens in a sentence , followed by number of repetition tokens and overall length . as a sanity check , we also include the average number of repetention tokens to ensure that our model does not rely on repetition tokens . the results are presented in table 2 .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , or in neither . table 3 shows the relative frequency with which tokens belong to each category and the length of tokens in which they are predicted to belong . as expected , the content - content tokens are much more common in the repair disfluency , indicating that the content word is important in generating the correct output . however , the function - function tokens are less frequently predicted to be disfluent , indicating the extent to which the lexical features captured by the tokens are specialized for the task at hand .
the results are presented in table 1 . we observe that the text model performs best when trained and tested on a single dataset , with the exception of the case of " text + raw " . moreover , the performance gap between " text " and " raw " models is narrower when training on both datasets , indicating that text alone alone does not improve predictive performance . further , the model performs better when trained on a larger dataset , indicating the advantage of incorporating innovations into the model design . finally , we observe that training on more than one dataset at once results in overfitting , leading to an overfitting scenario that is less appealing to some users .
performance comparison of our model with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the state of the art . the difference in accuracy between the original embeddings and our model is most prevalent in sentence prediction accuracy , with rnn - based models performing slightly worse than self - attention . however , our model exhibits a slight improvement in general accuracy over other approaches , confirming the value of domain adaptation .
table 2 compares the performance of our model with the best previous approaches on the apw and nyt datasets . our unified model significantly outperforms all previous models , confirming the value of attention .
table 3 compares the performance of our neuraldater model with and without word attention . our approach shows that both word and graph attention are comparable in terms of accuracy , confirming the effectiveness of our approach . additionally , the accuracy decreases as a result of replacing word attention with graph attention .
table 1 presents the performance of different approaches for event prediction on the training data . our model outperforms all the alternatives with a gap of 10 . 5 points over the best previous state of the art . embedding + t improves performance on both datasets with 1 / 1 and 1 / n tests , but does not improve on the 3 / n test set . jrnn and jmee both outperform dmcnn and cnn in both event prediction and argument prediction .
table 1 presents the results on event identification , classification and event recall . our proposed method significantly improves upon the state of the art on both event and argument identification . the results are statistically significant with bootstrapping using f1 scores indicating that our proposed method is more useful for production use . specifically , we see that the proposed method identification and classification methods benefit from a significant drop in error from using pre - training data .
the results are shown in table 1 . as a baseline , we also consider the results of english - only , spanish - only and french - only models , as well as the slightly better results of " shuffled " and " fine - tuned " models . all models perform slightly better than " cs - only " when trained and tested on the same dataset , indicating that the training data structure is more useful for brevity . however , fine - tuning the model results in a significant drop in performance compared to the original model . when training on a single dataset , the results are less clear , but still indicate a performance drop of less than 0 . 5 % . the best results are obtained by using the " shuffled - lm " model , which shows the advantage of redundancy removal . further , the " fine - tuned " model further boosts performance by more than 5 % . as shown in fig . 2 , the performance gap between " original " ( cs - last - lm ) and " original + vocab " model is less pronounced when training on both english and spanish - language datasets .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned model outperforms the naive approach by a noticeable margin . the results are statistically significant even when training with only 50 % of the dev data as the training data , which shows the advantage of finetuning the model to improve the generalization ability .
as shown in table 5 , fine - tuning improves the performance on both the dev and test sets , and on the test set as well . the difference in accuracy between the two sets is less pronounced for the former , but the difference is still significant . fine - tuned cues result in significantly better precision on the dev set compared to monolingual cues .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . our approach shows a significant improvement in precision and recall over the baseline model , confirming the effectiveness of our approach . additionally , the f1 score improves from a low baseline to high f1 scores indicating that our approach can further improve interpretability .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features on the conll - 2003 dataset as compared to using the baseline model . our approach shows a statistically significant improvement in precision and recall compared to baseline on both datasets . additionally , the f1 score improves from 94 . 03 to 94 . 35 , which shows a significant drop in performance .
the results on the test set of belinkov2014exploring ’ s ppa are shown in table 1 . we use the hpcd and glove - extended embeddings described in faruqui et al . ( 2015 ) as our source . the results are slightly better than the results of using syntactic - sg , indicating the advantage of hand - crafted tokens . however , the difference between the two approaches is less pronounced when using the lstm - pp type - based approach . overall , both approaches outperform the ontolstm approach , showing that handcrafted tokens are a better complement to syntactic tokens .
table 2 shows the performance of our system with various pp attachment predictors and oracle attachments . our hpcd - based system outperforms the best ontolstm - pp model with a large margin .
table 3 shows the effect of removing the sense priors and context sensitivity ( attention ) from the model . the results are statistically significant ( ppa acc . = 0 . 05 ) compared to full , indicating that the importance of context sensitivity is lessened .
table 2 shows the bleu % scores of the subtitle data and domain tuning for image caption translation using the three models described in table 2 . the results are slightly better than the results with marian amun et al . ( 2017 ) using the ensemble - of - 3 model , indicating that domain tuning improves the interpretability of subtitle data . however , the results are still slightly worse than those with sub - subsfull subtitle data , showing the advantage of domain tuning .
we observe that domain - tuned h + ms - coco outperforms the plain h + hoco model on three out of the four domains ( table 3 ) when labels are added . however , the results are less pronounced for en - de and flickr17 compared to mscoco17 . adding domain - specific labels improves the results for both datasets , the results are particularly striking for flickr17 , as the results show , when the domain - aware h + domain - tuning model is added to the subs1m dataset , the model performs better than both monolingual and asymmetric models .
table 4 shows the bleu scores of the models using automatic captions and marian amun . adding automatic image captions improves the general performance for all models except for those using multi30k dataset . amun model outperforms both en - de and mscoco models , indicating the advantage of finetuning word embeddings during training .
table 5 compares the performance of our encoder and decoder with the best performing approaches using transformer , multi30k + ms - coco + subs3mlm and detectron mask surface , the results are presented in table 5 . our encoder achieves the best performance with a bleu % score of 68 . 38 % , slightly higher than the previous best performance of 62 . 86 % by using en - de and mscoco17 embeddings . the decoder performs similarly to encoder , however our model obtains the better generalization performance .
we observe that the ensemble - of - 3 approach outperforms subs3m and subs6m when we switch from text - only to multi - lingual features . moreover , the results are slightly better than the results of " text - only " and " visual features " . we notice that the performance gain comes from a larger ensemble size , indicating that the training data size that is important for multilingual models may vary depending on the underlying data distribution . adding ms - coco features improves results for both datasets , however , the biggest performance gain is seen for the larger ensemble , sub - 3m achieves the best results with 62 . 45 % on average compared to the 54 . 40 % average of " g2048 " .
table 3 presents the results for english on the hidden test set of hotpotqa . the results are presented in tables 3 and 4 . as the table indicates , the performance gap between en - fr - ht and en - es - ht is narrower than that between the two baseline models , indicating that the performance gain comes from a better model design . moreover , the average number of frames per second for each translation is also slightly higher than the average of both baseline models .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model splits 1 , 467 , 489 sentences , which means that there are 5 , 734 instances per sentence that can be considered as a train / test and development split .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to both src and trg scores , confirming the effectiveness of our model .
table 5 shows the bleu and ter evaluation scores for the rev systems . our system outperforms both the best previous approaches with an absolute improvement of 2 . 8 points over the baselines .
table 2 shows the test set on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised by ng et al . ( 2017 ) . the results are presented in table 2 . as expected , the vgs model significantly outperforms the rsaimage model in recall @ 10 and average mfcc score .
results on synthetically spoken coco dataset are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the model supervised using rsaimage data . the results are statistically significant ( t - test , p < 0 . 01 ) with a mean mfcc score of 1 . 0 and a median rank of 0 . 9 , which indicates that the model performs well in terms of recall and chance . audio2vec - u shows lower recall , but higher chance to predict answers , confirming the importance of word embeddings adaptation .
table 1 compares the effectiveness of the different classifiers with the original on sst - 2 . the dan model outperforms cnn in terms of sentence selection . it turns in a screenplay that is slightly better than the original , but still requires a lot of data to turn in . as a result , we report further examples in the appendix . rnn also outperforms dan in sentence selection ,
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same since fine - tuning the original sentence in sst - 2 . as the table indicates , the number of occurrences has increased as a result of the improvements in the part - of - speech metric . however , the overall effect is less pronounced for nouns as compared to verbs . rnp outperforms cnn and dan in terms of overall performance .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels result in a significant drop in sentiment score compared to flipping the negative labels to positive .
table 1 presents the results of experiments on the pubmed and sst - 2 datasets . results are presented in table 1 . the results are statistically significant ( p < 0 . 001 ) with an absolute improvement of 98 % on average compared to pubmed . overall , the results show that pubmed outperforms sift significantly .
