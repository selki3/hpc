table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as shown in the table , the size of the training instances and the number of instances per iteration are the most important factors in the success of our approach , as it reduces the training time and increases the inference time . with a training size of 10 instances , the recursive approach is more efficient than the iterative one . the folding technique is more appealing for production use , since it allows faster iteration of the tree nodes .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . table 1 shows that the balanced dataset exhibits the greatest performance improvement over the linear one when the size of the batch is increased to 25 .
the max pooling strategy consistently performs better in all model variations . hyper parameter optimization results are shown in table 2 . the best results are obtained with conll08 as the representation and ud v1 . 3 as the filter size . with co - nll8 as the filtering size , sb achieves the best performance with a f1 score of 1 . 83 / 2 . 57 and a dropout probability of 0 . 63 / 0 . 87 , respectively , compared to the sigmoid and softplus representation ( 1 . 79 / 1 . 86 and 1 . 87 / 1 ) . with ud as a filter size , the dropout rate is less than sb , but is higher than softplus , 2 . 03 / 1 and 3 . 01 / 1 , indicating that the selection of the best filtering size is more important than selecting the size of the feature maps .
table 1 shows the results of using the shortest dependency path on each relation type . our macro - averaged model achieves the best f1 score in 5 - fold test set , outperforming all the baselines without using sdp .
the results are shown in table 3 . the results show that the best performing model is the y - 3 model , which achieves a c - f1 / f1 score of 100 % and 50 % on average . the best f1 score is achieved by the model with a f1 / r score of 53 . 57 % , which shows that the model is well - adapted to the task at hand . on the other hand , the lower f1 scores show that it is harder for the model to learn new tasks .
table 3 shows that mst - parser outperforms all the other approaches on average in terms of paragraph accuracy . the results are shown in bold . as shown in the table , the average accuracy of paragraph prediction is significantly higher than that of any other approach . in general terms , all the models that perform well on paragraph prediction outperform the others on average .
as shown in table 4 , the average c - f1 score for the two systems is 60 . 40 ± 13 . 57 % lower than the majority performances over the runs given in table 2 . the difference is most pronounced at the paragraph level , where the lstm - parser performs significantly worse .
the results are shown in table 3 . the results show that when the training data is cleaned , the original tgen model performs better than tgen − and tgen + when trained with sc - lstm . however , when training with the cleaned data , the results are slightly worse than when the original is trained with the clean data .
table 1 compares the original e2e dataset with the cleaned version . the results are presented in table 1 . the difference in the number of distinct mrs between the original and the cleaned versions is statistically significant ( p < 0 . 01 ) with respect to training and test sets . table 1 also shows that the difference in ser is less pronounced for the original dataset than for the cleaned one , indicating that the training set is more stable .
table 3 presents the results on the training and test set . the results are presented in bold . original and tgen + models outperform sc - lstm on all metrics except meteor and rouge - l . in general terms , the results are shown in table 3 . when trained with the original tgen model , the system performs better than tgen − and tgn + with a bleu score of 2 . 83 / 0 . 83 and 2 . 53 / 1 . 83 on average compared to the original .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances with missing values ( including those from the training set ) are shown in table 4 . as shown in the table , the majority of errors ( 71 % ) are caused by errors caused by missing training data ( 42 % ) or incorrect values ( 28 % ) ( 28 % ) .
table 3 presents the results on the external and internal test sets . graphlstm outperforms all the other approaches on both sets . the results are shown in bold . table 3 shows that the graph lstm model significantly outperforms the other methods on both the external and internal test sets , with the exception of seq2seqk ( konstas et al . , 2017 ) .
table 2 shows the performance of our model compared to previous work on amr17 . our dcgcn achieves 24 . 5 bleu points per iteration compared to seq2seqb ' s 21 . 6 and ggnn2seq ' s 27 . 1 . similarly , our ensemble model achieves 27 . 5 and 27 . 3 bleu points compared to the previous state - of - the - art . gcnseq ( damonte and cohen , 2019 ) achieves an overall score of 57 . 4 and 57 . 6 points per implementation , respectively .
table 3 presents the results for english , german , dutch , czech and slovak . the results are presented in table 3 . table 3 shows that the multi - model approach outperforms all the baselines except birnn and bow + gcn . multi - model approaches tend to have lower performance on english - german and czech - language datasets than on the other two languages . for example , seq2seqb ( beck et al . , 2018 ) achieves a single - score of 43 . 8 / 71 . 6 on average compared to bi - rnn ' s 36 . 6 / 59 . 8 and cnn ' s 33 . 9 / 43 . 4 on the english - czech dataset .
table 5 shows that the number of layers inside dc has a significant effect on the performance of the model ( i . e . , n = 2 , n + m ) with a bias score of 2 . 0 b = 0 . 5 , m = 3 . 0 and c = 2 . 1 b = 2 indicating that more layers are important for the model to perform well .
table 6 shows that the gcn with residual connections outperforms the baselines on average . rc denotes residual connections , and rc + la denotes connections with multiple connections . the results are shown in table 6 .
the results are shown in table 4 . the most striking thing about our model is that it achieves state - of - the - art results on par with the best previous work ( hochreiter et al . , 2016 ) . we observe that our dcgcn model outperforms all the other models on every metric by a significant margin .
table 8 shows the results of an ablation study on the dev set of amr15 . the results show that removing the dense connections in the i - th and j - th blocks significantly reduces the overall density of connections . the results are shown in bold .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results are summarized in table 9 . as can be seen , all the features discussed in the table are beneficial for the model , however , for the decoder , the results are less encouraging . we found that the coverage mechanism used in our decoder is beneficial , it improves the decoding performance by 4 . 5 % over the best previous work .
table 7 shows the performance of our initialization strategies on probing tasks . the glorot initialization strategy outperforms all the baselines except subjnum and topconst by a large margin . it achieves the best performance with an absolute improvement of 3 . 8 points over the previous state - of - the - art on all three metrics . as the table shows , the gloot initialization strategies achieve the highest performance with a relative margin .
table 3 shows that the h - cmow embedding model outperforms h - cbow in terms of generalization across all metrics . it achieves state - of - the - art results on all metrics with a gap of 3 . 6 points from the previous state - ofthe - art model . in particular , it achieves the best generalization on subjnum and tense metrics , and achieves the highest score on topconst metric .
table 3 shows the results of our method compared to cmow and cmp . the results are shown in bold . hybrid outperforms cmow on all metrics except for sst2 and sst5 , where it achieves a marginal improvement of 0 . 2 % over cmow .
table 3 shows the performance of our models on unsupervised downstream tasks . hybrid outperforms cbow and cmow in all but one of the three cases ( sts13 , sts14 , and sts16 ) . as shown in the table , when cbow is used in combination with cmow , the result is that the cbow model achieves higher scores on all three of the downstream tasks than hybrid .
table 8 shows that the glorot initialization strategy outperforms all the other initialization strategies on supervised downstream tasks . it achieves the best performance with an overall score of 87 . 6 % on mpqa , 86 . 4 % on sst2 and 86 . 7 % on sts - b .
table 6 shows the performance of our method compared to the state - of - the - art cbow - r method on the unsupervised downstream tasks . our method significantly outperforms the best previous approaches on all three of the four tasks , showing that our approach is more suitable for production use .
table 3 shows the performance of our method compared to the state - of - the - art cmow - c on the hidden test set of somo . the results are shown in bold . our method outperforms all the other methods on every metric by a significant margin . it achieves the best results on three of the four metrics .
table 3 shows the results of our method compared to the state - of - the - art approaches . our cmow - c model outperforms all the baselines except sick - e and trec by a large margin . it achieves a mean improvement of 3 . 6 points over the best previous methods on average .
the results are shown in table 3 . in general terms , our system outperforms all the state - of - the - art supervised and unsupervised systems on all metrics . name matching is the most difficult task for our system to solve , and it achieves the best results with an absolute improvement of 2 . 38 points over the best previous work on this metric . supervised learning achieves a marginal improvement of 0 . 03 points over previous work , but is still better than all the baselines except for loc and per . the best performance is achieved by τmil - nd , which obtains a marginal gain of 1 . 57 points over supervised learning . finally , the best performance achieved by mil - nd is achieved with a marginal increase of 3 . 36 points over our system .
table 2 shows the results on the test set under two settings . name matching and supervised learning achieve the best f1 scores , with τmil - nd achieving the highest f1 score . supervised learning achieves the best overall f1 and roc scores as well . the results are shown in table 2 . in both settings , the training set size and the number of iterations are the most important factors in the success of the model , as these are the only ones that are considered in the final evaluation .
table 6 shows that g2s - gat outperforms all the other models on average . the results are shown in table 6 . as shown in the results table , the gat model significantly outperforms the other two baselines in terms of average ref and average gen with a gap of 2 . 86 points from the last published results .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s model outperforms all the previous models on both datasets with a gap of 2 . 42 ± 0 . 03 points from the previous state - of - the - art . the results are shown in bold .
results in table 3 show that g2s - ggnn models are comparable to state - of - the - art models trained with additional gigaword data . the performance gap between bleu and external models is less pronounced when the gigaword data is used as the training data , however it is still significant .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are summarized in table 4 . our model significantly outperforms the previous state - of - the - art models in terms of bleu and meteor .
the results are shown in table 3 . g2s - gin outperforms all the other models on all metrics except sentence length with a margin of 2 . 51 % and 0 . 43 % improvement over the baseline s2s model , respectively .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence as well as in the missing sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - gat outperforms s2s in terms of the fraction of elements missing from the output ( added ) and miss ( fraction of miss ) as shown in table 8 . as shown in the table , the model with the best performance is gold , which obtains a f1 score of 50 . 35 % higher than the other models .
table 4 shows that pos tagging accuracy is comparable to that of sem , with the difference being that pos features are more useful for target languages , and sem features have higher correlation with target languages . pos also shows lower correlation with human judgement .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms unsupemb in both tasks , showing that the encoder encoder is better than the unsupervised embeddings at selecting the most frequent tags and the highest baselines .
table 4 shows the performance of our system compared to state - of - the - art systems on all metrics . our model outperforms all the baselines except pos with a large margin . it achieves the highest accuracy on all four metrics .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the residual encoder has the best performance , with a pos accuracy of 87 . 9 % and a sem accuracy of 81 . 9 % . the uni encoder shows lower precision , but higher precision overall .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . as shown in the table , the attacker significantly outperforms the adversary on all three datasets . also , the difference between the attacker score and the corresponding adversary ' s accuracy is small , indicating that the attacker is more likely to learn from the training data .
table 1 shows the performance of our model with respect to training on a single task . the results are shown in bold . our model significantly outperforms all the baselines on all metrics except gender and age .
table 2 shows the results of unbalanced and balanced data splits . the results are shown in table 2 . as expected , the unbalanced data splits cause more data to leak than the balanced ones , indicating that there is a need to design more complicated tasks to reduce the impact of data leakage .
table 3 shows the performance of our approach on different datasets with an adversarial training set . our approach outperforms all the baselines except pan16 by a large margin . the difference between the attacker score and the corresponding adversary ’ s accuracy is small , but significant .
the results are shown in table 6 . the rnn encoder significantly outperforms the guarded encoder in terms of accuracy . with different encoders , the rnn embedding leaky information is less accurate than the guarded one . guarded information is more accurate than leaky information .
table 3 presents the results of our model on the training data . the results are summarized in table 3 . our model outperforms all the state - of - the - art models on all metrics except finetune by a significant margin . it achieves the best results with a f1 score of 4 . 36 / 10 . 97 and a f2 score of 6 . 86 / 9 . 59 on the finetuned and dynamic benchmarks , respectively . as shown in the table , the size of the training set and the number of parameters are the most important factors in our model ' s success . table 3 shows that our model performs better than all the other models that rely on finetune alone .
table 3 presents the results of baselines trained on the lstm dataset from rocktäschel et al . ( 2016 ) . the results are summarized in table 3 . as shown in the table , the baselines used in this work are slightly different than those used in the previous work . table 3 shows that the ln - based baselines outperform all baselines except atr , gru and sru in terms of both time and accuracy . the difference is most pronounced for gru , which achieves an absolute improvement of 3 . 5 % over the previous state - of - the - art model with a bert time improvement of 2 . 7 % over atr . in addition , the difference is less pronounced for sru , with an absolute gain of 0 . 3 % over other baselines .
table 3 presents the results of baselines trained on the data from amapolar err and yahoo time . the results are presented in bold . as shown in the table , the hierarchical clustering approach by zhang et al . ( 2015 ) achieves the best results with an err of 4 . 836 / 4 . 861 and a time - error rate of 1 . 012 / 1 . 012 , respectively , compared to the previous state - of - the - art models .
table 3 shows the bleu score of our model on the wmt14 english - german translation task on tesla p100 . our gnmt model significantly outperforms all the baselines except olrn and sru . lrn achieves the best performance with an average case - insensitive tn score of 26 . 67 / 71 . 86 . as shown in the table , gnmt has the worst performance on the translation task . sru , on the other hand , obtains the highest case - inflation score . atr and atr have the best decoder performance with a combined score of 27 . 86 / 72 . 86 and 27 . 67 / 71 . 86 , respectively . finally , lrn has the highest decoder score with a weighted average score of 25 . 15 / 59 . 43 and 25 . 34 / 62 . 40 .
table 4 shows the f1 score of our model on the squad dataset . it can be seen that our approach outperforms all the state - of - the - art models on average . specifically , our lstm model obtains a higher match / f1 score than all the other models except lrn , atr , and gru . also , our approach achieves higher f1 scores than all other methods except atr and sru , indicating the scalability of our approach . table 4 also shows that our model is comparable to previous work ( wang et al . , 2017 ) in terms of the number of parameter number of base .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . the lstm model achieves the best performance . it achieves 90 . 56 f1 ( out of a possible 90 . 61 ) on the english evaluation set . as shown in table 6 , all the parameter numbers used in our model have a significant effect on the performance .
table 7 shows the test results on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn outperforms glrn and elrn on both tasks . with the base setting , lrn achieves higher accuracy than elrnn on both snli and ptb .
table 3 shows the performance of our system with respect to word embeddings . our system outperforms all the other systems on every metric by a significant margin . for example , when trained with oracle retrieval , our system achieves the best performance with a r2 score of 7 . 38 / 10 . 38 and a mtr score of 3 . 55 / 8 . 55 on average .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the human evaluation performed by our system is of high quality . table 4 shows that our system outperforms all the other systems that have been evaluated to date on this task .
table 3 shows the results for english , spanish , dutch , dutch and russian on the training data . the results are shown in bold . we observe that the performance gap between english and spanish is narrower than expected , but still significant . europarl outperforms both ted talks and dutch on all metrics . in particular , the difference between en and r = 0 . 5656 and 0 . 5984 points is statistically significant ( p < 0 . 01 ) with respect to english , while for dutch and german , it is less significant .
table 3 shows the results for english , spanish , dutch , dutch and russian on the training data . the results are shown in bold . we observe that the performance gap between english and german on the test data is relatively small , with p < 0 . 01 indicating that the training set outperforms the other two baselines on average .
table 3 shows the results for english , spanish , dutch , dutch and russian on the training data . the results are shown in bold . we observe that the performance gap between english and german on the test data is relatively small , with p < 0 . 01 indicating that the training set outperforms the other two baselines on average . europarl outperforms all the other baselines except tf and docsub with a gap of 2 . 5 points .
the results are shown in table 3 . europarl outperforms all the other systems on all metrics except for docsub . the average depth achieved by all the metrics is 11 . 05 / 11 . 46 , which indicates that the depth cohesion achieved by the dsim model is high . as shown in the table , the maxdepth achieved by dsim and slqs is higher than that by a large margin .
the results are shown in table 3 . europarl outperforms all the other systems on all metrics except for docsub . the average depth achieved by all metrics is 9 . 43 / 10 . 29 , which indicates that the depth cohesion achieved by the system is high . as shown in the table , the maxdepth and averagedepth metrics are the most important metrics for the success of our dsim model .
table 1 shows that the enhanced version of our system outperforms the baseline model in terms of ncdcg % on the validation set of visdial v1 . 0 . 7 . the enhanced model achieves a significant improvement over the baseline on all metrics ( r0 , r1 , r2 , r3 ) by a margin of 3 . 42 % on average compared to p1 . 7 % in the original visdial .
in table 2 , we show the results of ablative studies on different models on the visdial v1 . 0 validation set . adding p2 indicates that the hidden dictionary learning approach is more effective than applying p1 and p2 alone . the results are shown in table 2 .
table 5 shows the results for hard and soft alignments . the hmd - f1 model outperforms all the baselines except ruse and wmd - bigram on all metrics . it achieves the best results on hard alignments with a precision of 0 . 823 / 0 . 866 and a f1 score of 1 . 012 / 0 , respectively .
the results are shown in table 3 . the baselines used in this study are presented in bold . meteor + + and ruse ( * ) have the highest correlation with average scores , while bertscore - f1 has the lowest correlation . sent - mover achieves the best average score with a f1 score of 0 . 716 .
the results are shown in table 3 . sent - mover outperforms meteor and bertscore by a noticeable margin . the baselines used to train the model outperform the baselines in terms of f1 score , indicating that the selection of the best baselines is based on the information available from the training data , not on the quality of the model . in addition , the bleu scores are significantly higher than those of meteor , indicating the importance of selecting the best baseline for the task at hand .
the results are shown in table 3 . word - mover achieves the best results with a precision of 0 . 939 / 0 . 949 on m1 and m2 . the accuracy is high when using the baselines set by meteor , bertscore - recall , spice and leic . as shown in the table , word - mover has the highest precision with a m1 / m2 score .
the results are shown in table 7 . the results show that the model with the best generalization performance is the one with the highest accuracy . it can be seen that when the model is trained with the lexical features of shen - 1 , the model performs better than the model trained with lexical feature - values alone .
table 3 presents the results of our model on the training data . the results are shown in bold . table 3 shows that yelp significantly outperforms other models in terms of transfer quality , transfer quality tie and semantic preservation . it is clear from table 3 that yelp has significantly better transfer quality than other models . in particular , it achieves a transfer quality improvement of 2 . 7 points over google translate with a f1 score of 0 . 01 compared to the previous best model .
table 5 shows the results of human validation on yelp . the results are summarized in table 5 . it can be seen that both the human evaluation and the machine evaluation yield similar results . acc is the percentage of machine and human judgments that match a given metric that match the evaluation criterion , and positively matches the human ratings of semantic preservation and fluency .
the results are shown in table 6 . the results of the best performing model are presented in bold . as shown in the table , our model significantly outperforms all the baselines that do not use para - para feature - values . para - based models significantly outperform those that do .
table 6 shows the results for yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform all the previous work on this dataset by a large margin . acc ∗ : the definition of acc varies by row because of different classifiers in use . for example , multi - decoder achieves the highest acc with 22 . 3 % higher than the best unsupervised model ( fu - 1 ) and 22 . 4 % higher acc with the best trained model ( yang2018 , yang2018unsupervised ) .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent in the nested disfluency experiments as well as the overall number of disfluencies for each repetition token . reparandum length is the average length of the tokens that are considered to be disfluential , with rephrase tokens having the highest significance . regularization tokens have the smallest effect on prediction performance , as their average length is shorter than the length of repetition tokens .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , or in neither . as shown in the table , the length of tokens in a disfluency sentence is the most important factor in predicting whether a sentence is disfluential or not . reparandum tokens are shorter than repair tokens , but longer than function - function tokens , indicating that the content word is more important than the function word .
the results are shown in table 3 . we observe that the text - based approach outperforms the single - input approach in terms of both test set dev and average number of iterations . as expected , when the model is trained on raw data , it achieves the best results . however , the results are slightly less striking when trained on data with innovations . it can be observed that the model trained on the raw data is more likely to converge to the best performance when the innovations are combined with the text . this suggests that incorporating innovations into the model may improve the performance for the model , but it does not improve the overall performance . finally , we see that the combination of innovations and text improves the model ' s performance for both test sets . in particular , it improves the performance on the test set with the largest difference between average and maximum iterations .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embedding achieves the best performance with an accuracy of 83 . 43 % on the micro f1 test set ( table 2 ) . rnn - based embeddings achieve the highest accuracy with a f1 score of 82 . 43 % . self - attention sentences generate sentences with a higher accuracy than cnn - based sentences , but are less accurate than rnn . as shown in table 2 , the accuracy of rnn is higher than that of cnn , indicating that rnn relies less on superficial cues and more on the semantic information encoded in the sentence .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . attentive neuraldater improves upon the performance of ac - gcn and oegcn by 4 . 5 % and 6 . 2 % on average over the previous state - of - the - art methods on both datasets . also , neuraldater outperforms maxent - joint by 3 . 6 % on the nyt dataset , and by 4 % .
table 3 shows the performance of our neuraldater model with and without word attention . it can be seen that both word attention and graph attention improve the performance for both the neuraldater and the oe - gcn model .
the results are shown in table 3 . embedding + t improves upon the performance of dmcnn and cnn with a noticeable margin . however , it does not improve upon the jrnn model , which is still comparable with the best state - of - the - art . as shown in fig . 3 , the advantage of leveraging t - values comes from a lower error rate on the 1 / 1 and 1 / n test sets , which shows that leveraging t gives a significant performance boost .
table 3 presents the results of cross - event event detection on the training data . the results are presented in table 3 . as can be seen , our method significantly outperforms the state - of - the - art in terms of both event identification and event classification . specifically , our cross - event method significantly improves upon the previous state - ofthe - art on all three metrics .
results are shown in table 3 . the results are summarized in bold . as shown in the table , fine - tuned and shuffled - lm outperform all other approaches that use only one type of language learning algorithm . regularization has a generally positive effect on performance ( i . e . , it improves the average dev perp and average test wer ) , but it does not improve the average score for test perp . all models use the same vocabulary learning algorithm , but fine - tune it to improve the performance for both languages . in english - only and spanish - only languages , the best results are obtained by shuffling the word embeddings and applying the concatenation of the lexical features of both languages into a single sentence . fine - tuning also improves performance , but only marginally .
results are shown in table 4 . fine - tuned models outperform cs - only models on both the dev set and the test set , showing that fine - tuning improves the generalization ability of the model with a noticeable drop in performance when training with only subsets of the source and target data .
as shown in table 5 , fine - tuned - disc improves the performance on the dev set and on the test set , and upsampling has a generally positive effect ( p < 0 . 01 ) . fine - tuned - disc reduces the error of the gold sentences by 2 . 5 % on average compared to cs - only - disc .
table 7 shows the precision ( p ) and recall ( r ) numbers for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) and f1 - score ( f1 ) is significantly higher than that on the baseline dataset ( p = 0 . 005 ) indicating that the combination of gaze features improves the recall and precision of the model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvement in precision is statistically significant ( p < 0 . 01 ) and r = 0 . 03 , with a f1 score of 1 . 35 ( t - test , p < 0 . 005 ) indicating significant improvement in the performance of our method .
results on belinkov2014exploring ’ s test set are shown in table 1 . hpcd ( full ) and glove - extended embeddings outperform lstm - pp on the test set . the results are statistically significant ( p < 0 . 01 ) with respect to both type and length of tokens , indicating that the extended embedding scheme can further improve the generalization ability of the system . further improving performance with the use of syntactic - sg and syntactic skipgram improves the overall performance . finally , the performance of ontolstm improves with the addition of the word - synset embedding feature . with these features , the system achieves a general improvement of 3 . 7 points over lstmp - pp .
results are shown in table 2 . the hpcd system outperforms the original ontolstm - pp model in terms of accuracy on the full uas test set . it achieves a ppa acc . score of 98 . 60 / 98 . 97 and 94 . 59 / 88 . 97 on the partial uas set , respectively .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . as can be seen , the ppa acc . score of full models decreases as a result of removing the context sensitivity .
as shown in table 2 , using subtitle data and domain tuning improves the bleu % scores for both en - de and multi30k embeddings , and upsampling improves the overall performance for both sets . subsfolded subtitle data also improves the results for en - fr and mscoco17 , however , it does not improve the overall results for marian amun et al . ( 2018 ) because the subtitled subtitle data are too small to be useful in multi - task training ( table 2 ) .
table 3 shows that domain - tuned h + ms - coco outperforms the plain hoco model with a margin of 3 . 5 bleu / s compared to the strong baselines ( hochreiter et al . , 2016 ) on both datasets .
table 4 shows the bleu scores of en - de , en - fr and en - scoco17 models compared to multi30k models using only the best 5 captions . adding automatic image captions improves the performance for all models , the results are shown in table 4 . as the results show , when using only one caption per image , the model with the best captions performs better than the model using all 5 . when using multi - tasked captions , the improvement is less pronounced , but still significant .
the results in table 5 show that enc - gate and dec - gate strategies are comparable in terms of bleu % scores , with the former achieving higher scores than enc - de and mscoco17 . encoding information via dec - gates improves the generalization ability of the model , and the improvement over en - de is statistically significant ( p < 0 . 01 ) with respect to both img w and w - score . using multi30k + ms - coco + subs3mlm and detectron mask surface , the encoder and decoder perform similarly to en - fr . however , the improvement is less pronounced than en - deg because encoding information takes longer to process than decoding information .
table 3 shows the results for en - de , en - fr and mscoco17 compared to the baseline models . the results are summarized in table 3 . sub - 3m achieves the best results with a f1 score of 69 . 86 out of 100 . 00 . as shown in the table , the multi - lingual approach outperforms the plain text - only approach by a large margin . in particular , the ensemble - of - 3 approach is more effective than the monolingual approach , achieving f1 scores of 68 . 71 and 69 . 71 out of 99 . 86 on average , respectively .
table 3 shows that en - fr - ht and en - es - ht achieve similar results on the yule ’ s i and ii test sets as those on the mtld test set . the results are summarized in table 3 . in general terms , the results are as follows : en - frht achieves the best results with an absolute improvement of 2 . 27 % on the ttr and mtld scores over en - regexp , and a marginal gain of 0 . 01 % on mtld . as shown in fig . 3 , the best performance is obtained by en - e - ht , which achieves a marginal improvement of 1 . 57 % on both metrics .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model splits sentences into two parallel sentences for each training and a development split .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . our model outperforms all the other models with a large margin .
table 5 shows the evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems outperform their counterparts in terms of bleu and ter scores . as the results show , the smaller size of the rnn - rev baseline leads to a lower performance on rev tasks .
table 2 shows the results for rsaimage on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled segmatch is the one supervised using the rsaimage embeddings . the results are shown in table 2 . rsaimage has the highest recall @ 10 and average mfcc of 0 . 2 , indicating that the model is performing well .
results are shown in table 1 . the acoustic embeddings outperform all the alternatives except audio2vec - u in terms of recall . rsaimage achieves the highest recall @ 10 . 0 with a mean mfcc score of 1 . 4 / 2 . 0 and a median rank of 0 . 5 / 0 . 5 , indicating that rsaimage is more appealing to the task - solicitors . as shown in fig . 3 , the average recall number of frames per second is 1 . 0 / 1 . 5 while the average chance is 3 . 9 / 3 . 0 . in general terms , rsaimage performs better than all the other approaches .
we report further examples in the appendix . as shown in table 1 , the rnn turns in a screenplay that is slightly different from the original on sst - 2 . it turns out that the difference between the original and the dan is due to the size of the edges of the screenplay , the difference between cnn and rnn is less pronounced for dan , however , for rnn , it is more pronounced for cnn . table 1 shows that dan significantly outperforms rnn in terms of sentence selection . finally , the difference is less striking for cnn ,
table 2 shows that rnn fine - tuning has increased the number of occurrences in sst - 2 by 3 . 5 % over the original sentence . rnn has also increased the percentage of occurrences for nouns and verbs , and has decreased the percentage for adjectives by 2 . 5 % .
table 3 shows that the sentiment score increases when negative labels are flipped from positive to negative labels . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . also , the dan score increases as a result of the flipped labels .
table 1 presents the results of our experiments on the positive and negative evaluations . the results are presented in table 1 . our approach outperforms all the baselines except for corr et al . ( 2018 ) by a large margin . overall , our approach achieves an accuracy rate of 98 . 5 % on the negative evaluation .
