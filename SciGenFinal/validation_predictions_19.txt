table 2 compares the performance of our approach to tensorflow ’ s iterative approach and our recursive framework , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the summaries generated by our treelstm model are slightly larger than the size of the recur and fold datasets , but are comparable in terms of training and inference performance .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the fact that the balanced dataset has 25 instances , this small difference in performance does not represent a significant big performance drop . the difference between the balanced and linear datasets is less pronounced under the moderate distribution of the training set , but still indicates that there is a significant performance gap to be found with greater parallelization . we conjecture that this gap is caused by the high cost of scalability in the low - resource settings .
the max pooling strategy consistently performs better in all model variations . for example , ud v1 . 3 achieves the best performance with a f1 score of 75 . 57 out of a possible 75 . 05 , which shows that the selection of the best feature maps for each model is a critical factor in model performance .
table 1 : effect of using the shortest dependency path on each relation type . we show that macro - averaged models perform better than the best ones without sdp . additionally , our approach gives a significant performance boost in the best f1 results .
the results are shown in table 1 . results are broken down in terms of r - f1 and f1 scores . for brevity we only report results for those using pascal - voc table 1 shows that for all models , our three - step approach yields better results on average than the best previous state - of - the - art approach .
the results are shown in table 1 . for brevity we only report results for english , german , dutch , russian , turkish and turkish . the system performs well on both english and french , with an absolute improvement of 10 % on average compared to the previous state of the art .
table 4 shows the performance of our system compared to other widely used parsers for the essay and paragraph level . our lstm - parser outperforms all the other systems that do not use paragraph level parsers .
the results are shown in table 1 . from left to right each row displays the bleu , meteor , rouge - l , nist scores , cider scores and ser scores . the original and the cleaned tgen models perform slightly better than the original ones on average . the difference is most prevalent in the subtasks for add and miss , where the original tgen model performs slightly worse than the cleaned model .
table 1 shows the comparison of the original e2e data and our cleaned version . the difference in statistics between the two sets is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 .
table 1 shows the results for the four systems that we trained on the data augmentation task . the results are presented in bold . from left to right each row displays the bleu , meteor , rouge - l , nist scores , cider scores and ser scores . the summaries are broken down in terms of training , test set and error reduction . our system outperforms all the other systems that do not use tgen + or tgen ,
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) , as well as the number of instances that are actually missing from the test set . from table 4 , we can see that the majority of errors in our system ( 62 % ) are caused by missing data , however , there are a few cases where the missing data actually contains correct values .
the results are shown in table 1 . our joint model outperforms all stateof - the - art models on both the single - domain and ensemble test sets . the results show that our joint model achieves the best performance on both datasets with a gap of 10 . 6 % overall improvement over the previous state of the art .
table 2 shows the performance of our model on amr17 . our dcgcn achieves 24 . 5 bleu points per label improvement over the previous state - of - the - art models across all metrics . gcnseq ( damonte and cohen , 2019 ) achieves an overall improvement of 2 . 6 bleu points over previous work , which shows that our model can effectively complement the strong baselines of seq2seqb with a comparable number of parameters .
table 1 shows the results for english - german , czech and slovak , compared to english - czech . the results are broken down in terms of performance on single and multi - word embeddings . for english , we see that our proposed method outperforms the previous approaches on both datasets with two tasks . on the other hand , our seq2seqb model outperforms both the previous methods with a gap of 10 . 6 % on average .
table 5 shows that the number of layers inside our dc network is the most important factor in the success of our model . we find that , for example , having only one layer inside the network does not improve performance , and in fact hurts performance , leading to a drop of more than 10 % in performance overall .
table 6 compares the results of rc and rc + la with baselines for the gcns with residual connections . the results are presented in tables 6 . rc denotes residual connections and refers to gcns that are already in the decoder , while la refers to those that are still in the wrapper . gcn with rc ( 2 ) and la ( 6 ) generally perform better than those without .
the results are shown in table 4 . the first group shows that our dcgcn model outperforms all stateof - the - art models on average . specifically , it achieves 4 . 2 % higher performance on average compared to the previous state of the art model on all metrics with two tasks .
table 8 : ablation study for density of connections on the dev set of amr15 . the results of removing the dense connections in the i - th and ii - th blocks shows that our proposed dcgcn4 model exhibits a significant drop in performance compared to the previous state of the art model .
table 9 shows the results of an ablation study for modules used in the graph encoder and the lstm decoder . the results show that , under the " - graph attention " and " - direction aggregation " settings , our model obtains better results than the previous state of the art model .
table 7 shows the performance of our initialization strategies on a variety of probing tasks . our paper establishes a new state - of - the - art on all metrics , outperforming previous work on every metric by a significant margin . subjnum and topconst are particularly strong , achieving gains over glorot and zsgnet across all metrics . coordinv and tense are close second , but edge - checking gives a slight performance improvement . we conjecture that this is due to better data modeling ability .
we observe that the h - cmow approach is comparable to the best state - of - the - art cbow model on both datasets . the difference is most prevalent in the subtasks of bshift and coordinv , where the former performs better than the latter . subjnum and topconst are both strong baselines for concatenating multiple entities , but the difference between the two is less pronounced for bshift , which shows the advantage of selective attention .
the results are shown in table 1 . hybrid outperforms both monolingual and automatic methods . ame performs better than cmp . net , but does not exceed the best state - of - the - art results . cbow / 784 shows a significant performance gain over cmow / 783 ,
table 3 : scores on unsupervised downstream tasks attained by our models . the results show that our proposed method outperforms both the state - of - the - art cbow and cmow on average across all 13 sts datasets . however , the difference between the results on sts13 and sts16 is less pronounced for our model , which shows that our approach can rely less on superficial cues .
table 8 shows the performance of our system for initialization on selected downstream tasks . our system outperforms glorot and trec by a large margin . it achieves state - of - the - art results on five out of the eight datasets .
table 6 shows the performance of our method for different training objectives on the unsupervised downstream tasks . our cbow - r model outperforms the best state - of - the - art method on all the four datasets .
the results are shown in table 1 . the first group shows that our method outperforms the state - of - the - art methods on every metric by a significant margin . our method obtains the best results on three out of the four metrics . the difference is most prevalent in the subtasks of bshift , subjnum , tense and coordinv , where our cbow - r model significantly outperforms all the others .
the results are shown in table 1 . the first group shows that the cbow - r method outperforms all the other methods apart from the sub - category in terms of precision on sub - categories .
the results are shown in table 1 . supervised learning outperforms all stateof - the - art systems on both loc and misc datasets . specifically , it achieves a performance improvement over the previous state of the art model τmil - nd by 4 . 48 points in the eq . metric , which shows that supervised learning is more effective in the low - supervision settings . on the other hand , when trained and tested on the large - scale data set , our model mil - nd shows a performance gain of 2 . 95 points over the best state - of - art model , supervised learning , by a margin of 3 . 36 points .
results on the test set are shown in table 2 . our model outperforms both the best state - of - the - art methods in terms of f1 scores on both test sets . the difference between the performance of the best and worst models is most prevalent in the case of the former , where our model ( mil - 1 ) obtains the best f1 score . however , when we switch to the second set of training data , we get the best results . this indicates that our model can learn to reason over large distances in a relatively short period of time .
table 6 shows the results for entailment and ref on the test set of hotpotqa in the distractor and fullwiki setting . the results are summarized in table 6 . our model obtains the best results with respect to both ref and gen . the g2s - gat model significantly outperforms the other models with different feature sets in terms of both feature set . however , the difference between the two is less pronounced for gen , which shows the advantage of using pre - trained models .
table 1 summarizes our results on the hidden test set of ldc2015e86 and ldc2017t10 . the results are broken down in terms of performance on bleu , meteor and s2s . our model outperforms both published and unpublished work on every metric by a significant margin . for example , g2s - gat reaches a final score of 29 . 28 / 30 . 53 out of a possible 30 . 00 on the ldc 2015e86 baseline , while g - s2s achieves a final result of 25 . 42 / 25 . 59 . these results appear to indicate that the model developed by song et al . ( 2018 ) is well - equipped to perform on the smallscale ldc dataset . on the larger scale , the results are slightly less clear , but still show significant performance improvement .
table 3 shows the performance of our model on the ldc2015e86 test set when we add additional gigaword data . our g2s - ggnn model outperforms other models trained with similar gigaword data on the test set .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model outperforms the previous state - of - the - art models on every metric by a significant margin .
we observe that g2s - ggnn reaches a new state - of - the - art on sentence length improvement of 3 . 51 % over the previous state of the art model , and 2 . 66 % improvement over gat . from this data , we observe that , for brevity , we consider only the shortest paths between 0 - 7 δ and 10 - 240 δ , and consider all other paths as well .
table 8 shows the results for the test set of ldc2017t10 . the token lemmas are used in the comparison . our model outperforms all the other models that do not use g2s - gat in terms of the fraction of elements in the output that are missing in the input that are not present in the generated sentence ( added ) . the difference is less pronounced for our model , but it is still significant .
table 4 shows the pos and sem accuracy for different target languages trained on a smaller parallel corpus . pos significantly outperforms sem , showing that the semantic features extracted from the 4th nmt encoding layer are more useful for target languages . sem , on the other hand , is only comparable with pos in that it requires more training data .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . we use the best performing method , word2tag , which relies on unsupervised word embeddings and a baselines - based encoder - decoder . the results are statistically significant ( p < 0 . 01 ) with respect to most frequent tags and the upper bound of the pos / sem tagging accuracy , which shows that the encoder is more than 4 % better at predicting most tags .
table 4 presents the system ' s performance on the four types of tagging accuracy metrics . we observe that , for example , the system performs well on average when trained with only one error . on the other hand , when training with four errors , it achieves the best results .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer of our system exhibits the best performance . the second layer exhibits the worst performance . finally , the third layer shows the biggest performance drop .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary ' s accuracy is small , but significant enough to show that the proposed approach can significantly improve the prediction quality for gender - neutral tasks .
table 1 : accuracies when training directly towards a single task . our proposed approach outperforms all stateof - the - art methods across all metrics on both datasets . for example , the racial disparities in accuracy are less pronounced for gender and age compared to the previous state of the art approach . also , gender bias is less prevalent in our approach , but still leads to lower accuracy .
table 2 shows the results for balanced and unbalanced data splits . the results show that the gender - balanced approach outperforms the balanced approach , confirming the importance of gender balance in the task prediction phase . gender - balanced task prediction also outperforms unbalanced and balanced task prediction , indicating that gender balance is important in predicting task performance . the racial disparities are also evident in the results , as the average gender of the leaked data is slightly lower than that of the balanced ones .
the performance of our system on these datasets is shown in table 3 . our proposed approach outperforms state - of - the - art methods in terms of all metrics on each dataset .
the results are shown in table 6 . the rnn and guarded encoders perform similarly to each other when the protected attribute is encrypted . however , the rnn is more accurate and therefore requires less data to encode .
the results of applying our finetune and tuning on the training data are shown in table 1 . we observe that our approach outperforms previous state - of - the - art models on every metric by a significant margin . for example , our lstm achieves a final score of 63 . 97 % on the finetune test set , compared to 62 . 59 % by yang et al . ( 2018 ) . the difference is less pronounced on the ptb and wt2 test sets , but still suggests significant performance improvement . our approach exceeds traditional models by a noticeable margin . on the other hand , it performs slightly worse than the best previous work on the wt2 dataset .
table 1 compares the performance of our approach with previous state - of - the - art models on the training and test set . the results are presented in table 1 . we observe that our approach exceeds all state of the art models on both datasets with two tasks . the difference in performance between the base and the best - performing model is mostly due to small size of the training set ( micro - f1 ) and high number of parameters ( largely due to training on single - domain datasets ) . our approach outperforms all the other approaches that do not use multi - domain training .
the results of zhang et al . ( 2015 ) are shown in table 1 . in particular , we report results for amapolar err , amafull time and yelppolar time as well as the average number of tokens per second for each sub - step . our proposed approach outperforms both published and unpublished work on every metric by a significant margin . specifically , our proposed approach obtains an err / yahyy time improvement of 2 . 36 / 0 . 48 and a bleu / 4 . 59 % increase over the previous state of the art on average .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . from table 3 , we can see that our approach outperforms all stateof - the - art methods in both languages on average . the difference is most prevalent in the case of german , where our olrn model obtains the best performance . on the other hand , the comparison of our approach with other methods that rely on syntactic analogy is less clear , but still shows a significant performance improvement .
table 4 : exact match / f1 - score on squad dataset . we show that our approach outperforms all state - of - the - art methods on average . specifically , our approach obtains the best results with the help of elmo feature - values . the difference between elmo and other base models is less pronounced for our approach , but still significant enough to warrant a significant increase in performance .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model outperforms all stateof - the - art models with a large margin . the number of parameters in our model exceeds the size of the training set , which indicates that the model performs well in the competitive evaluation .
table 7 shows the performance of our model on snli and ptb task with base + ln setting compared to the previous state - of - the - art model with base and base setting . with the base setting , our model obtains the best performance on the snli task . with the ptb setting , we get the best result .
table 1 shows the system and sentence quality on the test set for english , spanish , french , dutch , russian and turkish . the results are presented in table 1 . our system outperforms all the other systems with two tasks . the difference is most prevalent for systems using oracle retrieval , with a gap of 10 . 8 % on average compared to human . retrieving the same word twice as often as matching the original sentence is beneficial , with an absolute boost of 2 . 6 % on b - 2 and 7 % on oracle . sentence quality is only slightly better than system quality , but still superior than human on both systems . we observe that the difference between human and system learning is less pronounced for english .
table 4 presents the results of human evaluation . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is indeed comparable to state of the art in terms of overall quality . however , seq2seq performs slightly worse than human on some of the subtasks , since it requires significantly more data and time to train . overall , our system outperforms the best previous approaches on all subtasks .
the results are shown in table 1 . our system outperforms all stateof - the - art systems on every metric by a noticeable margin . for example , we see that our approach outperforms the best previous approaches on three out of four metrics by a significant margin . on the four datasets , our system performs better than all the other systems apart from the one that does not have our data augmentation .
for completeness , here we also compare our system against the best previous approaches on three baselines : wikipedia , ted talks , and sub - tables . the results are shown in table 1 . from left to right , we observe that the performance gap between our system and the previous best state - of - the - art approaches is minimal , but significant . our system outperforms all the other systems that do not use text clustering .
the results are shown in table 1 . our system outperforms all stateof - the - art systems on every metric by a noticeable margin . for example , we see that our approach outperforms the best previous approaches on three out of the four metrics by a significant margin . on the four datasets , our system performs better than all the other systems apart from the one that does not have our data augmentation feature .
the system performs well on all metrics with two exceptions . for example , europarl ' s average depth is 11 . 05 % higher than the average of other systems . on the other hand , our hclust model performs better than most other baselines with a comparable number of features .
the system performs well on both datasets with different feature sets . for example , europarl has the best overall performance with a 17 . 29 % boost on average compared to the previous state of the art system . on the other hand , our hclust model performs slightly worse than the other two baselines .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . our enhanced model outperforms the baseline model by a noticeable margin . the difference between the performance of our enhanced model and the baseline is less pronounced , but still significant .
the performance ( ndcg % ) of these experiments on the visdial v1 . 0 validation set is shown in table 2 . note that only applying p2 indicates the most effective one ( i . e . , the hidden dictionary learning method ) , and that it is easier to apply than p1 and p2 to the baseline .
table 5 compares the results of different approaches for hard and soft alignments . the hmd - f1 approach outperforms all the other approaches that do not rely on bert , i . e . wmd - unigram , wmd - bigram , hmd - ( prec + bert ) and wmd ( hmd - recall ) . the results are summarized in table 5 , where we observe that , for hard alignments , ruse performs better than all the alternatives apart from wmd .
the results of baselines are shown in table 1 . the average score of bertscore - f1 and ruse ( * ) is slightly higher than that of meteor + + , indicating that bert is better at selecting the relevant features and its output is more interpretable . on the other hand , sent - mover performs slightly better than bert score on some of the baselines indicating that it has better interpretability .
the results are shown in table 1 . the system performs well on all metrics with two exceptions . for example , bertscore - f1 is only slightly better than bleu - 1 , while meteor is better than sfhotel . sent - mover achieves the best results with an f1 score of 0 . 7 . on the other hand , when using w2v as a baseline , our system performs better than both the other baselines . we notice that the quality of our approach is slightly higher than the baselines on two of the four scenarios .
the results are shown in table 1 . sent - mover achieved the best results with a bertscore - recall of 0 . 939 on m1 and 0 . 949 on m2 . the difference is less pronounced with respect to other metrics , but still shows significant performance improvement . we observe that the clustering quality of some of the baselines is relatively high compared to others . for example , leic ( * ) scores are higher than spice , but lower than meteor , indicating that there is a need to consider alternative ways of classifying the training data . we find that word - mover alone achieves the best performance , outperforming other methods which rely on word - based learning models .
the results are shown in table 1 . the first set of results show that the approach chosen by vaswani et al . ( 2017 ) outperforms all stateof - the - art methods in terms of all metrics on both datasets with two tasks . the difference is most prevalent in the lower - level categories , namely , in the case of para - lex , where the results are slightly worse than those by m6 .
table 1 shows the results for english , spanish , french , dutch , russian and turkish . our proposed system outperforms both published and unpublished work on every metric by a significant margin . for english , we observe that our proposed system improves the transfer quality by 3 . 6 % over the previous state of the art model . on the other hand , we see a drop of more than 2 % in performance for spanish .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . from table 5 , we can see that our approach verifies the accuracy of our summaries , and that the summaries generated by our approach match the human evaluation of the metrics .
the results are shown in table 6 . we observe that for all models , the performance gap between the baseline model with and without parameter sharing is minimal , but significant when we add in the effect of syntactic and syntactic distance - para - based learning on the training data .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher acc than prior work at similar levels of acc with different classifiers in use , and their transfer quality is better than that of simple - transfer . however , our best model is still slightly worse than the best previous work . acc ∗ : the definition of acc varies by row because of different classifier in use . our model achieves the highest acc level , which indicates that our classifiers are better at learning the task .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent for each type of nested disfluency . for example , the number of tokens for rephrase and restart disfluencies is less than the number for repetition tokens , but still exceeds the length of repetition tokens . reparandum length is shorter for both types , which means fewer tokens are required to generate a statement . overall , the system performs better than stateof - the - art methods for both languages .
table 3 shows the relative frequency of rephrases that are correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparence or repair ( both in the content - function ) or in neither . the length of tokens in each category is the most important factor in predicting whether a disfluency will be predicted as a disfuncion or not . reparandum length is the fraction of tokens that belong to each category that are in the repair ( the majority ) and the reparation ( the minority ) . function - function tokens belong to the part of the lexical lexical datum that contains the function - function . they are the ones that are used to derive tokens for the reparations .
the results are shown in table 1 . we observe that the text - based approach outperforms both the approach based on single - input and multi - input learning models . however , for the best performances , we need to consider the fact that the innovations developed during the training phase are the most important part of the model , and that their performance are only present in the test set during the development phase . the results of applying our text - adaptive learning model to the dataset are presented in tables 1 and 2 . they show that the model performs better when trained and tested on a larger corpus .
the performance comparison of our model against the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . our model shows marked performance improvement over the previous state of - art methods . the difference in accuracy between our model and the state of the art is most prevalent in relation to topics discussed , which shows that our proposed approach can significantly improve interpretability without sacrificing too many correct answers .
table 2 compares the performance of the different methods on the apw and nyt datasets for the document dating problem . our unified model significantly outperforms all previous models .
table 3 compares the performance of our approach with and without word attention for this task . our approach outperforms all stateof - the - art approaches across all metrics . the difference in accuracy between word attention and graph attention is minimal , however it is significant enough to show that our approach has comparable performance with other approaches .
the performance of each approach is presented in table 1 . we observe that , for example , the approach based on dmcnn achieves the best results on 1 / 1 , 1 / n and 3 / n , with a gap of 10 . 6 % overall improvement over the previous state of the art on all metrics . on the other hand , jrnn obtains the best performance on 3 / 1 and 2 / n . the gap between embedding + t and argument is narrower , but still significant , with jrnn performing better overall .
table 1 presents the results for event identification and event classification . the system performs well on both datasets with different feature sets . for event identification , we see that our proposed method leads to better identification and classification . on the other hand , our approach results are slightly worse than the previous state - of - the - art model on both event and argument identification .
the results are shown in table 1 . we observe that , for english - only and spanish - only languages , the precision on average is slightly higher than that on average for both languages . however , fine - tuning gives a significant performance boost , which is more than expected for a language that is only spoken in one language . the results of " shuffled - lm " outperform " fine - tuned - lm " . finally , we see that " all : cs - last " outperforms " cs - only " when trained and tested on the same dataset .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . the fine - tuned approach outperforms our approach , showing that the training set size and type of training data are the most important factors in model performance .
the results in table 5 show that fine - tuning reduces precision on the test set and increases accuracy on the dev set . moreover , precision increases with the type of gold sentence in the set , as shown in fig . 5 . the difference in accuracy between the two sets is less pronounced for the gold sentence , but still suggests some significant performance drop .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from baseline to current state - of - the - art model is statistically significant ( p < 0 . 01 ) which indicates significant performance improvement over the baseline model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our approach shows a significant improvement over the baseline model by 10 % in precision and r = 0 . 35 points , which indicates that our approach can significantly improve interpretability without sacrificing too many high - level features .
the results on belinkov2014exploring ’ s test set are shown in table 1 . our hpcd model outperforms the previous state - of - the - art model on both the test set and the evaluation set . the difference is most prevalent in the test case of lstm - pp , where our glove - extended embeddings outperform the original model by 3 . 8 points .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the first set of results show that our hpcd - based system outperforms the best ontolstm - pp model . the second set shows that our approach further boosts performance with the help of additional pp features .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . our model achieves the best performance with a 90 % ppa acc . on the full test set .
in table 2 , we report the bleu % scores of the models using subtitle data and domain tuning for image caption translation . the results are slightly better than the results with sub - subsparse subtitle data , but still significantly worse than using full subtitle data . our model obtains the best results with domain tuning .
we observe that the domain - tuned model outperforms the plain h + ms model , confirming the importance of domain - adaptive keyphrases . the results are slightly less pronounced for en - de , but still show that domain - aware models perform better than those without . for mscoco17 , the results are less striking but still consistent with the results of " tuning " .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . the results are slightly better than the results with marian amun . adding only the best 5 captions improves performance , but still results slightly worse than adding the rest .
the results in table 5 show that enc - gate and dec - gate achieve better results than en - de and mscoco17 on three out of the four datasets . however , for the flickr17 dataset , the difference is less pronounced , indicating that encoding information is more useful for visual information extraction . encoding information takes a considerable amount of time and effort compared to decoding information . we find that the advantage of using a gate - based encoder is also evident in the lower bleu % scores .
in the en - de and mscoco17 models , we find that the visual features - free approach outperforms the text - only approach , confirming the importance of the word " visual features " . however , the results are slightly less clear when we switch to the multi - lingual approach . perhaps the most striking thing about the results is that the combined effect of visual features on the overall performance is less pronounced than those of text features , which indicates that the semantic features extracted by ms - coco are important for the model to succeed .
the results are shown in table 1 . from left to right , en - fr - ht and en - es - ht achieve better results on the test set compared to the previous state - of - the - art models on both mtld and ttr . the smaller difference in performance between the two approaches is mostly due to the smaller size of the training data set , which results in lower precision on mtld .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr model splits into 7 different language pairs , each of which has 1 , 472 , 203 sentences . the total number of sentences in our model is 7 , 723 .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to each language pair , which shows that the training set size does not impact the performance of the models .
table 5 shows the evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems achieve decent performance , but are still significantly worse than the ter system . on the other hand , the en - e - smt - rev system is comparable with ter and bleu , obtains a better overall evaluation score and ter score , and is closer to the performance of the original rev system .
table 2 shows the performance of our approach on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the text - supervised model from our second submission . rsaimage , on the other hand , is supervised using the best performing feature set . our approach outperforms both the approaches .
the experimental results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the second row labeled rsaimage is the text - supervised model from the previous study . the difference in performance between the two approaches is minimal , however we see significant difference in the average recall @ 10 and chance for rsaimage , which shows the viability of finetuning the embeddings during training . we also observe that audio2vec - u outperforms the approach based on the fact that it has more training data , making it more suitable for production .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that is easier to hate than the original , but harder to hate . cnn is more difficult to hate , but easier to turn in than rnn . finally , we find that fan is more useful for use with < unk > , as it turns on a single sentence instead of multiple sentences .
table 2 shows the part - of - speech changes in sst - 2 since fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . since the training set is small , the effect on the number of occurrences has not been significant . however , the percentage of occurrences in the final sentence has increased , decreased or stayed the same , indicating that the quality of the output has been improved .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the flipped labels cause the sentiment score to decrease in the positive sentiment case , but the decrease in negative sentiment is less pronounced .
table 1 presents the results of the second study . our approach outperforms all state - of - the - art methods on both pubmed and sst - 2 . the results show that pubmed outperforms sift significantly in both cases .
