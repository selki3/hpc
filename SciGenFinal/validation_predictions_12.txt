table 2 shows the performance of our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the inference and training set , the iterative approach outperforms the recursive framework with a performance gap of 2 . 5 instances / s vs . 3 . 6 instances / s on average . the recur and fold approaches yield similar performance on the training and inference set , but the improvement is narrower on the inference set .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
the max pooling strategy consistently performs better in all model variations . for example , conll08 models with different representation perform better than ud v1 . 3 models with the best performing feature maps and dropout probabilities under all three scenarios ( table 2 ) . the softplus representation also improves over the sigmoid representation , but does not improve over softplus , indicating that the selection of the best feature maps may depend on the underlying model architecture .
table 1 : effect of using the shortest dependency path on each relation type . our macro - averaged model obtains the best f1 score ( in 5 - fold ) with and without sdp .
the results are shown in table 3 . we observe that our proposed method outperforms the state - of - the - art on all three metrics by a significant margin . on the c - f1 and f1 metrics , our method achieves 100 % and 50 % accuracy , respectively , on average , with an absolute improvement of 3 . 57 % and 6 . 45 % over the previous state of the art .
the results are shown in table 3 . we observe that mst - parser achieves a full 100 % accuracy on average on all three test sets , with a gap of 2 . 59 points from the previous best state - of - the - art .
the results are shown in table 4 . the average c - f1 score for the two systems is 60 . 40 ± 13 . 57 % and 56 . 24 ± 2 . 87 % respectively . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between the average and the average score is less pronounced for the paragraph - level system .
the results are shown in table 4 . the original tgen + model outperforms the original model on every metric by a significant margin . on average , the tgen model performs better than tgen − and sc - lstm on all metrics except bleu and nist .
table 1 compares the original e2e data with the cleaned version . the difference in quality between the original and the cleaned data is small ( 0 . 5pt / 2pt ) but significant ( 17 . 69 % vs . 11 . 42 % ) . the difference between the average number of distinct mrs and total number of textual references is much larger ( 4 , 862 vs . 4 , 693 ) .
table 3 presents the results on the training and test set for tgen + and sc - lstm . the results are summarized in table 3 . original tgen model outperforms the original model on all metrics except bleu and nist by a margin of 2 . 83 points . on the test set , it achieves a marginal improvement of 1 . 53 points over the original tgen − model .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances in the training set ( which we cleaned ) are shown in table 4 . the difference in the error numbers between the original and the cleaned set is small , however it is significant .
the results are shown in table 3 . our proposed method outperforms all the state - of - the - art methods on both the single - domain and ensemble test sets . the results show that our proposed method is comparable to previous approaches on the ensemble test set , with the exception of seq2seqk ( konstas et al . , 2017 ) . on the other hand , it performs slightly better than previous methods on the single entity test set .
table 2 shows the performance of our model on amr17 . our ensemble model achieves 24 . 5 bleu points better than the previous state - of - the - art on average compared to seq2seqb ( beck et al . , 2018 ) . similarly , our dcgcn model achieves 19 . 5 points better bleu points than the last state of the art ensemble model . table 2 also shows that our ensemble model is comparable to the best previous work on the micro - scale .
table 3 presents the results for english - german , czech and slovak . our proposed method outperforms all the base systems except birnn and bow + gcn . the results are statistically significant with respect to both english - and czech , with the exception of seq2seqb ( beck et al . , 2018 ) . the proposed method improves upon the previous methods by 4 . 6 points in the single - parity test set . it achieves a marginal improvement over the previous state - of - the - art on both english and czech .
the effect of the number of layers inside the network is shown in table 5 . our model obtains a significant performance improvement over state - of - the - art han models with a minimum of 10 layers . we observe that the han model can further improve with the addition of additional layers as shown in fig . 5 .
comparisons with baselines are shown in table 6 . our model outperforms the previous state - of - the - art in terms of rc and residual connections . the difference between rc and rc + la is less pronounced for the dcgcn model , but still suggests that residual connections are a significant factor in the performance of these gcns .
the results are shown in table 4 . our model outperforms the previous state - of - the - art on all metrics by a significant margin . on the dcgcn test set , it achieves a final score of 54 . 2 % higher than the previous best state - ofthe - art model .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . the results show that removing the dense connections in the i - th and j - th blocks significantly improves the model ' s performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . our model obtains the best results with an f1 score of 25 . 2 on the global test set and a f1score of 54 . 9 on the local test set .
table 7 shows the performance of our initialization strategies on a variety of probing tasks . our glorot - based model obtains the best results . it obtains a score of 35 . 8 / 71 . 3 on average and 35 . 2 / 59 . 9 on the subtasks with a gap of 3 . 5 / 10 . 9 points from the previous best performance .
we observe that the h - cbow model outperforms the previous state - of - the - art h - cmow model on all metrics except subtasks with a gap of 3 . 6 points . the h - mul feature - values are close to those of the original cbow model , but are slightly higher than the original cmow due to the small size of the data set .
the results are shown in table 3 . we observe that the hybrid method outperforms the original cbow / 784 model by a margin of 3 . 6 points . the difference between cbow and cmow is narrower than the difference between sick - e , sst2 and sst5 , but still suggests that there is a need to improve our interpretation of these datasets .
table 3 shows the results on unsupervised downstream tasks attained by our models . the cbow and cmow method outperform the hybrid method in terms of all three sts12 and sts16 tasks , indicating that the cbow approach has superior generalization ability . hybrid also improves the performance on the sts13 and 14 tasks , although the difference is narrower .
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our glorot - based system outperforms all the stateof - the - art systems except sick - e by a large margin . it achieves a final score of 87 . 6 % on sst2 and 86 . 4 % on sts - b , a gap of 3 . 5 % over the previous best state of the art .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our method outperforms the best previous approaches on all three datasets . the cbow - r achieves a final score of 43 . 2 / 71 . 2 on the sts12 and sts13 datasets , which shows that our method is comparable to state - of - the - art on both datasets . however , on sts16 , our method performs slightly worse than the previous best performing method .
the results are shown in table 3 . our method outperforms the previous state - of - the - art on every metric by a significant margin . our method obtains the best results on the subtasks with a gap of 3 . 6 points from the previous best result . on average , it achieves a weighted average precision of 82 . 5 % on subtasks and 79 . 6 % on average .
the results are shown in table 3 . the best results are obtained by the method of cbow - r , which improves upon the previous state - of - the - art cmow - c by 3 . 6 points . it achieves the best results on sub - sst2 and sst5 scores .
the results are shown in table 3 . name matching and supervised learning outperform all the base systems except for supervised learning , which shows that the supervised learning approach has the advantage of training on a larger corpus . in particular , it outperforms all the baselines except loc and misc by a margin of 3 . 36 points . moreover , it achieves a marginal improvement over the previous state - of - the - art on all metrics except loc .
results on the test set under two settings are shown in table 2 . name matching has the best performance with an f1 score of 15 . 38 ± 1 . 03 and supervised learning has the worst performance . however , when we switch to τmil - nd ( model 2 ) , the f1 scores improve to 42 . 42 ± 0 . 59 and 42 . 57 ± 1 points , respectively , which shows the diminishing returns from naive supervised learning under the current set of training conditions .
table 6 presents the results of our experiments on the hidden test set of g2s - gat . our model obtains the best results with an absolute improvement over the previous state - of - the - art on all metrics . on the other hand , our model performs slightly worse than the best previous state of the art .
the results are shown in table 3 . the g2s model outperforms the previous state of the art on three of the four datasets . on the ldc2015e86 and ldc2017t10 datasets , it achieves a marginal improvement of 0 . 55 ± 0 . 03 points over the previous best state - of - the - art model while surpassing the s2s baseline by 0 . 87 points . it achieves an absolute improvement of 3 . 36 points over g2cad on the other datasets , which shows the diminishing returns from relying on superficial similarity .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model improves upon the previous state - of - the - art on the test set by 1 . 5 bleu points .
the results of the ablation study on the ldc2017t10 development set are shown in table 4 . our model obtains the best results with a bleu score of 62 . 42 % and a meteor score of 59 . 62 % .
the results are shown in table 3 . we observe that g2s - gat has the best results , with a marginal gain of 3 . 51 % over the previous state of the art model on average .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when the model is added to the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s outperforms g2s - gat in terms of both accuracy andiss ( i . e . , it obtains 50 . 06 % f1 score compared to 48 . 67 % in the original test set ) . the difference is narrower when we compare the model with the alternative ggnn model .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . the pos tagging accuracy is slightly better than sem , indicating that the features extracted from the 4th nmt encoding layer are more useful for target languages . the semantic features also contribute more than expected to the model ' s overall performance .
table 2 compares pos and sem tagging accuracy with baselines and an upper bound . our unsupervised embeddings outperform the best previous approaches on both metrics . our encoder encoder decoder is better than the previous encoder with a baseline of 91 . 55 % pos and 91 . 41 % sem .
table 4 presents the system ' s performance on the pos and sem datasets . our model obtains the best results on all metrics with an absolute improvement of 3 . 8 points over the previous state of the art .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our proposed system exhibits a slight improvement over the previous state - of - the - art on average across all target languages , with the exception of english .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary ' s accuracy is statistically significant ( p < 0 . 01 ) with respect to all the three protected attributes .
accuracies when training directly towards a single task are shown in table 1 . our model outperforms all the state - of - the - art methods on all metrics except gender and age .
table 2 shows the results for balanced and unbalanced task data splits . our proposed method outperforms the best previous approaches by a large margin . the average task accuracy of pan16 is 82 . 5 % compared to 71 . 2 % on unbalanced data .
the adversarial training set is shown in table 3 . our proposed method outperforms the best state - of - the - art approaches by a significant margin . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . it is clear from table 3 that our proposed method can significantly improve the generalization ability of our proposed feature extraction algorithm .
classification accuracies of the protected attribute with different encoders are shown in table 6 . the rnn encoder performs better than the guarded encoder under all three scenarios . with the exception of the leaky encoder , rnn is more accurate than guarded in all scenarios .
the results are shown in table 3 . table 3 compares the performance of our method with previous state - of - the - art models on the four base systems . our lstm model improves upon the previous state of the art on all four systems . it achieves a final score of 62 . 36 % on average compared to 62 . 45 % on the previous best model ( yang et al . ( 2018 ) with a gap of 3 . 45 points in finetune fine - tuning .
table 3 presents the results of our experiments on the lstm and gru models . our model obtains the best results with an absolute improvement over the previous state - of - the - art on all metrics . the difference is most prevalent on the base acc metric , which shows that our model is more than 4 . 5x faster than previous state of the art models on average .
the results of zhang et al . ( 2015 ) are shown in table 1 . the results are summarized in bold . our proposed method outperforms the previous state - of - the - art on all metrics by a significant margin . on the amapolar and yahoo time datasets , our proposed method obtains an absolute improvement of 3 . 53 points over the previous best state of the art . on yelp time , it achieves an absolute gain of 1 . 55 points .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . our model obtains the best performance with an overall b + of 2 . 67 points on the metric of average time taken to train and decode one sentence .
table 4 : exact match / f1 - score on squad dataset . our model obtains the best results with an f1 score of 75 . 41 / [ bold ] 79 . 83 on average . the difference between elmo baseline and rnet * model is less pronounced when we consider only the parameter number of base . table 4 also shows that the gru model outperforms the lstm model with a large margin . finally , the sru model shows a slight improvement over the previous state - of - the - art on average , but still performs substantially worse than atr and gru . we conjecture that the difference is due to the large variation in parameter number between base and paramethods .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model outperforms all the state - of - the - art models with a gap of 3 . 56 points from the last published results .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 56 % accuracy and 169 . 81 % on the perplexity test set .
table 3 compares the performance of our system with the best previous state - of - the - art systems on the word analogy task . our system outperforms all the previous systems with a gap of 10 . 8 % on average . the difference is most pronounced for human , however , with an absolute margin of 2 . 5 % on oracle .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that our system is comparable to the best human evaluation performed by h & w hua and wang ( 2018 ) on grammatical accuracy ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . our system obtains the highest average score on all three metrics ( k 500 , k 1000 and k 2000 ) . retrieval is the only automatic system that obtains a lower average score than human on all metrics , indicating the high quality of the data generated by our system .
the results are shown in table 5 . table 5 shows that our approach outperforms the previous state - of - the - art on all three datasets by a significant margin . on the english datasets , our proposed hclust model outperforms both the previous best and the best - performing ted talks model by a margin of 3 . 5 points . on the spanish dataset , our approach achieves a marginal improvement of 0 . 3 points .
the results are shown in table 3 . table 3 shows that our approach outperforms the previous state - of - the - art on all three datasets by a noticeable margin . on the english datasets , our proposed hclust clustering scheme outperforms both the previous best state - ofthe - art and the best previous work on the spanish datasets . the difference is most prevalent on the ted talks dataset , which shows that it is harder to train a hclust model than on the original corpus dataset .
the results are shown in table 5 . table 5 shows that our approach outperforms the previous state - of - the - art on all three datasets with one exception . on the ted talks dataset , our proposed hclust model outperforms both the previous best state - ofthe - art and the best previous work on the sub - tables dataset by a margin of 2 . 5 points . on the europarl dataset , we get a marginal improvement of 0 . 3 points .
the results are shown in table 3 . we show the average depth and the average number of roots for each category . our hclust model outperforms the previous state - of - the - art on all metrics except for averagedepth . the hclust model obtains the best results on average , with a marginal improvement over previous work .
the results are shown in table 3 . we show the averagedepth and maxdepth scores for each metric . the averagedepth scores are slightly higher than the previous state of the art . europarl has the best results with a maxdepth score of 9 . 43 and a weighted average of 2 . 29 . according to the table 3 , the best performing feature set is the depth cohesion . our hclust model outperforms all the baselines except slqs .
table 1 compares the performance of our enhanced model with the baseline model using the best performing regressive loss and weighted softmax loss on the validation set of visdial v1 . 0 . the enhanced model shows a significant performance improvement over the original model , with an ndcg % of 3 . 42 % compared to the previous best performing model .
the performance ( ndcg % ) of the ablative studies on different models on visdial v1 . 0 validation set is shown in table 2 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . p2 indicates the most effective one ( i . e . , the hidden dictionary learning is the best performing one ) , and p1 indicates the least effective one .
table 5 compares the performance of our hmd - f1 + bert model with the previous state - of - the - art recall and bigram models on hard and soft alignments . the hmd - recall model achieves the best results with a precision score of 0 . 823 on hard alignments and a marginal improvement of 1 . 01 on soft alignment . on the other hand , the wmd - unigram model achieves a performance improvement of 2 . 01 points on hard aligned alignments , which indicates that the recall function is more useful .
the results are shown in table 3 . the average score of bertscore - f1 and ruse scores for each setting is reported in parentheses . bert score is significantly higher than ruse score on average , indicating that the method is more interpretable and interpretable .
the results of bertscore - f1 are shown in table 3 . the proposed method outperforms the previous state - of - the - art on all metrics except meteor by a noticeable margin . on the bleu - 1 dataset , it achieves a score of 0 . 45 / 0 . 55 and a gap of 3 . 43 / 4 . 55 points over the previous best bert score on the sfhotel dataset . sent - mover also achieves a significant performance improvement over previous methods , reaching a new all - time high of 1 . 57 / 2 . 55 on the bert scores .
the results are shown in table 3 . word - mover is the most consistent across all metrics with a gap of 0 . 7 points from the previous state of the art . it achieves the best results on m1 and m2 scores , while bertscore - recall is close to the best on m2 . we observe that word - mover is comparable across all the baselines with the exception of leic ( * ) where it performs slightly worse . sent - movers are comparable with the best previous state - of - the - art on all metrics except spice .
the results are shown in table 6 . para - para model outperforms all the base lines with a gap of 0 . 81 points from the best previous state - of - the - art on the sim and gm test sets .
table 3 presents the results on transfer quality , transfer quality and semantic preservation . our model improves upon the previous state - of - the - art on all three metrics by 1 . 5 points on average . on the transfer quality metric , it achieves a marginal improvement of 0 . 9 points over the previous best state of the art . semantic preservation is marginally better than transfer quality by 0 . 1 point on average , and on the semantic preservation metric by 1 point . the results are slightly worse than the results of last published work ( table 3 ) . on the fluency metric , our model improves by 2 . 6 points .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are presented in table 5 . acc is the percentage of machine and human judgments that match the true answer in the validation set when both sets are trained on the same dataset . refer to table 5 for additional information on the method of validation .
the results are shown in table 6 . para - para model outperforms all the base lines with a gap of 3 . 5 points from the best previous state - of - the - art results . it achieves the best results with a precision of 0 . 817 points over the previous best state of the art .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher acc scores than prior work at similar levels of acc with different classifiers in use , and their accuracy is comparable to the best unsupervised models ( table 6 ) . however , our transfer results are worse than those of simple - transfer , indicating that the transfer quality of our model can be improved with additional classifiers . we also note that our model achieves higher acc than the best previous work with a similar set of 1000 tokens , suggesting that our classifiers are better than the simple transfer method .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent when we filtered out repetition tokens and nested disfluencies . reparandum length is the average of the average number of repetition tokens divided into correct and incorrect tokens , and overall the average length of the disfluency tokens . the difference in performance between repetition and non - repetition tokens is less pronounced for nested disfuncions , although it is still significant .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . table 3 shows the percentage of tokens in each category that belong to each category when the content word is removed . percentages in parentheses show the fraction of tokens whose content word corresponds to a disfluency for each category . reparandum length is the average length of the tokens in the original reparation sentence followed by the repair sentence length . function - function tokens are the smallest .
the results are shown in table 4 . we observe that the best model is derived from a single model , while the average number of iterations per model is slightly higher than the best single model . when we add in the innovations and the text transformation step , our model achieves the best results .
word2vec embeddings outperform the state - of - art algorithms on the fnc - 1 test dataset . our model achieves an accuracy improvement of 3 . 43 % over the previous state of - the - art on average compared to self - attention sentence embedding . furthermore , our model achieves a 3 . 53 % improvement over the accuracy of the rnn - based cnn - based model , which is comparable to the current state of the art .
table 2 compares the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater outperforms the previous state - of - the - art on both datasets . attentive neuraldater improves upon ac - gcn by 4 . 5 points , while maxent - joint improves by 2 points . ac - gcn also outperforms other methods by 4 points .
table 3 compares the performance of our neuraldater model with and without word attention . our approach shows that word attention alone does not improve the performance , however , graph attention does improve it . this results show the effectiveness of both word attention and graph attention for this task .
the results are shown in table 3 . embedding + t improves the general performance for cnn and jrnn models , but does not improve the cross - domain performance for jmee models . trigger - based dmcnn improves upon the previous state - of - the - art cnn model , but still performs substantially worse than all the other methods except for jrnn . as shown in the next table , the ability to select the correct argument improves with the improvement of 1 / 1 and 1 / n over previous state of the art neural networks .
table 3 presents the results on event identification and event classification . our proposed method outperforms all the state - of - the - art methods on both event and argument identification . on the argument identification stage , our proposed method improves upon the previous state of the art by 4 . 8 points . on trigger identification , our method improves by 3 . 9 points and on argument identification by 2 . 1 points . our cross - event method improves further by 4 points .
the results are shown in table 3 . we observe that , for english - only and spanish - only languages , the dev perp and test wer accuracy are relatively the same while fine - tuned variants yield slightly better results . fine - tuning gives a performance gain of 0 . 36 points over the original model , but still puts it slightly better than the best previous state - of - the - art . further improving performance is seen when we switch to all - vocab - based learning schemes . the best results are obtained by using the best performing variation of the original vocabulary . finally , we notice that the best performance is obtained when using only the last layer of the lexical encoder .
results on the dev set and on the test set using only subsets of the code - switched data . fine - tuned fine - tuned neural models outperform cs - only models on both sets , showing that the training quality can be further improved with a drop of only 0 . 5 bleu over the strong baselines ( tables 4 and 5 ) .
the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set , is reported in table 5 . fine - tuned - disc improves the performance for both sets , with fine - tuned - disc achieving 75 . 40 % and 75 . 53 % better results , respectively , than fine - tuned - fixed - disc .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . our approach shows a statistically significant improvement over the baseline model with a f1 score of 3 . 61 points compared to the type combined baseline .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . our approach shows a statistically significant improvement over the baseline model by 2 . 5pp over the best previous state - of - the - art model ( gillick et al . , 2008 ) .
results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . our hpcd model outperforms the previous state - of - the - art on both syntactic and semantic word embeddings . the difference is most pronounced on the syntactic - sg dataset , which shows the effectiveness of our enhanced glove embedding scheme . further improving performance by 1 . 7 points over the previous model , however , the difference is narrower on the wordnet dataset .
results are shown in table 2 . our system obtains the best performance with respect to uas acc . on the four pp attachment predictors and oracle attachments . our hpcd dependency parser outperforms the original ontolstm - pp on all four pp prediction features . on the other hand , it performs slightly worse than the original lstm on the oracle pp .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves a ppa acc . score of 89 . 5 on the full test set and 87 . 4 on the attention set .
table 2 shows the bleu % scores of our model with domain tuning and subtitle data added . our ensemble - of - 3 model improves upon the original en - de model by 3 . 7 points . however , it still performs substantially worse than the model with sub - subsfull subtitle data .
the results of domain - tuned h + ms - coco are shown in table 3 . our model outperforms the previous state - of - the - art on all metrics except for en - de , where it is slightly better than the best previous state of the art .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding automatic captions improves the general performance for all models except for en - de , which shows a drop in performance when using multi - domain multi - attn . further improving performance with autocap 1 - 5 ( concat ) and multi30k ( + autocap 2 ) shows that the model with the best captions can be further improved with the addition of all 5 captions . finally , when using only the best five captions , the model improves further .
table 5 compares the bleu % scores of our encoder + dec - gate and en - de encoder + dec - gate strategies for integrating visual information in the multi30k + ms - coco + subs3mlm task . our encoder achieves the best results , with a b + u % score of 69 . 86 . the decoder is slightly better than the encoder , but still performs better than en - fr and mscoco17 . en - de relies on a layer of layer - layer - layer encoder with a gap of 2 . 53 points from the last published results .
the results are shown in table 3 . multi - lingual models outperform the text - only and visual features - only variants , sub - 3m and subs6m lm detectrons are both comparable to the performance of en - de and mscoco17 models on the test set of flickr16 and flickr17 . however , the results are slightly worse than those on the original flickr17 set when we switch to ms - coco encoders . the ensemble - of - 3 model outperforms the original model with a gap of 3 . 45 points .
the results are shown in table 3 . the average number of frames per second for each translation according to the ttr and mtld metric is reported in parentheses . for english , en - fr - ht achieves the best results with an average of 2 . 27 points per second . for french , it achieves a marginal improvement of 0 . 01 points .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr embeddings have 1 , 467 , 489 parallel sentences and 1 , 472 , 203 total words , respectively .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to the average number of errors for each language pair .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . our en - fr - rnn - rev system outperforms the previous state - of - the - art on both metrics by a noticeable margin . on the other hand , our en - es - trans - rev is comparable with the best previous state of the art on ter .
table 2 shows the results for flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the trained model from the previous study ( chan et al . , 2017 ) . rsaimage has the highest recall @ 10 and median rank , indicating that the model is well - equipped to handle high recall . segmatch , on the other hand , has the worst performance .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled u is the audiovisual supervised model . the average recall @ 10 rank of rsaimage is 0 . 4 , which means that the model has 27 % chance to match the rsaimage generated by the original embeddings . the mean mfcc score is 1 . 9 % higher than the average score of audio2vec - u .
table 1 compares the original on sst - 2 with the new embeddings using the different classifiers . we report further examples in the appendix . originally , the dan embedding was used to turn in a screenplay that had edges at the edges and a on ( the edges are the most difficult part ) . since the cnn embedding is easier , we can also use it for the screenplay as well . finally , rnn also improves upon the original by adding additional classifiers to improve interpretability .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same as a result of fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sst - 2 sentence . note that the number of occurrences for some words has increased while the number has decreased for others . this indicates that the semantic information extracted from the cnn embedding has been fine - tuned . for example , the presence of the word " b * tch " indicates that cnn has increased , while the occurrence of " cid " has decreased .
the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the positive sentiment score increases by 9 . 6 points in sst - 2 .
the results are shown in table 2 . our method outperforms all the state - of - the - art methods except sift by a large margin . our approach shows that the ability to distinguish between positive and negative sentiment is relatively unaffected by the fact that the sample size is small . however , our method does not generalize well , since it relies on superficial cues . the results of our approach show that it is difficult to generalize from small sample size to larger ones .
