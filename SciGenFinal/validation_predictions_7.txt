table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . table 2 shows the performance of the recursive and the iterative approaches on the training and inference tasks , respectively . it can be seen that the recursive approach is more suitable for training , since it requires fewer instances , but is more efficient on inference . the folding technique performs better on training , however , as it requires more instances to train the model .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization .
table 2 : hyper parameter optimization results for each model with different representation . the max pooling strategy consistently performs better in all model variations . conll08 and ud v1 . 3 significantly outperform sb and softplus models with different feature maps . sigmoid and softplus models perform similarly well with the same number of feature maps , however , the performance drops significantly with the size of the filter size and the number of dropout probabilities . the performance drop - out probabilities are less pronounced with the softplus model , but are still significant .
table 1 : effect of using the shortest dependency path on each relation type . the results are shown in table 1 . the macro - averaged model achieves the best f1 score ( in 5 - fold test set ) and the model - feature model obtains the second - best f1 ( in 10 - fold ) .
table 3 shows the performance of our model on the f1 and r - f1 test sets . the results are presented in table 3 . the results show that our model outperforms the previous state - of - the - art models on all test sets except for f1 , where it achieves the best performance . our model obtains the best f1 score of 50 % on both datasets .
table 1 compares the performance of mst - parser with other parsers on different test sets . the results are presented in table 1 . the performance of all parsers is reported in terms of the average number of words per paragraph and the average accuracy of each argument per paragraph . as shown in the table , the average percentage of correct answers obtained by each parser is significantly higher on the test set with respect to all test sets except for the one where it is less than 50 % .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level and paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between the average and the mean performance is less pronounced for paragraph level , indicating that the lstm - parser performs better on the essay level .
table 3 shows the performance of the original model compared to the model with and without tgen + and tgen − on the test set . the results are shown in bold . as expected , when the model is cleaned , it performs significantly better than when it is trained . however , the difference between original model and clean model is less pronounced when the models are trained with tgen + , as shown in table 3 .
table 1 shows the data statistics comparison for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the difference in ser between the original and the cleaned version is less than 0 . 5 % ( table 1 ) .
table 3 shows the performance of the original and the tgen + models compared to the original model on the training and test sets . the results are shown in bold . the tgen model outperforms both original and tgen − models by a significant margin . the difference in performance between the two models is less pronounced on the test set , but is still significant . on the training set , the difference is less than 0 . 5 % on average , which indicates that the model performs better on test set with less training data .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) and the number of instances that were added and subtracted from the test set . the results are shown in table 4 . adding , missing , and disfluency are the most significant types of errors , followed by missing and incorrect values . disfluencies are small but significant enough to show that tgen can be improved .
table 3 shows the performance of the best models on the single - domain and multi - domain datasets . the results are presented in table 3 . the average number of iterations for each model is reported in parentheses . table 3 compares the performance on single and ensemble datasets . our model outperforms all the other models except for tree2str , which achieves the best performance on the ensemble dataset . however , the performance gap between our model and other models is much smaller than that of the other approaches .
table 2 : main results on amr17 . our model achieves 24 . 5 bleu points compared to seq2seqb ( beck et al . , 2018 ) and gcnseq ( damonte and cohen , 2019 ) by a significant margin . table 2 shows the performance of our model compared to the previous state - of - the - art ensembles . the size of the model increases with the number of parameters as shown in table 2 , but the performance drop is less pronounced than that of the ensemble models , indicating that the model size does not need to be increased significantly to achieve high performance . the performance drop of the ensembling model is less significant than the difference in performance between the single and ensemble models .
table 1 presents the performance of the models using bow + gcn on the english - german and czech datasets . the results are shown in table 1 . table 1 shows the average number of frames per second ( p < 0 . 05 ) for each type of embeddings for each language compared to the previous state - of - the - art models . we see that the best performance is obtained by using the multi - layer bow - gcn model with birnn + birnn ( bastings et al . , 2017 ) . the best performance on the czech dataset is obtained using the seq2seqb model , which uses the best adaptive clustering scheme . however , the performance gap between the single - layer model and the baselines is much smaller than that of the single layer model . this indicates that the model using the bi - rnn model is more suitable for the task .
table 5 : the effect of the number of layers inside the network on the performance of our model is shown in table 5 . our model obtains the best performance with a maximum of 6 layers . the effect is most pronounced for blocks with more than 3 layers , however , the effect is less pronounced for those with fewer layers .
table 6 : comparisons with baselines . our model outperforms the baselines in terms of rc and rc + la by a large margin . the results are shown in table 6 . the difference in rc scores between baselines and our model is small , but it is significant enough to show that our model can outperform both baselines with residual connections . furthermore , the difference between rc scores and baselines is much larger than those of previous models .
table 4 shows the performance of our model compared to the best previous state - of - the - art models . the results are shown in bold . our model outperforms all the previous state of the art models except for dcgcn by a large margin .
table 8 : ablation study for density of connections on the dev set of amr15 . the results of the ablation study are shown in table 8 . the results show that removing the dense connections in the i - th and x - th blocks significantly reduces the number of connections , and that removing these connections significantly decreases the overall density of the network . as shown in the table , removing the connections from the ith block reduces the network density significantly , but does not significantly reduce the overall network density .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the coverage mechanism used by the decoder achieves the best results , with the exception of the direction aggregation module , which achieves the second best result .
table 7 : scores for initialization strategies on probing tasks . the glorot initialization strategies outperform all the other initialization strategies except subjnum and topconst by a large margin . as table 7 shows , the difference in performance between the two initialization strategies is small , but it is significant enough to show that it is possible to improve upon the performance of somo with the help of additional data augmentation . table 7 also shows that somo has the advantage of using shorter initialization times , which means that it obtains higher performance on the probing tasks , but still performs better overall .
table 3 shows the performance of h - cbow and h - cmow with respect to concatenated layers . the results are shown in bold . the h - cow model outperforms the cmow model by a large margin . it achieves an absolute improvement of 3 . 6 % over the previous state - of - the - art model on average . it achieves the best results with a weighted average of 10 . 8 % improvement over the best previous state of the art model .
table 3 shows the performance of the models with different approaches compared to the baselines . hybrid models outperform all baselines except sst2 and sst5 by a large margin . the results are shown in bold . we see that the hybrid model outperforms both baselines with a margin of 2 . 6 % and 3 . 2 % over the baseline on average .
table 3 : scores on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change with respect to hybrid . the difference in performance between the cbow and cmow is less pronounced than that between hybrid and the original model , but still significant . hybrid outperforms both the original and the hybrid models on un supervised downstream tasks . as shown in table 3 , the difference between the performance of the original method and the hybrid model is less significant than that of the hybrid model .
table 8 : scores for initialization strategies on supervised downstream tasks . glorot and trec outperform all the other initialization strategies except subj and mpqa , indicating that it is better at selecting the correct sub - decoder for each supervised downstream task . subj and mrpc also outperform the other three initialization strategies , but the difference is less pronounced for sst2 and sts - b .
table 6 : scores for different training objectives on the unsupervised downstream tasks . the cbow - r method outperforms the cmow - c on all the tasks except for sts13 and sts14 , which shows that it is able to achieve the training objectives even when supervised . on the sts15 tasks , it achieves the best performance .
table 3 shows the performance of our method compared to the previous state - of - the - art cmow - r and cbow - c models . the results show that our method outperforms both the state of the art cmow model and the cbow model by a significant margin . the difference between the two models is most striking when we consider the subtasks of bshift , subjnum , tense , coordinv , and topconst . our method obtains the best performance on all three subtasks . it achieves the best results on all metrics except for topconst , where it achieves the second - best performance .
table 3 shows the performance of our method compared to the best previous state - of - the - art approaches . our method outperforms all the baselines except sst2 and sst5 by a large margin . it achieves a c - score of 90 . 6 % and a sick - r score of 87 . 6 % , both of which are significantly higher than previous state of the art methods .
table 3 shows the performance of our supervised learning model compared to the best state - of - the - art supervised learning models on the loc and misc datasets . our model outperforms all the baselines except for τmil - nd by a large margin . the performance gap between supervised learning and supervised learning is small , but it is significant enough to show that it is possible to improve upon the performance by using supervised learning with the right combination of training data .
table 2 : results on the test set under two settings . the results are shown in table 2 . name matching and supervised learning achieve the best f1 scores , while τmil - nd achieves the highest f1 score . supervised learning achieves the best overall score , and achieves the most consistent results with respect to all metrics . in particular , the model with the best performance is the one with the least training data , which shows that the model trained on the training data alone is more than sufficient to achieve the task .
table 6 shows that g2s - gat outperforms all the baselines except the gin baseline by a large margin . table 6 : entailment ( ent ) and ref ( ref ) are the most important factors in the performance of the models . the results are shown in table 6 . we see that the model with the best performance is the gat baseline , followed by the gns - ggnn baseline , which has the best generalization ability . finally , we see that models with the worst performance are the ones with the least amount of data .
table 3 presents the performance of our model compared to previous state - of - the - art models on the ldc2015e86 and ldc2017t10 datasets . the g2s - gat model outperforms all the baselines except the s2s model by a large margin . it achieves the best results on both datasets , surpassing the previous best state of the art models by a significant margin . we note that the performance gap between our model and the best baselines is narrower than that between the best baseline and the worst baseline , indicating that our model is more suitable for low - resource settings .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model outperforms all the previous models on the test set by a large margin . it achieves a bleu score of 32 . 23 % on the ldc dataset , which is higher than the previous best state - of - the - art model .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are summarized in table 4 . the results show that the combination of bilstm with get improves the performance of the model by 4 . 6 % compared to the baseline model . moreover , the size of the models improves by 3 . 6 % .
table 3 shows that the g2s - gin model outperforms all the baselines except gat and gat - ggnn in terms of average sentence length and average number of words per sentence . moreover , the model performs significantly better than the gat baseline on all metrics except for sentence length , indicating that it is more suitable for the task .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence as well as in the missing sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . g2s - ggnn outperforms s2s in terms of both fraction of tokens in output ( added ) and miss ( fraction of elements missing in generated sentence ) by a large margin . table 8 also shows the performance of the models with different derivation schemes . the model with the best derivation scheme is the one with the highest percentage of tokens missing ( gin ) . the models with the worst performance are the ones with the least amount of tokens .
table 4 : sem and pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . the pos and sem tags are significantly better than the previous state - of - the - art models on the smaller corpus , indicating that these features are well - adapted to the task .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the unsupervised embeddings outperform word2tag in terms of most frequent tags and the upper bound encoder - decoder accuracy .
table 3 shows the performance of our model with respect to the tagging accuracy metric on the pos and sem metrics . our model obtains the best performance on all metrics , with the exception of the pos metric , which shows that our model can improve upon previous state - of - the - art models by a significant margin .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . the uni encoder achieves the best performance with respect to pos , while the residual encoder obtains the second - best performance . the bi encoder shows the best overall performance .
table 8 : attacker ’ s performance on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ' s accuracy . for pan16 , the difference is less than 0 . 1 % ( p < 0 . 001 ) . for pan17 , it is more than 2 % . for the pan16 training set , we see that the attacker performs better than the adversary on all three datasets .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . the pan16 model outperforms all the baselines on all metrics except for gender and age . for example , pan16 achieves the highest accuracy on the dial and sentiment task , and the highest recall on the mention task , both for gender - specific and age - specific datasets .
table 2 : protected attribute leakage : balanced & unbalanced data splits . the results are shown in table 2 . as shown in the table , the difference between the unbalanced and balanced data splits is small , but still significant . the difference between balanced and unbalanced task splits is much larger than in the case of gender , indicating that there is a need to consider the impact of these data splits in the design of future models .
table 3 : performances on different datasets with an adversarial training . as shown in table 3 , the difference between the attacker score and the corresponding adversary ’ s accuracy is less than 0 . 5 % for pan16 and pan16 , indicating that the training data are more suitable for training adversarial tasks . however , the performance on pan16 is still significantly worse than pan16 .
table 6 : accuracies of the protected attribute with different encoders . the rnn encoder performs better than the guarded encoder on the leaky and leaky embeddings , indicating that rnn is more suitable for the task .
table 3 shows the performance of the models with different finetuning schemes . the models with the best performance are the ones with the highest finetune scores . these include the lstm and lrn models , as well as the work of yang et al . ( 2018 ) and sru ( yang et al . , 2018 ) . the performance of these models is reported in table 3 . table 3 presents the results of our model with the finetune - adapted wt2 base and the wt2 + finetune models . it can be seen that these models perform better than previous state - of - the - art models with more training data . moreover , these models outperform the state of the art baselines with a large margin .
table 3 shows the performance of the baselines on the lstm and gru models . the results are presented in table 3 . as shown in the table , the gru model achieves the best results with a time - to - performance ratio of 2 . 5x the base time and a time to completion ratio of 3x the previous best state - of - the - art model . table 3 also shows the results of rocktäschel et al . ( 2016 ) and ( 2016 ) . the performance gap between the best state of the art models and the best baselines is small , but still significant .
table 1 presents the results of zhang et al . ( 2015 ) and zhang et al . , ( 2015 ) . the results are summarized in table 1 . table 1 shows the performance of our model compared to previous state - of - the - art models . our model outperforms all the baselines except the lstm by a large margin . the performance gap between our model and previous state of the art models is small , but it is still significant .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task . the gnmt model outperforms all the baselines except olrn and sru by a large margin . the difference between gnmt and other baselines can be explained by the small size of the training set and the small number of training steps on the tesla p100 . lrn outperforms the models by a significant margin , but is still less than gnmt , sru and atr . table 3 also shows that the difference between lrn and gnmt is less pronounced when the model is trained on the newstest2014 dataset compared to the original dataset .
table 4 : exact match / f1 - score on squad dataset . table 4 shows the results of the model with the parameter number of " params " and " elmo " and the number of parameters for each parameter . the model achieves the best f1 score with an absolute improvement of 2 . 41 % over the previous state - of - the - art lstm model . moreover , the model obtains the highest match rate with a f1 of 7 . 14 % compared to previous state of the art models with the same set of parameters . lrn outperforms all the other models with a large margin .
table 6 shows the f1 score on conll - 2003 english ner task . the lstm model achieves the best performance with an f1 of 90 . 56 on the ner test set . lrn and gru achieve the highest f1 scores . table 6 : the model performs better on the english nert task than any other model with the same number of parameters .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn outperforms elrn and glrn on the snli and ptb tasks with base setting .
table 1 compares the performance of human and machine learning models with the best performance on the word embeddings . the results are shown in table 1 . the average number of words per sentence is significantly higher for human models compared to systems using oracle retrieval than for systems using mtr . table 1 also shows that when using the mtr feature , the performance gap between human models and systems is much smaller . when using the oracle feature , however , the difference between human and system models is much larger . this is evident from the difference in the r - 2 and r - 4 scores .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 6 % ( candela et al . , 2018 ) and the second highest is 2 . 3 % ( h & w hua and wang , 2018 ) . table 4 also shows the percentage of evaluations a system receives in the top 1 or 2 for overall quality . table 4 : the average number of evaluations for each human evaluation is reported in table 4 . top - 1 / 2 : % of evaluations obtained in the human evaluation , and the percentage obtained by the automatic system in the second evaluation . the results are shown in bold . the best results are reported in the table 4 .
table 3 shows the performance of the models trained on the ted talks dataset compared to those trained on europarl . the results are shown in bold . the results show that the model trained on ted talks outperforms both the baselines in terms of p < 0 . 05 and r = 0 . 5 , while the results are slightly worse than those on the other baselines . table 3 also shows that the performance gap between en and ted talks is small , but still significant .
table 3 shows the performance of our approach compared to the baselines on the ted talks dataset . the results are shown in bold . the results show that our approach outperforms the previous state - of - the - art baselines by a significant margin . europarl outperforms all baselines except the ted talks dataset by a margin of 2 - 3 % on average .
table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms all the baselines except ted talks by a significant margin . the results are shown in table 3 . we show that our model outperforms the previous state - of - the - art baselines by a large margin . the difference between en and europarl is less pronounced , but still significant .
table 3 shows the average and maxdepth scores of all the metrics for each dataset . europarl outperforms all the baselines except for the maxdepth metric by a large margin . the average depth of the dataset is 11 . 05 % lower than that of other baselines , but still significantly higher than most baselines . as shown in table 3 , maxdepth and average depth are the most important metrics for baselines to achieve high cohesion scores . also , the number of roots and number of number of rels used by a dataset are important for the model to achieve the best cohesion score .
table 3 shows the average and maxdepth scores of all the metrics for each dataset . europarl outperforms all the baselines except for the maxdepth metric by a large margin . the average depth of the dataset is 9 . 43 % higher than the mindepth metric , which indicates that the dataset has better cohesion . also , the average number of roots per row is higher than that of any baselines , indicating that the data are more compact .
table 1 : performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . we show that the enhanced version of the model ( lf ) outperforms the baseline model ( p1 ) by a significant margin . moreover , the enhanced model performs better on question type , answer score sampling , and hidden dictionary learning , as shown in table 1 . the difference in performance between baseline and enhanced model is less pronounced , but still significant .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . the model with p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . the models with p1 and p2 are significantly better than models with only p1 , indicating that p2 alone is more effective than p1 alone .
table 5 : comparison on hard and soft alignments . hmd - f1 and hmd - recall outperform wmd - bigram and wmd - unigram on both soft and hard alignments , respectively . the results are shown in tables 5 and 6 . we observe that the hmd prec model outperforms the hmd recall model on both sets of datasets . moreover , hmd recall outperforms wmd bigram and bert on both datasets , indicating that the model is more suitable for the task at hand .
table 3 shows the performance of the baselines on the direct assessment and sent - mover datasets . the average bertscore scores are significantly higher than those of meteor + + and ruse ( * ) on both datasets , indicating that the training data are more suitable for the task at hand . table 3 also shows that bert score - f1 is significantly higher on some datasets than on others .
table 3 shows the performance of bertscore - f1 , bleu - 2 , meteor and sfhotel on the sent - mover task . the results are presented in table 3 . as shown in the table , the baselines are significantly better than the bert score on the smd task , and the w2v task is significantly worse than the baseline on both datasets . also , the performance gap between bert and smd is much larger on smd than on the other datasets .
table 3 shows the performance of the word - mover task using the baselines for leic , spice , meteor , and bertscore - recall . the results are shown in bold . word - mover is the most important metric , followed by recall , while the average number of iterations is the second most important . as shown in table 3 , the performance gap between leic and the other baselines is small , but it is still significant .
table 1 shows the performance of the models with and without para - para . the results are shown in bold . para - based models outperform models with the same meta - classifier by a large margin . moreover , the performance gap between m0 and m6 is much smaller with the model with the least amount of para compared to the model without .
table 3 shows the transfer quality and semantic preservation scores of all the models trained on the yelp dataset . the results are shown in bold . table 3 compares the performance of models trained with the best transfer quality with models trained only with the worst transfer quality . it can be seen that the best models are those trained only on datasets with transfer quality less than 50 % of the average number of instances , and those trained on datasets that have more than 50 instances .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are shown in table 5 . the accuracy of the summaries is 94 % , the percentage of machine and human judgments that match is 84 % , and the human ratings of semantic preservation are 0 . 67 .
table 3 shows the performance of the models with and without para - para . the results are presented in table 3 . para - based models outperform models with both meta - and lexical features . for example , the model with para + para outperforms models with only lexical feature - rich training data by a large margin . the performance gap between m0 and m1 is less pronounced when the model is trained with both lexical and syntactic features .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the best models ( fu - 1 and multi - decoder ) outperform the best unsupervised models ( yang2018 , yang2018unsupervised ) in terms of acc , but are still worse than the best trained models . as shown in table 6 , the training data for both sets of models is comparable , but the performance of the trained models is lower than the untransferred ones , indicating that training data alone is not enough to improve the accuracy of the model . adding the classifiers improves the accuracy by a large margin , but only marginally .
table 2 : percent of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of tokens in a sentence divided into disfluency and repetition tokens , and the average number of repetition tokens . the average length of the disfluencies is the fraction of tokens that are correctly predicted to be disfluent . as shown in table 2 , the length of disfluences decreases with repetition , while the average length increases with rephrase tokens . this indicates that the disfuncions caused by repetition are less severe than those caused by rephrase .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair contexts , or in neither . the average number of tokens per disfluency prediction is 0 . 83 ( 50 % ) for reparandandum and 0 . 58 ( 52 % ) for repair contexts . table 3 also shows the percentage of tokens that belong to each category .
table 3 shows the performance of the models with different combinations of text and innovations . the results are summarized in table 3 . we see that the model with the best performance is derived from the combination of text + innovations and the best test set , while the model using only the raw data is the only one that achieves the best result . moreover , we see that when the model is combined with innovations , it performs better than the single model .
table 2 : performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . word2vec embeddings outperform self - attention and rnn - based neural networks in terms of accuracy . the average of the average number of instances where a sentence is agree or disagree is significantly higher than the average of instances when it is disagree , indicating that the model is better able to learn new sentences . also , the accuracy of the embedding algorithm is higher than that of rnn , suggesting that it is better at learning new sentences faster .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . attentive neuraldater outperforms the previous state - of - the - art models on both datasets . ac - gcn and maxent - joint models significantly outperform all previous state of the art models , and outperform the original burstysimdater model on the nyt dataset . moreover , the joint model outperforms both the original and the adaptive neuraldater . as shown in table 2 , the unified model performs significantly better than any of the previous methods .
table 3 : accuracy ( % ) comparisons of component models with and without attention . this results show the effectiveness of both word attention and graph attention for this task . neuraldater outperforms the oe - gcn model with both word and graph attention . the accuracy of the neuraldater model is significantly higher with graph attention compared to with word attention .
table 3 shows the performance of different approaches on the 1 / 1 , 1 / n , and 3 / 10 epochs . the model with the best performance is the jrnn model , followed by the dmcnn model . the models with the worst performance are the cnn model and the jmee model . the performance gap between the two approaches is small , but still significant . the difference in performance between cnn and the other approaches can be seen in table 3 . as shown in the table , the difference between the average performance of all approaches is less than that of any other approach . however , the performance gap is much smaller between the cnn and jmee models , which shows that the model with more training data is more suitable for the task at hand .
table 1 presents the results of cross - event event detection . the results are presented in table 1 . cross - event event detection outperforms the state - of - the - art on all three domains . table 1 shows that event detection is a relatively simple task , with only a slight performance drop compared to the state of the art . as the results show , event detection on all domains is relatively simple , with the exception of event classification , where the performance drop is much larger .
table 3 shows the performance of different approaches for english - only , spanish - only and french - only languages . the results are summarized in table 3 . the average number of tokens per second and test wer for each language is reported in the table . as expected , the results of all approaches show that the best performance is obtained by using the best subset of the lexical features of each language . however , when using only the vocab - based embeddings , fine - tuned models outperform all the other approaches except for the case where the model with the best clustering performs best , i . e . when using both the original and the shuffled - lms . moreover , the performance drop from the best to the worst is less pronounced when using all the features of the original language .
table 4 : results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned model outperforms the cs - only model on both sets . the results are shown in table 4 . fine - tuned models outperform the cs model with a large margin . in particular , fine - tuned models perform better than cs models with 25 % training dev and 50 % test dev compared to full train dev .
table 5 : accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) and fine - tuned ( fine - tuned - lm ) . the results are shown in table 5 . the fine - tuned - disc model achieves the best performance on both sets , with an absolute improvement of 3 . 53 % over the original model on both test sets .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows significant improvements in precision and recall compared to the baseline model .
table 5 : precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined model shows significant improvements in precision and recall over the baseline model , indicating that the model can be further improved with the addition of additional gaze features to improve recall .
table 1 : results on belinkov2014exploring ’ s ppa test set . syntactic - sg and glove - extended embeddings outperform ontolstm - pp on the same test set , but the difference is less pronounced on wordnet 3 . 1 . the difference between hpcd ( from the original paper ) and the original model ( from faruqui et al . ( 2015 ) is less than 0 . 5 % on the wordnet test set ( table 1 ) . the performance gap between the two models on the original and the second test set is smaller on the second set , indicating that the performance gap is narrower on the former set , and larger on the latter .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . hpcd ( full ) and ontolstm - pp ( partial ) outperform both the full uas and the original uas by a large margin . also , the results are slightly better than the results obtained using the original lstm model .
table 3 : effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . the effect of removing the context sensitivity decreases the ppa acc . score of the model , but does not affect the accuracy .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . subsfolded subtitle data significantly improves the bleu % score for multi30k and en - de datasets , but decreases the score for en - fr and mscoco17 datasets . adding domain tuning also improves the performance of the ensemble - of - 3 dataset , but does not improve the overall performance . as shown in fig . 2 , when domain tuning is added to the ensemble of 3 datasets , the performance drops significantly , but only when using subtitle data is used .
table 3 shows that domain - tuned h + ms - coco models outperform the baseline model with a margin of 2 . 5 - 3 . 5 % improvement over the baseline h + hoco model . the difference between en - de and en - fr baseline is less pronounced , but still significant , and the difference between subs1m and subs2m is larger than the baseline . domain - tuning improves the performance of both models , but only marginally .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best 5 captions improves the performance of the model by 3 . 7 % compared to using only en - de captions . the model with marian amun ( marian amun et al . , 2017 ) achieves the best performance . multi - tasked models with automatic captions achieve the best results . as shown in table 4 , the model with the best five captions achieves the highest performance .
table 5 : comparison of strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the encoder and decoder strategies outperform the en - de and mscoco17 strategies in terms of bleu % score . however , the encoder performs better than the decoder , as shown in fig . 3 . the difference between en - fr and ende is less pronounced than that between enc - gate and dec - gate , but the difference is still significant . the decoder outperforms the ende and encoder , and the difference between the w and w - score is less significant than that of encoder + decoder . this indicates that the decoding strategies are more suitable for large datasets .
table 3 shows the results for en - fr and en - de ensembled models . the results are summarized in table 3 . multi - lingual models outperform the text - only models on average by a large margin . for example , the ms - coco model outperforms the subs3m model by a margin of 3 . 86 % on average . sub - 3m models with multilingual features perform better than those with only visual features . however , the results are slightly less pronounced for multilingual models , as shown in fig . 3 .
table 3 shows the performance of the three approaches for yule ’ s i , ii , iii and iv . the results are summarized in table 3 . as expected , the models using en - fr - ht and en - es - ht outperform all the baselines except for enfr - smt - back . however , the results are slightly better than the baseline models for the second set of models , as shown in fig . 3 . the difference between the baseline and the final results is small , but still significant .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the average number of sentences in each split is 1 . 467 , 489 , and 1 . 487 , 487 , respectively , for english and french .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the average number of words for each language pair is shown in table 2 . for english , it is 113 , 132 , for french , 131 , 104 and 168 , 195 , for spanish , and 113 , 692 , respectively .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems outperform the standard rev system in terms of bleu , ter , and rnn - rev scores . however , the performance gap between the two systems is narrower than expected by chance . the performance gap is less pronounced between the standard and unsupervised systems , but is still significant .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model trained on the same dataset . rsaimage achieves the best recall @ 10 % with a mean mfcc of 0 . 8k . the vgs model achieves the highest recall with a recall of 17 . 2 % .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled u is the second - generation audio2vec - u model . the results are shown in table 1 . the vgs model achieves the best recall @ 10 % with a mean mfcc of 1 . 414 and a median recall of 0 . 9 % . it achieves the highest chance score with a chance of 3 . 9 % to match the rsaimage .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . the cnn classifier turns in a screenplay that is at the edges and the edges of the screenplay , while the rnn classifiers turns on a on ( at the edges ) and on ( in the the the edges ) . the dan classifiers turn in scripts that are in the center of the screen , while rnn turns in ones that are at edges . we report additional examples in table 1 . as the table 1 shows , the cnn classifiers are easier to train than the original rnn , but more difficult to train .
table 2 : part - of - speech ( pos ) changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the percentage of occurrences for each part of the sentence that have increased or decreased as a result of fine tuning . the rnn model outperforms the rnp model by a large margin . the difference between the two models is less pronounced for nouns , but larger for verbs and adjectives . for adjectives , the difference between rnn and rnp is smaller , but still significant . for verbs , rnn shows a larger improvement .
table 3 : sentiment score changes in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the results are shown in table 3 . for positive sentiment , the rnn model shows a significant increase in the sentiment score . for negative sentiment , it shows a slight decrease .
table 1 compares the performance of pubmed and sst - 2 with corr and pubmed . the results are shown in table 1 . the results show that pubmed outperforms corr by a large margin . corr outperforms pubmed by a significant margin .
