table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the training set size and the number of instances for each training and inference set is the most important factors in the model performance , as shown in table 2 . with a training size of 25 , the recur and fold approaches yield similar performance on inference and training , but with a bigger difference in performance in terms of training instances , since the folding approach requires more data and time to train , making it more suitable for production use . on the inference set , the iter and recur approaches yield comparable performance , but the larger size of the data set makes it less suitable for training . finally , the training data size of 10 gives a significant performance boost , since it eliminates the reliance on concatenated training instances .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . also , the size of the balanced dataset decreases as the training set grows , meaning that the model performs less well in the more balanced setting .
results in table 2 show that the max pooling strategy consistently performs better in all models with different representation configurations , confirming the value of the dropout prob . in the selection of the best filtering size and the number of tokens for each representation . additionally , the model performs better when trained with softplus instead of softplus , indicating that softplus is a better complement to softplus in the production setting . finally , we see that selecting the best activation func . gives a significant performance boost when training with sb as well as conll08 . these results show that selecting only the features with the highest f1 scores helps the model to learn more about human judgement in the task at hand .
table 1 shows the results for relation types without and with sdp in the best f1 models . we show that using the shortest dependency path on each relation type bridges the gap between the performance of the macro - averaged model with and without sdp . further , it helps the model to converge further with the current state - of - the - art model across relation types .
consistent with the observations by vaswani et al . ( 2017 ) , we report the results of models trained on pascal - voc on the unlabelled test set in table 3 . the results are shown in bold . for brevity we only report results for r - f1 and f1 while ignoring the effect of y - 3 on the other two metrics . in general terms , the results appear to be consistent with what we expect from a single model trained on a single dataset : the model performing best on the two metrics with the same training set is the one trained on the larger corpus .
table 3 presents the results for english and german for english . the results are presented in table 3 . for english , we see that mst - parser achieves 50 . 69 % and 50 . 59 % accuracy on average compared to the previous best state - of - the - art systems on both sets .
for the two indicated systems , we report the mean and average c - f1 scores of the best performances for each paragraph level and the average number of paragraphs per paragraph for each system . note that the mean performances are lower than the majority performances over the runs given in table 2 . the results are shown in table 4 .
table 3 shows the results for tgen and sc - lstm trained and tested on the hidden test set of relis . the results are shown in bold . relis outperforms both the original and the cleaned tgen model on every metric except bleu , indicating the advantage of redundancy removal in the low - supervision settings . also , relis performs better than tgen − and tgen + when the training data is cleaned .
table 1 shows the comparison of the original e2e data and the cleaned version for the original train and test sets . the difference in overall statistics between the two sets is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . for the test dataset , the difference is much larger , about 2 . 5 % compared to the original . in the train dataset , we see a significant increase in the number of distinct mrs and total number of textual references , which indicates that the training data is more useful for this task . the difference is less pronounced for dev , but still significant .
table 3 shows the results for tgen and sc - lstm trained on the hidden test set of nist . the results are shown in bold . from left to right , we can see that original and original tgen models perform comparably to each other , with the exception of the case of tgen + which is slightly worse than tgen − . adding the correct answer for each error causes a significant drop in performance between the original and the original model , as shown in the next table .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . it can be seen that the cleaned tgen instances are much more error - free than the original ones , which shows the extent to which the language modeling approaches can be improved with a reasonable selection of the correct values from the training data .
table 3 presents the performance of our model on the hidden test set of tree2str + graphlstm with respect to different classifiers . all the models show significant performance drops relative to the previous state - of - the - art models on this data set when trained and tested with the same set of data . for example , tree2str ( fancellu et al . , 2016 ) achieves a performance drop of 2 . 9 % compared to parallelism with the previous best state - ofthe - art model , while reaching a new high of 25 . 6 % on the seq2seqk dataset . relative improvements are also seen on the snrg dataset , which shows significant performance gains relative to pbmt with a drop of 4 . 4 % in performance compared to snrg with a similar ensemble .
table 2 shows the performance of our model with respect to amr17 . our ensemble model achieves 24 . 5 bleu points better performance than the previous state - of - the - art models with a comparable number of parameters . we show that our dcgcn model is comparable in performance to the best previous models with the same set of parameters across all three scenarios .
table 3 presents the results for english - german , czech and slovak , compared to english - czech . the results are presented in table 3 . we show that for both languages , our proposed method significantly outperforms the previous approaches in terms of average number of tokens per sentence , with the exception of german , where it is slightly better than birnn .
table 5 shows the effect of the number of layers inside dc on model performance . as the results of adding more layers inside the model shows , the performance of the model decreases as a result of more data being added in the model .
table 6 shows the results for baselines and residual connections for the gcns with residual connections compared to the baselines with rc and la in combination . the results are presented in bold . rc denotes residual connections and refers to the part of the gcn with which the model is derived from the supporting documents . adding rc information helps the model to improve its performance relative to both the baseline and the residual connections .
table 4 presents the results of models trained on the hidden test set of pascal - voc in the distractor and fullwiki setting . the results are shown in bold . our proposed model outperforms all the state - of - the - art models except for dcgcn in terms of overall performance .
table 8 shows the ablation study results for density of connections on the dev set of amr15 . we show that removing the dense connections in the i - th and iii - th blocks reduces the overall density of the connections , as shown in table 8 .
table 9 shows the results of an ablation study for modules used in the graph encoder and the lstm decoder . we used the language modeling approach described in section 4 . 2 . 4 and the data augmentation technique described in table 9 . the results of " - linear combination " compared to " - global node " show that the model performs better when using only one type of data structure , namely , when the global node is combined with a domain - aware attention layer . however , when using both languages as ensembles , the performance drops significantly .
table 7 shows the performance of initialization strategies for various initialization tasks on probing tasks . glorot initialization is particularly beneficial for data augmentation since it reduces the repetition rate and allows more data to be added to the objnum during initialization . subjnum initialization is beneficial for scalability as well , as it reduces repetition and gives a better generalization performance . finally , it helps the model to learn more about the relations between concatenated entities .
table 3 presents the results for the h - cbow and h - cmow models . the results are shown in bold . as can be seen , the smaller size of the cbow / 400 and the relative lower precision of the two models mean that the accuracy of these models is impacted by different factors . for example , in the case of the former , the accuracy drop is much lower than that of the latter .
table 3 presents the results of models trained on the stacked test sets of sst2 , sst5 , and sts - b . hybrid models outperform both monolingual and multi - task learning models . the results of cbow / 784 outperform cmow and cmp . ow in all but one of the tests except for the one where it is trained on sick - r .
table 3 shows the performance of our models on these unsupervised downstream tasks . hybrid outperforms both monolingual and multi - task modes , showing that the performance gain comes from a better model design in the low - supervision settings . as expected , in the more supervised settings , cbow shows a significant performance gain relative to cmow , while in the balanced setting , it shows a smaller performance gain . with respect to sts13 and sts16 , the cbow model shows a slight performance drop relative to hybrid , but still represents a significant gain in performance . we conjecture that this is due to the larger size of the training set in which these tasks are contained and the relatively high overlap between the training instances in which the models are tested .
table 8 shows the evaluation results for initialization strategies on supervised downstream tasks . glorot outperforms the best previous approaches with a large margin . its performance on subj and mrpc datasets indicates that it has the advantage of training on a larger corpus of data with fewer errors , making it more suitable for production use .
table 6 shows the performance of each method for different training objectives on the unsupervised downstream tasks . the cbow - r model outperforms the cmow model in all but one of the four cases .
the results are shown in table 3 . we can see that the compactness of our system is high relative to the competition across all three metrics , with the case of subjnum being particularly pronounced in the deep layers . topconst is the most stable of the three and closely matches the performance of the other two .
the results are shown in table 3 . the best performances are obtained by cbow - r and cmow - c . these models outperform the best state - of - the - art methods on all datasets except sst2 and sst5 .
table 3 shows the test bias scores for all loc and misc datasets , in addition to τmil - nd . in general terms , the results are presented in table 3 show that supervised learning is more effective than naive supervised learning in all but one case . name matching and entity prediction are the most difficult aspects of the task for naive models to solve , as expected , for supervised learning , precision is relatively high while in case of misc , it is low . precision is relatively consistent across all data types with the exception of loc , where it is lower than the others .
results on the test set under two settings are shown in table 2 . name matching and named entity recognition are the most difficult aspects of the task for model 1 and model 2 , respectively , since the training data are small and the f1 scores are low . however , when trained with only one type of name matching algorithm , the model performs well in both settings . supervised learning under the best performing model γmil - nd achieves the best results with a f1 score of 43 . 57 in the training set and 35 . 42 in the final test set . with the additional training data augmentation , model 2 gets a better performance than model 1 .
table 6 presents the results of models trained on the hidden test set of s2s in the unsupervised setting . the results are shown in bold . g2s - gat shows much better performance than the model trained on snowboard in terms of both ref and concatenated keyphrases . it is clear from table 6 that the model developed with the best model has the advantage of training on a larger dataset . moreover , the model is more than 4x more likely to converge with the pre - trained model when trained on a single dataset .
table 3 summarizes our results on the hidden test set of bleu and meteor in the distractor and golbeck et al . datasets . the results are shown in bold . g2s - gat shows marked performance improvement over the previous state - of - the - art models on both datasets with a gap of 10 . 32 ± 0 . 03 points compared to the previous best state - ofthe - art results .
table 3 shows the model performance on the ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model significantly outperforms the best previous models across all 3 domains with a gap of 10 . 60 % in bleu score .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model outperforms the previous state - of - the - art models in terms of precision on both metrics with a large margin .
results are shown in table 1 . we observe that the g2s models with the shortest sentence durations outperform the models with longer sentences . for example , s2s - gin shows a reduction of 2 . 51 % in sentence length and a gain of 9 . 42 % in average number of words over the baseline model . the model with the highest average sentence length outperforms all the other models with a gap of 3 . 18 % in terms of overall performance .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when gold is added to the input graph for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s outperforms g2s - gat in terms of both the miss metric and the overall effect on the model performance , confirming the viability of parameter sharing in low - supervision settings .
table 4 shows pos and sem accuracy for the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , pos tagging accuracy is relatively high compared to sem , showing that the semantic features extracted from the 3rd nmt encodings are useful for target languages . additionally , semantic features contribute significantly to the overall model ' s generalization performance as well .
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . word2tag classifier outperforms both mft and unsupemb using unsupervised word embeddings and the upper bound encoderdecoder . pos is the most frequent tag while sem is the second most frequently tagged . with these baselines in mind , we can further calculate that word2tag has 96 % accuracy in pos and 91 . 41 % in sem .
table 4 presents the system ' s performance on the hidden test set of pos tagging accuracy and sem tagging accuracy . the results are presented in table 4 . as can be seen , the system performs well on both datasets with different difficulty levels . for example , on the pos dataset , it achieves 8 . 28 % higher accuracy on average compared to the previous best state - of - the - art system with a gap of 2 . 36 % in accuracy with respect to semantic features . this indicates that the semantic information injected into the model by the additional cost term has a significant impact on the performance of the model .
table 5 shows pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . as the results show , the bi - sem model can further improve with the addition of additional bi - nmt features as required by the expansion of the data set .
table 8 shows the performance of the attacker in dfgn on different datasets for different protected attributes . results are on a training set 10 % held - out . as shown in the table , the diversity of the protected attributes seem to have little effect on the attacker performance , indicating that all the features discussed in table 8 are useful for prediction . however , for pan16 , the difference between the attacker score and the corresponding adversary ’ s accuracy is much larger , showing the challenge of predicting gender - based bias in the human judgement .
table 1 shows the performance of our system for training directly on a single task . our proposed method outperforms all state - of - the - art models across all three gender - based and age - based groups . for example , gender - neutral tweets are more accurate than those containing offensive terms such as " n * gga " and " b * tch " for all but one of the three groups . the racial disparities are most prevalent for the age group , which shows the extent to which gender bias can be overcome by language modeling .
table 2 presents the results of unbalanced and balanced data splits for task prediction using the protected attribute leakage and unbalanced data splits . the results are shown in table 2 . as expected , the gender and racial disparities are most prevalent in the task prediction data , followed by age and sentiment . overall , the results are very similar , with gender and sentiment as the only two groups that show significant differences in task prediction performance .
as shown in table 3 , the adversarial training set size and the average number of tokens for each task are used to calculate the attacker and the corresponding adversary ' s accuracy . as expected , gender - based and racial disparities in performance are the most prevalent underrepresented in these datasets , followed by age and gender . in pan16 , all the tokens for which there is a complaint are assigned a negative label indicating that the user is aware of the issue and has responded to the complaint . finally , the presence of the word " b * tch " in the sentence prediction box underperforms all the other tokens except for age , which shows the extent to which gender bias is aggravated in this dataset .
the results are shown in table 6 . the rnn encoders perform similarly to the naive embeddings when the protected attribute is protected with a guard . however , the difference in performance between leaky and guarded en ( 2018 ) is more pronounced when the guard is applied to the embedded instances . this indicates that the challenge of rnn decoding is in how to distinguish between the true response and negative responses .
table 3 presents detailed results on the hidden test set of wt2 from yang et al . ( 2018 ) and lstm . the results are summarized in table 3 . our model outperforms the previous state - of - the - art models on every metric by a significant margin . on the wt2 base and finetune datasets , it achieves gains of 1 . 36 and 0 . 59 points over the previous best state - ofthe - art model , respectively . in comparison , the number of entries in the lrn dataset is less than half of what the previous models had , making it easier for us to converge on a single model without sacrificing performance . additionally , our model exhibits a significant performance gain over previous models on the finetune dataset , showing that the training data augmentation is beneficial for both the training and the final model .
table 3 shows the turn - level evaluation results for the lstm and the gru models . the results are presented in table 3 . as the results show , the training set size and the number of iterations for each model seem to have little effect on performance , indicating that all the required parameters for the model to perform well are in the range of the human judgement . table 3 also highlights the performance of different combinations of training data for different task types . this model significantly outperforms the previous state - of - the - art models in terms of all metrics except for time .
the results of zhang et al . ( 2015 ) are shown in table 3 . table 3 presents the results for yelppolar err , amapolar time and yahoo time . as the results show , the training set size and the number of parameters used for each training set seem to have little effect on performance , indicating that the training data size and type of parameter are the most important factors in model selection . the performance of this model compared to the previous state - of - the - art models on the three datasets is very similar , with the exception of the fact that it is trained on a significantly larger dataset . this model significantly outperforms the other two baselines in terms of err and time .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task . we trained our model on the newstest2014 dataset , and tested it on the training set of tesla p100 . the results are shown in table 3 . our model obtains the best performance with a case - in - sensitive tokenization . compared to other methods , our olrn model is more accurate and therefore requires less training time to decode one sentence . it also requires less time to train compared to the other methods .
table 4 shows the exact match / f1 - score on squad dataset . the model with the best performance is achieved with the parameter number of elmo in the base set ( + elmo ) and the number of parameters in the parameter set ( + sru ) as the base . as the results show , the combination of these factors helps the model to improve its performance with only a slight drop in performance when using only elmo as parameter number . finally , the model becomes more realistic with the addition of sru and gru as parameter numbers as well as boosting the f1 score from 75 . 41 to 76 . 83 for atr and 75 . 45 to 75 . 79 for gru .
table 6 shows the f1 score of our model on conll - 2003 english ner task . as the results show , all the parameter numbers in table 6 represent a significant improvement over the previous state - of - the - art model in terms of f1 scores . although the number of parameters in question is small , we managed to significantly improve our model ' s performance with the help of a few additional parameter numbers .
table 7 shows the test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . with the base setting , our model obtains 85 . 26 % accuracy and 169 . 81 % perplexity compared to the previous best state - of - the - art model , glrn ( 84 . 72 % ) on snlo and ptb tasks .
table 3 presents the results for english , spanish , french , dutch , russian and turkish . our system outperforms the best previous approaches on every metric by a significant margin . the results are presented in table 3 . table 3 shows the system performance on the word analogy task . word analogy task is very similar on both systems , both for english and for spanish , with the exception of the case of spanish . sentence analogy is beneficial for both systems as it reduces repetition and allows more information to be extracted from word analogies . in addition , it improves the generalization ability of sentence selection .
table 4 presents the results of human evaluation . the best result among all automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that seq2seq is indeed comparable to human evaluation in terms of overall quality . retrieval is comparable with human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , while candela is comparable on syntactic and semantic quality ( k 2000 ) . overall , the system performs well on all three metrics , with the exception of syntactic quality , which is underrepresented in table 4 . it is clear from table 4 that the human evaluation process has a high impact on the performance of automatic systems .
table 3 shows the test bias scores for the " ted talks " subset and " europarl " subset compared to the " lang " subset . the results are shown in bold . from left to right , we can see p < 0 . 01 , p < . 01 and r > 0 . 005 for " lang " , " sea " and " docsub " . table 3 also shows the results for " europarl " . as the results show , the training set size and type of text used for each dataset seem to have little effect on predictive performance , however , this may be partially explained by the small size of the training data set and the relatively high number of training instances in which we choose to include them in the dataset for evaluation .
table 3 shows the test bias scores for the " ted talks " dataset and " europarl " dataset . from left to right , we can see p < 0 . 01 , r > 0 . 03 and svm scores for each dataset with different feature sets trained on the data augmentation task . the results are shown in bold . for the " ted talks " datasets , we see that our proposed approach outperforms the previous approaches on average , however it has the advantage of training on a larger corpus which results in a lower precision .
table 3 shows the test bias scores for the " ted talks " subset and " europarl " subset compared to the " lang " subset . the results are shown in bold . from left to right , we can see that p < 0 . 01 and r > 0 . 005 for both sets indicates that our proposed method outperforms the previous state - of - the - art approaches on both datasets with two tasks .
from table 1 , we can see that the average depth and the average number of roots per lexical entity are the most important factors in selecting the best features for each dataset . for europarl , maxdepth and mindepth are the only two that contribute significantly to the overall performance , while for df and docsub , it is the biggest difference . from the table , we also see that , for both datasets , the average roots and the number of tokens per metric is relatively consistent , which indicates that the coreference task is relatively straightforward .
from table 1 , we can see that the average depth and the average number of roots per lexical entity are the most important factors in the dsim model ' s performance . for europarl , maxdepth and mindepth are the only two metrics that contribute significantly less than average to the total terms , which shows the extent to which the word " depth " is a factor in selecting the lexical entities for a model . from this group of metrics , the most interesting ones are the roots and the number of tokens per entity , which show that the semantic information injected into the model by the additional cost term is significant .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the enhanced version of lf model outperforms the baseline model in terms of both question type and answer score sampling , indicating that it has better generalization ability . also , it exhibits a noticeable drop in performance when using the weighted softmax loss instead of the binary sigmoid loss , indicating the advantage of using the learned representation of the hidden dictionary in combination with the correct ranking loss . in addition , the model exhibits a significant performance drop when using p1 instead of p1 as in the baseline experiment . finally , it shows that the model is more stable under the weight of the additional training data .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . note that only applying p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . the model with the best performance is the one using p2 as well as the history shortcut in section 5 .
table 5 shows the performance of these models on the test set in the hard and soft alignments . the hmd - f1 model outperforms all the other models except for wmd - bigram , which shows the advantage of using bert training data augmentation . also , it outperforms the wmd - unigram model with a large margin .
the results for " direct assessment " and " sent - mover " are shown in table 3 . the first set shows the results for ru - en , zh - en and ru - tran , compared to " de - en " . the results are broken down in terms of baselines , with meteor + + contributing significantly less than bertscore and ruse ( * ) contributing significantly more than the other two baselines . for " sent - mover " the results are slightly better than the others , but still significantly worse than the baseline . we can further see from table 3 that the dependency distance based baselines and the w2v - based classification scheme are the most important factors in selecting the best baseline for a given task .
the results are shown in table 1 . we show the results for " sent - mover " and " w2v " using the best performing baseline configurations . the results displayed in bold are those of bleu - 1 + w2v with the best f1 scores . when using the " best performing baseline " method , the results are slightly worse than those in " sent " but still superior than " sent " . we can further see that the " best performing " baseline is bertscore - f1 , with an f1 score of 0 . 078 , which indicates that bert is well - equipped to handle the task at hand .
the results are shown in table 1 . word - mover performance on the training set is reported in bold . sentiment is relatively consistent across all metrics with the exception of spice , which shows significant performance drop when using only meteor or bertscore - recall instead of the word - based baselines . we observe that word - mover performance is significantly impacted by the fact that the semantic features of the w2v dataset are relatively small compared to those of the smd dataset , indicating the extent to which the semantic information injected into the models by the additional cost term can have an impact on the model performance .
the results are shown in table 6 . para - para models seem to be more useful than shen - 1 and 2d models for prediction in the low - supervision settings . in general terms , the results are similar for m1 and m2 while the results for m3 and m4 are slightly worse than those for m2 and m3 . further , the performance drop between m0 and m6 as a result of the additional syntactic and semantic information in these models is much larger than in those without .
table 3 shows the results for english , spanish , french , dutch , russian and turkish for the comparison set . the results are presented in tables 3 and 4 . semantic preservation and transfer quality are the most interesting ones , while fluency is the less interesting one . we show the results of models b and c with different transfer quality scores for english and spanish . in general terms , we see that yelp significantly outperforms the other two baselines in terms of transfer quality .
table 5 presents the results of human sentence - level validation for each metric for validation of acc and pp . as table 5 shows , the human evaluation method verifies the accuracy of the summaries , and the human ratings of semantic preservation as well as the fluency of the tokens . however , it does not generalize well across all metrics , which indicates the challenge in human evaluation .
the results are shown in table 6 . para - para models outperform the models with different syntactic and semantic features in terms of generalization . the most interesting ones are m0 [ ital ] + cyc and m6 , which show marked performance gains for both languages . further , the model with the best generalization performance is m6 [ ital ] , which shows the advantage of combining the features of lexical and syntactic features from the pre - training set with those from the training set .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc with the same number of tokens and human references , and their accuracy is comparable to that with untransferred sentences , so the training set size is small but the accuracy is high . with respect to transfer learning , we note that the multi - decoder approach by ng et al . ( 2018 ) achieves the highest acc and multi - classifier approach , so we do not need to rely on the learned rewards from previous work . we also note that combining the learned reward function with the lexical features of delete / retrieve and style embed significantly improves the model ' s performance . finally , adding the classifiers lm and classifier gives a 0 . 8 / 3 . 2 bleuu gain over simple transfer .
in table 2 , we report the percentage of reparandum tokens that were correctly predicted as disfluent for each repetition type and the overall number of repetition tokens for each disfluency type . reparandum length is the average of the number of tokens in a sentence , so repetition tokens alone do not represent a significant part of the model ' s performance . however , for nested disfluencies , their length is significant enough to result in a significant drop in performance . this indicates that the model can learn to reason more intuitively about the nature of some of the most difficult relationships in the past .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparation or repair ( error - function ) or in neither . note that for all but one of these cases , the tokens in question belong to one of the three categories and the rest to the other two . percentages in parentheses show the fraction of tokens belong to each category .
the results are shown in table 1 . in particular , we show the results for the dev mean , average number of iterations per test and best result for each test set . as the results show , the text transformation is beneficial for both datasets with different combinations of features contributing differently to the model performance . for example , in the case of text transformation , the benefits are most pronounced for the early models while the losses are most prevalent for the late models . moreover , for innovations , the results are even more pronounced for text transformation than for the other two types of variants . finally , we see that the model performs best when the innovations are added to the training data prior to the dev change . this indicates that incorporating the features of text and innovations helps the model to improve its performance .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset is shown in table 2 . word2vec embeddings significantly outperform the self - attention and neural network - based approaches , confirming the value of word2vec in the low - resource settings . rnn - based sentences are also comparable with the performance of neural networks in terms of accuracy , however the accuracy is lower than those of cnn - based ones . our model shows marked performance improvement over the previous approaches . it achieves the best results with 28 . 43 % accuracy on average compared to the previous best rnn model of 24 . 53 % .
table 2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater shows marked performance improvement over previous neural dater models on both datasets . ac - gcn also outperforms the other methods with a large margin . maxent - joint is comparable with the best previous model , but does not perform as well on the nyt dataset . attentive neuraldater shows a slight performance drop compared to previous neuraldaters . we conjecture that this is due to the large variation in the training data size between the two datasets .
table 3 compares the performance of neuraldater with and without word attention for this task . with and without attention , our model exhibits significant gains in accuracy . the difference in accuracy between word attention and graph attention shows that neuraldater is more effective in generation of sentence representations with word attention than it is without .
table 3 presents the performance of the models in 1 / 1 , 1 / n and 3 / n word embeddings . embedding + t models perform best in the one - to - x analogy task while argument argument is the most difficult for dmcnn and cnn . trigger and argument are the only two models that perform significantly worse than argument in all but one of the comparison tasks . the performance gap between argument and argument is less pronounced for jrnn , but still suggests that there is a need to refine the model for further performance improvement . we notice that the semantic information extraction task performed best on the cnn dataset is more difficult than the argument extraction task . further improving performance by leveraging the syntactic information available in the training data boosts the joint performance of all the models .
table 3 presents the results for argument identification and event prediction . our proposed method significantly improves upon the state - of - the - art approach in terms of both event identification and f1 . for argument identification , it achieves gains of 2 . 8 points in f1 while in event prediction it gains 4 . 7 points . for event prediction , it gains 2 . 9 points . the results are presented in table 3 . as can be seen , our proposed method is more useful for event prediction than for argument prediction .
results are shown in table 3 . the results are broken down in terms of dev perp , test acc and wer . regularization gives a significant performance boost for english - only and spanish - only languages , fine - tuned and all - shuffled models give a performance drop relative to the original models , as expected , fine - tuning gives a performance boost relative to all the other models except for the one that uses lexical features derived from the vocab dictionaries . finally , the results are slightly better than the results reported in the previous section when using only plain english , all models except the original ones show lower performance on average compared to the other two languages .
results on the dev set and on the test set using only subsets of the code - switched data are shown in table 4 . fine - tuned models perform well over both training sets with different distributions of the training data in the dev and test set . as the results show , fine - tuning reduces the noise in the model during training and upsampling , and thereby increases the general performance .
we can see from table 5 that fine - tuning the dev and test sets improves the performance for both languages for the gold sentences in the set , and for the test set as well . moreover , the accuracy increases for both sets for the monolingual and the code - switched variants . fine - tuned - disc is comparable in both languages with the original tuning scheme .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements in precision and recall from baseline to type combined gaze features are statistically significant ( p < 0 . 01 ) which indicates that the model can further improve its performance with the addition of these features .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for the conll - 2003 dataset as compared to the baseline model using the monolingual gaze features . as expected , type combined gaze features result in a significant improvement in precision and recall as well as f1 scores , which shows that the model can further improve its interpretability with the help of these additional features .
results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . ontolstm - pp with glove - extended tokens gives a 0 . 8 / 3 . 7 / 10 . 3 % boost in overall performance over the original embeddings . with the same number of tokens , the system performs similarly on the word - error level as in the original paper ( table 1 ) . the difference between the two is most prevalent in the syntactic category , which indicates that the semantic information injected into the tokens by the additional cost term has a significant impact .
results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . ontolstm - pp , hpcd and ontopp predictor perform well while rbg does not . with respect to uas performance , we see that onto - pp is comparable to lstm in terms of accuracy with respect to all the pp predictors except for oracle pp . further , it outperforms both onto and oracle pp with a large margin . finally , it achieves competitive oracle accuracy with the full uas system .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the model achieves the best performance with a ppa acc . of 89 . 5 % when only using full context sensitivity .
table 2 shows the bleu % scores for adding subtitle data and domain tuning for image caption translation in the multi30k dataset for flickr16 , 17 and mscoco17 . the results are slightly better than the results with marian amun ( marian amun et al . , 2017 ) with ensemble - of - 3 modeling taking advantage of the multi - domain tuning across the three datasets . as the results show , when domain tuning is added to the subtitle data , the model performs better in both languages . it is clear from table 2 that domain tuning has a significant impact on the model performance .
table 3 shows the results for en - de , en - fr and mscoco17 compared to monolingual models using the best performing h + ms - coco model . the results of domain - tuned models are slightly better than those without , but still significantly worse than those using the language - adaptive model with the same features .
table 4 shows the bleu scores for en - de , en - fr , and mscoco17 for flickr16 , 17 , and multi30k datasets . adding automatic image captions improves the general performance for all three datasets except for those using marian amun . the model with the best captions obtains 62 . 7 bleus and 35 . 2 % overall improvement over the model with only the best 5 captions . further improving performance with the addition of auto - categories improves the results for all but the last dataset .
table 5 shows the bleu % and overall results for visual information integration using transformer , multi30k + ms - coco + subs3mlm and detectron mask surface , encoding and decoding strategies seem to have similar impact on the visual information extraction in flickr16 and 17 whereas for mscoco17 , the effect is less pronounced . further , encoding and dec - gate have completely opposing effects on the performance of the model , as shown in fig . 3 , once the encoder and decoder are combined , the model performs better than either encoder or decoder with a comparable number of frames . finally , when en - de and en - frased models are used , the performance gap between encoder / decoder decreases significantly with respect to both datasets .
we show the results for en - de and en - fr for flickr 16 , 17 and mscoco17 when we switch from the text - only model to the multi - lingual model . the results are shown in table 3 . as the results show , the visual features that contribute most to the model ' s performance are the most important ones for the french - speaking models . overall , the results are very similar across the three models , with the exception of those using ms - coco in the case of " micro - foco " .
table 3 presents the results for english , german , french , dutch , russian and turkish for comparison . the results are presented in tables 3 and 4 . en - fr - ht and en - es - ht achieve relatively high precision on both mtld and ttr datasets , as expected , the performance of these models is markedly different when trained and tested on the unlabelled text of the two languages . translate the sentences from one language to the other and compare the results with the original ones in the unsupervised setting . in most cases , the model performing best on mtld is the one trained on the original text .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . en – fr and en – es contains 1 , 472 , 203 and 499 , 487 sentences , respectively , and 7 , 723 sentences in total , making it the smallest of the 10 language pairs .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . for each language pair , we set up a training sentence for each model and compare the results with the best performing model in terms of src .
table 5 shows the automatic evaluation scores for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems achieve decent performance , but are still significantly worse than the comparably trained systems in ter and bleu . as expected , the ter evaluation scores are slightly higher for the re - env systems , indicating that the translation task is more difficult for the model to learn .
table 2 shows the performance of our model on flickr8k dataset . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled segmatch is the text - supervised model from chrupala2018 . our model obtains the highest recall @ 10 and median rank among all the models .
results on synthetically spoken coco dataset are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the text - supervised model from kutuzov et al . 2018 . in rsaimage dataset , the average recall @ 10 is 1 . 414 , the median rank is 0 . 955 and the chance to predict answer is 3 , 955 . as the results show , acoustic embeddings with multi - task learning models can improve the performance for target models in terms of recall and precision . however , for target model , the performance drop is much larger than for any other model .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . originally , the rnn turns in a screenplay that is roughly balanced at the edges and ends up at the ends , so it is easier to turn on a on ( i . e . , a blank screenplay ) than a screenplay with edges edges edges and curves . in addition , the dan classifiers turn in sentences that are much more difficult to turn in than the original ones . we report further examples in the appendix .
table 2 shows the pos changes in sst - 2 as a result of fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . verbs and adjectives have increased , decreased or stayed the same while the number of occurrences have increased or decreased indicating that the vocabulary size has not changed . predicate part - of - speech ( pos ) refers to the occurrences of words in the speech that are part of the sentence that the model is tested on . rnp shows a significant percentage increase in the coverage of some of the most productive parts of the speech . it is clear from table 2 that the rnp model has learned a lot about the human interpretability of some words .
table 3 shows sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . in the positive sentiment case , sentiment is also increased as a result of the flipped labels being flipped from negative to positive .
table 2 presents the results of the second study . results are presented in bold . our proposed method outperforms the best previous approaches in terms of generalization on both sst - 2 and pubmed , confirming the value of cross - input validation .
