table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative and recursive approaches yield similar performance in terms of training and inference , but the latter has the advantage of using more data and is more suitable for production use .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . in ud v1 . 3 and conll08 models , the dropout probability dropout function is slightly worse than the activation function , but is still better than the sigmoid function . similarly , the activation func is better than softplus , indicating that the selection of the best activation function is a better value for the model , than selecting the worst one .
table 1 shows the effect of using the shortest dependency path on each relation type . with sdp , our model obtains the best f1 ( in 5 - fold ) and the best overall diff . score . with the macro - averaged model , we get the second - best f1 score .
the results are shown in table 1 . our model outperforms the previous state - of - the - art models on every metric by a significant margin . on average , it achieves 100 % on r - f1 and 50 % on f1 , while surpassing the performance of the best previous model by 3 points .
the results are shown in table 1 . our proposed parser outperforms all the base models except mst - parser on every metric by a significant margin . on average , it achieves better than 50 % on average .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference is less pronounced for paragraph level , indicating that lstm - parser is better at parsing paragraph - level sentences .
the results are shown in table 1 . the original tgen + and the cleaned tgen models perform slightly better than the tgen − model , but the former is still inferior to the latter in terms of bleu and nist scores .
table 1 compares the original e2e data with the cleaned version . the difference in mr statistics is significant ( 17 . 5 % vs . 17 . 5 % ) and total number of textual references ( 4 , 862 vs . 42 , 061 ) .
the results are shown in table 1 . the results show that the tgen + variant outperforms the original tgen − variant on every metric by a significant margin . in fact , the difference between the two is more than 2 . 5 points .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set . as expected , there are a significant number of errors ( i . e . missing , missed , wrong values , slight disfluencies ) that can be attributed to the fact that the tgen instances are pre - trained , and that the training data contains a lot of errors .
the results are shown in table 1 . our proposed method outperforms the previous state - of - the - art models on both the internal and external test sets . on the external test set , the graphlstm model ( song et al . , 2018 ) achieves the best performance with an all score of 25 . 9 % and a gap of 2 . 6 % from the previous best performance . similarly , the multi - headed entity model ( dcgcn ) achieves a performance gap of 3 . 2 % and 4 . 4 % with the ensembled model ( tsp ) and pbmt , respectively .
table 2 shows the performance of the ensemble models on amr17 . our model achieves 24 . 5 bleu points , which is slightly better than the previous best performing ensemble model , seq2seqb ( beck et al . , 2018 ) . similarly , our dcgcn model achieves a slight improvement of 2 . 5 points over the previous state - of - the - art ggnn model . gcnseq ( damonte and cohen , 2019 ) achieves 24 points improvement over the best ensemble model and achieves a significant gain of 3 . 7 points over previous state of the art seq2seq models .
table 1 presents the results for english , german , czech , french , spanish , dutch , russian and turkish . our model outperforms the previous state - of - the - art models on all three languages . it achieves the best results on the three languages with a gap of 2 . 8 points from the previous best performance .
table 5 shows the effect of the number of layers inside a network on the performance of our model . we observe that , for example , when we add 6 layers , our model exhibits the best performance . however , this is only true when we only add 3 layers .
table 6 shows that rcn with residual connections outperforms rcn without residual connections . rcn also outperforms the baselines in terms of generalization .
the results are shown in table 4 . we observe that the best performing model is the dcgcn model , with a gap of 2 . 4 points from the previous best model ( dcgcn ) on average .
table 8 shows the ablation study results for the dev set of amr15 . the results show that removing the dense connections in the i - th block significantly decreases the density of connections , and that the model is therefore less suitable for production .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . we used the best performing approach , namely , - global node / linear combination , and - direction aggregation . the results are statistically significant ( p < 0 . 05 ) with respect to both languages , with the exception of the case of dcgcn4 , where the best performance is obtained by applying the best coverage mechanism .
table 7 shows the performance of our initialization strategies on various probing tasks . our glorot - based system outperforms all the other methods except subjnum , which shows that it has better generalization ability .
we observe that the hcmow embedding method is comparable to the best performing cbow / 400 embeddings , and that it achieves state - of - the - art results in terms of concatenation , with a gap of 3 . 6 points from the previous best performance .
the results are shown in table 1 . the hybrid method outperforms the original cbow / 784 model in terms of all three sub - criteria with a margin of 3 . 6 % over the monolingual cbow baseline . it achieves state - of - the - art results on sst2 , sst5 , and sts - b , and sick - r .
table 3 shows the relative improvements on unsupervised downstream tasks that our models have achieved with respect to hybrid . the cbow and cmow have both seen considerable gains over the best state - of - the - art methods ( i . e . cbow + 26 . 5 % on sts12 , sts13 , and sts14 ) while hybrid has seen a significant ( 26 . 6 % ) increase in performance . with respect to sts16 , the gains are less pronounced , but still represent a significant gain of 2 . 6 %
table 8 shows the performance of our initialization strategies on supervised downstream tasks . our glorot - based system outperforms all the other systems except sick - e by a noticeable margin . it achieves state - of - the - art performance on all three sub - topics ( except sst2 ) .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . the best performing method is the cbow - r , with an absolute improvement of 3 . 2 points over cmow - c .
the results are shown in table 1 . we observe that the concatenation of concatenated keyphrases is the most important aspect of the cbow - r model ; however , it is still significantly worse than the cmow model in terms of generalization . subjnum and topconst are the only two groups that consistently show significant performance improvement over the previous models .
the results are shown in table 1 . the best performing method is the cbow - r , which achieves 90 . 6 % overall improvement over the cmow - c baseline .
the results are shown in table 1 . in general terms , our system outperforms all the stateof - the - art systems in terms of all metrics , with the exception of name matching , which is slightly worse than the other systems . in particular , we see that supervised learning is better than all the other methods apart from the fact that it requires more data .
results on the test set under two settings are shown in table 2 . supervised learning ( model 1 ) and τmil - nd ( model 2 ) achieves the best f1 scores , with an f1 score of 71 . 38 ± 1 . 03 and 69 . 42 ± 0 . 59 , respectively , compared to 69 . 03 ± 15 . 38 and 29 . 57 ± 7 . 03 f1 . with the same set of parameters , model 1 and model 2 achieve the best performance with a f1 of 69 . 38 ± 1 . 03 , and a 69 . 43 ± 0 . 55 f1 , respectively .
table 6 shows that the g2s - gat model is comparable to the best state - of - the - art models in terms of both generalization and performance on single - domain tasks . however , it is significantly worse than s2s , indicating that there is a need to design more sophisticated models to improve the generalization performance .
table 3 presents the experimental results on the ldc2015e86 and ldc2017t10 datasets . we observe that g2s outperforms all the base models except s2s , with the exception of konstas et al . ( 2017 ) , which shows a performance drop of 0 . 42 ± 0 . 03 from the previous best state - of - the - art model ( g2s - gat ) .
results on ldc2015e86 test set when models are trained with additional gigaword data . table 3 shows that the g2s - ggnn model improves upon the previous state - of - the - art model by 3 . 23 % in terms of bleu score .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model significantly outperforms the previous state - of - the - art models in terms of bleu and meteor scores .
we observe that the g2s - gin model has the best general performance , with an absolute improvement of 3 . 51 % over the baseline model , and 5 . 2 % more than the average of the other models . moreover , the improvement is larger across all metrics , with the exception of sentence length .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence when the model is added to the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , g2s - gat is comparable to s2s in terms of fraction of elements missing , however it is significantly worse than sggnn .
table 4 shows the pos and sem tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , the pos features significantly contribute more than the sem features .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms unsupemb in both metrics . the difference is most striking for pos , which shows that the encoder decoding encoder is better than the word embeddings encoderdecoder .
table 4 shows the system ' s performance on the four types of test data . our model outperforms the previous state - of - the - art models on all metrics except pos tagging accuracy . it achieves the best results on three of the four metrics .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the residual encoder performs best , while the uni encoder is close to the best in terms of overall performance .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . in general terms , the attacker performs better than the adversary on all three datasets .
table 1 shows the performance of our model with respect to training directly towards a single task . our model outperforms all the stateof - the - art models with a large margin .
as shown in table 2 , there is a noticeable imbalance in the performance between balanced and unbalanced data splits , with the unbalanced task data having the worse performance .
the performance of these models on different datasets is shown in table 3 . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . as expected , the gender - based features contribute the most to the loss of performance . however , age and race feature - values contribute the least .
as shown in table 6 , the rnn encoders are more accurate at decoding the leaked attribute than the guarded ones . the difference is most striking when the protected attribute is named " leaky " .
table 3 presents the results of our final model on the hidden test set of ptb + dynamic , wt2 + finetune , and lrn . the results are slightly better than those of yang et al . ( 2018 ) and lstm ( yuan et al . , 2018 ) , but still significantly worse than the previous best state - of - the - art models . the performance gap between the best performing models is narrower with respect to finetune tuning , but still suggests that there is a significant performance gap to be overcome .
table 3 presents the results of our final model on the base acc and time extraction task . the results show that our lstm model is comparable to previous state - of - the - art models in terms of all metrics , with the exception of the time taken to compute the last parameter .
the results of zhang et al . ( 2015 ) are shown in table 1 . the results are statistically significant with respect to all three domains , with the exception of yelppolar , where the model performed slightly worse than the previous state - of - the - art .
table 3 shows the case - insensitive tokenized bleu score on wmt14 english - german translation task . our model outperforms all the base systems with a large margin . the gnmt neural network ( gnmt ) achieves the best performance with an absolute improvement of 2 . 67 points over the previous state - of - the - art model , while the olrn neural network achieves a slight improvement of 0 . 55 points over gru .
table 4 shows the exact match / f1 - score on squad dataset . the models with the least elmo parameter are slightly worse than those with the mostelmo parameter , indicating that more parameters are required to improve the model ' s performance . further , the models with more parameters perform slightly better than those without . lrn outperforms the lstm and gru in terms of f1 score . sru obtains the best overall performance , however , it is significantly worse than the gru ,
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our lstm model significantly outperforms the previous state - of - the - art models with a gap of 9 . 56 points from the last published results .
table 7 shows the test performance on snli and ptb task with base + ln setting and test perplexity on base setting . the best performing lrn model is elrn , while glrn shows lower performance . as shown in the table , the best performing model is the e - lrn model .
table 1 shows the system and word analogy task learning performance on word analogy tasks . our system outperforms all the other systems with two tasks . retrieving words from word analogies is easier than system and sentence analogy tasks , however , it requires more data and time to learn .
table 4 shows the human evaluation results on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that seq2seq is comparable to human in terms of overall quality . the second best result is obtained by candela ( 30 . 2 % ) on the content richness scale , which shows that it is comparable in syntactic and semantic terms with human . the third best result by a significant margin is achieved by retrieval ( 38 . 8 % ) , which shows the high quality of automatic systems .
table 3 shows the performance of our approach compared to the best previous approaches . our approach outperforms both published and unpublished work on every metric by a noticeable margin . for example , we see that our proposed hclust model outperforms the previous state - of - the - art model on all metrics except lexical similarity .
the results are shown in table 1 . table 1 shows the performance of our system compared to the best previous state - of - the - art systems on the test set . our approach outperforms all the base the results show that our proposed method outperforms the previous state of the art systems on every metric by a significant margin .
the results are shown in table 1 . table 1 shows the performance of our system compared to the best previous state - of - the - art systems on the test set . our model outperforms all the base the results show that our approach is comparable to those of previous models with only one error .
as shown in table 1 , the average depth and maxdepth are the most important metrics for each feature , while the average number of roots is the least important . europarl has the best performance among all the metrics .
as shown in table 1 , the average depth and maxdepth are the most important metrics for each feature , while the depth cohesion is the second most important . for europarl , the difference between maxdepth and averagedepth is less pronounced , but still suggests that there is a need to improve the feature extraction performance .
performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our lf model outperforms the baseline model in terms of both question type and answer score sampling , and hidden dictionary learning , while performing slightly worse on the ranking loss metric .
performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . the model with the best performance is the one with the shortest gap between baseline and p2 .
table 5 shows that the hmd - f1 model performs best on hard and soft alignments , while wmd - unigram and wmd - bigram perform the worst on soft alignment . further , hmd - recall and bert perform similarly on both sets , but do not have the advantage of fine - tuning .
the results are shown in table 1 . the baselines show that bertscore - f1 and ruse ( * ) have the best performance on the direct assessment metric , while meteor + + and w2v have the worst performance .
the proposed bertscore - f1 model outperforms the bleu baseline on all metrics except for the sent - mover metric by a margin of 0 . 012 points . on the other hand , it achieves the best f1 score on the sfhotel metric by 3 points .
word - mover achieved the best results with an f1 score of 0 . 939 on m1 and m2 while bert scored 0 . 866 on m2 . the classification performance of the word - mover is reported in table 1 . we observe that the best performance is obtained with the combination of word - and sentence - based classification .
the results are shown in table 7 . para - para model outperforms all the other models that do not rely on lexical prefixing . as expected , the performance drop is most severe in the lower - middle - of - the - scale settings , where m0 and m0 + para are the worst performing models .
table 3 shows the transfer quality , transfer quality and semantic preservation metrics for yelp , semantic and fluency . the results are slightly worse than those for google translate , indicating that the model with the best transfer quality is more likely to be trained on a more realistic data set . semantic preservation metrics show that the best performing model is likely the one with the highest f1 score .
table 5 shows the human evaluation results for each metric . our approach verifies that the summaries generated by the machine and human are comparable in terms of syntactic and semantic recall , and that the generated summaries match the human ratings of semantic preservation and fluency ( p < 0 . 01 ) .
the results are shown in table 7 . para - para model outperforms all the other models that do not rely on lexical prefixing . it achieves the best results with an absolute improvement of 3 . 5 points over the baseline .
results on yelp sentiment transfer are shown in table 6 . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc with the same 1000sentences , and their best acc is 31 . 4 % higher than the best unsupervised model ( left table ) . the difference is less pronounced with respect to multidecoder , indicating that the multi - decoder approach has better performance .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . reparandum length is the average of the number of repetition tokens over the length of disfluencies , and overall , it represents the average number of tokens over which a disfluency prediction can be made . as the table shows , the more disfluent tokens , the harder it is to predict .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparandum or repair ( error - function ) or in neither . as expected , the content - function - disfluency category has the highest percentage of tokens that belong to each category , indicating that it is particularly difficult to disambiguate between the two types of tokens .
the results are shown in table 1 . we observe that the best performances are obtained when the model is trained with only one type of information type : text + raw + innovations , or text + innovations + innovations . this confirms that incorporating all the information available from the raw data helps the model to learn more about the input context and the task at hand . moreover , it improves the generalization ability of the model , improving from a state - of - the - art single - input model to a model that relies on multiple types of information .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . in table 2 , we compare word2vec embeddings with self - attention , rnn - based and cnn - based neural networks . our model shows considerable performance improvement over the state of the art . it achieves the best average f1 score of 7 . 43 / 10 . 43 and the highest accuracy ( 83 . 43 % ) among all the algorithms .
table 2 shows the performance of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . neuraldater outperforms the previous best performing neuraldater model , ac - gcn , and oegcn by a significant margin . attention - attentive neuraldater is also comparable to the best previous model , and achieves the highest accuracy .
table 3 shows the performance of our neuraldater model with and without word attention . our approach outperforms the best previous approaches with a significant margin .
the results are shown in table 1 . embedding + t achieves the best results , surpassing both cnn and dmcnn in terms of 1 / 1 and 1 / n , while surpassing jrnn in 3 / n and 5 / n . as expected , the argument argument is the most difficult part to solve , since it requires a lot of data and leads to incorrect predictions .
table 1 shows the system ' s performance on event and event prediction tasks . our proposed method significantly outperforms the state - of - the - art on all metrics . on the event prediction task , it achieves the best performance with an f1 of 7 . 7 and f1 - score of 6 . 9 . on argument prediction , it gets the best f1 score of 5 . 8 .
the results are shown in table 1 . we see that the best performing variant is the " shuffled - lm " variant , followed by " fine - tuned - lm " . however , fine - tuned - lm is inferior to all the other variants in terms of dev perp and test wer because it relies on lexical features derived from the vocab - based learner instead of the spoken word embeddings . finally , the " spanish - only " variant is slightly better than " english - only " .
results on the dev set and on the test set using only subsets of the code - switched data . note that fine - tuned models perform better than cs - only models on both sets . fine - tuned models outperform the original cs - trained models in terms of both training and test set performance .
as shown in table 5 , fine - tuned - disc improves the performance on the dev set and on the test set , and upsampling has a generally positive effect ( p < 0 . 05 ) . fine - tuned - disc , however , does not improve performance on either the test or dev set .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f1 ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvements are statistically significant ( p < 0 . 01 ) with respect to the baseline and the f1 score ( p > 0 . 05 ) .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvements are statistically significant ( p < 0 . 05 ) and r > ( r = 0 . 03 ) with type combined features representing a 3 . 5 % improvement over baseline .
results on belinkov2014exploring ’ s test set are shown in table 1 . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on wordnet 3 . 1 , while ontolstm - pp is derived from the original hpcd ( from faruqui et al . ( 2015 ) . syntactic - sg embedding achieves the best performance with an f1 score of 88 . 7 % on the test set . further improving performance by 4 . 8 % over the baseline is achieved with a glosdk - retro embedding of 84 . 3 % . the best performance by far is obtained with the onto + glove extension , reaching 89 . 8 % .
results shown in table 2 show that the hpcd dependency parser ( full ) and ontolstm - pp ( partial ) outperforms rbg with features coming from various pp attachment predictors and oracle attachments . the same is true for onto - lstmp - pp as well as rbg ( partial ) .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a ppa acc . of 89 . 5 % on the full test set .
as can be seen in table 2 , the domain tuning approach improves the bleu % scores for both en - de and mscoco17 , and improves the multi30k embeddings as well . adding subtitle data and domain tuning improves the translation performance for en - fr , as well as for both flickr16 and flickr17 ( by a margin of 3 . 5 bleus ) . however , the improvement is narrower for mscoca17 , as it requires more data and multi - domain tuning .
the results of domain - tuned h + ms - coco are shown in table 1 . we see that the model with the best results is the subs1m model , while the best lm model is the mscoco17 model .
table 4 shows the bleu scores for en - de , en - fr , and mscoco17 , as well as marian amun . adding automatic image captions improves performance for all three models , and for multi30k as well . however , the performance drop is most pronounced for enfr , where automatic captions alone give a performance drop of 2 . 7 points . multi30k is the only case where the performance gain is less pronounced .
table 5 shows the bleu % scores of our encoder + dec - gate and en - de + dec - gate strategies for visual information integration . our encoder + decgate approach achieves the best overall performance , with an absolute improvement of 4 . 38 % over the previous state - of - the - art en - fr model .
we observe that the ensemble - of - 3 approach outperforms subs3m and subs6m in terms of multi - lingual performance when we switch from visual features to text - only features . however , the results are slightly worse when we use ms - coco , indicating that the visual features contribute less than the linguistic features .
the results are shown in table 1 . en - fr - ht and en - es - ht achieve the best performance on the yule ' s i and mtld test sets , respectively , with an absolute improvement of 2 . 5 % and 3 . 7 % over the previous state of the art models , respectively . the same trend is observed for mtld as well .
table 1 shows the number of parallel sentences in the training , test and development splits for the language pairs we used . the en – fr and en – es language pairs have the most parallel sentences , with 1 , 472 , 203 and 499 , 487 sentences , respectively .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the results are statistically significant ( p < 0 . 01 ) with respect to the src and trg .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - smt - rev systems both receive high bleu scores and ter scores , indicating that they are well - equipped to handle the task at hand . however , the en - de - rev system is less than capable of performing at the state - of - the - art on rev tasks .
table 2 shows the evaluation results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled rsaimage is the model supervised from the previous work ( chan et al . , 2017 ) . rsaimage has the highest recall @ 10 and the median rank , indicating that it is well - equipped to handle high recall .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . com while the row labeled u is the audiovisual supervised model . the acoustic embeddings are the ones from the previous work ( kutuzov et al . , 2016 ) and are supervised using pascal vocoder .
table 1 shows the comparison of the different classifiers using the original on sst - 2 . we report further examples in the appendix . the dan classifier turns in a screenplay that is slightly worse than the original at the edges , but is still much better overall . cnn also shows that it is more difficult to turn a screenplay into a tweet , but easier to turn it into a sentence . finally , rnn shows a marked improvement in performance .
table 2 shows the percentage of occurrences that have increased , decreased or stayed the same through fine - tuning in sst - 2 . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the second row indicates that the number of occurrences has increased . note that the presence of " good " and " evil " words in the sentence is not a change , as those are not considered to be part of the speech . the symbols are purely analytic without any notion of goodness .
sentiment score changes in sst - 2 . the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment .
table 1 presents the results of our experiments on the sst - 2 and pubmed datasets . our proposed method outperforms both published and unpublished work on every metric by a significant margin . for the pubmed dataset , our proposed method achieves an acc / f1 score of 98 . 5 / 98 . 7 % with an absolute improvement of 2 . 2 points over the previous state of the art .
