table 2 shows the throughput for training and inference on our treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . the iterative approach outperforms the recursive one on both inference and training .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
the max pooling strategy consistently performs better in all model variations . for example , conll08 achieves the best performance with a dropout probability of 9 . 66 % compared to the sigmoid model ( 8 . 63 % vs . 7 . 57 % ) in all but the case of ud v1 . 3 . similarly , the sb model performs better than the softplus model in all models with different representation .
table 1 shows the effect of using the shortest dependency path on each relation type . our macro - averaged model achieves the best f1 score in 5 - fold test set without using sdp and with sdp . our model - feature model obtains the best result .
the results are shown in table 3 . we observe that our model outperforms all the state - of - the - art models on all metrics except for f1 and r - f1 by a significant margin .
the results are shown in table 3 . we show that mst - parser outperforms all the other models on all three test sets except for the one where it gets 50 % or less accuracy .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph level and paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . however , the difference between paragraph and essay level performance is much smaller than that for essay , indicating that the difference in performance is less pronounced for paragraph - level systems .
the results are shown in table 3 . the results for the original and the cleaned model are presented in bold . as the table shows , when the model is cleaned , it performs significantly worse than when it is trained with tgen + and tgen − .
table 1 compares the original e2e dataset with our cleaned version . the difference in the number of distinct mrs and total number of textual references between the original and the cleaned version is small , but significant ( 17 . 5 % vs . 17 . 5 % ) .
table 3 shows the results of training and testing on the original and the original models with different tgen + and tgen − models . the results are shown in bold . the original model outperforms the original model on all metrics except for meteor and rouge - l .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) , and the number of instances in the training set . as shown in table 4 , adding the correct values to the original tgen instances leads to more errors than removing them .
table 3 shows the performance of our model compared to all the state - of - the - art models on the single - domain and ensemble datasets . our model outperforms all the models except for tree2str , which achieves the best performance on the ensemble dataset .
table 2 shows the performance of our model on amr17 . our model achieves 24 . 5 bleu points , which is marginally better than the previous state - of - the - art model , seq2seqb ( beck et al . , 2018 ) . similarly , our ensemble model achieves 28 . 5 points , a full 4 . 5 point improvement over the previous best result by beck et al . ( 2018 ) . the difference between our model and the other models is less pronounced , but still significant .
table 3 shows the results for english - german and czech , compared to english - czech . our model outperforms all the other models except birnn + gcn and seq2seqb on both languages . the difference between the two models is most pronounced on english , where our model obtains the best results .
table 5 shows the effect of the number of layers inside the network on the performance of our model . our model obtains the best results with n = 6 layers , and m = 23 . 5 .
table 6 shows the performance of our model compared to the baselines for all gcns with residual connections . our model outperforms all baselines except for the dcgcn3 model , which is slightly better than the previous state - of - the - art model .
table 4 shows the performance of our model compared to the best state - of - the - art model , the dcgcn . our model achieves the best results with a d / b score of 54 . 2 / 54 . 4 and a f1 score of 14 . 4 / 14 . 4 .
table 8 shows the results of an ablation study on the dev set of amr15 . the results show that removing the dense connections in the i - th and ii - th blocks significantly decreases the density of connections , and that the dcgcn4 model exhibits the best performance .
table 9 shows the ablation study results for modules used in the graph encoder and the lstm decoder . the results are summarized in table 9 . our model obtains the best results with respect to both coverage and attention .
table 7 : scores for initialization strategies on probing tasks . our paper achieves the best performance , with a score of 35 . 8 % on the depth / length and subjnum tasks . the glorot initialization strategy achieves the highest score on the tense / subjnum task and the topconst task .
table 3 shows the results of our h - cmow model compared to the previous state - of - the - art h - cbow model . our model obtains the best results with a score of 87 . 9 % on the weighted average of all metrics . it outperforms both the previous best state of the art model and the best state - ofthe - art model by a large margin .
table 3 shows the results of our model on the sub - test set of sst2 , sst5 , and sts - b . our model outperforms all the other models except for sick - r , which achieves the best results on all metrics .
table 3 : scores on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our model outperforms all the state - of - the - art models on all the downstream tasks except sts13 and sts16 . hybrid mode outperforms both cbow and cmow on all but sts15 . it also outperforms cmp . model with a large margin .
table 8 : scores for initialization strategies on supervised downstream tasks . our system outperforms all the alternatives except glorot ( 86 . 6 % vs . 86 . 4 % ) .
table 6 shows the scores for different training objectives on the unsupervised downstream tasks . our model outperforms all the stateof - the - art methods on all the downstream tasks except sts13 and sts15 .
table 3 shows the performance of our method compared to the state - of - the - art cmow - c model . our method outperforms both the best state - ofthe - art models on every metric by a significant margin . the difference is most striking on the length metric , where our model obtains the highest performance .
the results are shown in table 3 . the best results are obtained by the cmow - r model , which outperforms all the sub - jurisdictions except sst2 and sst5 by a large margin .
table 3 shows the performance of our system with all loc , misc and named entity features . our model outperforms all the state - of - the - art supervised and unsupervised models on all metrics except for name matching .
table 2 shows the results on the test set under two settings . name matching and supervised learning have the best performance , while τmil - nd has the worst performance . supervised learning has the best f1 score , but the best overall f1 scores are obtained under all settings .
table 6 shows that g2s - gat outperforms all the base models except the s2s model , with a gap of 3 . 86 points from the last published results . the gap is narrower than the gap between the best and worst models , but still significant . the difference between the performance of the best model and the worst one is less pronounced for the gat model , but it is still statistically significant .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s model outperforms all the models except for konstas et al . ( 2017 ) and damonte et al . ( 2019 ) . it also outperforms the previous state - of - the - art models on both datasets by a significant margin .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . our g2s - ggnn model outperforms all the previous models except for song et al . ( 2018 ) and guo et al . , ( 2018 ) .
table 4 shows the results of the ablation study on the ldc2017t10 development set . our model outperforms all the stateof - the - art models except meteor and bleu by a large margin .
the results are shown in table 3 . the model with the best performance is the g2s - ggnn model , which achieves 36 . 51 % on average compared to the baseline model with a f1 score of 35 . 2 % and a graph diameter of 7 . 8 .
table 8 shows the fraction of elements in the output that are missing in the generated sentence for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s outperform all the other models except gold , as shown in table 8 . the difference between gold and other models is less pronounced than in the previous experiments .
table 4 shows the pos and sem tagging accuracy using different target languages on a smaller parallel corpus ( 200k sentences ) . as expected , the pos tagging accuracy is significantly better than sem and sem , indicating that the features extracted from the 4th nmt encoding layer are more useful for target languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . word2tag significantly outperforms both unsupemb embeddings on both baselines , with an absolute improvement of 4 . 41 % and 3 . 41 % , respectively , over the best previous model ( word2tag ) .
table 3 shows the performance of our system with respect to all three metrics . our model outperforms all the state - of - the - art systems on all metrics except for pos tagging accuracy . the model achieves the best performance with an absolute improvement of 3 . 9 % over the previous state of the art model .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our model obtains the best results with three layers of the four - layer model .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary ' s accuracy on each dataset is shown in δ . as the table shows , for pan16 , the attacker has the worst performance .
table 1 : accuracies when training directly towards a single task . the results are shown in table 1 . our model outperforms all the stateof - the - art models on all three tasks except gender .
table 2 : protected attribute leakage : balanced & unbalanced data splits . our model outperforms all the baselines except pan16 by a large margin . the difference between balanced and unbalanced task splits is less pronounced for pan16 , but still significant .
table 3 shows the performance of our adversarial model on different datasets with an adversarial training set . our model outperforms all the state - of - the - art models on all three datasets except pan16 .
the results are shown in table 6 . the rnn encoder significantly outperforms the embeddings with different encoders . the difference in performance between rnn and embedding guarded is most noticeable when using the leaky encoder .
table 3 shows the results of our model on the ptb and wt2 datasets . our model outperforms the previous state - of - the - art models on all three datasets with a large margin . it achieves the best results on the wt2 dataset , outperforming all the previous models except for yang et al . ( 2018 ) and sru ( which achieves the worst results ) .
table 3 shows the results of our model with respect to training and testing on the 250k word embeddings of the lstm and atr datasets . the results are presented in table 3 . the results show that our model outperforms both the previous state - of - the - art models on all metrics except for the base acc metric .
table 3 shows the results of our model compared to previous work on the yelppolar and amapolar datasets . our model outperforms all the previous models except for zhang et al . ( 2015 ) by a large margin . the difference between our model and the previous state - of - the - art model is less than 0 . 5pp , but still significant .
table 3 shows the case - insensitive tokenized bleu score on the wmt14 english - german translation task on newstest2014 dataset . our model outperforms all the base the gnmt model and all the other models except olrn , sru , atr , and gru .
table 4 : exact match / f1 - score on squad dataset . our model outperforms all the state - of - the - art models with a large margin . the difference between the f1 score of rnet * and rnet + elmo is less pronounced than that of atr and gru , but still significant enough to warrant further study . as shown in table 4 , the model with the least number of parameters outperforms the other models with the most parameters . finally , the difference between elmo and atr is less significant than that between gru and lstm , indicating that the model using atr has less training data .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . our model outperforms all the state - of - the - art models on both english and german ner tasks . the difference in f1 scores between our model and the other models is most striking when we consider the number of parameter number in the training set . the lstm model has 245k parameters , while the lrn model has 129k .
table 7 : test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . lrn outperforms elrn and glrn on both snli and ptb tasks with the base setting as well as with the perplexity setting .
table 3 shows the performance of our system with and without oracle retrieeval on the word embeddings . our system outperforms all the other systems except human , with a large margin . the difference between human and other systems is most pronounced on b - 2 and b - 4 , with oracle retrieval having the smallest performance gap .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 2 , and the smallest is 0 . 6 , indicating that all the automatic systems are comparable in terms of overall quality .
the results are shown in table 3 . table 3 shows the results for english , german , french , spanish , dutch , russian , spanish and dutch . our model outperforms all the other models except df and tf on all metrics except for english .
the results are shown in table 1 . table 1 shows the results for english , german , french , spanish , dutch , dutch and russian on the test set . our model outperforms all the other models except for english and german on all but german and french datasets .
table 3 shows the performance of our model on the test set using the best - performing embeddings for english , german , french , spanish , dutch , russian , dutch and spanish . our model outperforms all the other models except df and tf on all metrics except english .
table 1 shows the performance of our model on all metrics . our model outperforms all the baselines except for europarl , where it is slightly better .
table 1 shows the performance of our model on all metrics . our model outperforms all the baselines except for europarl , where it is slightly better .
table 1 compares the performance of our enhanced model with the baseline model on the validation set of visdial v1 . 0 . the enhanced model ( lf ) achieves the best performance with a ndcg % of 73 . 42 % compared to our baseline model ( 71 . 36 % ) . the difference between the baseline and the enhanced model is due to the large variation in the question type and answer score sampling .
table 2 shows the performance of the ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . adding p1 and p2 only slightly improves the performance for all models , but only for coatt .
table 5 compares the performance of all the models with respect to hard and soft alignments . the results are shown in table 5 . hmd - f1 outperforms all the other models except ruse and wmd - bigram , with a gap of 0 . 817 points from ruse . also , hmd + recall and bert perform significantly worse than wmd + bigram and hmd * prec . finally , wmd * bigram outperforms ruse , but does not outperform hmd + f1 . we note that the difference between the performance on soft and hard alignments is less pronounced , but still significant .
the results are shown in table 3 . the average score of all the baselines for each setting is reported in bold . for example , meteor + + has the best average score , while bertscore - f1 has the worst average score . sent - mover has the highest average score and ruse ( * ) is the worst .
table 3 shows the results for bertscore - f1 on the sent - mover task . the results are presented in bold . the bleu model outperforms all the baselines except for meteor , which achieves the best results with a f1 score of 0 . 176 on the smd task . on the other hand , the sfhotel model achieves the highest score with 0 . 174 .
the results are shown in table 3 . word - mover and sent - movers outperform all the baseline models on both m1 and m2 metric . the results show that word - mover outperforms all the baselines except leic ( * ) and bertscore - recall by a significant margin on both metric .
the results are shown in table 7 . we observe that the model trained on shen - 1 achieves the best results with a minimum of 0 . 81 acc and a maximum of 39 . 2 % gm .
table 3 shows the results of our model on the semantic and transfer quality datasets . our model outperforms all the other models on both transfer quality and semantic preservation metrics . on the transfer quality metric , our model obtains the best results . it also achieves the best transfer quality .
table 5 shows the results of human sentence - level validation on yelp for each dataset for each metric . our model uses spearman ’ s [ italic ] ρ ρ b / w sim and human ratings of semantic preservation and fluency to validate acc and pp , respectively . the results are shown in bold .
the results are shown in table 6 . we observe that the model trained on shen - 1 achieves the best results with a minimum of 0 . 8 acc and a maximum of 37 . 7 % gm . the model performs best when trained with the language feature addition .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) outperform all the previous work on both datasets with similar amounts of acc ∗ and with the same number of human references . however , our best model achieves higher acc than both the best previous work and our best unsupervised model , with a gap of 3 . 5 points from the previous best model .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . for nested disfluencies , the average number of tokens per disfluency is slightly higher than for repetition tokens , but still significantly lower than for rephrase tokens , indicating that the repetition tokens are more difficult to predict than the rest . for both types , the maximum length of the disfluential tokens is less than the average of the overall length .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the repardiation of the disfluency , or in neither . percentages in parentheses show the fraction of tokens belong to each category . the average number of tokens in each category is shown in table 3 .
table 3 shows the performance of our model with different combinations of text and innovations on the test set . the results are shown in bold . we observe that the model performs best when using both innovations and text alone , and when using the combination of both .
table 2 shows the performance of our model compared to the state - of - art embeddings on the fnc - 1 test dataset . our model achieves the best performance with an accuracy of 83 . 43 % on the micro f1 test set . the rnn - based embedding achieves the highest accuracy , and the self - attention embedding has the worst performance .
table 2 : accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . the ac - gcn model , for example , has the best performance with 69 . 2 % accuracy compared to the previous best model , the attentive neuraldater ( 66 . 2 % ) and 62 . 9 % compared to maxent - joint .
table 3 shows the performance of all the component models with and without word attention for this task . our neuraldater model outperforms all the other models with both word attention and graph attention .
the results are shown in table 3 . we observe that all models perform similarly to the previous state - of - the - art models on the 1 / 1 and 1 / n tasks , with the exception of dmcnn , which achieves the best performance on both tasks . however , the difference between all models is less pronounced on the 2 / n task , where jrnn achieves 75 . 2 % performance compared to the last published results .
table 1 shows the performance of our method compared to state - of - the - art cross - event event detection systems . our method significantly outperforms all the state of the art methods on both event detection and event prediction .
table 3 shows the results for english , spanish , french , german , dutch , russian , turkish , italian , spanish and russian on the test set . the results are shown in bold . the results of all models are statistically significant with respect to the dev perp and test wer metric , with fine - tuning having the least effect on the results . as expected , all models perform significantly worse than the original model with the exception of spanish - only , which achieves the best results .
table 4 shows the results on the dev set and on the test set using only subsets of the code - switched data . fine - tuned fine - tuned models outperform cs - only models on both sets with a large margin .
table 5 shows the performance of fine - tuning on the dev set and on the test set , compared to the monolingual and code - switched versions of the gold sentence in the set . fine - tuned - disc outperforms both the monolinguistic and the fixed - disc models on both sets , with an absolute improvement of 3 . 53 % and 4 . 87 % over the original model , respectively .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the results are statistically significant ( p < 0 . 01 ) compared to using the baseline model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset , compared to using the baseline model . the improvement in precision is statistically significant ( p < 0 . 05 ) compared to baseline , with a f1 score of 1 . 35 compared to 0 . 03 for type combined model .
table 1 : results on belinkov2014exploring ’ s ppa test set . syntactic - sg embeddings outperform both ontolstm - pp and glove - extended on both test sets . the difference between the two approaches is most pronounced on the wordnet test set , where syntacticsg embedding gives a performance gain of 3 . 7 % over glove - retro . similarly , the difference is less pronounced for wordnet 3 . 1 , but still significant . on the other hand , the gap between the performance on wordnet and lstm is less than 1 % .
table 2 : results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . our hpcd model outperforms all the alternatives except ontolstm - pp and ontooracle pp with a large margin .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . our model achieves the best performance with a precision of 89 . 5 % on the ppa acc . metric .
table 2 : adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results of en - de , en - fr , and mscoco17 models are shown in table 2 . subsfolding the subtitle data improves the bleu scores for all models , but only marginally for en - fro . adding domain tuning improves the results for all three models , however , the improvement is less pronounced for en ‑ de , as it requires more domain tuning than for multi30k models .
table 3 shows that domain - tuned models outperform both en - de and en - fr models on both datasets when using the h + ms - coco model with domain - aware labels . however , the results are slightly worse than those using the lm + ms model .
table 4 : adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . adding only the best 5 captions improves marian amun ’ s model by 3 points . the performance of en - fr and mscoco17 models improves by 2 points and 3 points , respectively , compared to en - de . multi30k model improves by 4 points .
table 5 shows the bleu % and w - score of our encoder and decoder strategies for integrating visual information into multi30k + ms - coco + subs3mlm models . our encoder achieves the best performance with 62 . 45 % and 62 . 38 % bleus , respectively , compared to the en - de and mscoco17 models , both of which use the same decoding scheme ( hence , the difference is less pronounced ) .
table 3 shows the results for en - fr , en - de and mscoco17 models compared to the baseline models using the multi - lingual approach . the results are summarized in table 3 . the results show that , when using only the visual features , our model outperforms all the other models except for the case where it is trained using the ms - coco embeddings . however , the results are slightly worse than those using the text - only approach .
the results are shown in table 3 . as the table shows , en - fr - ht and en - es - ht significantly outperform the models using both mtld and ttr . moreover , the performance gap between the two models is much narrower than that between the baseline and the best state - of - the - art models , indicating that the model trained on the original embeddings may have better interpretability .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the en – fr and en – es language pairs have the most parallel sentences , with a total of 1 , 472 , 203 and 459 , 633 sentences , respectively .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the models trained on the english and spanish datasets have the best performance .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the en - fr - rnn - rev and en - es - trans - rev systems achieve the best performance , while the en - e - smt - rev system achieves the worst performance .
table 2 shows the results of our model on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com and the row labeled rsaimage is the one supervised by the rsaimage model . our model obtains the highest recall @ 10 and median rank .
table 1 : results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the model from rsaimage . the rsaimage model has the highest recall @ 10 and the highest chance to match the rsaimage image .
table 1 shows the example sentences of the different classifiers compared to the original on sst - 2 . we report further examples in the appendix . for example , the cnn classifier turns in a screenplay that has edges at the edges , and a sentence containing the word “ want to hate it ” . the rnn classifiers turn in a sentence with edges and edges , while the dan classifiers turns in sentences with curves and edges . as the table 1 shows , the difference in sentence size between cnn and rnn is small , but significant .
table 2 shows the percentage of occurrences for each part - of - speech that have increased , decreased or stayed the same through fine - tuning . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the number of occurrences of each word in sst - 2 . for example , for nouns , there is a drop of 3 . 5 % and for verbs , a decrease of 4 . 5 % . for adjectives , the drop of 5 % and the increase of 1 % is significant .
the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the results are shown in table 3 .
table 1 shows the results for pubmed and sst - 2 . the results are summarized in table 1 . our approach outperforms all the alternatives except for corr ( 98 % vs . 98 % ) .
