table 2 shows the performance of our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . moreover , the iterative and recursive approaches perform similarly on training and inference .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . t parallelization . table 1 shows that the balanced dataset has the best performance in terms of the number of instances per second .
the max pooling strategy consistently performs better in all model variations . the results are shown in table 2 . we observe that the best performance is obtained when the model is trained on conll08 , ud v1 . 3 , softplus and sb models with different feature maps .
table 1 shows the results of using the shortest dependency path on each relation type . the results are shown in table 1 . we observe that the best f1 ( in 5 - fold ) is achieved when using the best dependency path with sdp . as shown in the table , the macro - averaged approach outperforms the model - feature approach by a significant margin .
the results are shown in table 3 . we observe that the best performing model is the y - 3 model , which outperforms all the other models in terms of f1 score and r - f1 and f1 50 % on average . the performance gap between the best and worst models is small , but it is still significant .
table 3 shows that mst - parser outperforms all the other systems in terms of accuracy . the results are shown in table 3 . as shown in the table , the best performance is obtained by using the paragraph level acc . scores of all the systems . these results show that the best performing systems are the ones with the highest accuracy scores .
table 4 shows the c - f1 ( 100 % ) in % for the two indicated systems ; essay vs . paragraph and paragraph vs . sentence level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the difference in performance between the two systems can be seen in table 4 , with the paragraph - level system performing better than the essay - level one .
table 3 shows the performance of our system compared to the best state - of - the - art systems . the results are shown in table 3 . our system outperforms all the state of the art systems except for sc - lstm , which outperforms both the original and the clean - up systems by a significant margin .
table 1 shows the results for the original e2e data and our cleaned version ( number of distinct mrs , total number of textual references , ser as measured by our slot matching script , see section 3 ) . the results are shown in table 1 . we observe that the difference between the original and the cleaned version is small , but it is significant enough to show that our slot - matching script performs better than the original .
table 3 shows the performance of our system compared to the best state - of - the - art systems . the results are shown in table 3 . it can be seen that our system outperforms all the state of the art systems except for sc - lstm , which shows that it is better than all but one of them . further , it outperforms both the original and the original versions of the original tgen + model .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . these results are shown in table 4 . as can be seen , adding , missing , and incorrect values leads to significant increases in the number of errors . adding , missing and wrong values also leads to a significant drop in the overall number of correct answers .
table 3 shows the performance of our system on the external and internal test sets . our system outperforms all the previous state - of - the - art systems except for graphlstm ( song et al . , 2018 ) and pbmt .
table 2 shows the main results on amr17 . our model achieves 24 . 5 bleu points , which is significantly higher than the previous best performance of seq2seqb ( beck et al . , 2018 ) , which achieves 21 . 3 points . in contrast , the best performance achieved by the second - best ensemble model ( dcgcn ) is 21 . 5 points , while the best single - parametre model ( ggnn2sq ) achieves 27 . 6 points . as shown in table 2 , the performance gap between our model and the best ensemble model is small , but still significant .
table 3 shows the performance of the best models for english - german , german - czech and czech . the best models are bow + gcn ( bastings et al . , 2017 ) and seq2seqb ( beck et al . 2018 ) , both of which outperform the best single - domain models in terms of performance . in addition , these models outperform all the other models except for birnn , which outperforms all the models except bow + .
table 5 shows the effect of the number of layers inside the network on the performance of our system . we observe that when we add more layers to the network , we see that our system performs better than the previous state - of - the - art system .
table 6 shows that the results of our model are comparable with those of the baselines in table 6 . our model outperforms both the previous state - of - the - art and the previous best state of the art in terms of the number of connections .
table 3 shows the performance of our model in the real - world setting . our model outperforms all the previous state - of - the - art models except for dcgcn ( 1 ) , which shows that it is able to improve upon the performance by a significant margin over previous state of the art models .
table 8 shows the ablation study for density of connections on the dev set of amr15 . the results are shown in table 8 . the results show that removing the dense connections in the i - th and i - tth blocks significantly reduces the number of connections , and that the dcgcn4 model obtains better performance .
table 9 shows the results of the ablation study for modules used in the graph encoder and the lstm decoder . the results are shown in table 9 . the results show that the coverage mechanism used in our model outperforms the previous state - of - the - art coverage mechanism , and the direction aggregation mechanism outperforms all the other coverage mechanisms .
table 7 shows the performance of the initialization strategies on probing tasks . glorot and somo perform better than our paper on all three of the four probing tasks ( table 7 ) . however , it is clear that somo performs worse than our system on the last two tasks .
table 3 shows the performance of the h - cmow and h - cbow systems compared to the previous state - of - the - art approaches . the results are shown in table 3 . we observe that the best performance is obtained by using the best state of the art method , namely , the hcmow / 400 system , which achieves the best results in terms of accuracy and precision . further , it achieves the highest precision and accuracy with the best precision .
table 3 shows the performance of our method compared to the previous state - of - the - art approaches . we observe that our hybrid approach outperforms all the other approaches except for the cmow / 784 model , which shows that it is better than the previous best state of the art in terms of performance on sub - domain coverage .
table 3 shows the results on unsupervised downstream tasks attained by our models . the results are shown in table 3 . our models outperform the best state - of - the - art cbow and cmow models on all the downstream tasks except for sts13 and sts14 . as shown in the table , the performance of our models improves with respect to hybrid .
table 8 shows the performance of the initialization strategies on supervised downstream tasks . glorot and trec outperform all the other initialization strategies except for sick - e , which shows that their performance is comparable to that of subj and mpqa . we also observe that trec and mppc outperform the other three initialization strategies .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . we observe that the cmow - r model outperforms the cbow - c model on all the downstream tasks except for sts13 and sts14 , where it performs slightly worse than the best performing cbow model .
table 3 shows the performance of our method compared to the previous state - of - the - art cmow - c and cbow - r systems . our method outperforms both the previous best state of the art systems in terms of accuracy , precision , and precision on all metrics .
table 3 shows the performance of our method compared to the state - of - the - art cmow - c and cbow - r systems . our system outperforms all the other methods except for sick - e , which shows that our system is better than all the others . our model outperforms both the best and worst - performing systems in terms of accuracy .
table 3 shows the performance of our supervised learning system ( mil - nd ) compared to the best state - of - the - art supervised learning systems ( tmtmil and τmil ) in terms of both loc and misc . the results are shown in table 3 . we observe that our system outperforms all the state of the art systems except for the one in which it has the worst performance . in particular , we observe that the performance gap between our system and those of the other systems is very large , with the difference between the two systems being less than 0 . 5 points . this indicates that our model is more sensitive to the type of data that it is trained on .
table 2 shows the results on the test set under two settings . in both settings , the model with the best f1 scores , τmil - nd ( model 2 ) outperforms both the best supervised and unsupervised learning systems . the results are shown in table 2 . as expected , the best performance is achieved by the supervised learning system , which shows that it is able to learn the best human - supervised model with a high degree of accuracy . moreover , it shows that the best performing model is the one with the highest precision .
table 6 shows the results of our model in the real - world setting . the results are shown in table 6 . our model outperforms all the state - of - the - art systems except g2s - gat , which shows that it is able to perform better than all the other systems . we observe that our model is better than the best state of the art systems in terms of precision and accuracy .
table 3 presents the results of our model on the ldc2015e86 and ldc2017t10 datasets . our g2s - gat model outperforms the previous state - of - the - art models in terms of bleu and meteor scores . it also outperforms both the best - performing models of the previous two sets of models . the results are shown in table 3 .
table 3 shows the results on the ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model outperforms all the other models in terms of bleu score . it also outperforms both the external and the internal models .
table 4 shows the results of the ablation study on the ldc2017t10 development set . the results are shown in table 4 . our model outperforms the previous state - of - the - art models in terms of precision and size . it is clear that the combination of our model with bilstm and get improves the precision of our system .
the results are shown in table 3 . we observe that the g2s - gin model outperforms all the other models in terms of the average number of sentences and the average length of sentences . it also outperforms the models that use gat and gat - gat with a margin of 0 . 51 % and 0 . 43 % , respectively .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( miss ) , for the test set of ldc2017t10 . the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - gin outperform both gat and gat - ggnn in terms of accuracy . we observe that the accuracy of the gat model is higher than that of gin , indicating that the model is more sensitive to human attention .
table 4 shows that the pos and sem tagging accuracy are comparable across the different target languages on a smaller parallel corpus ( 200k sentences ) . however , the pos tagging accuracy is lower than that of the other two languages .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . unsupemb classifier and word2tag outperform both the best - performing word embeddings in terms of most frequent and most frequent tags .
table 4 shows the performance of our system in terms of accuracy and precision . we observe that our system outperforms the previous state - of - the - art systems on all metrics except for pos tagging accuracy , which shows that it is able to improve upon the performance by a significant margin over the previous best state of the art systems .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the best performing models are the ones with the highest precision , i . e . , the ones that have the highest recall of all the features . these models have the best performance on all the target languages except english .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . as shown in table 8 , the difference between the attacker score and the corresponding adversary ’ s accuracy is small , but it is significant enough to indicate that the attacker is performing well .
table 1 shows the results of training pan16 on a single task . the results are shown in table 1 . as can be seen , pan16 outperforms pan16 with respect to the accuracy and sentiment scores . it also outperforms the pan16 model in terms of recall .
table 2 shows the results of the balanced and unbalanced data splits . the results are shown in table 2 . as shown in the table , the best performance is achieved when the data is balanced , and the worst performance is obtained when it is unbalanced . we observe that the best performing model is the pan16 model .
table 3 shows the performance of our system on different datasets with an adversarial training . the results are shown in table 3 . as can be seen , our system outperforms the previous state - of - the - art approaches on all three datasets except for pan16 , where it performs worse than pan16 .
table 6 shows the performance of different encoders on the protected attribute . the rnn encoder performs better than the guarded encoder on the leaky and guarded embeddings .
table 3 presents the results of our work on the ptb and wt2 systems . the results are shown in table 3 . our work shows that our model outperforms all the state - of - the - art systems except the lstm , which shows that it is able to improve upon the performance of the previous state of the art in terms of the number of params and finetune . it also shows that the work performed on the wt2 system is comparable to that on the other systems .
table 3 presents the results of our work on the lstm and gru models . the results are shown in table 3 . as can be seen , our model outperforms all the previous state - of - the - art models except for the work of sru , which shows that it is able to improve upon the performance of the previous work by a significant margin . also , the results show that the work performed by the gru and sru models is comparable in terms of performance to the work done by the previous best state of the art models .
table 3 presents the results of our work on the amapolar err and yelppolar time datasets . the results are shown in table 3 . our model outperforms the previous state - of - the - art models in terms of err , time and precision . it also outperforms both the previous best state of the art systems , namely , the lstm and the gru models .
table 3 shows the bleu score on the wmt14 english - german translation task on tesla p100 . as shown in table 3 , gnmt and olrn outperform all the other models except atr and sru in the case of case - insensitive tokenized tokens . however , the performance gap between the gnmt model and the other three models is small , indicating that gnmt is better suited to the task . in addition , it outperforms all the models except for atr , which shows that it is better at decoding sentences in english than the other two models . finally , it shows that the difference between the performance of the three models may be due to the small number of tokens required to tokenize a sentence .
table 4 presents the results of our model on the squad dataset . the results are shown in table 4 . our model outperforms all the state - of - the - art systems in terms of match / f1 score . we observe that our model has the best performance with respect to the parameter number of base . as shown in the table , our model obtains the best f1 score when using only elmo - based parameters . however , when using all the parameters of the lstm and lrn models , the results are slightly worse than those of the other systems .
table 6 shows the f1 score on conll - 2003 english ner task . the results are shown in table 6 . we observe that the lstm model outperforms all the other models except for lrn and sru , which have lower f1 scores than lrn , sru and lrn .
table 7 shows the test results on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . lrn outperforms elrn and glrn on the snli and ptb tasks with the best performance .
table 3 shows the performance of our system with respect to human and machine learning models . our system outperforms all the other systems in terms of word and sentence recall . the results are shown in table 3 . in particular , we see that our system performs better than all the others except for human , with the exception of retrieval .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 3 points , which indicates that the best automatic system has the best overall quality . table 4 also shows the percentage of evaluations a system gets in the top 1 or 2 for overall quality , indicating that the system is performing better than the best human evaluation . the best automatic systems are candela ( 2018 ) and h & w hua and wang ( 2018 ) .
table 3 shows the performance of our approach compared to the best state - of - the - art systems . our approach outperforms all the state of the art systems except for ted talks , which shows that our approach is better than all the other systems except ted talks . the performance gap between our approach and those of other systems is small , but still significant .
table 3 shows the performance of our system compared to the best state - of - the - art systems . our system outperforms all the state of the art systems except for ted talks , which shows that our system is better than all the other systems except ted talks . the performance gap between our system and those of other systems is small , but it is still significant .
table 3 shows the performance of our system compared to the best state - of - the - art systems . our system outperforms all the state of the art systems except for ted talks , which shows that our system is better than all the other systems except ted talks . the performance gap between our system and those of other systems is small , but it is significant enough to warrant further study .
table 3 shows the performance of our system with respect to the number of roots , total terms , and number of numberrels . we observe that our system outperforms all the other systems in terms of total roots and total terms . our system also outperforms the best - performing systems on all metrics except for maxdepth .
table 3 shows the performance of our system in terms of the number of roots , maxdepth and maxdepth cohesion . we observe that our system outperforms all the other systems on average , with the exception of numberrels , which are slightly worse than the others . our system also outperforms other systems , such as dsim and docsub .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . the enhanced version of our system outperforms the baseline model in terms of qt , s and d scores , and hidden dictionary learning , respectively . in addition , the enhanced model outperforms both the baseline and the enhanced version on the question type and answer score sampling set . finally , it achieves the best performance on the weighted softmax loss .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . p1 indicates the best performing model , and p2 shows the worst performing one . we observe that p1 + p2 is more effective than p1 alone , indicating that p2 alone can improve the model performance .
table 5 shows that hmd - f1 and hmd - recall outperform wmd - unigram and wmd - bigram on hard and soft alignments , respectively . we observe that the performance gap between the two approaches is less pronounced with respect to hard alignments . however , the difference in performance between the hmd and hmd models is still significant .
the results are shown in table 3 . we observe that the bertscore - f1 model outperforms all the other methods except for meteor + + and ruse ( * ) by a significant margin . as shown in the table , the average score of all the models is significantly lower than those of the other models , indicating that the selection of the correct set of models is difficult .
table 3 shows the performance of our bertscore - f1 model on the sent - mover and w2v models . the results are shown in table 3 . our model outperforms the previous state - of - the - art models on all metrics except for bleu - 2 , which shows that it is able to achieve the best performance on all the metrics . in particular , our model achieves the best results on the " sent - mover " and " w2v " metrics , both of which show that our model can achieve the highest performance on both of these metrics .
table 3 shows the performance of our system in terms of word - mover and recall on the m1 and m2 metrics . the results are shown in table 3 . our system outperforms the previous state - of - the - art models on both the m2 metric and the leic metric .
table 6 shows the performance of our model with and without para + lang . the results are shown in table 6 . our model outperforms the previous state - of - the - art models with a significant margin . we observe that our model performs better with the addition of para - lang , as shown in the table .
table 3 presents the results of our model on the yelp dataset . the results are shown in table 3 . our model outperforms all the other models in terms of transfer quality , transfer quality and transfer quality tie . it also outperforms both the best and worst - performing models on the semantic preservation and the fluency metric . we observe that our model performs better on the transfer quality metric than all the others .
table 5 shows the results of human sentence - level validation of the metrics for each dataset for validation of gm . the results are shown in table 5 . as can be seen , the accuracy of the system is high , and the human ratings of semantic preservation and fluency are high , indicating that the model is performing well .
table 3 shows the performance of our model with and without para + lang . the results are shown in table 3 . our model outperforms the previous state - of - the - art models with a significant margin . we observe that our model performs better than all the previous models with the exception of shen - 1 , as shown in the table 3 .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the best models ( fu - 1 , multi - decoder ) and the best classifier ( yang2018 , yang2018unsupervised ) outperform all the best previous work on this topic . as shown in table 6 , the best models outperform the best unsupervised models , but are worse than the best supervised ones . we note that the classifiers used in our system outperform those in the previous work , but the difference between the best and worst models is less pronounced in the untransferred set .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . as can be seen in table 2 , nested disfluencies are more difficult to predict than those that are not . however , the number of tokens that are correctly predicted to be disfluent is higher than for repetition tokens , indicating that the training set is able to distinguish between the different types of disfluency .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , either the reparation or repair ( error ) or in neither . as shown in table 3 , content - content and function - function tokens have the highest percentage of tokens that belong to each category of disfluency , indicating that these tokens are more likely to belong to the disfluence category . in addition , the number of tokens belonging to the content - function category is higher than in the other two categories . these results show that content - disfluencies are more difficult to predict than those in the repair category .
the results are shown in table 3 . we observe that the best model is the one with the best dev and best test scores . the best model with the highest dev and test scores is the single - input model , followed by the best single - out - of - sample model . it is clear that the combination of innovations and text improves the performance of the model , as shown in the table .
table 2 shows the performance of the word2vec embeddings on the fnc - 1 test dataset . our model outperforms all the state - of - the - art algorithms except for self - attention sentence embedding , as shown in table 2 . it also outperforms rnn - based sentence - eq . we observe that the accuracy of our model is higher than that of the state of the art .
table 2 shows the accuracy ( % ) of different methods on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms all previous models . in particular , the ac - gcn model outperforms the previous state - of - the - art neuraldater and the attentionive neuraldater . the ac - gnc model also shows significant performance improvement over the previous best models .
table 3 shows the performance of our neuraldater model with and without graph attention . the results are shown in table 3 . our neuraldater model outperforms all the other approaches except for oe - gcn , which shows that it is more sensitive to word attention . also , the accuracy of our model is higher than that of other approaches .
as shown in table 3 , the performance of our model outperforms all the other approaches except for the dmcnn model , which shows that it is able to handle multiple stages at once . the performance gap between our model and the other models is small , but it is still significant .
table 3 presents the results of our model on the event detection and event prediction systems . the results are shown in table 3 . our model outperforms the previous state of the art in terms of event detection , event prediction , and event recall . it also outperforms previous state - of - the - art models in the cross - event event detection task . we observe that our model is able to detect all the events in the event of a single event , with the exception of one .
the results are shown in table 3 . we observe that the best performance is obtained by using the best models for english - only and spanish - only languages , followed by the best model for french - only . the best performing models are the best performing ones for both languages , and the worst performing ones are the ones that use the best training data . as can be seen from table 3 , these models perform better than all the other approaches except for the fine - tuned one , which shows that fine - tuning the training data can improve the performance of the model .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the fine - tuned model outperforms the cs - only model by a significant margin . the results are shown in table 4 .
as shown in table 5 , the fine - tuned - disc system outperforms both the best - trained and worst - trained models on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the type combined approach shows a significant improvement in precision and recall over the baseline model .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset ( table 5 ) as well as the f1 score ( f1 ) of the type combined gaze features . the results are shown in table 5 . we observe that the precision and recall scores are significantly improved over the baseline , indicating that the combination of the two types of gaze features has a significant impact on the performance of the model .
table 1 shows the results on belinkov2014exploring ’ s ppa test set . the results are shown in table 1 . we see that the glove - extended embeddings outperform the lstm - pp and ontolstm in terms of both type and type - of tokens . further , we observe that the hpcd model outperforms both the ontolpstm model and the original syntactic - sg model , which uses syntactic skipgram . finally , we see that our system outperforms the previous state - of - the - art wordnet model ( faroqui et al . ( 2015 ) on the ppa tests set .
results in table 2 show that the hpcd system outperforms the ontolstm - pp model with respect to ppa acc . and ppa accuracy . in addition , it outperforms both the lstm model and the oracle pp model in terms of accuracy .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we observe that the ppa acc . scores are significantly higher when the context sensitivity is removed .
table 2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the results are shown in table 2 . subsfull and domain - tuned models outperform both en - de and multi30k models in terms of bleu % score . in particular , the domain tuning model outperforms en - fr , mscoco17 , and flickr17 models . moreover , it outperforms all the models with sub - subsfull or domain tuning . as can be seen in the table 2 , the best performing models are those with the best domain tuning , which shows that domain tuning can improve the translation performance .
as shown in table 3 , domain - tuned h + ms - coco outperforms all the other models except for en - de and en - fr , where it performs better than all the others . in addition , the results are comparable across all the models , with the exception of flickr16 and flickr17 , which perform worse than the other two models . moreover , the difference in performance between the best and worst models can be attributed to the high performance of domain - domain - tuning , as shown in the table .
table 4 shows the bleu scores in % . adding automatic image captions ( only the best one or all 5 ) . the table shows that the multi30k and en - de models outperform the en - fr and flickr16 models by a significant margin . further , the multi - de model outperforms all the other models except for the mscoco17 model , as shown in table 4 . as shown in the table , using only the best 5 captions improves the performance of these models .
in table 5 , we show the bleu % scores of the different strategies for integrating visual information . we observe that the enc - gate and dec - gate strategies outperform the en - de and mscoco17 strategies , respectively , in terms of the percentage of visual information that is encoded into the img and sub - sub - subs3mlm layers . however , the performance of en - fr and flickr17 outperforms both of these strategies . in particular , we observe that when using transformer , multi30k + ms - coco + subsmlm , detectron mask surface , and detectron mask surface , our enc - gates outperform those of the other two strategies .
the results are shown in table 3 . we observe that the multi - lingual approach outperforms all the other approaches except for ms - coco , which shows that it is more difficult to detect the presence of visual features in the captions of the images . moreover , the performance gap between the two approaches is much larger when we consider only the visual features , as shown in the table 3 .
table 3 shows the performance of the three approaches for yule ’ s i and ii . the results are shown in table 3 . we observe that the best performance is achieved by the en - fr - ht and en - es - ht models , while the worst performance is obtained by the models using the trans - trans - ff model . these models outperform all the other approaches except for enfr - smt - back , which shows that it is easier to train these models than the others .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the number of sentences in each split is shown in table 1 . as can be seen , our model outperforms the previous state - of - the - art models in terms of parallel sentence splits .
table 2 shows the training vocabularies for the english , french and spanish data used for our models . the results are shown in table 2 . we observe that the models are able to learn the english and french language pairs better than the spanish ones .
table 5 shows the automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . our system outperforms the previous state - of - the - art systems in terms of bleu , ter and rev system evaluation scores .
table 2 shows the results on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the one supervised by chrupala et al . ( 2017 ) . the results are shown in table 2 . we observe that the vgs model has the best recall @ 10 % and the highest recall @ 711 .
table 1 shows the results on synthetically spoken coco . the results are shown in table 1 . rsaimage outperforms all the other models except for vgs , which shows that it has better recall than all the others except vgs . as shown in the table , the vgs model has higher recall than the rsaimage model . moreover , it has higher precision than the other two models .
we report further examples in the appendix . in table 1 , we show that the different classifiers can be trained on the original sst - 2 screenplay , and that their performance is comparable to that of the original on cnn . the difference is that the dan classifier turns in a screenplay that is slightly worse than the original , and the rnn classifier is better than the cnn one .
table 2 shows that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the second row shows the percentage points that the rnn has increased or decreased in terms of number of instances of the words in the sentence .
table 3 shows that when negative labels are flipped to positive labels , the sentiment score increases in both positive and negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence . the results are shown in table 3 . as can be seen in the table , the positive label flips to positive and vice versa , and the negative label flips from negative to positive .
table 1 presents the results for pubmed and sst - 2 . the results are shown in table 1 . we observe that sift outperforms all the other methods except for corr ( p < 0 . 001 ) and pubmed ( p > 0 . 01 ) in terms of accuracy .
