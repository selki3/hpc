table 2 shows the throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the numbers show , both the number of instances and the training time are important factors in determining whether a model can be trained and inference performed well .
table 1 shows the throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t .
we can see from table 2 that the max pooling strategy consistently performs better in all models variations . the best performance is achieved with the conll08 model , with a f1 score of 75 . 83 . we can also see that ud v1 . 3 has the best performance with the hyperparametrization optimization results .
table 1 shows the results for relation type 1 on each relation type . our macro - averaged model significantly improves over the best f1 model without sdp .
consistent with the observations by vaswani et al . ( 2017 ) , we observe that in all but one of the comparisons , our model obtains better results than the previous state - of - the - art on average . the only exception is the case of f1 50 % where our model ( y - 3 ) obtains significantly better results .
we show the precision numbers for each paragraph in table 1 . for example , we show that on average , mst - parser achieves 100 % accuracy on average with a gap of 10 . 5 % from the last published results . on the other hand , our model achieves 50 % accuracy .
we report the overall performance of our system in table 4 . our proposed lstm - parser parser outperforms the original embeddings and performs on par with the best performing stagblcc parser . however , it is slightly worse than the majority system in terms of c - f1 score .
the results are shown in table 2 . the original tgen model was significantly worse than the cleaned model on all metrics except meteor , so we observed that it had to be completely redesigned . the results of re - scoring our model after replacing these 10 files with the ones from the original are presented in table 2 . we observe that the difference in bleu score between the original and the cleaned model is minimal , however we see significant difference in ser score due to different training sets and the number of iterations of the model used to train the model .
table 1 compares the original e2e data with the cleaned version . the difference in mr statistics is minimal , however we see significant difference in ser ( p = 0 . 00 ) due to different training and test set sizes . the difference is most prevalent in slot matching , see section 3 .
table 3 shows the results for the second variation of tgen . our model tgen + improves upon the original tgen model by 3 . 94 points in bleu score . however , it is still significantly worse than tgen original under all metrics .
the results of manual error analysis of tgen on a sample of 100 instances from the original test set are shown in table 4 . we found that the majority of errors in our system ( 62 . 4 % ) were caused by errors caused by missing training data or incorrect values ( 14 . 6 % ) in the initialization step .
table 2 compares our proposed approach against previous approaches . our proposed dcgcn model outperforms all the base lines with a gap of 10 . 2 % in external validation set vs . the previous state - of - the - art .
as shown in table 2 , our dcgcn model achieves 24 . 5 bleu points improvement over the previous state of the art on amr17 . our ensemble model improves upon the performance of seq2seqb with a gap of 10 . 5 points from the previous best performance .
table 3 shows the results for english - german and czech . our model outperforms the previous approaches in both languages . the difference is most prevalent in the second language category , we see that ggnn2seq ( beck et al . , 2018 ) achieves state - of - the - art results , outperforming all the other models apart from birnn + gcn when using bow embeddings . also , we notice a drop in performance between english - and czech - language models .
we observe that the number of layers inside the dc network is important , as it affects the performance of prediction quality . our model achieves state - of - the - art results with a 17 . 8 % improvement on average over the best baseline ( kutuzov et al . , 2016 ) with a reduction of 10 % in absolute terms .
table 6 shows that gcns with residual connections outperform gcns without residual connections . the smaller difference in bias metric between gcn + rc and gcn + la ( 2 ) indicates that the residual connections in gcns are less significant , but still contribute significantly to the overall performance . gcn with rc and la connections outperforms the baseline on two of the four gcns ( i . e . gcn2 + la ) by a margin of 2 . 8 points .
we observe that the coreference signal is localized on specific objects and that these objects are in the deep layers of the network ( e . g . dcgcn , ps graphs , etc . ) . the performance of these models is reported in table 4 . generalization is relatively consistent across all metrics with the exception of bias metric , which shows that the model can rely on superficial cues .
we also performed an ablation study on the dev set of amr15 . in table 8 we report the results of removing the dense connections in the i - th block . the results show that our dcgcn4 model exhibits a significant drop in the density of connections .
the results of an ablation study for the graph encoder and the lstm decoder are shown in table 9 . the results show that the models using the coverage mechanism achieve better results when the global node is combined with the local node , indicating that the coverage mechanism helps the decoder to more precisely detect events of armed conflicts termination ( where no insurgents should be predicted for a location ) , and less frequently misclassified insurgents .
table 7 shows the performance of our initialization strategies on probing tasks . our framework establishes a new state - of - the - art on all three high - level probing tasks , and on all subtasks except coordinv . it significantly improves the generalization ability of our framework , and its performance on subtasks improves over previous state of the art models .
we noticed that the compactness of our h - cmow model compared to previous approaches is impacted negatively by the fact that it is trained on a smaller subset of the data set , hence leading to incorrect predictions . our proposed h - cbow model improves upon the previous state of the art . it achieves state - of - the - art results , outperforming previous methods by 10 . 6 points in the coherent index metric .
our results reconfirm that the cbow model can significantly improve over the monolingual approach when trained and tested on the multi - news dataset . it achieves 90 . 2 % improvement on average over the previous state of the art model on all subj and mpqa metrics . hybrid mode also achieves competitive or better results than the original model on three of the four datasets .
table 3 shows the performance of our models on the four downstream tasks as well as the improvements in overall performance over the best baseline model , cmow . hybrid mode outperforms cbow in all but one of the four tasks , and in sts13 and sts16 , it achieves a relative improvement of 44 . 2 % over the baseline on sts12 .
table 8 presents the performance of our system in the supervised downstream tasks . our system outperforms glorot and mpqa by a large margin . it achieves state - of - the - art results , improving upon the previous state of the art on all three sub - tasks .
table 6 shows the performance of our method compared to the best previous approaches across the five downstream tasks . our cbow - r model significantly outperforms the cmow model in all the three tasks .
we compare our proposed method with the previous state - of - the - art methods on the hidden test set of somo . in general terms , we observe that the new cbow model significantly outperforms the previous methods . its compactness and recall results demonstrate that it can rely less on superficial cues . moreover , the accuracy is superior than previous models across all metrics , with the exception of tense .
we observe that the best performing method is the cbow model . it outperforms all the mod table 3 shows that cbow - r can significantly improve over the sub - jurisdiction of subj when trained and tested on the multi - news dataset . it achieves state - of - the - art results , outperforming all the alternatives except sick - e .
the results are shown in table 4 . in all but one of the comparisons , our system obtains the best results . the difference is most prevalent in the loc category , supervised learning underperforms all the alternatives except name matching . we observe that in this particular case , our model τmil - nd is better than the other systems in both loc and misc categories . moreover , the difference is narrower in the subtasks of loc and eps , indicating that the model can rely less on superficial cues . our model outperforms other models across all subtasks . in particular , it achieves the best performance in the food category , the largest of the three subtasks , with an absolute improvement of 2 . 36 points over the previous state of the art .
the results on the test set are shown in table 2 . in all but one case , our system obtains the best f1 score . the other two settings give considerably worse performance . supervised learning under automatic metrics results are slightly better than our system but still inferior to both the best baseline and the best previous state of the art model . we observe that τmil - nd ( model 2 ) is significantly better than the model in both settings . name matching is indeed easier than in mil - 1 , but still requires a considerable amount of data and time to train . precision improvements are modest but consistent , and we note that we consider them as a lower - bound on the actual improvements as the current test set comes from the same distribution of the training set , and also contains similarly noisy pairs .
we show the results of our experiments in table 6 . our model obtains the best results with a ref / gen score of 73 . 86 . it closely matches the performance of the best previous models . however , g2s - gin significantly outperforms other models with different feature sets in terms of both ref and gen . its average number of iterations per model is significantly higher than that of other models , indicating that the model is more interested in features specific to its target domain .
we compare our model against previous stateof - the - art models on the ldc2015e86 and ldc2017e86 datasets . the results are presented in table 2 . our g2s model outperforms all the other models apart from the case of the meteor metric in which it obtains the best performance . note that our model performs on par with the best previous state of the art on both datasets in terms of bleu metric , while it performs slightly worse than the s2s baseline .
table 3 shows the results for ldc2015e86 test set when models are trained with additional gigaword data . the g2s - ggnn model achieves a new best performance of 32 . 23 % on the test set , which shows the effectiveness of our model design .
we report the results of an ablation study on the ldc2017t10 development set . the results are summarized in table 4 . our model significantly outperforms the previous state of the art on all metrics except meteor .
we noticed that the g2s model with the shortest sentence length outperforms the models with the longest sentence length . this confirms the importance of word embeddings . we observed that the number of tokens in a sentence is relatively less important than the length of the sentence , which explains the lower precision . nevertheless , our model did not have the advantage of using more data , and consequently , we observed lower accuracy .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added " and miss " according to our system . gold refers to the reference sentences . the token lemmas are used in the comparison . we find that g2s - ggnn significantly outperforms s2s in terms of gold - associate tokens ,
we also evaluated the pos tagging accuracy using features extracted from the 4th nmt encoding layer , trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . our model pos model shows marked improvement in accuracy over the best previous state - of - the - art model . pos now tags 96 % of the sentences in the correct language . sem also shows a slight improvement , but still performing substantially worse than pos .
table 2 shows pos and sem tagging accuracy with baselines and an upper bound . our embeddings outperform the best previous approaches across all three tags . pos is the most frequent tag , followed by sem , word2tag is the better performing tag . the upper bound on pos shows that our encoder - decoder is more than 4 . 5 % better than the previous state of the art encoder .
the results are presented in table 4 . precision is the average of the number of frames per second for each tag , with precision being the most important metric . precision is relatively consistent across all three types of tags , with the exception of pos tagging accuracy . it is clear from table 4 that the refinements made during the development of our system have resulted in significantly better precision . our system outperforms previous state of the art models .
we can see from table 5 that pos and sem tagging accuracy with features from different layers of our four - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . our proposed system exhibits marked improvements in accuracy over the best baseline model . pos tagging accuracy improves from 0 . 9 % to 1 . 8 % over the strong baselines . residual features also improve , but to a smaller extent than that .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference in accuracy between the attacker score and the corresponding adversary is minimal , however we see significant difference in sentiment score due to different protected attributes being present in different training sets . gender - neutral features like age and race seem to have little effect on the performance , however it does impact the overall results . sentiment features have a significant effect , showing that the presence of gender - neutral words in the training set helps the attacker to improve his performance .
accuracies are shown in table 1 . our system outperforms all state - of - the - art methods with a gap of 10 . 8 % in accuracy .
the results in table 2 show that the balanced task accuracy and unbalanced task accuracy splits are caused by different aspects of the data being split into tasks that are assigned to gender - neutral and race - neutral categories . gender - neutral task accuracy is relatively high while racial accuracy is low . we find that the presence of the word " gender " in the label helps the model to distinguish between the true response and negative responses . it also helps to more precisely detect instances of data leakage when it comes to task accuracy .
the performance of our system on the test datasets is shown in table 3 . the difference in accuracy between the attacker and the corresponding adversary is minimal , however we see significant difference in leakage . our system outperforms other methods with an adversarial training set . gender - neutral features like age and race contribute similarly to the task performance , however the difference is less pronounced for pan16 . we find that pan16 is particularly sensitive to gender bias , in that it is gender - neutral and age - based , and consequently requires significantly less training data to train . this corroborates our intuition that gender bias is a significant part of the gender bias in how the human judgement works .
the results reported in table 6 show that the rnn encoders easily distinguish between the true response and negative responses . guarded embeddings also perform better than unencoded ones .
we show the results for the best performing model on the ptb and wt2 base and dynamic set in table 2 . the results show that our model can significantly outperform the previous state of the art on both datasets with a gap of 10 . 36 points in finetune score . the difference is most prevalent in the dynamic set , where our model obtains 73 . 36 % improvement over the previous best state . on the base and finetune set , our lrn model achieves 73 . 37 % improvement . we notice that the number of parameters for the dynamic set is slightly more than the base set , but still superior to other models with smaller training data . also , we notice a drop in performance between lrn and lstm due to larger training data set size .
the results of the best performing model on the base acc metric are reported in rocktäschel et al . ( 2016 ) in table 2 . the results show that our model obtains the best performance with a gap of 10 . 5 % in acc time from the last published results ( 83 . 27 % vs . 82 . 03 % ) on the base acc metric .
table 3 presents the results of the second study in the set of yelppolar err and time extraction on the amapolar and yahoo time datasets . we benchmark against the best previous models on both datasets . the results are presented in zhang et al . ( 2015 ) . the results reconfirm that the translation quality of our model can be further improved with an increase of precision in the low - supervision settings . our model obtains the best results with an err of 37 . 242 on the yelptime dataset , which shows the advantage of finetuning word embeddings during training .
we also show the case - insensitive tokenized bleu score on the wmt14 english - german translation task . it can be seen in table 3 that the gnmt model significantly outperforms other methods in decoding one sentence in milliseconds . gnmt even outperforms the best previous model , olrn , in completing the task in a single training batch . lrn also outperforms atr in terms of time in milliseconds used to decode one sentence , as shown in the second group of table 3 . though the number of tokens in question is less than the size of gnmt dataset , the model performs better on newstest2014 dataset , as the average time taken to decode each sentence is greater than the time to train . we also observe that the model is more than 4x faster in decoding task with a training batch size of 0 . 2k training steps on tesla p100 .
we report the exact match / f1 - score on squad dataset in table 4 . it can be seen that the model performs well with the assistance of elmo ( elmo ) parameter in the parameter number of base . however , the model do not benefit from the elmo parameter , which explains ∼ 9 % drop in performance . we managed to regain some accuracy with the β modulation with the help of the elmo parameter in our final model .
table 6 shows the f1 score of our model on the conll - 2003 english ner task . the model obtains the best performance with an f1 of 90 . 56 on the three parameter number . it closely matches the performance of the best previous model ( lample et al . , 2016 ) in both language .
table 7 shows that our model obtains 85 . 26 % accuracy improvement on snli task with the base setting and 169 . 81 % increase in perplexity score over the strong base setting .
table 2 shows the system and sentence recognition results . our approach dkrn outperforms all state - of - the - art methods in terms of all metrics on both systems with two tasks . oracle retrieval is more than 4 . 5 times faster than human on all systems , with the exception of system . sentances are slightly higher than human for oracle , but still superior on system . we show the results for b - 2 , b - 4 and b - 5 to show that our approach can significantly improve interpretability without a drop in performance .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that our system is well - equipped to perform in the low - resource settings . among all the systems , seq2seq is the best in grammatical accuracy ( grammaticality ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the second best result is achieved by candela ( 30 . 2 % ) on the content richness scale , which shows the extent to which the syntactic patterns captured by the embeddings can be improved with a reasonable selection of the lexical resource from which the grammatical patterns were derived . the third best result by a large margin is obtained by h & w hua and wang ( 2018 ) on the sentiment analysis scale , at 38 . 8 % .
the results are shown in table 6 . our system outperforms the best previous approaches across all three domains . we observe that the performance gap between en and pt is minimal , but significant with respect to subsets , it should also be noted that our system performs better on datasets with fewer training examples , namely , those in the " food & beverage " category . these results show that our approach can significantly improve the generalization ability of our model across all datasets .
for completeness , here we also compare against the best previous approaches across the five domains . we benchmark against the following baselines : lang , corpus , patt , dsim , slqs , tf , tf - sqs and docsub . results are presented in table 2 . the results are broken down in terms of performance on extractive and abstractive keyphrases . for extractive keyphethics , we observe that our proposed approach outperforms the previous state - of - the - art on average . on the other hand , it performs slightly better than the previous best baseline on abstractive part - ofspeech . this suggests that there are still some areas where our approach can improve upon the performance of the previous approach .
for completeness , here we also compare against the en / pt scores of the best previous models on the training set . we observe that for the europarl dataset , our model obtains the best performance . on the other hand , our pt model performs slightly worse than the other two baselines on the ted talks dataset .
the system performs well on synthetic dataset with a minimum of 21 . 05 % overall improvement over the baseline . europarl has the best overall performance . our numberrels are 1 , 588 , which means that more than half of the words in the dataset are actually relevant to the task at hand . our average depth measurement is 11 . 05 % , which shows that the number of tokens in the europarl corpus is relatively balanced .
from table 1 , we find that the average depth of our numberrels is 9 . 43 % , which means that our average number of roots per row is 1 . 005 . our europarl dataset is 1 , 527 . 29 % over the previous state of the art . we find that our depth - based metrics are comparable , but do not exceed the maxdepth threshold , which indicates that there is a need to design more complicated features .
in table 1 , we compare the performance of our approach against the baseline model with the enhanced version of visdial v1 . 0 . 7 . the enhanced version exhibits significant gains in ndcg % . it now achieves 73 . 42 % ndcg % on the validation set , which shows that it can significantly improve the interpretability without sacrificing too many answers .
in table 2 , we report the performance ( ndcg % ) of ablative studies on different models on the visdial v1 . 0 validation set . the best performing model is the coatt model , showing that hidden dictionary learning is very effective . the model achieves 73 . 44 % ndcg % improvement over the baseline on coatt in table 2 .
we compare our proposed hmd - recall model with previous stateof - the - art models on hard and soft alignments . the results are summarized in table 5 . we observe that the hmd - f1 model significantly outperforms the pre - trained hmd model on all metrics except for gold - aligned test set .
the results are presented in table 2 . our method outperforms all the base lines with a gap of 10 . 3 % in direct assessment score from the last published results ( cs - en ) on ru - en . we observe that the method significantly boosts the generalization ability of question answering . the average score of bertscore - f1 is significantly higher than the previous state - of - the - art method on all metrics except for meteor + + . sent - mover scores significantly outperform other methods ,
the results are shown in table 2 . our system outperforms all the base lines with a gap of 0 . 012 points under the smd baseline . the difference is most prevalent under the " inf " and " qual " metrics , which shows that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . we further compare our system with other methods of leveraging semantic information . we observe that meteor significantly outperforms bleu in both semantic and quantitative metrics , in particular , it beats bertscore - f1 by a noticeable margin .
we noticed that the word - mover task is relatively easier for the smd model to achieve when the accuracy is set to m1 than it is for wmd - 1 . moreover , the accuracy drop between m1 and m2 indicates that the semantic information injected into the model by the additional cost term is significant enough to result in a significant drop in predictive accuracy . we observed that word - movement accuracy is relatively consistent across the multiple metrics , with the exception of leic ( * ) where it shows a slight drop .
we noticed that the performance reach its best when the model is trained with only one type of wrapper layer , namely , para - para , with an accuracy drop of 0 . 81 points in sim . adding the second layer of wrapper helps improve the results for all models , but the drop is most prevalent in m1 and m2 . we notice that the model performing best when trained with both meta - and meta - language features is the result of more training data available in the supplementary material .
table 3 shows the results for english and spanish . our model obtains the best results in the semantic preservation and transfer quality . semantic preservation results are significantly better than the previous state of the art . we observe that our model b has the best overall performance in all three semantic preservation tests . transfer quality is relatively consistent with the best previous state - of - the - art models , with a gap of 10 . 5 % in english . we observe a slight drop in transfer quality between yelp and yelp , however , when we switch from m0 to m7 . this suggests that yelp may rely on superficial cues .
table 5 shows the results of human evaluation . our approach verifies the accuracy of our summaries with a minimum of 94 % on each metric . the difference in acc between human evaluation and machine is minimal , however we see significant difference in human evaluation percentage due to different phrasing of the metrics .
we further compare our model with other models trained on simnet with different classifiers trained on the simnet dataset . the results are presented in table 7 . the results of the best - performing classifiers are reported in bold . our model obtains the best results with a gap of 10 . 2 points from the last published results ( m0 ) with the help of our pre - trained model .
results in table 6 show that our transfer model achieves higher bleu than prior work on yelp sentiment transfer , and acc is restricted to the same 1000 sentences . the multi - decoder approach achieves the highest acc , and we note that it also applies the best feature set , the style embeddings , in that the number of tokens in each row is limited to 1000 , which means that the accuracy obtained by the model can vary depending on the classifier in use . we notice that the multi - decoder approach also achieves higher acc than previous work , because the training set contains more tokens , and therefore requires fewer tokens to transfer . sentiment transfer is indeed better than simple transfer .
we report the percentage of reparandum tokens that were correctly predicted as disfluent as disfluencies in table 2 . as a baseline , we also include the number of repetition tokens in the original reparation sentence for each repetition token as a percentage of the total number of tokens , in case the repetition tokens are less than 1 . reparandum length is relatively consistent across the three types , with the average being 0 . 66 . however , this analysis fails to account for the fact that repetition tokens can have multiple meanings , which leads to incorrect predictions .
we observe that for disfluencies that contain a content word in both the reparandum and repair ( content - content ) , the model predicts the rephrases as disfluent even when the content word is in the repair sentence . it also shows the percentage of tokens that belong to each category that are correctly predicted as disluent for each category . reparandum length is the average of the number of tokens in the reparation sentence , repair is the extent to which the tokens can be repaired in the correct context . function - function tokens are the smallest fraction of tokens , but are the most frequently predicted to be disfluential .
in the en - de news commentary , we report the best performances of the models in the single - domain and multi - domain setting . the results are broken down in terms of performance on dev and test set . the early results show that text alone contributes significantly less to the model ' s performance than the text alone contribution . however , this gap is less pronounced when the model is trained with both raw and uniform variants of the word embeddings . the model performs much better when trained with innovations . this corroborates our intuition that incorporating all the information available in the input data helps the model to improve its performance . moreover , the model performs better when training with innovations as well . we observe that this helps to more precisely detect trends in the output attention .
we observe that our model exhibits considerable performance improvement over the state - of - art algorithms on the fnc - 1 test dataset . it closely matches the performance of the best previous embeddings ( hochreiter and schmidhuber , 2009 ) in terms of accuracy . in fact , our model achieves 8 . 43 % higher accuracy on average compared to the previous state of the art algorithm .
table 2 shows the performance of each method on the apw and nyt datasets for the document dating problem . our unified model significantly outperforms all previous methods . the accuracy is higher than the previous state of the art on both datasets , indicating that the model can rely on superficial cues more effectively .
we compare the performance of our method with and without word attention for this task in table 3 . our approach shows that both word attention and graph attention have a significant impact on the performance , improving the t - gcn by 9 . 6 % in the word attention task and to parity in the graph attention task .
the results are shown in table 1 . our model significantly outperforms the previous state of the art on all metrics . it achieves state - of - the - art results in 1 / 1 and 1 / n tasks , significantly improving over the performance of previous methods . we notice that the argument argument stage is the most difficult part of the model to solve , as it requires a significant amount of data and time to train . fortunately , this gap is less pronounced for jrnn , since it relies on word embeddings pre - trained on a single dataset . the model performs well on both validation and argument stage , with a gap of 10 . 5 % on average compared to previous models . embedding + t improves results , but does not improve significantly over argument stage .
table 1 shows the results for event identification and event classification . our approach establishes a new state - of - the - art in this regard , significantly improving upon the previous state of the art . cross - event event identification is significantly better than f1 on all datasets with a gap of 10 . 5 points from the last published results . the method is signifi cantly better than dfgn when trained and tested on the multi - event dataset . it exhibits marked improvements in interpretability with bootstrapping and permutation tests .
consistent with the results of the second variation experiment , we observe that spanish - only - lm is slightly better than english - onlylm in both cases . however , it is inferior to both languages in terms of acc score . the difference is less pronounced for english , as it is trained on a significantly larger corpus . we observe that fine - tuning has a generally positive effect , improving the acc scores of both test set and target perp by about 2 % .
the results in table 4 show that fine - tuning the model improves the results for both subsets of the training set . the fine - tuned model improves upon the strong lemma baseline by 3 . 8 points in the standard task formulation .
we can see from table 5 that fine - tuning has a generally positive effect ( i . e . accuracy improves ) on the dev set and on the test set , according to the type of the gold sentence in the set , both when the gold sentences are gold - two - mention - qualified and when they are in the standard gold sentence set . additionally , fine - tuned - lm also improves accuracy . the difference is less pronounced for monolingual sentences , but still noticeable .
we report precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement in precision from the baseline to the current state - of - the - art is statistically significant ( p = 0 . 0088 , double - tailed ttest ) which shows that the ability to combine gaze features with object detectors has a significant impact on the interpretability of our model .
we show precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features for evaluating the conll - 2003 dataset in table 5 . the improvement from the baseline to the current state - of - the - art is significant , which shows that the ability to combine gaze features with the correct recall metric has improved the interpretability of our model .
we apply the best adaptive decoding scheme , ontolstm - pp , to wordnet 3 . 1 , and glove - extended embeddings obtained by running autoextend rothe and schütze ( 2015 ) . the results on the test set of belinkov2014exploring ’ s ppa test set are shown in table 1 . the improved performance by about 9 % over the baseline indicates that adaptive decoding is possible with a reasonable selection of the lexical resource set and the correct token embedding scheme .
results in table 2 show that our hpcd system obtains full uas acc . on par with the best previous state - of - the - art systems across all pp attachment predictors and oracle attachments .
we observe that the effect of removing sense priors and context sensitivity ( attention ) from our model is minimal , however it is significant enough to show that it has a significant effect on predictive performance .
we find that incorporating subtitle data improves the results for both en - de and mscoco17 models . the resulting bleu % scores are slightly higher than those of marian amun ( marian amun et al . , 2017 ) when the subtitle data are added to the multi30k dataset . however , the improvement is less pronounced for en - fr : the ensemble - of - 3 model achieves 60 . 2 % improvement over the strong baselines across all three domains .
we show that the domain - tuned model outperforms the plain model with a large margin . h + ms - coco achieves 60 . 3 % hoco improvement over the baseline model on three out of the four datasets . the difference is most prevalent in en - de , where the h + loc model improves by 3 . 3 % . however , for subs1m , the improvement is only 2 . 5 % higher .
we show bleu scores in percentage for en - de and en - fr when adding automatic image captions . the results are slightly better than marian amun et al . ( 2017 ) on multi30k dataset , but still significantly worse than the original model on flickr16 . adding automatic captions improves performance on both flickr16 and flickr17 . it improves the general performance , but does not improve the better performing model on mscoco17 . the difference is most prevalent in multi - task mode , adding the captions bridges the gap between the performance of " auto - attract " and " automatic captions " .
in table 5 we compare the results of our multi30k + ms - coco + subs3mlm model with other approaches for visual information integration . our model achieves the best overall performance with a bleu % of 62 . 38 on the fixed - element img dataset .
we notice that the multi - lingual approach is comparable to the approach described in ( fancellu et al . , 2018 ) in that it achieves the best generalization . however , it does not outperform the monolingual approach . in en - de , the results are slightly worse than those in en - fr . ( see x4 ) .
in the en - fr - ht translation task , the results are presented in table 2 . our model obtains the best performance on three out of the four metrics . the difference is most prevalent in mtld metric , it is clear that adapting the word embedding for the task at hand , boosts the generalization ability of the model , since the translation quality is better than that of the original embeddings . in particular , our model outperforms the previous state - of - the - art in all metrics .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . our en – fr split is 1 , 467 , 489 words , which means that there are 7 , 723 sentences in total that can be considered as train sentences and 5 , 734 as development sentences .
the training vocabularies for the english , french and spanish data used for our models are shown in table 2 . the total number of words for each language pair is 113 , 132 , which means that the models can easily distinguish between the true meaning of the words .
in table 5 , we report the automatic evaluation scores ( bleu and ter ) for the rev systems . our system obtains the best performance with a bleu score of 43 . 7 out of 100 . the en - fr - trans - rev system is slightly better than the previous state - of - the - art rev system .
table 2 shows the performance of our model in flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . com . it achieves the best recall @ 10 @ k9 with a mean mfcc of 0 . 7 . segmatch also outperforms rsaimage ,
the results on synthetically spoken coco are shown in table 1 . audio2vec embeddings outperform rsaimage in terms of recall @ 10 , while the average number of recalls per second is slightly higher than rsaimage . we find that the acoustic features boost recall , and thereby improve the generalization ability of the model .
we report the example sentences of the different classifiers compared to the original on sst - 2 . in table 1 , we show that rnn turns in a screenplay that is slightly better than the original , but still requires a lot of data parsing . it is clear from table 1 that the use of the best classifiers helps the model to improve interpretability .
we note that the number of occurrences have increased , decreased or stayed the same through fine - tuning of the original sentence in sst - 2 . however , the percentage of occurrences in the correct sentence has increased , so that the overall number of tokens in question has increased . this indicates that the part - of - speech transformation has a positive effect .
the sentiment score changes in sst - 2 . and indicate that the score increases in positive and negative sentiment . the numbers indicate the changes in percentage points with respect to the original sentence . in the flipped to positive sentiment case , the positive sentiment score increases by 9 . 3 points while the negative sentiment score decreases by 6 . 6 points .
for evaluating the results , we compare against pubmed and sst - 2 . results are presented in table 2 . our best pmi score is 98 % , but we should note that it is significantly better than the best pubmed score . also , we find that our approach improves the generalization ability of pmi by 9 % in the bad case .
