table 2 : throughput for processing the treelstm model on our recursive framework , fold ’ s folding technique , and tensorflow ' s iterative approach , with the large movie review dataset . the results are shown in table 2 . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . as the results show , both approaches benefit from the scalability of using a small number of instances to train the model , and from the advantage of faster iteration between training instances .
table 1 : throughput for the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as much as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t . considering the fact that the balanced dataset has 25 instances , this small improvement does not account for the significant drop in performance between balanced and linear datasets .
results for each model with different representation are shown in table 2 . the max pooling strategy consistently performs better in all models , indicating that it is better to select the features with the highest recall and accuracy for each iteration . we observe that ud v1 . 3 achieves the best results with a f1 score of 75 . 83 % compared to sigmoid et al . ( 2014 ) and conll08 ( 2014 ) . as hard coreference problems are rare in ud , we do not have significant performance improvement . however , these results show that softplus also performs better than softplus under all three scenarios , confirming the importance of the feature - pooling strategy .
table 1 shows that using the shortest dependency path on each relation type bridges the gap between the best f1 and the worst f1 scores . it also improves the generalization ability of our model , both in terms of f1 score and overall ranking .
the results are shown in table 1 . results are broken down in terms of r - f1 100 % and f1 50 % on average . for both groups , our system outperforms the best previous state - of - the - art model by a noticeable margin .
the results are shown in table 1 . for brevity we only report results for paragraph level 1 and 2 . all other metrics are statistically significant at significance level 0 . 01 . pretrained mst - parser outperforms all the other systems except for mate . it achieves a gap of 10 . 69 % on average with respect to paragraph accuracy .
we note that the average performances for both systems are lower than the majority performances over the test set , indicating that the parser performs on par with the essay level .
the results are shown in table 1 . the first group shows that original tgen model is significantly better than tgen + and sc - lstm when the error reduction is removed . next , we see that tgen is slightly worse than original when trained and cleaned , indicating that it has been trained and tested on a significantly larger corpus . finally , we observe that rouge - l is comparable to tgen − but does not generalize well , likely in part due to the high overlap between training and test set size . all metrics we consider here are statistically significant ( p < 0 . 001 ) with respect to errors reduction . table 1 shows that the smaller performance gap between original and cleaned model indicates that training on a larger corpus leads to a better model .
table 1 shows the comparison of the original e2e data and the cleaned version . the difference in mr statistics is minimal , however we see significant difference in ser as measured by our slot matching script , see section 3 . the difference is much larger for training data , as shown in table 1 .
the results are shown in table 6 . the first group shows that original tgen + outperforms tgen − and sc - lstm , while the second group shows the best performance . next , we compare our model with other sophisticated neural models like bleu , nist , meteor , rouge - l and cider . we observe that the difference in performance between original and original is less pronounced for all these systems , indicating that training on a larger corpus of data helps the model to improve its generalization ability .
table 4 : results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) as well as the number of errors caused by cleanups ( for each error we found ) are shown in table 4 . the difference in errors between the original and cleaned set is minimal , however we see significant difference in the percentage of errors for both sets due to different classifiers using different training data . disfluencies are mostly caused by the presence of incorrect values in the training data , as shown in fig . 3 .
the results are shown in table 1 . all models show that their performance is comparable to or better than the best previous state - of - the - art on all metrics . the only exception is the graphlstm model , which shows much worse performance than all the other models except for pbmt . note that the difference is mostly due to small size of the ensemble : dcgcn ( ensemble ) is only 0 . 2m users , while seq2seqk is more than 3m users .
the results on amr17 are shown in table 2 . our model achieves 24 . 5 bleu points . by comparison , the previous state - of - the - art on seq2seqb achieves 52 . 6 points . also , we notice a drop of more than 2 points in performance when we switch from a single to an ensemble model .
the results are shown in table 1 . first , we compare against the single - and multi - class models . for english - german , we see that the model using bow + gcn achieves the best performance with a bias score of 43 . 8 % on average compared to the previous best performance of 39 . 9 % by birnn . for czech - german we see a gap of 2 . 6 points with the previous state - of - the - art model .
the effect of the number of layers inside the network is shown in table 5 . as table 5 shows , the larger the network , the more complicated our model is able to perform . we observe that m = { 1 , 2 , 3 , 4 } has the greatest effect , while n = m has the smallest effect .
comparisons with baselines are shown in table 6 . the results show that gcns with residual connections are stronger than those without . gcn with rc as well as la features are particularly strong , outperforming all the alternatives except for dcgcn1 . note that rc and la are the only two groups that are strongly related to each other .
the results are shown in table 4 . our model outperforms all the other stateof - the - art models with a large margin . for example , dcgcn ( 1 ) achieves a performance improvement of 10 . 2 % over previous state of the art models on average . it closely matches the performance of oracle with only 0 . 5 % absolute difference .
the results shown in table 8 indicate that removing the dense connections in the i - th block significantly decreases the density of connections on the dev set of amr15 . the results also indicate that dcgcn4 is more stable and therefore requires fewer connections to reproduce .
the results of an ablation study for modules used in the graph encoder and the lstm decoder are shown in table 9 . the results show that - global node & linear combination achieves the best results , while - direction aggregation achieves the worst results . we observe that both combinations of features give better performance overall when compared with the original embeddings .
the performance of our initialization strategies on probing tasks is shown in table 7 . our framework establishes a new state - of - the - art on all three high - level neural networks , and on all subtasks except for subjnum . it improves upon the strong baselines by 4 . 8 points in the depth and tense measures , and by 1 point in the length measures .
the results are shown in table 6 . our h - cbow model outperforms the previous state - of - the - art methods and the best - performing variant of somo . it shows that the compactness and recall are the most important factors in selecting the correct subsets of objnum and topconst . subjnum is the only one that is trained with cbow / 400 as the training medium . its performance is significantly worse than that of h - cmow , both when trained and tested on the large scale web content . the difference is less pronounced for smaller datasets , but still suggests some reliance on superficial cues .
the results are shown in table 6 . our model outperforms all the other methods except sick - e . it improves upon the previous state - of - the - art on all metrics by 3 - 4 points . it achieves the best results on three of the four metrics .
table 3 shows the performance on unsupervised downstream tasks attained by our models . the results show that cbow has significantly outperformed cmow in all but one of the 13 cases ( and hybrid in sts13 and sts16 ) when we switch from cbow to cmow as the training set grows larger . however , the difference between cbow and hybrid is less pronounced for sts15 , indicating that the cbow model has better generalization ability . when we switch to sts14 and 16 , we see a drop of 1 . 5 % in performance relative to hybrid .
table 8 presents the performance of our system for initialization and supervised downstream tasks . our system establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except mpqa . it outperforms previous models with a gap of 10 . 2 points in terms of roc score .
table 6 shows the performance of our method compared to state of the art cbow - r on the unsupervised downstream tasks . the difference is minimal , however we see significant performance drop for sts12 and sts13 as compared to those on sts14 . our model outperforms all supervised methods except cmow in terms of training objectives . it achieves state - of - the - art results on all the three tasks , and even achieves competitive or better results than the best previous methods .
the results are shown in table 1 . we observe that the compactness and length are the most important factors in the success of our model , followed by the bshift metric and coordinv . our model obtains the best generalization results . it outperforms all the other methods apart from subjnum in terms of both depth and length . moreover , it improves its generalization ability over previous models by 3 . 8 points on average .
the results are shown in table 6 . our model outperforms all the other methods apart from sick - e . it establishes a new state - of - the - art on all metrics with a gap of 10 . 6 points from the previous state of the art .
the results are shown in table 1 . in general terms , the results are broken down in terms of loc and misc . our system outperforms all supervised and unsupervised systems except for the case of name matching when using all loc and all org . it closely matches the performance of org and contains only fragments of human - generated utterances , so it does not have the advantage of training with all the information available from the other systems . moreover , it performs much worse than the best supervised system under all three metrics . supervised learning is only slightly better than random learning , and slightly worse than τmil - nd . we observe that given the correct loc and org , the difference between error reduction is minimal , but large under misc , which indicates the difficulty of selecting compact regions to train on .
results on the test set under two settings are shown in table 2 . in general terms , the results show that both systems perform well in terms of f1 score . name matching and supervised learning have the highest f1 scores , respectively , with a 95 % confidence intervals . however , when trained with only tn and rn as inputs , both systems show much better performance . the difference is less pronounced for mil - nd , showing that the model can rely less on superficial cues . supervised learning has the upper hand on both sets , showing the effectiveness of finetuning neural networks during training . to test the contribution of rn and eq . , we compare our model to other models trained with different learning rates . we observe that our model performs slightly better than the other two models in both settings .
table 6 presents the results on paragraph selection . our model outperforms all the base lines with a gap of 10 . 86 % in terms of ref and gen while g2s - git performs slightly better than s2s . as can be seen , the difference is less pronounced for nested systems , but still suggests some reliance on superficial cues . we observe that the architectural choice that directly affects paragraph selection is crucial for the model to succeed , as it helps to improve the generalization ability of the model .
the results are shown in table 3 . our model outperforms previous stateof - the - art models on all metrics by a noticeable margin . for example , g2s - gat achieves a final score of 29 . 16 % on the ldc2015e86 score compared to 29 . 28 % by konstas et al . ( 2015 ) and 29 . 42 % by guo et al . , ( 2017 ) . similarly , it achieves a provisional score of 30 . 53 % ± 0 . 88 % higher than the previous best state of the art on ldc2017e86 . by further adding bleu and meteor scores , our model achieves an absolute improvement of 2 . 36 % and 1 . 55 % points , respectively , compared to previous work . the results reconfirm that pre - training of g1s leads to a better model performance , as the performance decreases as a result of training with fewer training examples .
table 3 : results on ldc2015e86 test set when models are trained with additional gigaword data . the results show that g2s - ggnn significantly outperforms both bleu and svm with a gap of 10 . 60 % on the test set .
as a result , we report the results of an ablation study on the ldc2017t10 development set . the results are summarized in table 4 . our model outperforms the previous state - of - the - art models on all metrics except meteor .
the results are shown in table 1 . we observe that g2s - gin outperforms all the base models with a gap of 10 . 51 % from 0 - 7 , the average sentence length of s2s is 34 . 2 % longer than that of other models , while for 10 - 20 is longer than other models .
table 8 shows the fraction of elements in the output that are not present in the input that are missing in the generated sentence ( added " and miss " , for the test set of ldc2017t10 . the token lemmas are used in the comparison . as shown in the table , the smaller miss fraction indicates , the model is better at selecting the relevant elements and generating the correct sentence when the required ones are absent . g2s - gat shows much worse performance than s2s and s2r , indicating that the model relies on superficial cues .
the pos and sem tagging accuracy improvements are shown in table 4 . the difference in accuracy between different target languages is minimal , however we see significant difference in sem and pos tagging accuracy due to different target language having different grammatical genders . pos also shows a significant racial disparity in terms of accuracy compared to sem , showing that pos is more sensitive to the gender - neutrality of the target language .
pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . word2tag significantly outperforms the plain embeddings and unsupemb in both metrics . the difference is less pronounced for pos , however it is much larger for sem . regularization reduces repetition and allows more tags to be tagged per label , which results in higher accuracy .
the performance of the system is presented in table 4 . we observe that , let alone a reduction in performance , the system performs on par with state of the art models across all metrics . it achieves the best results with a minimum of 80 % accuracy .
pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . we observe that the first layer of our model exhibits the best performance . the second and third layers show the worst performance . finally , we observe the fourth layer shows the highest performance .
table 8 shows the attacker ’ s performance on different datasets . results are on a training set 10 % held - out . the difference between the attacker score and the corresponding adversary is minimal , however we see significant difference in accuracy due to different classifiers being used for different tasks . gender and race are the most difficult classifiers to detect , as shown in table 8 . on the other hand , age and sentiment are the only ones that are relatively easy to pick out as features of interest for the attacker .
as shown in table 1 , training directly towards a single task improves the performance for all models except for those trained using pan16 . gender bias also reduces the accuracy for some of the models , as it increases the recall for gender - neutral tweets .
table 2 shows that both balanced and unbalanced data splits result in significantly less data leakage than in the balanced set , indicating that the racial disparities in the task prediction are less pronounced for gender - balanced tasks .
the performance on different datasets with an adversarial training set is shown in table 3 . as the results show , the difference between the attacker score and the corresponding adversary ’ s accuracy is minimal , however we see significant difference in terms of leakage . this indicates that the presence of gender bias in the task design can contribute to the performance of the model , as it can be seen from the large difference in error between the fielded and unbalanced test sets .
the results are shown in table 6 . rnn encoders significantly outperform embedding guarded and leaky embeddings , indicating that the protected attribute is a stronger baseline for future research into neural network design . as hard coreference experiments show , the difference between the protected and unencoded tokens is less pronounced for rnn , but still suggests some difficulty in predicting the distribution of the protected tokens .
the results are shown in table 1 . our model outperforms the previous stateof - the - art on all metrics with a gap of 10 . 36 % on the ptb and wt2 bases . it also outperforms both published and unpublished work on the finetune test set . the difference is much larger on the wt2 test set , where our model obtains a final score of 84 . 15 % on finetune and 94 . 97 % on ptb . the results reconfirm that the dynamic nature of the training set alone leads to a better model performance . we observe that the number of parameters considered for each training set is relatively small , but large enough to result in a significant improvement on the final score . table 1 compares our model with previous work on three of the four datasets . the results show that the size and type of training set that can be used for each variation of the model is important . for example , the lstm model has a total of 22m training examples , while the gru model has 17m .
the results are shown in table 1 . the first group shows that the training time and the average number of iterations for each parameter is the most important factors in determining the model ' s performance . the second set shows that it is relatively easy to set up and maintain a working model with a low error rate . the third set shows the performance of previous models when using the correct combination of base acc , time and length of training epochs . table 1 shows that gru is comparable to lstm and sru in terms of both acc and time to train , with a gap of 2 . 5 % on average compared to rocktäschel et al . ( 2016 ) . in addition , the difference is less pronounced for smaller models , as shown in the second group of table 1 , smaller models tend to have higher error rates , this confirms the effectiveness of redundancy removal .
table 3 presents the results on the test set for amapolar err , yelppolar time and google + time . we also include the results for nathan et al . ( 2015 ) as a sanity check . the results are summarized in table 3 . our model obtains the best results on all metrics with a gap of 10 . 08 % on average compared to the previous state of the art . the difference is mostly due to small size of the training set : this model performs on par with oracle , lstm and sru . on the other hand , it achieves the best time - averaged err and average number of tokens per user , which shows the effectiveness of our model .
table 3 : case - insensitive tokenized bleu score on wmt14 english - german translation task . as shown in the table , all the models used to train on tesla p100 have decent bleus scores . however , olrn is inferior to all the other models except gnmt and atr . the difference is less pronounced for gru , as seen in table 3 , the smaller training batch size makes it easier for lrn to decode one sentence per training batch than for other models . also , the average time taken to train and decode is shorter than for any other model , lrn is comparable to gnmt , however , it is faster than atr and sru on newstest2014 dataset , which shows the advantage of finetuning word embeddings during training . in addition , the model is more accurate in case of german as compared to english . table 3 shows that lrn has better generalization ability than other methods . it outperforms atr ,
we present the exact match / f1 - score on squad dataset as well as the percentage of responses scored against elmo for each parameter as a metric for scalability . the results are shown in table 4 . as the results show , the number of parameters in the base is the most important factor in model performance . besides , the presence of elmo also affects the model performance negatively , leading to a drop of more than 2 % in f1 score . table 4 shows that all the parameter numbers we consider have a significant impact on model performance , specifically , they reduce the error rate as measured by the rn @ k9 score .
table 6 : f1 score on conll - 2003 english ner task . as the results show , all the parameter numbers in table 6 indicate , the lstm model significantly outperforms other models with different parameter numbers as reported in lample et al . ( 2016 ) . the difference is less pronounced for gru , however it is still significant . sru also shows a significant drop in performance compared to lrn . table 6 shows that the number of parameters considered by our model is the most important factor in the success of the model . next , we consider the reported result of the last experiment in table 7 . it can be observed that , let alone a reduction in performance , the model performs well across all parameter numbers , indicating the model has a high degree of interpretability .
the performance on snli task with base + ln setting and test perplexity on ptb task with base setting are shown in table 7 . the difference in accuracy between snli and ptb is minimal , however we see significant difference in perplexity due to different training set configurations . lrn shows much worse performance than glrn and elrn in terms of training set selection .
table 2 presents the results on paragraph prediction . our system outperforms all the other systems with a large margin . for example , it retraces sentences with a gap of 1 . 55 words from the last published results ( micro - f1 ) and sets up new ones with a comparable number of words . it also outperforms human in terms of sentence prediction . retrieving the word " sent " is the most difficult part as it requires much more data than available in the training set . to simplify this task , we have decided to focus our work on sentence prediction using oracle retrieval . this approach uses word prediction with a minimum of 200 words per sentence . it outperforms both human and system prediction using the current set of features . the performance gap is modest but significant with bootstrapping and permutation tests . we observe that it takes significantly less data to set up a sentence prediction than for human .
the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 8 points , which indicates that seq2seq is indeed more accurate than human in generating most of the sentences . retrieval is only slightly better than human on grammaticality ( grammaticality ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the second highest ranking is achieved by candela ( 30 . 2 % ) on the content richness scale , followed by h & w hua and wang ( 2018 ) ( 38 . 8 % ) and ng et al . ( 2018 ) . in table 4 , we highlight the percentage of evaluations a system receives that are ranked in the top 1 or 2 for overall quality . table 4 shows that , overall , all the systems are comparable in terms of grammatical quality , with the exception of reaper .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , our model outperforms all the other baselines except ted talks . moreover , it achieves a new state - of - the - art on all datasets except for isdn , which shows the performance of our model is still significantly better than the other base on the other hand , we observe our model performs slightly worse than the others on isdn and tf .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , our model outperforms all the other baselines except ted talks . for example , it achieves an en / pt score of 0 . 4485 / 0 . 5710 and 0 . 5941 / 0 . 861 on the corpus and ted talks datasets , respectively , with an absolute improvement of 2 . 3 / 4 and 6 . 2 / 4 points over the previous state of the art . the relative lower performance of our model compared to previous models is mostly due to small size of the training set : on the smaller scale , our europarl model performs slightly better than the other large - scale baselines . on the larger scale , it performs slightly worse than our model .
the performance of these models on the test set is presented in table 6 . we observe that , let alone a reduction in performance , our model outperforms all the other baselines except ted talks . moreover , it achieves a new state - of - the - art on three of the four domains : it graphs , dsim , slqs and tf . the results are slightly superior than those of ted talks , but still inferior to the other two . table 6 shows that our model performs on par with the best previous models .
the performance of all metrics for europarl is presented in table 1 . from left to right , we see : total terms , roots , length of roots , averagedepth , numberrels and quality of clustering . we observe that , overall , all metrics show low correlation with the original embeddings .
from table 1 , we compare the performance of our system with previous stateof - the - art models on various metrics . our system outperforms all the base the competition on all metrics with a gap of 10 . 29 % on average . among all the metrics , the difference is most prevalent in terms of depth . our europarl system achieves the best performance with a weighted average depth of 9 . 43 % .
the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . 3 is shown in table 1 . the enhanced version of our model outperforms the baseline model with a large margin . it shows that it has significantly better performance with respect to both question type and answer score sampling . moreover , it exhibits a significant drop in performance against the strong baseline model due to the reduced number of question types and the fact that it is trained with a significantly worse ranking loss . we conjecture that this indicates that future work may need to consider principled ways of boosting the predictive performance of future models .
the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set is shown in table 2 . the results indicate that p2 indicates the most effective one ( i . e . , hidden dictionary learning ) is the most efficient . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . it also improves the generalization ability for both hciae and coatt .
the results on hard and soft alignments are shown in table 5 . hmd - f1 and hmd - recall perform comparably to wmd - bigram and wmd - unigram , as can be seen , both systems rely on word - error detector pretrained on the learned representations of pascal - voc , which are pre - trained on the fixed entities of the principals ' tree ( lin et al . , 2016 ) . the results of ruse are slightly better than those of wmd ( by a margin of 0 . 005 points ) when trained on the hidden entities , the results are slightly worse than those on the plain ones .
the results are shown in table 1 . the first set of results show that bertscore - f1 is significantly better than other baselines on the direct assessment metric and on the w2v metric . next , we see that sent - mover is better than meteor + + and ruse ( * ) on both sets , the second set shows that it is better at selecting the relevant features and its output is more interpretable . we observe that the training set size and the average number of frames per second are the most important factors in selecting the features for each training set , as these are used to train the model .
the experimental results are shown in table 1 . all metrics shown in bold are statistically significant with a bleu - 1 score of 0 . 95 or better . the sfhotel scores are significantly higher than any other baseline except for meteor . when we add w2v as a baseline , our results are slightly higher than the previous state - of - the - art . we observe that the clustering quality is relatively high across all metrics with two exceptions ( ' inf ' and ' nat ' achieving higher scores than ' qual ' . sent - mover is the only exception which shows lower performance than the baseline on all metrics except for those for hotpot . our system outperforms all the other baselines except for smd .
we further analyze our results with respect to word - mover . the first set of results shows that word - mover is relatively stable under all the various setting with a gap of 0 . 7 % on m1 and m2 . however , when we add elmo and bert scores , the gap grows significantly , showing that the model is already well - equipped to perform this task . the second set shows that the semantic features extracted by meteor are particularly useful for this task as the accuracy increases with the growth of the clustering of elmo nodes .
the results are shown in table 6 . the first group shows that the performance reach the best when the model is trained with only one type of classifier , namely , shen - 1 . the second group shows the performance at the cost of losing a lot of accuracy . while the error reduction is slim , it is significant : m0 has the highest percentage of out - of - vocabulary terms for all classifiers , so we suspect that there are not enough data to train models with all classes .
table 3 presents the results on transfer quality and semantic preservation . our model establishes a new state - of - the - art on all three high - level tasks , and on all subtasks except transfer quality . it closely matches the performance of oracle with only 0 . 3 % absolute difference . semantic preservation is very similar to transfer quality , with a gap of 10 . 5 % on m0 and m2 . in addition , the gap is much larger on m7 , showing the syntactic patterns discussed in section 3 . 3 are very similar . to test the contribution of parameter sharing , we compare our model to other models trained on the same dataset . we observe that yelp significantly outperforms other models with different feature sets . for example , google translate performs much better than svm with a large corpus of tweets , confirming the importance of word embeddings . on the other hand , word2vec topics perform much worse than those on yelp , likely in part due to the high overlap between training and test set size . this gap is modest but significant with a reduction of 1 . 8 % on average compared to previous work .
table 5 : human sentence - level validation of metrics ; 100 examples for each dataset for validating acc ; 150 each for sim and pp ; see text for validation of gm . the results are statistically significant ( p = 0 . 0088 ) with respect to both machine and human evaluations , confirming the effectiveness of our model . pretrained neural models generally outperform human models in terms of accuracy and match rate .
the results are shown in table 6 . the first group shows that the performance reach the best when the model is trained with only one type of classifier , namely , shen - 1 . the second group shows the performance of models trained with all classifiers except for para - lex , with a gap of 10 . 2 points with m1 and m2 achieving the best results .
table 6 : results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . the results show that the transfer baseline achieved by our model achieves higher acc than previous work on similar sentiment transfer benchmarks . however , the difference is less pronounced for untransferred sentences , which shows that the training set size does not vary widely across classifiers . we observe that transferring the raw sentences to the correct context leads to a reduction in acc , as shown in fig . 3 . again , this is seen in table 6 . sentiment transfer by itself achieves higher bleus than prior work , as the number of transferred sentences is smaller than that to retrieve the original ones . also , the average number of acc decreases as a result of different classifiers in use .
the results are shown in table 2 . as a baseline , we also consider the percentage of reparandum tokens that were correctly predicted as disfluent . while the average number of tokens for each disfluency is slightly higher than for repetition , it is still lower than for rephrase tokens .
the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . it is clear from table 3 that the content word contains a lot of tokens , as it is the majority of tokens in the tokens belong to each category . additionally , the repair token is the smallest , making it less likely to appear in a disfluency .
the results are shown in table 2 . in general terms , the results are broken down in terms of performance on text and innovations . for both datasets , the text model performs best when trained and tested on the single dataset , while the innovations model is the best on the multi - domain test set . as can be seen , the performance gap between the early and late stages is much narrower when we only consider text , compared to when we consider both raw and invented word embeddings . moreover , the average number of iterations per generation is shorter than for both sets , indicating that text alone is more useful for training .
performance comparison with the state - of - art algorithms on the fnc - 1 test dataset . our model shows marked performance improvement over the previous state of - art on all metrics . it closely matches the performance of rnn - based and cnn - based embeddings , and even outperforms self - attention sentence embedding ( 23 . 43 % vs . 69 . 42 % ) , as shown in table 2 . however , it is inferior to the accuracy achieved by rnn and cnn , showing the non - triviality of human judgement . accuracy is relatively high compared to both neural networks , pretrained word2vec embedding has the advantage of training on a larger corpus , and therefore requires less data to train , making it more suitable for production use . additionally , the accuracy is higher than for rnn , indicating that human judgement is more useful in predicting future events .
table 2 shows the performance of all the methods that we consider for the document dating problem on the apw and nyt datasets . the unified model significantly outperforms all previous models . neuraldater shows much higher performance than any previous approach , and even slightly outperforms ac - gcn . in addition , the accuracy is higher than previous methods , indicating that neural dater is better at selecting the relevant documents and its output is more accurate .
table 3 compares the performance of our neuraldater model with and without graph attention . our approach shows that both word attention and graph attention are effective for this task , improving the performance by 9 % and 4 % respectively compared to previous work .
the performance of all models is reported in table 1 . embedding + t improves over cnn , but does not exceed the performance of dmcnn , jrnn and argument . when trained with only one argument , the model performs much worse than it does with any other combination of all the other stages . the difference is less pronounced for jrnn , but still suggests some reliance on argument selection . further , if a model has access to the training data , it can further improve its performance by training additional models .
table 1 presents the results on event identification and event classification . our method establishes a new state - of - the - art on all aspects of event identification , with a gap of 10 . 5 % in terms of identification . on the other hand , it is nearly 5 % better at classifier identification . all other methods show lower performance than our method .
results are shown in table 2 . the first group shows that all configurations give similar improvements on the test set . however , the results are slightly worse than the previous state of the art when we add out - of - vocabulary training data . perhaps this is due to the small size of the training set and the fact that all models only use one type of language word - error detector , namely , english only , spanish - only , french - only and russian - only are the only ones that are trained with all the correct utterances , the second group shows the results of fine - tuning . we further compare our model with other methods of leveraging syntactic or semantic information . we find that , let alone a reduction in performance , the model performs better overall , both when training and testing , than when using only the original lm .
results on the dev set and on the test set using only subsets of the code - switched data . the results are shown in table 4 . fine - tuned neural models perform better than cs - only on both sets , showing that the training set can be further trained with a greater diversity of training instances to improve the generalization ability . regularization also improves predictive performance , improving the bias metric by 9 . 8 % in the standard task formulation and to parity in the gold standard case .
the results are shown in table 5 . fine - tuned - disc improves the performance for both test and dev set . the difference in accuracy between the two sets is minimal , however we see significant difference in the test set due to different types of gold sentences in the set : code - switched ( cs ) vs . monolingual ( lm ) is much higher than that for both sets . the difference is less pronounced for dev set , but still suggests some bias in the selection of the gold sentences for training .
table 7 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset . the improvement from the baseline to the current state - of - the - art is statistically significant ( p = 0 . 0088 ) . as can be seen , type combined gaze features improve the recall and precision , and improves the f1 score by 1 . 61 points over baseline .
table 5 shows the precision ( p ) , recall ( r ) and f1 - score ( f ) for using type - aggregated gaze features on the conll - 2003 dataset . the improvements from baseline to type combined are statistically significant ( p < 0 . 01 ) and r > r = 0 . 03 , both for baseline and type combined .
results on belinkov2014exploring ’ s test set . the results are shown in table 1 . hpcd ( full ) and syntactic - sg outperforms wordnet , verbnet and ontolstm - pp by a noticeable margin . glove - extended refers to the synset embeddings obtained by running autoextend rothe and schütze ( 2015 ) on top of wordnet 3 . 1 , and it uses syntactic skipgram . wordnet is augmented with a new type of prefixing wordnet , and its type is slightly worse than the original one . further , the difference between type 1 and type 2 indicates that there is a need to design more complicated neural networks and to refine the feature extraction for further performance improvement . onto - lstmp - pp shows a significant performance drop from the original lstm to glove - retro , which shows the diminishing returns from using pre - trained wordnet vectors .
detailed results from rbg dependency parser with features coming from various pp attachment predictors and oracle attachments . the results are shown in table 2 . as table 2 shows , the hpcd model outperforms rbg and ontolstm - pp in terms of ppa acc . achieving the best performance boosts the precision of the uas by 1 . 59 points over the baseline model .
we observe that removing sense priors and context sensitivity ( attention ) completely degrades the model ' s performance . the results are shown in table 3 .
in table 2 we report the bleu % scores for adding subtitle data and domain tuning for image caption translation . the results show that both variations of our model benefit from the domain tuning step . the ensemble - of - 3 model improves upon the model with marian amun ( marian amun et al . , 2017 ) by 3 . 7 points in terms of bileu % . the difference is less pronounced for en - de , but still suggests some reliance on superficial features .
the results are shown in table 1 . the first group shows that the domain - tuned h + ms - coco model outperforms the plain model on all three datasets except for en - de . the second set shows that it is better at selecting the correct objects and its variants for each domain , confirming the importance of selecting compact regions of the web content . finally , we see that the t - test results are slightly higher for both subs1m and mscoco17 .
table 4 shows bleu scores in % . adding automatic image captions improves the general performance for all models except for those using marian amun . the difference is narrower than for en - de : auto - captions reduce noise , but still result in significantly worse performance than adding all 5 captions individually .
the results in table 5 show that enc - gate and dec - gate achieve better results than en - de and mscoco17 on flickr16 and 17 , respectively . the difference is less pronounced for en - fr , but still suggests some reliance on pre - trained word embeddings . encryption achieves a lower bleu % score than dec - gate , indicating that pre - training of the encoder does not give a significant improvement in visual information integration . using multi30k + ms - coco + subs3mlm , detectron mask surface , and transformer achieves a marginal improvement of 2 . 7 % on top of img [ italic ] w .
we observe that the ensemble - of - 3 approach by itself achieves gains over both the text - only and multi - lingual approaches , the results are slightly worse than those by " visual features " for both subs3m and subs6m , as the results show , the visual features alone contribute less to the overall improvement , but help the model to improve its general performance . also , the results of " everything else " are slightly better than the others , when we add in ms - coco , we get a 0 . 9 / 3 . 4 bleu improvement over the previous state of the art model .
the results are shown in table 1 . en - fr - ht and en - es - ht achieve relatively high precision scores compared to the previous state - of - the - art on all metrics . as can be seen , the difference is most prevalent in relation to ttr , mtld and chime - 4 , in particular , all the variants that perform poorly on the back - end are much worse than those on the front - end . the difference is particularly pronounced for enfr - rnn , which performs in the range of 10 % to 15 % .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the total number of sentences for each language pair is 1 , 467 , 489 , 499 , 487 and 7 , 723 .
the vocabularies for the english , french and spanish data used for our models are shown in table 2 . as the table shows , the training vocabulary is relatively small , making it easier for the models to learn the required vocabulary . however , the difference between en – fr and spanish is much larger .
as shown in table 5 , the automatic evaluation scores ( bleu and ter ) for the rev systems are relatively high , indicating that the training set is well - equipped to perform the task . however , en - fr - rnn - rev is only comparable with en - es - trans - rev , which shows the performance of the original embeddings is less effective . the same tendency is observed for rephrase - based systems , as seen in fig . 3 .
table 2 shows the performance on flickr8k . the row labeled vgs is the visually supervised model from chrupala2017representations . the other rows are rsaimage models supervised by ng et al . ( 2017 ) . the median rank of rsaimage is 0 . 9 , slightly higher than segmatch . as seen in table 2 , both the average recall and mean mfcc score are significantly higher than rsaimage , indicating that the model is more effective at selecting compact regions and generating compact objects .
results on synthetically spoken coco are shown in table 1 . the row labeled vgs is the visually supervised model from chrupala2017representations . the other rows are the tested models from rsaimage . in general terms , the results are similar : acoustic2vec outperforms all the other approaches except segmatch . among all the models , audio2vec has the highest recall @ 10 and median rank , which shows that it is more effective at generating captions with fewer errors . when using rsaimage as the embeddings , the average number of frames per second is 1 . 414 , slightly higher than the chance of chance for any other approach . as shown in fig . 3 , acoustic - 2vec also outperforms other approaches with a large margin .
table 1 : example sentences of the different classifiers compared to the original on sst - 2 . as can be seen , all the classifiers turn in a screenplay that is similar at the edges , except for dan ( see x4 ) . as the picture shows , the difference in quality between rnn and cnn is less pronounced for dan , however , for cnn , it is much worse . want to hate it ? as shown in table 1 , hate speech is the most difficult class to solve for rnn , it is very difficult to pick out the parts of the screenplay that are hate speech , as shown in the second group of examples .
part - of - speech changes in sst - 2 : , , and indicate that the number of occurrences have increased , decreased or stayed the same through fine - tuning respectively . the numbers indicate the changes in percentage points with respect to the original sentence . as the table 2 shows , the importance of the word " good " is not important , as the scope for the improvement is limited . the symbols are purely analytic without any notion of goodness . however , the presence of " hate " and " opportunity " are significant enough to indicate that finetuning has changed the nature of the speech .
as shown in table 3 , the sentiment score increases as a result of the flipped labels being flipped from positive to negative sentiment . the numbers indicate that the rnn model learns to interpret both positive and negative sentiment more accurately .
the results are shown in table 2 . first , we compare against the best performing system , pubmed , for both positive and negative sentiment . for negative sentiment , we see that pubmed performs better than sst - 2 and corr . however , for positive sentiment , it performs slightly better than pubmed . we observe that sift is comparable to pmi , but does not outperform it , likely in part due to the training size .
