table 3 presents the results for the amr 2015 , 2017 and 2018 studies . we show that our psd model significantly outperforms the previous state of the art on all metrics . the average number of frames per second improved over previous models , with the exception of psd .
in the en - de news commentary , we compare our proposed approach against 2 baseline models - mnli ( m ) and snli ( s ) on the bert dev and biobert test set . we observe that the expanded mednli model significantly improves over the original snli model . it achieves state - of - the - art results on both datasets with good recall scores . moreover , it improves its generalization ability over both snli and mnli when the accuracy is increased . this corroborates our intuition that the semantic information injected into the model by the additional cost term encourages the model to rely less on superficial cues .
distill also improves the general performance of elmo over the no - distill model . it achieves 87 . 53 % on the sst2 task . however , the difference is less pronounced for negations , showing that the model can rely less on superficial cues . negations also have a positive effect on performance , however it is less significant .
the average accuracies of the baseline and elmo models ( 87 . 54 % and 87 . 55 % ) on non - neutral sentences is shown in table 3 . the elmo model outperforms the no - distill baseline and no - project model , indicating that the ability to select neutral sentences without loss of accuracy leads to a better interpretability .
the results are presented in table 1 . glove tags significantly outperform the competition on both datasets with a gap of . 005 points from the t - test set . with the boost in f - measure , we see that the effect of the additional cost term is only statistically significant on results with a reduction of more than . 5 points in coverage . using the label information improves results for both datasets , with the boost being mostly due to the reduced effect of cost term .
our system works well on both datasets with two tasks . glove tags significantly outperform both tf and tn with a gap of . 365 / 6 points from the last published results . with the additional cost reduction of . 005 / 0 . 005 points , we achieve 10 . 42 % f1 score improvement on both tasks with the boost being more pronounced on tf .
the results of topic embeddings experiments are shown in table 2 . the first set of results shows that avg_cossim outperforms google with a large margin . the second set shows that it can further improve with the addition of idf features . we notice that the difference between t / n and f / n is less pronounced for topic - science , but still suggests that there is a need to consider principled ways of improving the concatenation of input and output . we note that the best approach is leveraging the semantic information available from google translate ( gong et al . , 2018 ) and topic - specific features .
the results are presented in table 4 . we show the turn - level evaluation results . our model obtains the best overall results with a 21 . 3 % overall improvement on the lstm full score . we also observe that our model significantly outperforms the baseline in terms of precision .
the results are presented in table 4 . we show the results for the best performing models . our model obtains the best results on all metrics with a gap of 10 . 18 % in bleu and 3 . 58 % in speedup . the difference is most prevalent in the iwslt en - de section , where our model improves 3 . 36 % over the previous best state - of - the - art model . in addition , the improvement is much larger on the wmt en - fr section , improving 3 . 03 % over previous state of the art model . we also observe that the sat scores considerably improve over the baseline when the number of iterations in the en - de section is reduced . this confirms the effectiveness of our model .
hidden size is slightly larger than the average size of a hidden node , so we find that more data is required to train an s - lstm model . the total number of hidden nodes in our model is 8 , 768k , which means that more than half of the model can be hidden in a single hidden node .
we show the ablation results on bilstm in table 2 . as a baseline , we consider only the time taken to setup , maintain and test the model , and the number of parameters used for each iteration . our model obtains the best results with an accuracy of 81 . 53 % , which is slightly better than the previous state of the art . we further analyze our results with respect to concatenation of the bi - and cnn datasets . we find that , in bi - stm , more than half of the time ( 59 . 53 % ) are spent on initialization , while the remaining 48 . 53 % is spent on optimization .
we show the test set results on movie review dataset in table 4 . as a baseline , we also compare our model with previous models trained on the same training dataset . bilstm achieved the best results with an accuracy of 81 . 67 % , which indicates that the model can easily distinguish between the true response and negative responses . transformer ( n = 8 ) also outperforms our model , but does not match our model in terms of accuracy . we suspect that there are not enough data to pretrained models to pretrain the transformer , and that the number of models in the training set may vary depending on the underlying data distribution .
the results are shown in table 2 . the first set shows that the generalization ability of our bilstm model exceeds the competition on all datasets with two tasks . however , the difference is most prevalent in the food domain , perhaps the most striking thing about the food category is that our model performs significantly worse than the other two on both datasets in terms of time . in comparison , the time and accuracy are relatively comparable in the other categories , with the exception of the music category , where our model obtains a slight improvement ( 83 . 59 / 83 . 86 ) on average compared to the previous state of the art . further , the differences are less pronounced for the video category , our model outperforms all the other baselines except the health category . perhaps this is due to the smaller size of the data set ( 60 . 5 × 50 . 5 pixels ) and the fact that the training time increases when increasing the training epochs . we noticed that the music and dining categories are slightly more difficult than the others . however , if we switch to the library - based approach , this helps improve results . we show that the feature - rich , multi - task learning model can reduce the error of prediction when training time . the second set shows the results of phase 1 on a single dataset . we trained on a large scale dataset , containing 200 training examples . since this is a more data - intensive task , we used a lower precision rate for each training epoch . this resulted in a lower accuracy rate .
we show the results of our second study in table 6 . our bilstm model achieved a final accuracy of 97 . 55 % , which is slightly better than the previous best performance of manning2011 . however , it is still significantly worse than other models . we observed that the number of frames in the bi - stm is less than the size of the training set , which indicates that there is a need to design more complicated models and to refine the model for further performance improvement . we noticed that the transfer task that we performed on ma2016end was very difficult , as it required a lot of data shuffling . we found that the only solution that worked well for ma2016 was the stack transfer task . we managed to improve the model ' s performance with the help of a stepwise learning algorithm .
we show the f1 scores of all models trained on the stacked bilstm configurations . our model achieves the best results with a f1 score of 91 . 93 on the bi - stm dual task .
table 2 shows that the best e2e model on the development set is harvardnlp & h . elder , while thomson reuters ( np 3 ) is slightly better than the baseline . the difference in performance between dev and dev set is less pronounced for char . however , for character , we see a drop of 0 . 8 points in bleu score .
table 3 shows the ablation results on the development set of webnlg . our own model obtains the best performance . it outperforms all the other systems with a gap of 10 . 3 % in bleu score .
we find that increasing the number of layers in synst ’ s parse decoder significantly lowers the speedup while marginally impacting bleu . as the results of random sampling ( k = 0 . 01 ) shows in table 4 , when the max chunk size of the decoder is increased from 3 . 8 × to 25 . 1 × the original size , there is a negligible impact on speedup .
the results in table 4 show that e2e and webnlg development set results in the format avg ± sd . the average number of frames per second for each human reference is 43 . 2 , so the improvement in accuracy is only 2 . 6 % over the baseline . however , this improvement is much larger over using only human reference as prediction once . our model outperforms both bleu and rouge - l in predicting most human references .
we show the results for the content errors and overall correctness in table 2 . overall , we see that our system exhibits marked performance improvement . the error reduction over the best previous state - of - the - art method is 15 . 6 % on average .
we report the results of our second study in table 2 . we compare our model with the best previous approaches on three types of word embeddings . we observe that the average number of grammatical gender - parity scores for each word is relatively consistent , with the case of " unique word " having the best performance . however , the difference is less pronounced for " human " compared to " word " . " human " has the highest percentage of new texts , so we observe lower e2e scores for both groups . " unique word " is the smallest group of words , but the most representative of the human class , with an average of 7 . 5 % new texts per year compared to the average of 6 . 2 % in the previous study . " character " is slightly higher than " word " but still comparable with the average . " sents " are smaller groupings of words that are similar in some sense but have completely different semantic meaning . these results show that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement .
we also performed manual evaluation on 10 random test instances of our word - based model trained with synthetic training data from two templates . the results are in table 7 . our model obtains the best performance with a cagr of 1 . 9 % on the selected test instances .
table 1 : experimental results of abstractive summarization on gigaword test set with rouge metric . our approach outperforms the state - of - the - art on three out of the four baselines . the difference is most prevalent in the gold - two - mention case , where our approach obtains a significant improvement over the baselines with a lead - 75c score of 23 . 86 on the test set . however , this improvement is less pronounced than for other approaches , since our approach relies on prefix baselines instead of attention - based learning . contextual match is only slightly better than seq2seq , but still superior to seq - to - seq model with attention , and the bottom section is our model ’ s oracle performance . we also observe a drop in performance between goldtwo - emention and contextual match . this suggests that there are ways to improve the interpretability of our approach .
table 2 shows the experimental results of extractive summarization on google data set . the high f1 score indicates that the model can extract significant information from the input documents and synthesize these multiple facts with a minimum of 90 % accuracy .
table 3 compares the results of different contextual representation choices . our model achieves the best results with respect to extractive f1 score . it outperforms all the other models with different contextual representations in terms of extractive rl score . moreover , it achieves the highest score on extractive cr score as well . we observe that contextual representation has a significant impact on both extractive and abstractive rl scores , it improves the generalization ability of both temp5 and temp10 models .
the evaluation results of the submitted runs on the test set . in table 3 we report the official evaluation results . the results of mv ext . sub - task is presented in table 3 . it can be seen that our model can easily distinguish between the true response and negative responses . the negative responses are rare in a single - task model , but not uncommon in multi - task setups .
table 3 shows the f1 and exact match comparisons of predicted chunk sequences ( from the parse decoder ) , ground - truth chunk sequences , and chunk sequences obtained after parsing the translation produced by the token decoder . the difference in f1 score between the predicted and actual match set is slim , but significant enough to show that jointly training the two decoders helps the gold parsing task .
uniform sampling also improves over always sampling , and for sick - r , it improves by 3 . 6 points in acc / f1 . however , it does not improve over uniform sampling , which shows the advantage of finetuning word embeddings during training . we also observe that automatic decoding improves over both uniform sampling and always sampling .
for the sick - r dataset , we also consider the msrp ( acc / f1 ) and sst14 scores of the original embeddings . the results are presented in table 3 . we show that rnn models significantly outperform cnn models in both datasets with different decoding dims . for example , rnn with a dim of 0 . 8530 shows that it is more than 4 . 5x better than cnn with a similar number of encoders , in terms of sentence representation . with a typical rnn model having a single encoder , the number of decoder dims is low but consistent with the realistic requirement of a minimum of 2 . 2 .
in table 2 we report the bleu scores for training nmt models with full word and byte pair encoded vocabularies . the models are trained with annealing adam and scores are averaged over 3 optimizer runs . full word models limit vocabulary size to 50k . byte pair encoded models allow more vocabulary to be trained , but do not exceed 32k words , which results in a drop in performance . we find that adapting the wmt en - fi model to the byte pair encoder reduces performance by about 9 % .
as shown in table 1 , pre - selection results show , all models perform well in the low - supervision settings . syl - avg - a achieves 91 . 6 % ppl improvement over the best baseline model ( syl - cnn - 2 ) . however , the difference is narrower than that with respect to test set perplexity , ∼ 5m parameters in total . we find that the best performing model is char - cnn ,
the results are shown in table 1 . our data augmentation method outperforms traditional cnn models like char - cnn , syl - cnn and syl - concat . it achieves state - of - the - art results on all metrics with a gap of 10 . 4 points from the last published results ( 78 . 9 % vs . 75 . 3 % ) on data .
we can see from table 5 that adapting the lstm embedding scheme to the variational rhn improves the results for both datasets . the average number of iterations of our model is now 69 . 4 % , which is slightly better than the previous state of the art . additionally , the concatenated part - ofspeech embeddings perform better , providing a 2 . 4 % boost in precision .
table 1 shows that adding titles to premises improves the support accuracy for fever title one . the difference in support accuracy between esim and transformer is only . 005 , which shows that the ability to add titles improves the interpretability .
the results of concatenating evidence or not are shown in table 2 . our transformer model outperforms the transformer on fever title one and title five datasets . on the title five dataset , transformer shows a significant drop in support accuracy . however , the difference is less pronounced on the title one dataset , with transformer showing a slight ( p = . 005 ) improvement over esim .
according to table 3 , the system has retrieved 80 . 1 % of the evidence ( out of 900 claims ) in the first half of development set . the remaining 50 . 2 % of evidence is classified as supporting documents .
table 4 shows the fever scores of the systems that we trained on . all use ne + film retrieval . our system obtains a fever score of . 5736 , which means that it can easily distinguish between the true response and negative responses .
we report the precision @ k9 for the isolated example experiment in table 2 . it shows that our proposed method has comparable performance with dfgn on both datasets with a gap of 10 . 9 % in accuracy .
the results are presented in table 4 . in - vocabulary pairs only show that the task completion ability of the system is improving with the addition of oov features . the performance improvement is most prevalent in the invocabulary , both for the current and previous vocabularies . separate instances of the same word are much less likely to result in an error , in that the vocabulary overlap is narrower . further , the number of instances in the vocabulary is less than that of the previous pairs , which indicates that there is a need to design more complicated vocabulary features .
table 3 presents the results on indirect test set . our model obtains the best results with an absolute improvement of 3 . 2 points over the previous state of the art . indirect test set results show that the tl2rtl models are well - equipped to perform on synthetic dataset with a minimum of 80 % f1 score .
table 3 shows the results for ebay all , all onion and all onion half 2 . the results are broken down in terms of performance on the hidden test set . all onion is significantly better than all onion half 1 and all onion is better than the average of any other two aggregated sets . illegal onion is slightly better than legal onion in all but one of the comparisons but still inferior to all onion except for the case of legal onion . we observe that the difference between the average performance of all onion and the illegal onion is less pronounced for ebay half 1 than for all onion , indicating that there are fewer ways to distinguish between the hidden and visible targets .
according to table 2 , the average percentage of wikifiable named entities in a website per domain , with standard error is 38 . 6 % , which means that 43 . 6 % of the named entities on ebay is actually wikifiable . the rest are illegal or misclassified as such .
we note that the ukb model outperforms mfs in terms of all metrics on three out of the four datasets . the difference is most prevalent in s2 and s3 , where ukb ( 67 . 8 % ) and mfs ( 65 . 3 % ) are both significantly better than the best previous state - of - the - art models . on the other hand , in s13 , we see that ukb is only slightly better than mfs , but still superior to the other baseline models .
table 2 presents the f1 results for supervised systems on the raganato et al . ( 2017a ) dataset . our system outperforms the best previous supervised systems across all three domains . in particular , it improves upon the performance of ims zhong and ng ( 2010 ) by 3 . 8 points in f1 score . it improves further on s2 and s3 score by 2 points .
table 3 shows the results for the single context sentence and multi context sentence tests . our model outperforms all the other models apart from the case of the context sentence test set where pprnf achieves the best performance . in all but one of the comparisons , our model ppr achieves better results than the other two baselines . the reason is that our model relies on word embeddings optimised for the task at hand , and hence requires less data for each context sentence . this result along with the semantic improvements show that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement .
we report the accuracy ( in % ) of our model and other swbd models on the hidden test set of swbd2 . the results are summarized in table 2 . our model obtains substantial gains in accuracy over the baselines across all three domains . it closely matches the performance of tf - idf with only 0 . 18 % absolute difference . logistic features alone improve the results for both swbd , and the average number of correct answers per label is slightly better than the previous best swbd model . however , the difference is much larger under automatic metrics ( bow + svm ) than that under bow + logistic , showing that the ability to select compact regions induces the model to rely less on syntactic stereotypes . further improving performance by high margins
we show the results for the second variation of our seq2seq test set in table 1 . our approach obtains a slight improvement over the baselines on sql queries with an accuracy drop of 0 . 7 % compared to the previous state of the art .
table 2 shows the performance of our approach compared to previous approaches on test set hard and easy subsets . our irnet model outperforms all the other models with a large margin . irnet even outperforms syntaxsqlnet when trained on the hard subset .
we show the performance of our approach with respect to matching accuracy on the development set of semql in table 3 . our approach obtains a significant improvement in accuracy over the baselines : syntaxsqlnet now achieves 25 . 5 % matching accuracy , compared to the previous best performance of 10 . 9 % ( hochreiter et al . , 2016 ) . ( table 3 ) moreover , our bert model outperforms the previous state of the art in syntactic analogy task .
table 3 shows the classifiers ’ accuracy on the supports and refutes cases from the fever dev set and on the generated pairs for the symmetric test set in the setting of without ( base ) and with ( r . w ) re - weight . the bert model shows marked improvement in accuracy over the strong lemma baseline . nsmn also improves its performance over esim ,
the results are presented in table 4 . the first set of results show that the training set size and the number of tokens in the lmi metric are the most important factors in predicting the answer . the second set show that for each training and development set , the importance of the starred movie is very important . it helps to distinguish between the roles played by different actors . for example , if a movie has won an award , the actor may assume that the award has been won by other actors . however , this analysis fails to account for the fact that most of the award winners are unknown . the third set shows the results of rephrase training data without adding out - of - sample data for each domain . from this set , we can observe that the largest difference in development and training time is caused by the presence of multiple actors in the same training set . this suggests that the actor could rely on tips from other actors to predict answers based only on their own performance .
we apply the best newsqa model on squad development set . results in table 3 show that our model significantly outperforms the baseline mnewsqa model with a 2 - stage synnet data structure .
table 2 compares the results of seq2seq model with human and simulator models . we observe that , in both setups , the model that expresses the goal better achieves the most turns . the difference in accuracy between simulator and human is minimal , however we see significant difference in terms of number of turns taken to achieve the goal , with the former achieving 78 . 3 % of the turns while the latter is only 69 . 3 % . the results are shown in table 2 .
we also evaluated the pos tags for english . the results are shown in table 2 . pos tags with gender - neutral pronouns outperform other tags with different type nouns in terms of productivity . pos tags with gender specific pronouns perform best in the fertility category , with an overall improvement of 5 . 5 % over the previous state of the art . gender specific pronouns alone account for 3 . 7 % of the total pos tags , however , it is significantly less than the other categories . we noticed that the pos tags tags with verbs as adjectives outperform the others in predicting ovulation . this confirms the competitiveness of gender - specific pronouns in the infertility category . pos tags containing verbs and nouns outperform others , however it is less clear whether this is due to the higher percentage of verbs or the type of nouns in the vocabulary or the number of tokens in question .
we further analyze our results with respect to accuracy in the macro - f1 and precision in the semantic tasks . the results are presented in table 2 . semisupervised approach outperforms the naive approach in both semantic and syntactic tasks . it achieves 8 . 24 % overall improvement over the previous state of the art on the semeval - 15 task . on the semantic task , it achieves 6 . 45 % higher accuracy . this confirms the effectiveness of our approach . we also observe that our approach exceeds traditional neural models like atae - lstm wang et al . ( 2016 ) and tfslstm by a noticeable margin .
we show the results of our ablation studies in table 4 . exact match ( em ) and span f1 results on newsqa test set of a bidaf model finetuned with a 2 - stage synnet . the results show that our approach works well , matching the human - annotated answers to generate questions . although the number of mini - batches used to synthesize questions is small , the accuracy is high enough that we report both the em and f1 scores of our model ( 27 . 2 % ) on the test set with respect to question synthesis .
table 3 presents the results of the second set of experiments . we show that all - biomed systems transfer well , with the exception of es2en bio . health improves over bio , but does not improve over health , in which it is comparable with the former state of the art . overall , the improvement is slim but consistent , and we note that it is encouraging to continue researching into the underlying causes of low precision in the low - supervision settings .
uniform ensemble en2es health improves over all - biomed en2en bio on the khresmoi dataset . it achieves the best overall results with a gap of 10 . 5 % in uniform ensemble score from the last published results ( gillick et al . , 2016 ) on the en2e test set . the results reconfirm that the uniform ensemble approach is beneficial for both generalization and specifically for es2en . health improvement over uniform ensemble is modest but significant , and we note that it helps improve the generalization ability of both en2ed and disambiguated models .
in table 4 we report the results of bleu validation on the news dataset in english - german language pair submissions . uniform ensemble outperforms all other approaches except for the case of uniform ensemble when en2de is tested on the khresmoi dataset . bi ensemble ( with α = 0 . 5 ) and news → all - biomed ensemble ( α = 0 . 7 ) significantly outperform the uniform ensemble ( i . e . , by a margin of 3 . 2 points ) , but do not exceed the threshold for cochrane dataset validation . with α - 0 . 7 as the upper bound on the joint validation objective , we do not have significant improvement over uniform ensemble . however , bi ensemble with uniform ensemble under - performs the news ensemble by a large margin . this corroborates our intuition that selecting the features of contiguous regions of the data leads to a better understanding of the underlying data , and , consequently , better interpretability .
uniform ensembling outperforms bi with varying smoothing factors on the wmt19 test data . we find that uniform ensembles perform best , but bi outperforms them when using the smoothing factor of 0 . 5 . we find small deviations from the official test scores on three of the four scenarios ( table 5 ) due to tokenization differences . bi also outperforms the uniform ensemble in the one - to - x analogy task .
the bleu scores of the training set and the test set are presented in table 1 . the results show that our proposed method significantly improves the interpretability by increasing the precision for both input - specific and output - specific rl tasks .
uniform ensembling model outperforms the ensemble model in ter and pe test set , confirming the effectiveness of our model selection . the results of the best ensemble model is reported in table 3 . our model obtains the best performance in the peformance test set .
uniform ensembling improves the bleu scores over the base model by 1 . 08 points in the standard task formulation . by further adding multi - factor count , the improvement becomes 2 . 45 points . further improving performance by high margins
for completeness , here we also compare against the baseline . as this easier task requires fewer iterations , we show that diversity is relatively high while expressing it in terms of accuracy . more importantly , diversity is high in both languages , which means that more than half of the answers for each question are likely to contain words that are distinctive of one language . this highlights the differences in expressing diversity across domains . our proposed method significantly boosts accuracy for both datasets , with the boost being more pronounced for languages that are more diverse .
the results are presented in table 1 . nmpu is able to do exceptionally well on synthetic dataset with a f1 score of 43 . 08 / 71 . 03 on the standard task formulation . it significantly outperforms the previous state - of - the - art on all metrics . we observe that the object identification method developed in cotterell et al . ( 2017 ) achieves outstanding results , outperforming all the base lines with a gap of 10 . 45 / 0 . 59 points from the previous best state .
we report the evaluation results on the dataset of polysemous verb classes in korhonen et al . ( 2003 ) . it can be seen that our approach outperforms the previous approaches in terms of f1 score . the difference is most prevalent in the gold - two - mention case , where our lda - frames model ( 52 . 14 % ) outperforms all the other methods apart from the case of k - means , by a large margin .
performance of our method in french - english is shown in table 1 . our approach outperforms the baseline in terms of max f1 score , indicating that it has achieved state - of - the - art performance .
we also evaluated it in french - english , both for the original and the debiased embeddings ( see table 4 ) . in both cases , we report the results of the best performances on the 11 - point iap scale . the results are summarized in table 4 . rr achieved the best performance with a max f1 score of 63 . 79 .
table 1 shows the overall size of the data set , and the number of entity types for each entity type . our approach uses 10 object - based and 9 measure - based entity types , which allow more data to be categorized into task and event categories . our proposed method uses a three - step approach , which can be seen as an alternative to finetuning word embeddings . the approach uses a stepwise learning method based on the three aspects of biology and chemistry . it abstracts from the abstracts of 10 scientific journals and includes full - length , open - access journal articles about biology . the total number of articles in our data set is 561 , 015 . we show the full set in table 1 .
correlation coefficients vary between - 1 ( negative correlation ) and 1 ( positive correlation ) . zero means no correlation . positive correlation indicates that the tvc model pretrained models can rely on superficial cues .
in table 5 we compare our best performance on a single - domain dataset with the best performing pretrained models , which are pretrained on much larger corpora . our model improves upon the performance of the widely available ones . it improves by 9 . 45 % over the conll2003 baseline on glove embeddings .
we observe that the hyper - parameter setting proposed in our framework has a significant impact on the effectiveness of our pretrained word vectors . our model achieves gains in accuracy over both the baseline and the default setting . it closely matches the performance of the best previous model ( chiu et al . , 2016 ) with only 0 . 86 % absolute difference .
table 3 shows the percentage of discrepancies in context - agnostic translation caused by deixis ( excluding anaphora ) as explained in section iv . it is clear from table 3 that there is a significant percentage of discrepancy in the frequency with which the t - v distinction can be detected .
table 1 shows that rc with sglr as input is significantly better than rc without sg on all training subsets . with the training data of 0 - 100 , 100 - 500 and 500 + as the training intervals , rc achieves a f - measure improvement of 3 . 5 points over random initialization . further improving performance by high margins over sg initialization
table 3 compares the results of rc with no specialized resources and with specialized resources . with specialized resources , our model achieves 60 . 2 % improvement on average compared to the best previous results . rc initialization is more stable than rc with random initialization , confirming the effectiveness of sg initialization . we find that rc with sg initialization is better than rc ( sg fixed ) in both datasets when the specialized resources are used as part of the training set . moreover , leeuwenberg et al . ( 2017 ) and parallelism produce remarkably similar results : of the 2000 example pairs in the dataset , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples . regularization is beneficial the majority of the time , lowering recall scores when using sg initialization , however it does not improve significantly over random initialization .
table 3 shows the error analysis on 50 fp and 50 fn ( random from test set ) for different settings . frequent errors are more than 50 % in fp , but are less than 20 % in fn . regular errors are less prevalent in fn , but still represent a significant percentage of fp errors . cross - clause relations ( ccr ) are particularly difficult to detect , as we find that most errors in our system are caused by incorrect declaration of clauses .
the results for all the models that we trained on the three datasets in our experiment are shown in table 2 . the results show that ling models significantly improve over random and ling + n2v on all three datasets . in particular , ling model with the best performance improves on the hate speech dataset .
most of the words for each language are already known , but still there are a few thousand words that are not . our system can be seen to be significantly better than other systems for some reason . it is able to distinguish between low - frequency and high - frequency speech , which means that more than half the words in question are actually spoken in one language , and more than 50 % are dialects .
table 4 shows the percentage of discrepancies in context - agnostic translation caused by ellipsis . according to our results , about 66 % of the discrepancies in our system are caused by wrong morphological forms ( vp - ellipsis ) and 20 % by other errors .
as shown in table 1 , the model achieved a significant drop in accuracy when applying bonferroni correction for sentences with female nouns and sentences with male nouns . the difference in f - m from bow + logreg model to bilstm model ( 0 . 827 ) is significant enough to result in a drop of more than 2 % in accuracy .
the results are shown in table 2 . we observe that the seen and unseen pairings are comparable in terms of cosine similarity . however , in the more realistic second case , when we consider the cosine similarity metric in wm18 , the seen pairings have higher correlation with the standard deviation . this indicates that the presence of some unseen modifiers in the ensemble can impact the performance of the model . we observed that in the realistic case when we only consider the original pairings , the accuracy is relatively the same while in the unrealistic case , we see that it is significantly worse .
the bleu scores of our model ( cadec ) trained on the concatenated test set are shown in table 6 . concat concatenation outperforms s - hier - to - 2 . 5 . the difference is less pronounced for cadec trained with p = 0 . 005 compared to the baseline , but still suggests some advantage to concatenating training data .
we show the f & c dataset size in table 2 . all labels represent the original dataset with all the labels . subset labels are the subset labels which are inferable by the resource . the total number of labels in the dataset is about 7 , 322 , which means that there are 71 . 5 % overlap between the original and subset labels .
the results on the noun comparison datasets are shown in table 4 . our model obtains the best results with a f & c score of 0 . 78 on the clean dev and new data dev tasks , which shows that it can easily distinguish between the true response and negative responses . the difference is less pronounced for the new data , but still suggests some performance improvement .
as shown in table 5 , the best performance on the relative dataset is achieved by our model with different thresholds , which surpass previous work by 10 % on average .
we also evaluated the method for intrinsic evaluation in table 7 . it shows that our proposed method can significantly improve the interpretability by increasing the precision and the average number of objects within range of our proposed median , without sacrificing too many correct answers .
we noticed that in rc5 , the proportion of casual particles to causal verbs in sentence is relatively high , indicating that there is a high correlation between the number of words in a sentence and the rate at which it can be considered as satire , i . e . more than 80 % for nouns and more than 90 % for verbs . we observed that for verbs with two tokens in the sentence , the correlation is low but still high with respect to pca components . regarding word embeddings , we found that grammatical gender - parity was the most important factor in sentence comprehension , followed by word concreteness . overall , the system performed well , with an f1 score of 0 . 86 / 0 . 88 improving over the previous state of the art . however , we noticed that sentences containing multiple words tend to be more difficult to analyze , this indicates that there are ways to improve the predictive performance of future models .
results of classification between fake news and satire articles using bert pre - trained models , based on the headline , body and full text . bold : best performing model . in table 2 we report precision , rn and f1 scores for both headline and text body only . the difference in precision between headline and body only shows that the text body alone does not have a significant effect on classification performance , however it does impact recall .
we show the results for the second variation of our deixis model in table 2 . our concatenated model outperforms all the other models apart from the case of lexical cohesion . concat concatenation improves upon the baseline performance by 3 . 6 points in the semantic cohesion metric , and on average , it improves further in the syntactic cohesion metric by 3 points .
table 3 presents the results of classifiers trained on the test set of fake news and satire articles . the best performing model is pre - trained bert model . it shows marked improvements in precision and recall over the baseline model .
we show f1 scores on aida - b test set in table 1 . the ment - norm model significantly outperforms the comparably trained guorobust model on all test sets except q15 - 1011 . ment - norm also outperforms rel - norm in terms of f1 score . in particular , it improves over the strong baselines by 3 . 5 points in aida - b ( k16 - 1025 ) . rel - norm , in turn , improves by 2 points over the model based on k15 .
we also evaluated the models trained on msnbc , aquaint , wiki and ace2004 datasets . the results are summarized in table 2 . our approach significantly outperforms all the other methods except guorobust ( 86 . 5 % vs . 75 . 2 % ) on all metrics except ace2004 . relative error reductions range from 0 . 2 - 27 . 4 % over previous state of the art models .
we also note that the bilstm variants achieve unrealistically high performance in the training set when using the traditional cross - validation setup . the performance improvement over the lstm - based variants is due to overlapping sub - dialogues in the train and test sets , which means that the model can rely on false positive answers along with the true answer . this is evident from the significant drop in uar score over the best previous state - of - the - art model .
in table 8 we show the performance of our concatenated concatenation algorithm compared to the baseline on ellipsis test set . concat outperforms s - hier - to - 2 . 6 , significantly improving over the baseline performance . concat also improves over the strong lemma baseline by 3 . 2 points in the accuracy metric .
we note that the bilstm with attention mechanism performs best in all evaluation metrics . it achieves the best performance with a uar score of 0 . 88 on the dialogue - wise cross - validation set , which shows that the attention mechanism is strictly superior to the non - att mechanism in terms of all metrics . the performance improvement over the lstm - based variants is modest but consistent , and we note that it is encouraging to continue researching into the ways in which attention mechanisms can improve interpretability .
we report f1 - measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters . the results are shown in table 1 . our model improves significantly over the previous state of the art model by 9 . 3 % in the f1 metric .
we report the mean accuracy of our models for md performance in table 2 . as in the ner evaluation , we report accuracies over the test dataset averaged over 10 replications of the training . our joint model obtains a mean accuracy improvement of 1 . 61 points over the previous best state - of - the - art model , joint1 .
as shown in table 1 , incorporating sentiment information improved the performance of the models for fasttext and glove by 4 . 2 % . on adding knowledge graph information , we were able to improve these results to 78 . 76 % , which is an absolute improvement of 4 . 97 % over the performance in the previous state - of - the - art model . further adding sentiment information further improves the performance to 79 . 04 % .
we also show the bleu scores for different probabilities of using corrupted reference at training time . for ellipsis , we show inflection / vp scores . it is clear from table 9 that the use of corrupted reference can reduce the precision of the output predictions , since it reduces the precision with respect to grammatical gender . however , it does not harm the predictions for deixis . we show that it is beneficial to use a corrupted reference when training with multiple context sentences .
the accuracy of the traditional classifier in phase 2 given the seen and unseen rates is reported in table 5 . it shows that our approach significantly improves the interpretability by increasing the coverage with a 50 % boost in seen rate .
the results are presented in table 1 . we report both the fine and fine r scores of our model with and without augmentation . our model obtains extremely fine results , and its average f1 score is close to the level of the state of the art . however , we also observe that our model is slightly more than fine - tuned , in that it requires more augmentation to achieve its full performance . this confirms the extent to which the augmentation step can improve the general performance .
we show that our model achieves substantial gains over naive augmentation on the test set , and closely matches the performance of bert - base , uncased . elmo w augmentation gives a performance gain of 3 . 2 points over glove w / o augmentation , and 2 . 4 points over the best previous approach . we also observe that our labelgcn enhances the interpretability by further adding entity type - specific features .
the results are presented in table 1 . our proposed approach outperforms the tried and tested method in terms of el & head p , el andhead r scores . the results reconfirm that the heuristic baselines that our proposed approach relies upon are well - equipped to handle boundary - pushing tasks .
we show that our model achieves state - of - the - art results , outperforming all the other models with different augmentation schemes in terms of precision on mi - f1 score . elmo w augmentation improves precision on both datasets with good recall scores . it augmentation boosts precision by 9 . 5 points on the standard model , but does not improve results on the multi - factor classification task .
the average number of types added or deleted by the relabeling function per example is shown in table 5 . fine - fine add is the most common type of addition and removal , followed by fine add , fine del and ultra - fine del . selective filtering reduces the number of examples that can be discarded by the filtering function . it also reduces the overall error of the output predictions .
table 1 shows the properties of the ubuntu and samsung qa datasets . the message and response are presented in bold , the tokens are in bold and the response is in bold . it is clear from table 1 that the training set is very small : ubuntu - v1 has 1 , 609 messages and samsung - v2 has 18 , 920 messages . however , the difference in accuracy between the two sets is minimal , we managed to show that our approach can further improve with an increase of accuracy of 1 . 48 % in the standard task formulation .
we show the results for ubuntu - v1 1 in 2r @ 1 , 10r @ 2 and 5r @ 5 on the training set of tf - idf . the results are broken down in table 2 . our approach outperforms all the base lines with a gap of 10 . 5 % on average . in particular , we see that our approach rde achieves competitive or better results than the previous state - of - the - art models on all datasets in terms of both roc scores .
we compare our proposed approach against 3 baseline models - lstm , cnn [ 5 ] , rnn - cnn [ 5 , 6 ] , compagg [ 3 ] and attention \ scriptsize { 6 ] } on the ubuntu - v2 test set . in general terms , we see that the proposed approach improves upon the baseline models with good recall scores . however , it is slightly worse than the other baseline models in terms of accuracy on 2r @ 1 and 10r @ 2 . this indicates that there is a need to design more sophisticated neural models and to refine the decoding algorithm for further improvements . we noticed that the attention - adaptive decoding algorithm proposed by peyrard et al . ( 2018 ) is slightly better than the baseline model in both datasets on two of the four scenarios .
in table 5 , we report the model performance results for the samsung qa dataset . our approach outperforms the previous state - of - the - art methods on three out of the four scenarios . the difference is most prevalent in the second scenario , when we compare tf - idf with rde , rde - ltc and hrde . in this easier setup , we see that the rde model performs better in both scenarios with a gap of 10 . 3 % on average from the baseline . however , the difference is narrower in the third scenario , with hrde achieving a performance gap of 2 . 3 % .
table 2 compares our model with previous stateof - the - art models on the bleu test set . the iwaqg model outperforms all the baselines except meteor by a noticeable margin . it closely matches the performance of the best previous models with a gap of 10 . 53 points from the last published results . we observe that our qg module outperforms the other baselines on three out of the four test sets . the difference is most prevalent in meteoor , the fourth test set , where our model performs best .
the performance of the models on bleu - 1 , 2 , 3 , 4 and 5 is presented in table 4 . it can be seen that all the models only slightly outperform the strong baselines in terms of accuracy . however , iwaqg ( 73 . 8 % ) outperforms all the other models except meteor in accuracy in all but one of the scenarios . this indicates that the performance gain comes from an increase in interpretability of the input documents .
table 4 shows the recall of interrogative words for the qg model without our interrogative - word classifier zhao et al . ( 2018 ) . it shows that the model can easily distinguish between the true response and negative responses . the lower bound on the interrogative word recall is slightly higher than the upper bound , but still superior to the iwaqg model by a noticeable margin .
the results of an ablation study are shown in table 6 . our interrogative - word classifier achieves a performance improvement of 3 . 3 % over the best previous state - of - the - art model .
the precision and recall numbers of interrogative words that our interrogative - word classifier can handle are shown in table 7 . it shows that the classifier is well - equipped to perform this task with a precision of 87 . 7 % on the interrogative vocabulary . however , it is still significantly worse than most stateof - the - art methods .
we report the statistics for the forests generated with various γ ( upper half ) and k ( lower half ) on the development set . we observe that the average number of trees per edge is 1 , 2 , 3 , 4 and 5 , respectively , with an absolute boost of 10 % in γ as compared to the baseline .
the results of the second study are shown in table 2 . our model outperforms the previous stateof - the - art on biocreative vi cpr . it achieves 53 . 4 % f1 score improvement over deptree , which is significantly better than the kbesteisnerps model ( 52 . 4 % ) on bootstrap tests .
the results on pgr testest are shown in table 3 . our model obtains a significant improvement over the previous best performing model , deptree , by 3 . 6 points in f1 score . however , we should note that the improvement is slim and that we only tested our model on 1000 bootstrap tests .
as shown in table 4 , the best f1 score on semeval - 2010 task 8 is achieved by our model , kbesteisnerps , which significantly outperforms the previous best performing model .
the results are shown in table 2 . the first group shows that the training set size and the number of training instances for each macro are the most important factors in model performance . the model performs significantly worse when trained with hatt features instead of features like attract and attract , indicating that features like this are important for model performance in the low - supervision settings . the second group of results show that incorporating features like hatt improves the generalization ability of the model , both when training with all 1000 instances in the macro - domain and when matching the training size of the original embeddings . when training with only 1000 instances , the model performs slightly worse than the model with features like att and hatt .
the results of ablation study are shown in table 2 . our approach outperforms the baseline word2vec embeddings with a gap of 10 . 5 % in accuracy . we observe that the impact on generalization is also not notable , i . e . gcns perform better than katt models in most cases .
we observe that path2vec embeddings outperform node2vec in semantic analogy task , and in syntactic analogy tasks as well . the semantic relations are strongly positive in spearman correlations ( p < 0 . 001 ) with respect to human judgments , confirming the importance of syntactic relations in low - supervision settings . in our particular case , the correlation is strongest for wup , the largest of the three semantic relations .
table 3 compares the results of our graph - based vs vector - based measures on the senseval2 , senseval3 and semeval - 15 datasets . we find that our approach significantly outperforms the baselines on both datasets with a gap of 10 . 5 % on average .
in table 3 we replicate the results from ( kutuzov et al . , 2017 ) on the rareword set . we trained varembed on a 20 - million token dataset , polyglot on a 1 . 7b - token dataset , and fasttext on a 2 . 51m token dataset . the results are similar , measured as spearman ’ s ρ × 100 . the difference in similarity between dim and full vocab is minimal , but significant for in - vocab pairs , as in our second variation .
table 2 compares our system with other systems . we benchmark against the following baselines : system retrieval , system logistic time complexity ( seti ) , mtr and len . our system achieves outstanding results , outperforming all the other systems with a gap of 10 . 2 bleu points from the last published results ( 15 . 7 vs . 15 . 2 ) on the oracle test set . seq2seq achieves competitive or better results than our system on all metrics except for system comparisons . with respect to comparisons with oracle , we observe that our system performs better than the competition on both systems with two tasks .
the results for the second variation of experiment 2 where we trained on the no - char dataset are shown in table 4 . we observe that both the training and the mimicking set are comparable , with the case of the mimick having slightly better performance . the difference is less pronounced for the full data set , but still suggests that there is a need to design more complicated models and to refine our approaches for future iterations .
the results are presented in table 4 . we show the results for the models trained on a single data subset and tested on both the full data and the mimicked data set . the system performs well on both datasets with different feature sets in play . for example , ta significantly outperforms kk in both metrics . it closely matches the performance of kk with only 0 . 03 % absolute difference . with the additional data augmentation , the result is slightly worse than kk but still superior to ta . we notice that tags are important for the model to perform well both on the training and the test set . they help improve the recall scores for both datasets . when using the mimick dataset , the model performs better than the model with no data at all .
the results are shown in table vi . we report the percentage of tokens in the test set that are missing from the full vocabulary or the missing embeddings for each language . in general terms , we see that spanish is the worst performing language , with an absolute drop of 9 . 2 % in the oov metric . however , other languages perform comparably with english , french , german , dutch , russian and turkish . italian is the only one that performs much worse than spanish in the od metric .
the performance of each human model according to each scenario is reported in table 1 . it is clear some scenarios are more difficult than others to solve than others . the most difficult one to solve is the grocery shopping scenario , which shows the extent to which the human model can rely on superficial cues . however , it is encouraging to continue researching into this difficult topic as it helps improve the generalization ability of our model . we noticed that the error reduction over superficial cues is mostly due to high precision in low - supervision settings . we observed that the task completion ability that accompanies grocery shopping is relatively easy , but still requires a lot of data and time . fortunately , this helps to refine our model performance . we also evaluated it in word analogy task . we included only the accuracy and the number of instances in which the model had difficulty answering questions . we found that the tasks that required the most data were those that the model could easily distinguish between human and machine speech .
table 1 presents the results of our experiments on the standard decoder and seq2seq models . our models outperform both the baseline and the best previous state - of - the - art decoder models . we show that our model can easily distinguish between the true response and negative responses . moreover , our model significantly improves over the baseline with a p @ 1 improvement of 1 . 59 points over the previous state of the art . our model significantly outperforms other models with different feature sets in terms of decoding performance .
table 3 shows the error numbers for human , human and script . error base indicates that the accuracy of our model is relatively high while error percentage is low . we observe that our model can easily distinguish between the true response and negative responses based only on the error level of the error prediction .
we also evaluated the models in the deep layers of the network , specifically , the keller dobj , nsubj and keller amod scores . the results are shown in table 2 . our model obtains the best results in the second group . it closely matches the average score of the original embeddings . however , it is slightly worse than glove in terms of amod score . we observed that it is comparable on the superficial layers , but is inferior on the semantic ones . this indicates that the semantic information injected into the model by the additional cost term is significant enough to result in a noticeable drop in performance .
we noticed that dobj embedding model outperforms nsubj when trained with only one type of embeddings layer , namely , dobj model with dobj as en ( 2018 ) coder , dovec with a minimum of 80 % accuracy . we observed that the difference is most prevalent in the adjective category , where dobj dovec model ( h , t ) consistently shows much better performance than nubj model ( t , m ) with an absolute improvement of 2 . 5 % on average compared to glove model ( g , t ) . in particular , the improvement is much larger for adjective category than for noun category , showing the syntactic patterns discussed in the previous section hold high predictive accuracy for both genders . when using d - embedding , we observe that the model performs better on adjectives than the noun category . this corroborates our intuition that the semantic information injected into the word2vec by the embedding layer is significant enough to result in a measurable improvement .
comparison of our mwe model with elmo embeddings on the ws task is reported in table 4 . our model obtains 4 . 17 % overall improvement over elmo in the embedding dimension and a 3 . 66 % improvement in the training time .
table 5 compares the results of different training strategies . we find that the alternating optimization approach outperforms the baseline approach , confirming the value of word embeddings adaptation in the low - supervision settings .
for example , in table 3 we trained 8 transformers to convergence on 1m wat ja - en , batch size 4096 . linearized derivation also improves the bleu by 0 . 2 points over plain derivation .
for example , in table 4 we compare our model with the best wat17 result morishita et al . ( 2017 ) on ja - en . seq2seq with 8 models in the ensemble achieves 28 . 2 % improvement on dev bleu over the plain bpe model . linearized derivation also improves over plain derivation , but does not improve significantly over pos / bpe ,
in table 5 , the best performance on the test set is achieved by our ja - en transformer ensembles , which shows significant improvement on plain bpe baseline shown in table 4 . our model improves upon the pos / bpe baseline by 3 . 3 points in the bleu metric .
table 3 presents the results of models trained on english . our system works well on compact subset - based learner ensembles . it improves the results for both english and french , with the boost being most pronounced on the small - scale learner subset . large - scale learners outperform our system in both languages .
table 3 shows the results of models trained on the multi - news dataset . our dnn model outperforms all the other models apart from the case of spmrl . it shows that the performance gain comes from a better model design . we noticed that udpipe significantly outperforms the udpipe model in terms of perf score . this confirms the value of word embeddings adaptation . besides , udpipe , our model also outperforms udpipe with a large margin .
we noticed that the expansion options for wiki5k are slightly worse than the alternatives when we switch from the pre - trained model to the final model . the difference is most prevalent in spmrl , where the " - expansion " option results in a drop of more than 10 % in performance .
table 3 shows the maximum perturbation space size in the sst and ag news test set , which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification . as the number of tokens in question grows , the size of the perturbed space grows , which means that more sentences can be replaced with different tokens to maintain the correct answer .
the results are shown in table 2 . in general terms , the results of adversarial training seem to indicate that adversarial supervision is beneficial , improving upon the weak baseline performance of sst - char - level by 3 . 8 points in the adversarial task . however , it does not improve significantly over standard supervised supervision . predicate agents generally outperform supervised agents in both adversarial and unsupervised tasks . the difference is less pronounced for oracle , but still suggests some reliance on superficial cues . we observed that training on adversarial data leads to incorrect predictions , as the semantic information injected into the model by the additional cost term is significant enough to result in an incorrect prediction . this corroborates our intuition that there is a need to design more complicated training models and to refine the algorithm for further improvements .
the results for the non - anonymized and anonymized set are shown in table 1 . in general terms , we see that the nonverbal cues are specific about coin - toss , with the exception of bow - svm . the majority of the models that perform well in this task are anonymous , but there are exceptions such as bigru - att , whose anonymous model obtains higher f1 scores than the other two . also , contrary to intuition , the hier - bert model significantly outperforms the other models in terms of f1 score . when trained on the hidden test set , the average number of tokens per second is slightly higher than for the anonymous set , but still superior to most other models . it is clear from table 1 that the anonymity helps the model perform better in the more realistic task of prediction when the data is hidden .
we observe that seq . locates most of the errors in its labeling strategies in low - supervision settings . accuracy is shown in percentage points . for example , a - hmm ( 76 . 08 % ) and e - kmeans ( 63 . 69 % ) are both significantly better than the best previous approaches in terms of accuracy in all but one of the five target languages . in addition , the difference is less pronounced for ru , a language that is similar in many ways to the japanese ones , but has fewer clusters , making it harder to detect errors .
the best performance on parallelism is achieved using the cipher - avg algorithm . it improves the results for all languages except for those using ps graph pre - trained on pascal - voc it and ps graphs . the improvement is most pronounced for en , but it does not exceed that of fr , despite fr having spoken the parent language for more than a decade . ps graph improves further for all but es , with a gap of 10 . 5 points from the previous state of the art .
table 3 shows that our combined cipher grounder ( cipher - avg ) and a supervised tagger ( supervised tagger ) perform comparably to the best noun tagger . supervised taggers perform similarly to both en and fr ( see x4 ) . however , in our particular case , the precision improvement is much larger than that of en , indicating that the precision gain comes from a better model design .
we observe that the impact of grounded unsupervised pos tagging on mal is also not notable , i . e . it uas shows only marginal improvement , while the impact on las is much larger . it is clear from table 4 that the gold uas tags have a significant impact on the mal performance . they help improve the generalization ability of uas models by about 9 % in the standard task formulation .
the results are presented in table 2 . we observe that muse is comparable to guo in both accuracy and interpretability . it closely matches the performance of muse with only 0 . 40 % absolute difference . it achieves state - of - the - art results , outperforming both guo and muse in gold and silver . in particular , muse is the only one that can be seen to perform better in the low - supervision settings . this confirms the value of word embeddings adaptation . we observed that muse , like guo , is comparable in accuracy with both visual and syntactic descriptions .
the results are shown in table 1 . overall , bow - svm outperforms other models with different label types in terms of f1 score . it appears that the frequent label - based approach gives a significant performance boost , since it eliminates repetition . however , this boost is less pronounced for frequent labels , which indicates that the model relies on superficial cues . in our particular case , frequent and frequent labels are crucial for predicting events in the deep layers of the network . when combining all labels , the accuracy is only slightly better than the previous state of the art .
for each language , we show the number of tokens for each training and test set and the time period for which the dataset was last updated . religious tokens are the largest group , with a total of 233 , 947 tokens in the training set and 45 , 996 tokens for the test set . scientific tokens are smaller than the religious ones , but are comparable to the size of the religious tokens . we show the results for english , spanish , french , dutch , russian and turkish .
the results are presented in table vi . we show that norma , lookup and predicate schemas perform well over multiple domains . the maximum score achieved on each dataset is close to the maximum , but still significantly inferior to the minimum .
table 4 shows the mean absolute error and spearman ’ s ρ for case importance . our model obtains the best performance with an absolute improvement of . 369 ± . 000 over the strong baselines .
table 3 compares our model with previous work on the hidden test set of camr wang et al . ( 2015 ) . in general terms , the results are similar , with the exception of the case of jamr - style , where our model obtains 71 . 3 % local edge improvement . the difference is less pronounced for camr - adaptive models , but still suggests some reliance on superficial cues .
table 3 provides detailed results on the 2015 and 2017 ftd datasets . we show in bold our best summaries . our system outperforms all the other methods with a gap of 10 . 5 ftd points from the last published results . negations are uncovered through word embeddings ( see x4 ) that are significantly better than the unlabeled ones . wikification is the most difficult part of the task for our system to solve , as it requires a significant amount of data and time . fortunately , this gap is less pronounced for 2015 and 2016 compared to previous years . we present the results of our second study in table 3 .
the results are shown in table 2 . moving distance is the most difficult part of the task for our system , as it requires a lot of data and time to setup and maintain . moving distance also affects performance negatively , and we find that noinducedrulerule can significantly improve over moving distance . additionally , the jamr - gen model shows a drop in performance when using the conceptrule as a constraint .
the results are shown in table 3 . our best decoding method ( 1 - best ) obtains 39 . 9 % accuracy on the terminal dataset .
label distribution of the training , dev , and test set is shown in table 2 . the smaller training set indicates that the dev set is more likely to contain items of interest to the learner , and therefore , there is less variation in the label distribution . however , the difference between training and testing set size does not represent a significant big difference in label distribution , indicating that the training set size is a less significant factor in predicting sentiment . overall , we see that the distribution of happiness and anger is relatively uniform , with the exception of sad tweets .
table 1 shows the macro - f1 scores and its harmonic means for each of the four models in terms of happy , angry , sad and happy subsets . we observe that hrlce achieves the best results in the happy subset , outperforming bert , sl and sld by a noticeable margin . in addition , bert also achieves the highest average f1 score in the sad subset , which shows the extent to which the model can rely on the emotions of the user .
intrinsic evaluation results . our jamr agent outperforms all the other agents with a large margin . the results are presented in table 3 . our agent obtains the best performance on the hand - align metric .
the parsing results of our aligner model are shown in table 4 . our aligner significantly improves the precision of the jamr parsing results . the precision increases as a result of the reduced overlap between the aligned and unaligned spaces in the vocabulary .
our ensemble shows that the word embedding quality of our aligner is relatively high when we only consider pos and word pos , improving upon the results of ba17 by 3 . 2 points in coefficient metric .
table 3 presents the results of models trained on the hidden test set of all 3 domains . our model obtains the best results in all three domains . it outperforms the previous stateof - the - art models in all but one of the subtasks . the difference is most prevalent in the pos3 category , where our model ( ours ) obtains 45 . 2 % improvement over the previous best state of the art . we also observe that our model significantly boosts the generalization ability of all the domains .
the results are shown in table 7 . we can observe that the location and type of cell type are the most important factors in predicting the location of a location , followed by the type of gene expression . for example , if a location eq . 5 has the best performance , the gene eq . will predict the location best . however , if the location is the worst , the results are less important than the cell type . we can also see that the type and location of the gene expression vary depending on the underlying data type . in our particular case , the variation between the cell type and the location is less significant , but still indicates that there are some differences in expressing the opinions of the users .
table 3 presents the f - score of all models trained on the biocreative ii dataset . we show that the eaa annotation mode and hfa features significantly improve the results for all models , with the exception of conll - 2003 .
we show the number of examples for each language in table 1 . it is 1 , 590 , 885 total . in total , 69 , 6455 examples are written and 68 , 455 are spoken . the number of tweets in the black - box is more than 50 , 000 , which means that more than half of the examples are actually spoken .
confusion matrix for test data classification we show the results in table 2 . predicted sg and pl numbers are slightly different than actuals , which means that the predicted sg number can be confused with the actual number of tokens in the dataset . this leads to a significant drop in precision .
agreement patterns across genres are shown in table 3 . most of the agreement patterns shown in the table are notional , but still constituting a significant percentage of the overall agreement , as the percentage notional is higher in the spoken genre . in addition , the percentage of notional agreement that is spoken is slightly higher than the average for both written and spoken genres , indicating that there is a need to consider the ways in which the vocabulary is presented and the interpretation quality when selecting the appropriate genre .
ampere contains 10 , 386 propositions divided into 9 categories and 1 , 911 are non - a statements . the total number of propositions per type is 3 , 982 , which shows that more than half of the propositions are of interest to academics .
our results in table 4 show that our joint model bilstm - crf achieves significantly better precision on all comparisons than the previous stateof - the - art models . in fact , it is better than all the other models apart from the case of the partial comparison with respect to prec . segmentation .
we compare our proposed t - bilstm with previous stateof - the - art models on the factbank mae and meantime mae datasets . the results are presented in table 3 . we observe that the new approach significantly outperforms the previous state of the art models on both datasets in terms of mae . for example , lee et al . ( 2015 ) and parallelism produce remarkably similar results : of the 2000 example pairs in the dataset , the two have completely opposing predictions ( i . e . name a vs . name b ) on only 325 examples . in addition , the difference in precision between factbank and non - factbank mae is minimal , but significant on uds - ih2 datasets , in which our proposed lstm achieves the best score .
the results are presented in table 4 . with gold - standard segments , predicted segments and factored inference scores , cnn significantly outperforms svm in predicting most of the factoids . however , svm does not outperform predlexicon in predicting some of the most important facts . this is mostly due to small size of the dataset ( table 4 ) and the fact that predictions are based on word embeddings of the same word , which results in incorrect prediction of many of the facts . with gold - standard segments , our proposed method significantly improves over the svm baseline . it predicts 73 . 26 % of all facts in the gold standard and 69 . 43 % of gold - standard segments .
the model and its hyper - parameters . we show in table 1 that the beam size and dropout size are the most important factors in our model performance . additionally , we also consider the word vector dim and the attention type for scalability . our model achieves state - of - the - art results with a dropout dropout of 0 . 3 % on average .
the results on the wmt17 it domain english - german ape test set are shown in table 2 . the best performance on ter is achieved by our model , mt bojar et al . ( 2017 ) . we also observe that our model outperforms other models trained on a similar training set . note that the training set size and the number of iterations for each training set vary , so that the improvement may vary depending on the training size .
the performance of the best setting for each property is shown in table 3 . our word2vec sg embeddings outperform glove in all aspects ( except precision in f1 ) .
we also show the mean predictions for linear ( l - bilstm - s2 ) and tree models ( t - bi - lstmm - s ( 2 ) on uds datasets in table 6 . the tree models generally outperform the monolingual models in terms of precision , while the linear ones do not . we notice that acl : relcl has the best performance in both linear and tree modes . relational relations are relatively consistent across all relation types , however , the difference is most prevalent in the relation of concatenated relations , in our particular case , this is mostly due to small size of the dataset ( low some trees that perform poorly in linear mode ( e . g . xcomp , ccomp , xcompcomp ) are able to regain a lot of accuracy when trained in the tree mode . also , we notice that the adjacent relation labels are slightly less consistent with the original embeddings , adjacent relations are stronger in our case , perhaps this indicates that the semantic information injected into the model by the additional cost term has a significant impact on the model performance .
we show the lexicons used as external knowledge . it is clear from table 1 that the liwc lexicons contain a significant amount of information about sentiment , as the average number of words per lexica is 73 , 504 . however , sentiment is only one of the 25 % of words that are classified as sentiment , making it less useful for sentiment analysis . we filtered some of these because they were too generic ( " expression " , " expression " and " symbol " ) .
we also evaluated the models trained on sent17 , sent17 and sent17 - sent18 datasets . results are summarized in table 2 . in general terms , we observe that the affine model generally outperforms the baseline model in terms of generalization . however , affine models do not generalize well compared to the baseline , in part due to the small size of the training set ( https : / / www . uservoice . com / the results of concatenated test sets are slightly less clear than in baseline , regarding transfer learning experiments ( scv2 ) , we note that the embeddings can be improved with a reasonable selection of the features that the model has in its training set . these improvements are mostly due to small size ( around 5 . 2 % improvement over baseline ) we observed that the transfer learning method can further improve the generalization ability of the model when using additional training data . this is reflected in the average improvement of conc . conc . model over the baseline across all three datasets .
we show the results for linear mae and tree mae in table 2 . the results are broken down in terms of mean labels . perhaps the most striking thing about the linear mae results is that it appears to be more interpretable when the negative label is removed , indicating that the model can distinguish between the true answer and negative responses . however , this analysis fails to account for the fact that many of the negative responses are caused by the presence of negative labels in the training set , such as those for " no " and " will not " .
we observe that siguni outperforms sigvac in the coefficient of determination ( r2 ) on all metrics , indicating that the ability to select compact regions induces the generation of better captions .
table 3 shows the coefficient of determination ( r2 ) between automated metrics and crowdsourced topic - word matching annotations . we include metrics measuring both local topic quality and global topic quality . the local metrics generally perform better than the global metrics , but are slightly worse than the global average . we observe that the difference is less pronounced for newsgroups , indicating that the local metrics are more useful in predicting sentiment . additionally , we include metrics to measure the coherence of our algorithm , as this is the only metric that can be measured in a single domain , and therefore has a relatively high correlation with local sentiment .
we noticed a noticeable margin in performance between the baseline and the best performing method locations ( + 2 . 10 % p @ 01 and 0 . 3555 on average ) when we switch from frequency to pagerank ranking . also , we noticed a drop in the auc between baseline and current state of the art models due to larger variation in the training set size .
the results are presented in table 10 . we observe that the four feature - values that had the greatest effect on our model were the loc and event features , followed by frequency . the results show that the event features alone contributed the most to the model ' s performance . further , the combination features of loc and entity improved the results , but did not improve the results significantly over the baseline . finally , the number of event events in a row increased slightly , but still significantly less than the background noise reduction . we observed that the combination feature of event and frequency improved results ,
table 7 shows the five instances with the highest absolute prediction error . most of the errors in our uds - ih2 - dev instances are caused by grammatical errors . it is clear from table 7 that there is a significant amount of grammatical error in our multisim implementation .
in word2vec embeddings , we find that the cosine similarity of some event entity pairs is relatively high while others are low . it is clear from table 5 that some event have high correlation with human judgement . the most interesting ones are the 911 attack ( e ) and the hotel incident ( e ) .
for wt103 , we show the percentage of unique n - grams and tbcs for each self - trained model . as a baseline , we also consider the average number of unique ngrams in the training set and the number of unique self - learnt tokens in the wt103 dataset . we observe that bert ( large ) model significantly outperforms all the other models with a gap of 10 . 59 % from the last published results .
bert ( large ) and gpt ( small ) models generate 10 . 28 % and 7 . 47 % better precision scores , respectively , on the wt103 and tbc datasets . the difference in precision is less pronounced for wt103 , but still significant . gpt also produces significantly better precision . it is clear from table 3 that bert models are well - equipped to perform this task in the low - resource settings .
table 3 presents the results on msnbc , aida - b , ace2004 and wiki datasets . we show that our unlabelled model outperforms all the other methods apart from the case of aquaint . for example , it achieves 82 . 4 % on average improvement over the previous state of the art model on all metrics . on the msnbc dataset , it improves to 82 . 5 % from 75 . 2 % in 2008 . this confirms the value of word embeddings adaptation . we also observe that our model can further improve with the addition of lexical features . adding entity tags improves results , improving the average score by 3 . 7 % on ace2004 , though still performing substantially worse than other methods .
table 2 shows the f1 scores of our model when it is weakly - supervised on msnbc , aquaint , ace2004 , cweb , and wiki . the model also performs well on aida conll , a variation of the aida development set . as the table shows , the difference in f1 score between weakly and fully supervised aida models is minimal , but significant enough to warrant a correction .
we show the results of an ablation study on the aida conll development set . our model obtains the best f1 score of 86 . 05 on the evaluation set , which shows that the local attention model can distinguish between the true response and negative responses .
we report the accuracy ( % ) by ner type on aida - a . it can be seen that the loc model significantly outperforms the other types of models in terms of accuracy . the org model even outperforms loc in conll task . per also achieves competitive or better performance than loc , indicating the extent to which the semantic information injected into the model by the additional cost term affects the model performance .
the results are presented in table 6 . our h - combined model improves upon the strong baselines by 3 . 67 points in the discontinuous category and on the slimline category by 2 . 88 points . the difference is less pronounced for mwe - based models , but still suggests some reliance on superficial cues . we observe that the gcn - based model is more than 4 . 5 points better than the baseline on both the standard and the dissimilar categories .
we observe that in uds - ih2 - dev , the average number of tokens for each event is determined by the infinitival - taking verb , so that the precision of the predictions is relatively consistent even under the difficult requirement of xcomp - governed verbs . adding lexfeats improves the precision , but does not improve significantly over plain l - bilstm .
we also evaluated the system ' s performance in terms of mwe - based f - score . the results are summarized in table 2 . our h - combined system outperforms all the other systems that do not use gcn - based embeddings . it improves upon the baseline performance by 3 . 59 points in the discontinuous en metric . however , it does not improve significantly over the strong baselines . this indicates that the performance gain comes from a better model design that can rely on syntactic or semantic information alone .
table 3 presents the performance of the models using the cola and sst baselines . we included only the averages of the best performing models for each sub - category as a metric for brevity . our model outperforms all the other models apart from the case of the single - task case when using glue tasks as pretraining tasks . the performance gap is modest but significant with pretr . measure taking into account the fact that we only consider tasks that are already known to the model , and that the model is pre - trained on this dataset .
table iv presents the results of task training on the multi - task task . our approach improves upon the previous state - of - the - art on all metrics with a gap of 10 . 8 % in cola elmo with intermediate task training . we show the results with respect to task training . the smaller performance gap between the single - task and task training modes indicates that there is a need to design more complicated task training models and to refine the algorithm for further performance improvement .
the performance of the model on the cola task is presented in table 4 . we observe that it significantly outperforms all the other models apart from the case of the qqp task . the average cola score of 0 . 86 is slightly better than the previous best state - of - the - art model , while the sts score is slightly worse than the mnli score . mrpc , on the other hand , obtains the best performance . it appears that the model is more effective when trained and tested on a larger corpus .
table 1 shows the ne - tags of numbers in wikipedia . according to the table , the number of occurrences in wikipedia is 54 . 28 % , but the accuracy is only 2 . 86 % . relation cardinality is only 0 . 26 % , but it is a significant improvement over the original embeddings . time is also an important factor in the accuracy of our system , as it helps to more precisely detect events of armed conflicts termination ( where no insurgents should be predicted for a location ) , not only their start .
table 2 shows the number of wikidata entities as subjects ( # s ) of each predicate ( p , rn and k ∈ 1 ) for the vanilla and multi - nummod datasets . the average number of tokens for each predicate is reported in table 2 is p = 0 . 005 . for the vanilla dataset , we report only the vanilla p and r numbers , as this is the only dataset that comes with a nummod pre - trained on the training set . multi - nummed datasets generally outperform the baseline dataset in terms of both p and f1 scores .
we also evaluated the models in word embeddings . int is one of the better performing classifiers under automatic metrics . it significantly outperforms the average of both w2v and cnn , and lstm . moreover , it achieves state - of - the - art results in terms of precision on all metrics with a gap of 10 . 5 % in word metric from the last published results ( outsios et al . 2018 ) . int is significantly better than average in all metrics except for forestry . it closely matches the performance of oracle with only 0 . 25 % absolute difference .
table 2 shows the las improvements by cnn and lstm in the iv and oov cases on the development set of ara and baq datasets . δoov decreases significantly as a result of the reduced overlap between input and output labeled vectors , meaning that the cnn model can rely less on label labels . this indicates that the model is better able to interpret the input documents with a greater degree of accuracy .
table 3 shows the performance of our model on the test dataset in terms of accuracy scores . for each train and test dataset , we report both the official score ( from the belarusian and ukrainian newsletters ) and the result of re - scoring our second submission after replacing these 10 files with the ones from the ukrainian and belarusian newsletters . the results show that our model significantly boosts the performance in the low - supervision settings .
table 1 shows the bleu and exact - match scores over held - out test set . our model obtains the best overall performance with a coverage percentage of 71 . 43 % , which means that our dag transducer is more than 4 . 5 % better than the previous state of the art model .
we show the hyper - parameter values in table 3 . the average number of parameters for each parameter is 50 , which means that the lstm has 50 . 5 % chance to drop out of the lattice if the embram size is greater than 50 .
we noticed that the models performing best with the reduced reliance on word embeddings was the ' char ' baseline , indicating that the semantic information injected into the lstm by the additional cost term was beneficial . adding softword features improved results , but did not improve significantly over the baseline . we observed that the ' auto seg ' model performed best when using onlychar as input . moreover , the improvement was much larger when using ' char - bichar ' as input instead of ' softword ' features .
we show that the gold seg model achieved the best performance with the word baseline in the low - supervision settings . it outperforms both the baseline and auto seg models with a gap of 10 . 59 points in f1 score from the last published results ( yang et al . , 2016 ) . the difference is narrower with the smaller variation in input type , but still suggests some reliance on superficial cues .
we show the results of our final model on msra in table 6 . the results show that the lstm model significantly outperforms the baseline model with a gap of 10 . 18 points from the last published results ( kutuzov et al . , 2016 ) in terms of precision . the difference is narrower than for other models , with the exception of chen et al . ( 2006a ) . further , the difference between the average precision of " char baseline " and " softword baseline " is less pronounced for softword baseline , but still suggests some reliance on superficial cues .
the results reported in table 8 show that the lstm models significantly outperform the baseline models with a gap of 10 . 46 points from the baseline .
the results for english – estonian are in table 3 . we report both the official score ( from our dev set ) and the result of re - scoring our second submission after replacing these 10 files with the ones from our first submission . the results are slightly worse than the original evaluation set but still superior to the monolingual baseline . we note that the reduced indentation helps the model to improve interpretability . further , we notice that the bleu scores have increased as a result of the reduced effect of indentation .
the results reported in table 2 show that our baselines are comparable to the best previous state - of - the - art models ( sestorain et al . , 2018 ) . dual - 0 is indeed comparable , with the exception of pbsmt . however , supervised ( avg ) models perform slightly better than our baseline on both datasets , confirming the value of word embeddings adaptation .
we performed an ablation study on a single model trained with gold data only . the results of semantic feature ablation are in table 3 . we report the results of removing all the edge features and the presence of any node attr except num , tense and tense . in general terms , the results show that the model performs better when the bleu score is computed with respect to all the features removed .
the results of experiment 1 are shown in table 2 . our baselines adapted to the new pbsmt - 0 standard outperform the baseline on average . they yield significantly better precision scores than the original embeddings . dual - 0 is better than the best baseline on three out of the four datasets . the difference is most prevalent in experiment 1 , where the precision drop of more than 10 % in the translation window indicates that the trained model can rely less on superficial cues .
the results are presented in table 2 . our baselines generalize well across all three domains , with the exception of distill . in general terms , we confirm what klinger et al . ( 2018 ) report : the trained models perform well in both languages . however , in distill , our baselines are slightly worse than the other two domains . supervised models perform slightly better than the trained ones in both domains , confirming the importance of word embeddings adaptation .
the results on the official iwslt17 multilingual task are in table 4 . our baselines match the strong baselines of the original embeddings ( hochreiter and schmidhuber , 1997 ) and the best performing supervised model ( lebanoff et al . , 1997 ) . the difference is minimal , however we see that our adapted model significantly outperforms the original model .
the results on our proposed iwslt17 model are in table 5 . the results of the best performing model is reported in bold . our model significantly outperforms the best previous state - of - the - art model .
the results are presented in table 4 . our system outperforms the best previous approaches across all three subsets . in particular , we see that our en - et and en - ru subsets significantly outperform the previous approaches . subtitles in the europarl corpus are markedly gender - neutral , improving the results in the translation task . in addition , the number of tokens in the correct translations improves as well . overall , our system performs better in both subsets than the previous methods .
table 3 shows the bleu scores for the bilingual test sets . our model obtains significantly better performance than the contextual baseline on all the three sets . on europarl , it is better than the previous best performing model by 2 . 95 points . on the europarl dataset , it improves by 3 . 45 points . these results show that our model can interpret the word embeddings with significantly better precision .
table 4 shows the bleu scores for en - de bilingual test set . the current turn has the best performance , so we consider that the current context is the most useful . the other languages that the model can handle well in this setup are spanish , french , dutch and turkish .
we find that the best performance on the 300 - dimensions test set is achieved by our utdsm model , which achieves 69 . 2 % overall improvement over the previous state of the art . generalized attention mechanisms ( utdsm ) generally outperform random agents , but do not exceed the upper boundary of the realistic upper boundary , in which again demonstrating the competitiveness of our model . we notice that amiri et al . ( 2016 ) and lee and chen ( 2017 ) achieve significantly better results with their model with a gap of 10 . 5 % on average from the previous best state .
table 2 shows the bleu scores for domain match experiments . our model outperforms all the other methods with a large margin . the difference is most prevalent in the wsj domain , which shows that the training data better than the brown baseline can be trained on a larger corpus .
the performance of our method in multi - class classification is shown in table 2 . our average cd score improves over the previous state of the art model by 3 . 6 points in f1 - score . maxcd also improves significantly over the global - dsm model .
evaluation results on paraphrase detection task are shown in table 3 . our global - dsm model outperforms the other methods with a large margin . it achieves 69 . 7 % f1 - score improvement over the previous state of the art model .
we report the macro f1 - score of unknown intent detection with different proportion of classes as known intents in snips and atis dataset . lof ( softmax ) outperforms doc in terms of macro - f1 score while lmcl outperforms it on snips 25 % and 75 % of the time . the difference is less pronounced for doc , but still suggests some reliance on superficial cues . we observe that the ability to select compact regions induces the generation of better captions . moreover , the accuracy remains the same across both datasets , with the exception of snips . in snips , doc and lof , compact regions consistently outperform softmax on both datasets . atis , on the other hand , is comparable with softmax , but does not outperform lof substantially , in part due to the smaller size of the dataset .
we compare al with and without its truncated average , tracking time - indexed lag with a wait - 3 system . as the results show in table 1 , the effectiveness of our system is increased when the average number of statistics in a report is truncated , i . e . that the report contains enough information to cover all relevant statistics , without having to assume whether a report contains multiple entries .
table 4 shows the bleu scores for evaluating amr and dmrs generators on an amr test set . amr significantly outperforms dmrs in gold and silver , indicating that the amr generating method can significantly improve interpretability without a drop in performance .
the multinli matched dev set is presented in table 4 . it shows that the ability to reason in multi - domain contexts is improved with the increase of complexity . multi - domain reasoning achieves state - of - the - art results , outperforming the simple and medium reasoning baselines . complexity is the most difficult class to solve , as it requires much more data and time to compile . however , the improvement is slim , with multi - nli achieving 83 . 56 % on average compared to the simple reason baseline . in the more realistic multi - dev set , the complexity score improves to 99 . 27 % , but is still significantly lower than the level of the simple baseline .
table 1 shows the performance of the method for reference . memory - to - context bleu shows that the performance gain comes from a better leveraging of word embeddings . more importantly , the performance gains come from a higher correlation with the presence of the correct context in the bootstrapping context . when a context is used , the effect is only statistically significant on output . this confirms the effectiveness of our method . we show the results for reference in table 1 . for reference , we also consider the results of nc - 11 , the best performing state - of - the - art on this dataset . note that this particular dataset only applies to reference ; on the output dataset , it applies only to messages that are already in the context .
the results are presented in table 2 . we show the results of concatenating the extracted sentences from the two subsets of the word embeddings . our proposed algorithm outperforms all the other methods apart from the case of the syntactic treedepth . it achieves 60 . 3 % improvement over the baseline on average compared to the previous state of the art . we observe that it significantly boosts the semantic accuracy for both subsets . the average number of tokens in the concatenated sentences is significantly higher than the baseline , which shows the extent to which semantic features can be improved with a reasonable selection of the lexical resource from which the semantic features were extracted .
the results are presented in table 5 . we show the results for the subsets of sentiment analysis and relatedness / paraphrase section in the supplementary material . our model outperforms all the other models with a gap of 10 . 2 points in the relis score . relis significantly improves over the baseline on all datasets . it achieves state - of - the - art results , and its average number of frames per sentence is close to those of sst2 , though it is inferior on some datasets . relis features significantly improve over other approaches . it improves predictive performance across all datasets , and on sst5 , it achieves its best result since 2003 .
the results are presented in table 8 . we present precision @ k9 for the 20 - ng and 25 - ng datasets . it can be seen that the model performs well on synthetic dataset with a minimum of 80 % p and 90 % f1 score . moreover , p - means measures well above chance in both sets , with an average of 71 . 65 % on the 20ng and 27 . 27 % on 25 . 53ng dataset . on the sst - 5 dataset , our model obtains the best performance with 78 . 53 % on average .
we pruned the pruned cnet interjections and hypotheses with scores below 0 . 001 so that the coverage of the manual transcripts could be improved . the results are shown in table 1 . the reduced coverage of words indicates that the slots / values have a significant impact on the output quality of the dstc2 development set , and consequently , we need to consider re - scoring our work .
we noticed that the weighted pooling approach outperforms the no pruning approach when using the 1 - best baseline baseline in terms of all goals , but do not exceed the threshold for score threshold in any case . this indicates that the architectural choice that had the greatest impact on our model was the pruning removal of unnecessary branches . we observed that cnet with the minimum score threshold of 0 . 001 was significantly better than cnet without the threshold .
table 2 shows the dstc2 test set accuracy for 1 - best asr outputs for 10 runs with different random seeds in the format average maximumminimum . our system achieves the best performance with a minimum of 63 . 5 % on transcripts , a slight improvement over the previous best performance of 62 . 6 % on live asr .
the total number of tokens for training and test set is 31 , 545 , constituting 39 . 4 % of the total training tokens . the gold class is 5 , 563 tokens , and the requirement list is 2 , 860 tokens . for testing , we split the tokens into gold and silver classes . sentences / clauses are split into 9 categories to ease interpretation .
as a sanity check , we also evaluated it synchronically , that is when we only apply the test on tweets from one domain and only show the results for tweets from other domains , without penalizing the embeddings for yielding incorrect answers . for this easier task , we used only unigram probabilities for input validation . results are shown in table 1 .
the performance of the gold classifier is presented in table 4 . we show that all the metrics we consider have low correlation with the true response time . nevertheless , our model obtains the best results with an auc of 0 . 97 . the results show that precision is relatively consistent across all classifiers with the exception of obligation .
table 3 shows that for the two types of data , our proposed method results in significantly better results . the difference in bleu score between the original and pseudospecific embeddings is minimal , but we see significant difference in overall performance due to different sharing of the training data . our proposed method significantly improves the results for both categories . for example , the improvement on the ru → ja and en → ru data is 15 . 59 % and 8 . 63 % respectively . on the pseudo - parallel data , the difference is only 2 . 43 % and 6 . 53 % overall .
the results are shown in table 2 . proto significantly outperforms the other models in the 5 - way 1 - shot task . on 1 . 0 and 2 . 0 frames , proto achieves 8 . 32 % and 5 . 63 % better overall results on average compared to the results of bert - pair ( 78 . 28 % ) and gnn ( 71 . 63 % ) on the same shot . gnn also achieves competitive or better results than proto on two - shot tasks . when trained on a single shot dataset , the accuracy is slightly better than on the five - shot dataset , but still significantly worse than proto . we observed that proto - adv outperforms proto on both on - shot and on - off - shot tasks .
the results are shown in table 2 . proto ( cnn ) and proto ( bert ) models achieve extremely high accuracies in the accuracy on the five - way - 1 - shot task . they outperform bert - pair by 10 % in all but the last shot . accuracy is relatively consistent across all three models , with the exception of proto ( 70 . 28 % ) in the case of the second shot . in general , bert models perform better than the models trained on the plain cnn data set .
we also evaluated the accuracy of our models on the hidden test set of wiki / figer in the unsupervised setting . it can be seen that our proposed method outperforms the previous stateof - the - art methods in terms of both accuracy on both gold and silver - aligned targets . the difference is most prevalent in the accuracy on the mi - f1 metric , which shows the extent to which the semantic information injected into the model can be improved by the additional cost term .
we also evaluated our prec . level in terms of f - 1 score . it can be seen that our proposed system outperforms the previous state of the art on all three label types . however , the difference is most prevalent in the support category , which shows that our model can rely on superficial cues to predict responses based only on supporting documents without understanding the task .
we noticed that the most representative ones are the is_heavy and is_colourful features , followed by is_dangerous . in particular , we see that is the strongest under - represented class in all models , and is the only one that appears in the black - box . apart of the superficial differences in performance , the op scores of these features are relatively consistent , with the exception of is_expensive .
the training time for each model is reported in table 4 . it takes a negligible amount of time to learn the task , since the training time is based on the number of parameters in the input sentence . however , the time taken to train is significantly longer than that for x - bilstm , because the size and type of training instances is very small .
the results are shown in table 2 . crowd is slightly worse than the others under all three metrics , in that it is found to be more likely to flag items as dangerous when they are found in the wrong location . apart of the reduced performance of av - cos , full is the better performing classifier under all metrics . it should also be noted that full is not the only classifier that performs poorly under pressure . we also see that crowd is significantly worse under pressure when found to contain objects of interest other than the ones it is supposed to be protecting . this corroborates our intuition that the more information a user has about the location of the objects , the more likely they are to flag them as dangerous .
we noticed that the sub - category in which most models had the worst performance was currency country category category , sub - categories as adjectives antonyms and performer action had the highest percentage of out - of - vocabulary terms , so we observed lower performance in these categories for all models .
classification results are presented in table 2 . we show that the sub - category in which most models had the worst performance was currency country category . sub - categories as adjectives antonyms and performer action had the highest percentage of errors , so we observed lower performance in these categories for all models . our model outperforms the other models in semantic and syntactic similarity .
table 2 shows that for female pronouns , the system performs substantially worse on “ gotchas ” than it does for male pronouns . gotchas are words that are slightly gender - neutral but contain occupation information , and are difficult to solve . for example , the cbcefbyes system performs significantly worse on a sentence containing occupation information than the original embeddings . neural systems do not handle occupation information well , so we see that it is harder for them to solve instances where occupation information is gender specific .
the results of the models in question are presented in table 1 . our model obtains the best performance on three out of the four metrics . it significantly outperforms the previous stateof - the - art models in all metrics . in particular , it achieves the highest b - 1 score on the meteor metric , which shows that it has the best generalization ability .
accuracies for the approaches are shown in table 2 . advdat models outperform the baseline substantially in all aspects ( except for the hard subsets ) under all three metrics . infersent outperforms advdat in the hard subset , and advcls in the soft subset . the results show that the adversarial approaches can significantly improve interpretability without a drop in performance .
table 3 shows the percentage decrease in score for each category from baseline advdat to current advdat . in general terms , we see that sleeping is the most difficult category to solve , followed by driving and asleep . the only exception is empty , as it appears to be easier for the model to solve when there are no other words in the vocabulary for the category . apart of the sleeping category , there are two groups of words that are particularly difficult to solve : nobody ( i . e . , no , sleep , no , and asleep ) are the two groups that show the most significant decrease in performance . moving onto advdat , we find that empty is the only category that shows a decrease in percentage of score , compared to previous advdat models . this suggests that there are ways to improve the prediction quality for this task .
for completeness , here we also compare against [ empty ] and non - empty models . we observe that , let alone a reduction in performance , the cbow model significantly outperforms all the mod table 3 shows that cbow has superior generalization ability on synthetic dataset with a gap of 10 . 5 % on average from the last published results .
comparison of our gnbusiness dataset with other news datasets we compare gnbusiness with other widely used news datasets . the largest of these is ace 2005 , which contains 10 , 985 documents and news clusters . gnbusiness , in turn , is comparable to ace 2005 in terms of number of documents , but is slightly smaller than ere . additionally , astre has more than 50 , 000 documents in its news clusters , which means that gnbusiness is more likely to match existing datasets .
we observe that the preserved bleu and dal scores are slightly higher than the unpreserved ones , indicating that there is a need to maintain high precision in the low - supervision settings . with and without mass preservation , our milk scores are the same , but the difference is less pronounced for the dal score .
table 4 shows the overall performance of our method in schema matching . odee - fe outperforms the odee model with a gap of 10 . 8 points in f1 score from the last published results . schema matching is extremely difficult in dfblp : conf / acl / nguyentfb15 ; we find that more than 90 % of the time , the matching distance between the correct answer and the wrong answer is significant enough to result in a match .
averaged slot coherence results are shown in table 5 . our odee - fe model obtains 0 . 18 / 0 . 20 overall improvement over the odee baseline .
our method outperforms the other methods in both meteor and rouge - l metrics by a noticeable margin .
as the percentage of n - grams in test abstracts that appeared in training data as plagiarism check indicates ( table 3 ) , human also contributed significantly to the analysis . the system generated by system / human plagiarized 10 . 42 % of the abstracts . it is clear from table 3 that human was also responsible for some of the plagiarism .
we compare meteor and rouge - l with previous models on this data . in table 5 we report the average number of iterations for each iteration , and the percentage of iterations per label according to the number of frames in question . meteors significantly outperforms other models with different feature sets in terms of iteration comparison .
table 4 shows the turing test passing rates . as a baseline , we also consider the non - expert and expert t - test set . the results show that for both groups , the different titles have a negligible effect on performance . however , for the expert set , the difference is much larger . the difference between the average and the average number of tests for each title is much smaller . for example , different titles for a single test may result in different results , but this does not impact the results significantly .
the results of the best models are presented in table 4 . we observe that the visual cues alone result in significantly better results than the best baseline models . however , the difference is less pronounced for text - based models . still , we see that visual cues significantly outperform both the baseline and the best trained models across all three types . the results are broken down in terms of s and p scores , with visual and verbal cues leading to significantly higher s + p scores . random and human are qualitatively very different from each other in that they produce significantly worse results . in comparison , the score difference between visual and verbal is much smaller than that between random and human , but still suggests that there is a need to distinguish between the two types of training data . we notice that the ability to select compact regions induces the model to rely less on word embeddings . this is reflected in the average number of frames for each classification . for example , in the visual classification case , visual features alone results in 1 . 45 % better performance than random features . verbal features alone contribute significantly less than visual features to the overall score . to test the contribution of combining features , we compare our model with the best previous models . we find that it is harder for visual features to outperform the baseline than for verbal ones .
table 3 presents the results of models trained on the hidden test set of hotpotqa in the unsupervised setting . we show that tfn outperforms all the other models apart from random in terms of all metrics on both cc and s + p scores . for example , tfn achieves 1 . 181 s + p score with a gap of 10 . 5 % from the previous best baseline on random . with respect to random , the gap is narrower but still significant with tfn performing significantly worse than other models . we see that randomization has a generally positive effect on predictive performance , improving it by 10 % on average over the baseline model .
the model is trained on three different combinations of pmc and pubmed datasets ( top score marked as bold ) . each combination improves the performance on the mednli task . the difference in accuracy between pubmed + pubmedd and pubmed datasets is minimal , but significant enough to result in a drop of more than 2 % in biobert performance .
we also evaluated the models in terms of two - tailed t - test . our model outperforms all the other models apart from the case of bilstm when using the deat dataset . the results are shown in table 2 . dual - task learning models generally outperform the other baseline models . in particular , we see that the at model beats all the baselines except bimpm by a large margin . its performance is bolstered by the fact that it is trained on a single dataset , which means that it has less training data on which to bias its model . moreover , the model performs better when trained on multiple datasets , meaning that it can rely on the unbalanced distribution of the training data to refine its model performance .
as shown in table 3 , the model achieved an accuracy drop of 0 . 843 % when fed - forward with the current set of tokens from each paragraph .
we observe that the window position affects the performance of our model , simlex999 , negatively . the difference in performance between symmetric and asymmetric modes is minimal , however we see significant variation across all models due to different window positions . for example , in symmetric mode , our model obtains significantly worse performance than in asymmetric mode .
we observe that for simlex999 , the os true flag is slightly better than gw false in both cases , indicating that the semantic information injected into the model by the additional cost term helps the model to improve its performance in the cross - sentential contexts . we observe the same trends in simlex998 .
we can see from table 4 that for the simlex999 dataset , the removal of stop words has a generally positive effect ( i . e . an improvement in performance ) across all models , improving from 0 . 42 to 0 . 44 on the simulated word embeddings . however , it does not improve significantly compared to the os model .
table 1 shows the mean matched validation accuracies of our method with respect to the presence or absence of character embeddings . our method generally outperforms the best state - of - the - art pooling method across all 10 iterations .
table 3 shows the overall performance of our multinli model broken down by genre . our best model ( multinli overall ) outperforms all the other models across all three genres .
we show in table 1 the glue task performance of bert models with different sized training sets pre - trained , fine - tuned and initialized with normal distr . as the smaller size of the training set indicates , the accuracy decreases as a result of the smaller training set size . however , our model still obtains decent performance with a f1 / acc of 5 . 6 / 7 . 0 and 78 . 6 % on sts - 2 task . we also observe that our mnli model achieves competitive performance with other models with smaller training sets . the mnli - m model , trained on a single dataset , achieves 6 . 3 / 8 . 3 f1 and 87 . 4 % acc on the sts task .
recent points in the literature that our system can compare against are summarized in table 3 . scores are tokenized bleu . our system outperforms all the other systems with a gap of 10 . 3 points in bpe .
character versus bpe translation . we show in table 2 that our translated bleu char embeddings outperform the original ones in terms of all metrics .
table 4 shows the error counts for 100 randomly sampled examples from the deen test set . most of the errors in the sample are caused by lexical mistakes . our proposed method can reduce the number of errors for some cases , however it does not improve significantly for others .
we also provide a quantitative evaluation on the wmt15 deen encoder in table 6 . the comp . column shows the ratio of total computations carried out in the encoder . our model bilstm achieves a performance improvement over the best previous state - of - the - art model with a bpe size of 32 . 2k . by further adding hm layers , the performance increases to 29 . 7k .
we compare our proposed approach against 2 baseline models - bilstm , zubiaga et al . ( 2017 ) in terms of μr and ef1 scores . the results are broken down in table 1 . our proposed bert model outperforms the pre - trained model in both metrics by a noticeable margin . for example , bert achieves an average μr score of 0 . 699 ± 0 . 012 on the smallscale news commentary sub - scale compared to the previous best state - of - the - art model ( 0 . 652 ) on the large - scale web content . moreover , the puc score of bert is slightly higher than the other baseline models , indicating that the model performs better in the low - supervision settings . we observe that bert and wiki both contribute similarly to the task , with the difference being narrower .
table 3 shows the mean precision ( map ) of our model bert on political speeches . bold indicates best performance , underline indicates second best . we further report that bert achieved the best performance with a map of 0 . 346 on the political speeches dataset .
table 4 compares the results of manual relabelling of the top 100 predictions by puc model with the original labels in each dataset by two different annotators . we choose to compare twitter , wikipedia , and google + for each dataset , as the source domains . the results are presented in bold . the average f1 score of each annotator is reported in table 4 . twitter is significantly better than the other two annotators in both categories , in that the accuracy is higher in the politics category . however , in the general categories , google + is slightly better than twitter in both twitter and politics . we find that the difference is most prevalent in the education category , in which google + annotated tweets are 1 . 3 times as likely to make incorrect predictions as those by wikipedia , but this analysis fails to account for the large variation in the number of tweets in this category .
table 1 shows the results for the positive and negative recall . our approach outperforms all the base lines with a gap of 10 . 5 % in f1 score from the last published results . the results show that our approach can significantly improve the generalization ability of neural network models in the neutral task .
we show the results for cosine and 10rv in table 3 . in general terms , we see that cosine model significantly outperforms the competition on all metrics . its average score of 77 . 7 % is slightly better than the avg . of 75 . 2 % on the standard task formulation . however , cosine is still inferior to the other models on two of the four metrics . the difference is most prevalent in the currency country category , sub - categories as adjectives antonyms and performer action have the highest correlation with precision , so we observe lower precision in these categories for all models . semeval17 has the best performance on both currency country categories . we observe that it is better to select compact regions of the word embeddings and move them closer together when selecting the relevant features for a task . this helps improve the interpretability of the input documents .
the results are shown in table 6 . direct and source bridging bridges outperform target bridging , but do not exceed the upper boundary of our strong lemma , in which we observe that source and target are only slightly comparable . with respect to concatenation of the input and target states , rnnsearch * shows a slight improvement over vanilla on mt02 and mt03 , but a drop of more than 2 % on mt04 . smt * also shows a drop in performance on mt03 and mt08 . to test the contribution of concatenated part - ofspeech embeddings , we compare our proposed algorithm to the best previous state - of - the - art on the training data . we observe that the proposed approach significantly improves the generalization ability of our model .
results in table 2 show that our shared - private decoding model is significantly better than the vanilla transformer model in the wmt english - german translation task . the difference is most prevalent in the decoding task , with a bleu score of 98 . 79 % ( t - test , p < 0 . 05 ) vs . the previous best performance of 62 . 39 % .
table 3 shows the results for the iwslt { ar , ja , ko , zh } - to - en translation tasks . we shared the results of our best model , ar , with respect to the vanilla dataset , in table 3 . the results show that our model obtains a significant improvement in the translation task with a bleu improvement of 2 . 36 points over the previous state of the art . however , the difference is less pronounced for ko , with an overall improvement of 1 . 95 points .
table 4 shows the performance of our shared - private model with different sharing coefficients on the validation set of the nist chinese - english translation task . shared - private models perform better than the models using the vanilla sharing coefficient . we find that the difference in sharing coefficients between vanilla and shared private is less pronounced for zh - en , but still suggests that there is a need to consider the impact of sharing coefficients .
table 2 shows the average time for users to set up the tool and identify verbs in a 623 word news article . the differences between gate and either slate or yedda are significant at the 0 . 01 level according to a t - test . since the system is pre - trained , it takes a negligible amount of time to setup and use the tool . however , once the tool is set up , it is very difficult to remove it and re - install .
we also evaluated the models using syntree2vec embeddings . in table 1 we report the perplexity scores of our model using the base sentences of node2vec , syntree - 2vec and word2vec . the average number of tokens in each sentence is reported in table 1 , as the number of times the sentence appears in the correct sentence is the most important factor in determining the difficulty score . our model obtains the best performance with a score of 71 . 01 % on syntree 2vec . on the other hand , our model outperforms both the other methods with a large margin .
as the label description for asnq shows in table 1 , there are four types of labels that refer to answer sentence , long answer passage and short answer phrase respectively . the number of examples in our dataset is 19 , 446 , 120 , which means that more than half of the correct answer sentences are identified as no answer at all .
we compare our proposed approach against 2 baseline models - comp - agg + lm , lc + tl , and tl + tl ( qnli ) on the fixed - fold test set of asnq and wikiqa . the results are shown in table 2 . our proposed approach outperforms the previous approaches in terms of map , mrr and tnl score . the difference is most prevalent in tf mode , where our proposed model ( bert - l ft asnq ) achieves a map improvement of 2 . 3 % over the baseline bert - b model . however , it does not outperform the roberta - b approach , in which it receives a 0 . 9 % boost in map .
we compare our proposed approach against 2 baseline models - comp - agg + lm , lc + tl ( qnli ) on standard asnq and trec - qa datasets . the results are shown in table 2 . our proposed approach outperforms the previous approaches in terms of map , while roberta achieves a slight improvement on mrr .
the results are shown in table 2 . as a baseline , we also consider the drop in precision for bert - based and trec - qa models . in both cases , the drop is much lower than in the case of fine - tuning . according to our estimations , the noise reduction caused by fine tuning can range from 0 . 10 % - 22 . 73 % of the precision drop in wikiqa due to noise reduction .
as shown in table 6 , the three types of labels have different impact on fine - tuning bert . the most significant ones are map , mrr and trec - qa map . neg and pos refers to question - answer ( qa ) pairs of that particular label being chosen for fine - tune . as the table shows , the neg label has the least impact , but the impact is significant when we add multi - labeled question pairs to the baseline bert model .
table 7 compares the results of bert - base and trec - qa with asnq and qnli . as this table indicates , the accuracy improvements on wikiqa map are minimal but significant , which indicates that the translation quality of tanda can be further improved with a reasonable selection of the training data .
for completeness , here we also compare bert with asnq baseline on the three sets of data . as this easier setup allows more data to be used on multiple datasets , our model obtains significantly better results on all three sets . the results are presented in table 5 . prec @ 1 and map scores are the average of the precision @ 1 , average mrr and average precision @ 2 scores of the pre - trained models , while nad is the average number of points of precision for the map and mrr . as the results show , bert ' s performance improves with the addition of map and precision scores . moreover , the model ' s nad score improves as well . it is clear that bert has learned to rely less on superficial cues .
the results are shown in table 2 . we observe that has_diff has the best performance with an average of 1 . 0 % on average . it is clear from the results that has contributed the most to the model ' s performance . moreover , the bias term that has the greatest impact on the model is " bias term " . bias term refers to the number of tokens in a contribution tuple , and the type of bias term for each contribution . since the model only works with one type of contribution , and is strictly limited to ic = 1 , we observe that having a bias term helps the model to more accurately detect trends in the hidden messages .
table 1 , listing 1 & table 1 : ( right code snippet ) implementation of kiperwasser and goldberg ( 2016 ) ’ s neural parser in only a few lines using uniparse . we show that it is possible to improve upon the simple eisner et al . ( 2014 ) parser with a stepwise improvement in decoding speed . ( clemson et al . , 2014 ) generalizes best , and in our experiments , it achieves 19 . 31 % improvement over the performance of the original eisner ( generic ) parser .
we show the results for treebank ar_padt in fig . 2 . it can be seen that our proposed method significantly outperforms the previous stateof - the - art methods in both accuracy and precision . our proposed method improves upon the performance of the pre - trained model by 10 % in the standard task formulation and in the gold - two - mention case by 3 % .
comparison of the word form similarities , in % of 1 − vmeasure of clustering . we find that our jw clustering method achieves the best performance with a f1 of 8 . 17 on the 28 datasets .
in table ii , we compare the performances of the models with pre - trained embeddings and with unsupervised models trained only on one dataset . we find that the fasttext model significantly outperforms the w2v model . its average number of iterations per sentence is 10 . 2 vs . 7 . 8 for the previous best state of the art .
we compare the accuracy of our model with the previous stateof - the - art models on the development set of ucl . our model ( bert ) shows a slight improvement in accuracy ( ρ = 0 . 67 ) compared to hexaf ' s 80 . 18 % .
performance of the question generation system on fever dataset . it shows that the system can easily convert the total number of claims in the training set and the number of questions in the test set to questions when converting the claims into questions . the conversion accuracy is 88 . 63 % , which means that more than 90 % of the claims converted to questions are actually questions , and only 69 . 63 % of claims are errors .
performance of the question generation system on fever dataset . it can be seen in table 2 that the training set is significantly better than the test set in terms of accuracy , confirming the effectiveness of our label prediction . the accuracy increase over training set indicates that the label prediction ability of our system can be further improved with an increase of accuracy in the production set .
table 1 shows the results for the transductive scenario gap and the bias gap in fm1 . our model bertwikirand outperforms all the other models apart from the case of the scenario gap in which it is trained . it bridges the gap with respect to both f1 and fm1 , but does not improve significantly over the strong baselines on wnli dataset . this indicates that the translation quality of the model can be improved with additional training data .
comparison of pretrained and non - pretrained embeddings is presented in table iii . in pretrained embedding , the results are slightly better than those of non - p ( n ) pretrained embed , but still significantly worse than the pretrained baseline . pretrained embeddeddings perform similarly to that without pretraining , confirming that the ability to select compact regions induces the generation of compact sentences .
for completeness , here we also compare against the best previous models . the results are presented in table 2 . our model obtains the best results on three out of the four datasets . transactional errors are only . 005 % on average , which means that more than half of the errors identified by our model are actually caused by incorrect entries .
we also evaluated the model in terms of entity linking accuracy . transformed models generally perform better than unsupervised ones . in our particular case , the difference is most prevalent in trans - supervised mode , with an absolute improvement of 2 . 3 % in tigrinya and 0 . 9 % in oromo .
we show the results for entity linking accuracy with pbel , using graphemes , phonemes or articulatory features as input . the results are shown in table 3 . we find that the articulatory part of the grapheme vocabulary contributes significantly to the model ' s performance . it improves by more than it does not harm the model . specifically , it improves by . 28 % over the baseline . further improving performance by high margins
in table 1 , we report the precision numbers for each question as well as the number of correctly answered questions for each scenario as a percentage of the total number of questions in question , for both scenario and scenario - free iterations . as a baseline , we also compare against the best previous approaches : bimpm , esim , and diin . in both cases , the results show that the model performs better in the scenario when the scenario is considered . however , the difference is less pronounced for diin , since it uses a scenario - based model .
in experiment 1 , we replicate the results of experiment 1 with different embeddings for english , french , dutch , russian and turkish . the results are presented in tables 1 and 2 . as hard coreference problems are rare in english , we do not have significant improvement . however , our model obtains substantial improvements over the baselines across all three languages . our model significantly outperforms the other baseline models with a gap of 10 . 2 % in coefficient of locomotion ( coder et al . , 2018 ) on the two aggregated test sets .
in experiment 2 , we show the results of image retrieval on the second variation of our muse dataset . the results are broken down in terms of language embeddings . our model obtains significantly better results than the best previous models across all langauges . in particular , the en - de model significantly outperforms all the other models apart from the case of the french language .
table 3 shows that all models trained on multi30k dataset with different languages perform similarly in terms of recall @ 10 . however , the en - de model significantly outperforms the model trained in franc . this confirms that the translation quality of the model can be improved with an increase in precision .
table 4 shows that the improvement in recall @ 10 on multi30k dataset with different languages with bv embeddings is significant across all three languages . in particular , the improvement is most striking for en - de , the largest of the three languages , which shows that adapting the embeddings helps the model to perform better in both languages .
table 1 shows the performance of our system with a randomly - selected validator for each question . dbidaf outperforms all the other systems with a gap of 10 . 5 % em from the last published results ( hochreiter et al . , 2017 ) on average . droberta outperforms dbert , though it requires significantly less data and time to train , in comparison . as hard coreference problems are rare in dfgn , we do not have significant performance improvement . however , our system improves over other methods with a large margin .
the results are presented in table 2 . precision r - 1 and precision r - l are the best measures of recall while recall r - 2 is the best measure of precision . the models using m1 - latent features improve recall and precision , but do not improve precision . they show that the transfer learning method can reduce recall when using only one type of clustering feature , namely , when using all the features available in the training data , the effect is less pronounced for m1 . moreover , the precision drop when using multi - factor clustering reduces precision .
we show that the shallowness of the bi - lstm helps it achieve better precision in mape and mae . the model achieves state - of - the - art results in both mae and mape . the results are slightly worse than the baseline for mape , but better than the shallow baseline for mae , indicating that the superficiality of the embeddings helps the model to improve its performance in these high - level tasks . we observe that the model further improves its mape performance by 1 . 67 points in the standard task formulation .
the results are presented in table 5 . we observe that for news , the model t − 1 scores are relatively consistent while for imdb and snli , it is significantly worse . the results of bertsda subtraction remove some fraction of noise , however , remaining speech patterns are distorted as well .
table 3 shows the test set f1 on the four domains . in general , we confirm what klinger et al . ( 2018 ) report : the best model is bertbase , with an accuracy of 91 . 44 % on average . ulmfit also achieves high accuracy ( 86 . 44 % ) , but it does not have the best test set . yelp p . i . s . outperforms all the other models with a gap of 10 . 3 % in accuracy . though yelp f . 1 has the best overall score , it is slightly worse than the other two domains in terms of error rate .
fine - tuning the bert - large model ( bert - l ) , we report test error rate ( % ) . for snli , we report accuracy ( % ) . mt - dnn fine - tunes bert with multi - task learning . the resulting improvements in accuracy are slim but noticeable ( t - test , p < 0 . 5 ) with an absolute improvement of 0 . 95 % over the baseline bert model .
we also evaluated the models in terms of perplexity . in table 2 , we report the results of automatic evaluation on the high and low perplexity scores . our model obtains the best performance on the perplexity score . it closely matches the performance of transdg with only 0 . 5 % absolute difference . on the low perplexities score , we get a performance improvement of 2 . 36 points over the previous state of the art .
table 3 shows the system ' s performance on the entity evaluation . our model obtains the best overall score in the medium category . it closely matches the performance of the best baseline model , seq2seq , with only 0 . 7 % absolute difference . in the low - ov category , our transdg model achieves the best score with 1 . 232 % .
table 4 shows the bleu scores of all models trained on the hidden test set of seq2seq . our model obtains the best performance in the automatic evaluation set .
we also evaluated the models in terms of human evaluation . the results are summarized in table 5 . our model obtains the best performance in the field in all aspects ( fluency , relevance and correctness ) . the difference is most prevalent in relation to semantic relation extraction , where our ccm model ( 2 . 08 % ) is nearly 5 % better than all the other models apart from the case of copynet .
in table 7 , we report the ablation results of our transdg model . our model obtains the best performance in the multi - factor task with a bleu - 1 score of 43 . 53 . on the test set , we also observe that the model performs well in terms of entity score as well as perplexity .
table 3 presents the results of our final model on the opinion target extraction task . our method outperforms all the previous methods on average . it achieves 8 . 42 % improvement over the previous state of the art on average compared to han and yan ( 2018 ) .
the performances of the beam search for the reference is shown in table 1 . we discover that the look - ahead improves the model from the greedy search method — in fact , it improves it by 9 . 28 % over the baseline im2latex model .
table 3 shows the average number of words per question and answer , and the average length of n - gram overlap between passage and question . our dbert model obtains the best performance with an average of 10 . 8 % overlap .
the performances of the lstm model trained on the wmt16 multimodal translation dataset with different la steps . we show the look - ahead module is able to improve the model on the entire testing set . however , either the la module or the beam search method harm the models when the length of the target sentences is longer than 25 words . the results of the best performing la module are shown in table 2 . beam search also hurts the model when the target sentence size is greater than 25 , thereby leading to incorrect computation of the la step .
the results of applying la module to the transformer model trained on the wmt14 dataset are shown in table 3 . we find that the la module slightly improves the original model but harms the performance when the la time step is 5 . 56 seconds . we suggest one of the reasons of these results are caused by the eos problem .
we show the results of integrating auxiliary eos loss into the training state . the model performs better when using the greedy search strategy . we find that the loss function is more useful in that it helps the model to learn more about the greedy task and the hidden paths in the deep layers of the network . it also boosts the performance of the model when using greedy search , since the model is more likely to find items that are already known to the user .
table 1 shows the translation quality evaluation results . our joint self - attention model achieves the best performance with a bleu score of 43 . 7 on the wmt ’ 14 en - de and en - fr test set . it closely matches the performance of the best previous model , vaswani and co . ( 2017 ) .
detailed results of text - line extraction on the diva - hisdb dataset ( see section iv - a ) are shown in table and ii . our proposed method outperforms state - of - the - art results by reducing the error by 80 . 7 % and achieving nearly perfect results .
the results of the experiments are shown in table and ii . our proposed text - line extraction method is superior to state - of - the - art even if both methods run on the same perfect input . we show that it takes advantage of the pre - processing step taken to obtain the ground truth of the semantic segmentation at pixel - level , and thereby does not need to rely on superficial measures such as polygons or word embeddings . additionally , the improvement is much larger when we use the wavelength information as input , which gives a significant boost in performance .
table 1 shows the results for english . as a baseline , we also consider vqa questions on flickr30k . for english , we use referit . referit embeddings allow the training of sentences with a minimum of 80 % recall on a single dataset . since the size and type of dataset is small , this helps to more precisely detect errors . our approach verifies the effectiveness of our approach . we show the results in table 1 . with respect to image captioning , we show that our approach significantly boosts recall with a boost of 10 % on mscoco .
we show the results for the task on flickr30k . for word2vec embeddings , we see that average recall is 72 . 2 % , average accuracy is 70 . 2 % and average precision is 53 . 9 % . referit has the best performance , confirming the effectiveness of our model . we also observe that referit helps the model to improve recall and accuracy with a drop of 10 % in accuracy .
in table 4 we show the mean recall and accuracy of bleu - 4 models trained on single task training data . grovle ( w / o multi - task pretraining ) with ft as en ( 2018 ) coder improves upon the baseline model by 3 . 8 points in mean recall , but does not improve on accuracy by much . moreover , the accuracy drop of more than 2 points over pretraining indicates that the model is already well - equipped to perform this task well .
we show the results of multi - task pretraining on the training data in table 4 . the results show that the image decoding task is easier when the target task is pre - trained , meaning that the accuracy obtained during the pretraining can be further improved with the addition of target task training . moreover , the accuracy drop between pretraining and training data indicates that the model can further improve its task performance with the help of additional training data .
table 4 shows that the adversarial effect on our model is minimal , however it is perceptible when we switch from the original embeddings to the new ones . we observe that the model performs similarly under both em and f1 measures , with the exception of the case of bidaf .
the results are presented in table 2 . we observe that seq2seq achieves extremely high precision in the distinct - 1 and diff score ( differences are statistically significant with t - test , p < 0 . 05 ) on account of the high overlap between semantic and syntactic similarity . moreover , the diff score is significantly lower than the average for any other method , indicating that the semantic information injected into the model by the additional cost term is significant enough to result in a significant drop in performance . in our model , we concatenate the results of different approaches into a single score for each variant . rather than concatenating them , we can apply the best performing method across the multiple domains to compute the final score . this approach , named speaker - f , obtains significantly better results than the original seqseq model . it also outperforms the model with a large variation in difficulty level .
table 1 shows the evaluation results . our approach dbert outperforms all the base lines with a gap of 10 . 6 em points from the last published results . the difference is most prevalent in the epm category , which shows that the performance gain comes from a better model design . epm metrics are relatively consistent across all datasets , with the exception of dnq .
corpus - level bleu scores on the validation sets for the same model architecture trained on different data . we report both the official score ( from our second submission ) and the result of re - scoring after replacing these 10 files with the ones from our first submission . the results are summarized in table 4 . our model significantly outperforms the previous state of the art on both validation sets .
table 6 shows the evaluation results , as counts over the simple sentences predicted by each model for a random sample of 50 inputs from websplit 1 . 0 validation set . our model obtains 94 % accuracy on completing the tasks .
table 5 shows the results on the websplit v1 . 0 test set when varying the training data while holding the sentence - level bleu fixed . our model achieves 62 . 4 % improvement over the previous best model by aharoni : 2018 , which used the full websplit training set , whereas we downampled it . we further improve with a 1 . 8 % boost in sbleu .
table 2 shows the quality results for local embeddings . our model outperforms all the other methods with a gap of . 08 points in the ma and mc metrics . for example , refs has the best performance with an average of . 78 .
table 3 shows the quality results for sm . our embdi embeddings outperform all the other methods with a gap of . 75 points from the last published results .
table 4 shows the f - measure results for er . our supervised model outperforms the unsupervised baseline significantly in all metrics .
table 1 shows the evaluation results . our approach outperforms all the base lines with a gap of 10 . 5 em points from the last published results ( hochreiter et al . , 2017 ) on the training and evaluation set . the results reconfirm that the ability to select compact regions induces the model to perform better in the task at hand .
the results of the automatic evaluation procedure on a random sample of 1000 sentences are shown in table 4 . our system outperforms the best previous methods with a large margin .
the evaluation results reported in table 6 show that our minwikisplit embeddings are comparable in grammatical and syntactic quality to the best state - of - the - art wikipedias ( hochreiter and schmidhuber , 1997 ) . however , our summaries are slightly better than the others on two of the four metrics ( g & s ) by a margin of 1 . 36 points .
our dblp dataset shows that around 80 , 000 tweets in total are potentially offensive , and 20 , 000 are hate speech tweets . golbeck2017 shows that 35 , 000 tweets are abusive , constituting 25 % of the total tweets . the fountadclbsvsk18 dataset shows a similar imbalance , with 25 % tweets classified as offensive and 20 % as hate speech . we show that the majority of tweets in our dataset are not abusive , however those that are are are abusive are classified as harassment . our dataset contains 39 , 174 tweets , of which 13 , 000 are abusive .
the results are shown in table 1 . pointwise and hinge hinge models achieve higher precision @ k9 than pointwise pointwise models , indicating that the reliance on thresholding induces the model to rely less on recall . further , the hinge thresholding reduces recall , but does not improve precision . we find that the alternative approaches that aim to improve recall , namely , pointwise - pointwise ranknet and hnm , are less effective than the pairwise hinge model . to test the effectiveness of thresholding , we compare our proposed hinge - based pairwise ranking system against other approaches , such as ukp - athene [ athene et al . , 2016 ] and ucl - ucl [ ucl ] . in this easier setup , we pairwise rank - based ranking systems with different thresholding thresholds to maintain precision . rather than relying on the thresholding alone , we can apply the hnm feature - values across the multiple thresholding scores to train the model . this improves precision and recall ,
the results are shown in table 1 . we observe that the most representative aspect of the model is the degree to which the macro - f1 model can rely on superficial cues . directness is relatively high across all three metrics , and in particular , it is the most important aspect in macro - fs1 . the average number of frames per second for each macro - f1 model is reported in the table , followed by the average of the average ar and the average precision scores . we find that the directness metric is particularly important in the micro - f2 setting , as it helps to maintain high precision in low - supervision settings . this confirms that superficial cues alone do not translate well into effective models . the relative lower precision indicates that there is a need to design more sophisticated models to improve the interpretability of these features . we noticed that the stsl model significantly outperforms the other models in both macro - and micro - level metrics .
we also evaluated the models in terms of macro - f1 and macro - fr1 . the results are shown in table 1 . macro - fs1 average and average number of frames per tweet is reported in parentheses . micro - f2 average is the average of the average frames of tweet in the standard en and fr tweet embeddings , while stml average is computed on the average tweet of the tweet in question . we observe that for both macro - and micro - f1 datasets , our approach significantly outperforms the competition on average . this confirms the effectiveness of our model .
table 1 compares the performance of our multilingual bert model with the baseline on the french and japanese squad benchmark . in both cases , our model obtains the best performance . the difference in f1 and em scores between the two sets indicates that our model can easily distinguish between the true location and the ground truth location of the answer .
table 2 shows the performance of our bert model in multilingual mode on the four datasets where it occurs . our model obtains the best exact match , for each language , among the five datasets where they occur . the difference in f1 score between the baselines is minimal , however we see significant difference in em score due to different usage of the word embeddings in the different languages .
we further compare our model with previous models trained on the ukp - athene dataset . the results are shown in table 2 . pointwise and hnm models perform comparably , but with a gap of 10 . 59 points in fever score from the last published results ( oucl ) . with respect to label accuracy , the smaller difference between ucl and unc indicates that ucl models are more likely to rely on superficial cues . further , our model shows that pointwise cues alone do not improve classification performance , but help the model to improve its generalization ability . we notice that the hnm - trained models benefit from a reduction in false positive rate due to fewer training examples .
table 3 shows the doc - level bleu scores on the dgt valid and test sets of our submitted models in all tracks . our model achieved a comprehensive 60 . 2 % improvement over the best previous state - of - the - art model on all three tracks .
table 6 compares our model against the state - of - the - art on rotowire - test . our model obtains a 22 . 2 bleu improvement over the previous state of the art on the standard nlg test set .
we show the ablation results in table 7 . our model achieves 21 . 8 % bleu improvement over the strong baselines across all three subsets . standard deviation ranges between 0 . 1 and 0 . 4 .
in table 3 , we report f1 scores on the development set for low - resource training setups ( none , tiny 5k or small 10k labeled danish sentences ) . transfer via multilingual embeddings from medium ( 3 . 2k sentences , 51k tokens ) or large english source data ( 14k sentences / 203k tokens ) . regularization fine - tuning gives a 0 . 9 / 3 . 4 f1 gain over the best baseline . we additionally fine - tune the model for additional training instances when the required resource increases . this improves the f1 score by 1 . 18 points over the baseline .
table 4 shows that bilstm outperforms dkie in terms of f1 score for danish ner . the difference is most prevalent in loc , but polyglot also contributes significantly to the improvement . in fact , for all but one of the tests , the difference between loc and loc is greater than that in per . this indicates that the translation quality of the polyglot model can be improved with a reasonable selection of the lexical resource from which the model was trained .
we also evaluated the models in terms of f1 scores . our proposed approach outperforms all the base lines with a gap of 10 . 5 points in the kp20k f1 score from the last published results . we benchmark against the following baselines : inspec , semeval and nus . the results are presented in table 5 . the first set of results in the table shows that our proposed approach significantly outperforms the previous state - of - the - art models in all metrics . we observe that the new approach catseqd significantly improves over the tried and tested baseline on both datasets . the second set shows that it is comparable with the best previous approach on two of the four datasets .
we present the results of the best models in the absence of mae . it can be seen that the models that perform best in the absent mae are the ones that receive the best mae in the present setup . the best results are obtained by our model , catseqd , which obtains 3 . 087 % improvement over the best baseline on both rf1 and average mae scores . however , our model is slightly worse than the other two baselines in terms of rf1 score . we observed that it is better in the sparsely populated settings when the model receives the present mae as well as the present average .
we show the results of an ablation study on the kp20k dataset in table 5 . our approach uses the best adaptive rf1 reward function . the results show that our approach works well in the low - supervision settings . the f1 scores of the absent and present models are relatively the same , however , in the more realistic settings , our f1 @ 5 score drops significantly .
table 1 shows the results for the present and absent modes . our model obtains the best results with an f1 @ @ score of 0 . 382 on the old and new test sets . the difference is most prevalent in the absent mode , where our rf1 score drops significantly . on the other hand , in the present case , our model improves by 2 . 3 points over the previous best state - of - the - art model .
we noticed that ar models with diverse vocabulary outperform nonar models when trained on the standalone stopword dataset in terms of bleu score . moreover , the difference is less pronounced for distinct - 1 and distinct - 2 vocabulary , indicating that the diversity of the vocabulary helps the model to distinguish between lexically similar and non - similar alternatives . with respect to branching sentences , nonar model outperforms ar model with a large margin . ar + mmi model with different vocabulary helps interpret both the standalone and the variant - specific stopword data .
the results are shown in table 4 . ar and mmi achieve remarkably similar results on the coherence and content richness metrics . the difference in ar / mmi score between human and nonar is minimal , but substantial with respect to agr and coherence , showing that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . content richness is relatively consistent across all metrics , with ar + mmi achieving gains of 2 . 8 points on average over nonar . however , the difference is less pronounced for human , we notice that the ar model that relies on multi - domain embeddings is more likely to outperform the nonar model when using the coherence metric . this corroborates our intuition that incorporating the diverse features of the training data helps the model to improve its interpretability . we observe that the ability to combine ar and diverse features improves the interpretability of the model . moreover , the model achieves competitive or better results when using ar - mmi with nonar features .
table 4 shows the performances of our nonar + mmi methods compared to nat and lee et al . ( 2018 ) . the improvement over nat is slim but significant ( + 1 . 48 % ) on wmt14 en → de and wmt16 ro → en . ( table 4 shows that our approach exceeds the strong baselines of nat by nearly 3 points in the accuracy metric .
we report the results on the test set of the german compounds dataset in table 3 . transweight achieved the best performance with a dropout rate of 50 . 21 % on the standard metric tn2 . this indicates that the weighting variations use t = 100 transformations , word representations with n = 200 dimensions and the drop out rate that was observed to work best on the dev dataset ( see appendix b for details ) . results on the sample dataset in german are summarized in tables 3 and 4 .
the greek word analogy test set contains 39 , 000 questions divided into 9 categories . we show the results in table 1 . for english , the average number of tokens for each word is 5 . for both cos - d and n - grammatical word analogy questions , the system performs well . morph - based word analogy tests set up in the greek language outperform the competition on all metrics with a gap of 10 . 5 % on cos - d .
the results are shown in table 1 . ent - dep0 improves upon ent - only by 3 . 8 points in bilstm fine - tuning . it achieves state - of - the - art results , outperforming previous models by 10 . 3 points in f1 score . ent - sent , on the other hand , improves by 2 . 6 points over ent - dym .
table 2 shows the performance of our bertbase model on the five datasets with different epochs . our model achieves the best results with an average map of 0 . 942 on the wikiqa map and 0 . 909 on the yahooqas map . the sota results are from madabushi et al . ( 2018 ) ( trecqa ) , sha et al . , 2016 ) . the semantic threshold for semevalcqa is set at 3 . to test the effectiveness of our model , we set the epochs as 3 , 5 and 10 to mimic the realistic scenario of 3 - dimensional training data . we observe that our model outperforms all the other baseline models with a gap of 10 . 5 % on average .
the results are shown in table 1 . ent - dep0 improves upon ent - only by 3 . 8 points in bilstm fine - tuning . it achieves state - of - the - art results , outperforming both ent - sent and ent - dym on average .
the results are shown in table 2 . we report the total number of times for each of the three types of tokens in question and the number of instances for each tuple with semantic annotations . the results show that the semantic annotations help the model to distinguish between the true response and negative polarity . the positive polarity polarity contributes significantly to the overall performance of the model , improving from a 0 . 37 baseline to 0 . 64 baseline in a single shot .
table 1 summarizes the results of the second study . as we expect , the number of times a musician associated with a location is significant , with 43 , 842 instances ( 62 . 4 % ) “ associated ” with a musical artist . more than half of these ( 37 , 524 ) are in the presence of musicians whose name the associatedmusicalartist ( 6 , 273 ) is known to be associated with the musician . the other half ( 3 , 600 ) are at the location of the musician ( 1 , 308 ) .
we show the results for the frameset 1 . 5 and 1 . 7 tests set in table 2 . the results are broken down in terms of ambiguous and fullwiki embeddings . for the framenet test set , we see that our model obtains the best results with an ambiguous f1 score of 73 . 08 . on the other hand , our framenet model achieves the best overall score of 78 . 63 .
the results of bertbase and bertlarge are shown in table 3 . our model outperforms semevalcqa in test set of five datasets . the number of training epochs is 3 . 3 times the size of the original dataset , so the difference in performance between small and large datasets is less pronounced . nevertheless , our map metric significantly improves over the other baseline methods .
the results of random and manual search are shown in table 6 . in general terms , the results are similar , with the exception of the case of automatic search . when automatic search is used in combination with semeval , it achieves the best results ( 83 . 43 % ) on the standard i2b2 dataset . however , in the more realistic case of manual search , it gets worse performance than the automatic search method . with respect to semantic features , automatic search and random search are comparable , but do not outperform each other significantly . we find that the manual search method is more useful when using a single classifier instead of multiple classifiers . it improves the generalization of the task .
in the en - de news commentary , we compare the results of different classifiers trained on the same preprocessdataset . the results are presented in table 2 . we observe that the original classifier and the original embeddings are comparable in performance , but that the difference is most prevalent in the semantic layers . this indicates that the semantic information injected into the model by the additional cost term is significant enough to result in a significant performance drop . the results of entity blinding are slightly less clear than in the original , but still indicate that it is possible to detect the presence of hidden entities when the masking is applied . punct and digit features are very similar : they both result in significantly less accurate predictions ( i . e . , about 80 . 5 % on average compared to original , i . e . 69 . 5 % ) , but in the more realistic second case ( 76 . 5 % ) they show a slight improvement .
we compare our proposed bert - tokens with the previous state of the art models on the multi - news dataset . in general terms , we see that bert models perform better on both datasets with different classifiers trained on the same data . the difference is most prevalent in the detector classifier , where bert ' s interpretability improves significantly compared to crcnn . however , for the part - of - the - speech dataset , the difference is less pronounced ( p = 0 . 08 ) .
we also tokenized the bleu test results on iwslt 2017 de → en , ja → en and wmt 2014 en → de , respectively . α - entmax is significantly better than softmax in both cases , meaning that it requires significantly less data to train and interpret . we observe that the translation performance reach its best when trained and tested on the large scale dataset of kftt , the largest of the four datasets , in which our model obtains the best results .
as shown in table 6 , the models trained with cc and arxiv embeddings perform similarly on both subtasks . the difference in precision between subtasks is minimal , however we see significant difference in macro - f1 score due to different number of frames in the training set and the fact that the subtasks are small .
table 3 presents the results on unc testa , testb and referit test set . our system outperforms all the other systems with a gap of 10 . 63 points from the last published results . we observe that our approach applies the best adaptive decoding scheme , reaper , to both unc test set and referit test data . moreover , it improves upon the strong lemma baseline by 9 . 36 points in unc testb score . this corroborates our intuition that adaptive decoding strategies can improve interpretability without a drop in performance .
we show the results of an ablation study of our cross - modal attention method for the unc val set . our approach obtains 50 . 12 % improvement over the baseline iou method .
we compare our proposed method with 2 baseline models - cmsa , convlstm and rrn - cnn . in general terms , the results are presented in table 2 . our proposed method outperforms the pre - trained models on average . it achieves competitive or better results than the previous best state - of - the - art models on three out of the four scenarios . the difference is most prevalent in the second scenario , when we consider the accuracy @ 0 . 7 compared to the prec @ 0 . 5 baseline . this suggests that pre - training of the model can help the model to improve its performance in the realistic scenario when applying our proposed algorithm . moreover , our model further improves performance with the help of ppm feature - values . it outperforms cmsa - s with a gap of 10 . 95 points from the last published results ( 59 . 59 % vs . 48 . 53 % ) on the covec model .
the results of the evaluation metrics are shown in table 2 . first metric is the most important one in our system , followed by disp . perf . the system performs well in terms of all metrics with a gap of 0 . 08 points from the last published results .
table 3 presents the performance of our method in the setting of da classification accuracy . our method improves upon the previous state of the art on three out of the four datasets . the largest gains are on the mrda dataset , where our method improves by 3 . 8 points over the previous best state - of - the - art model .
inspec outperforms random in terms of f1 @ 5 while semeval achieves the best f10 @ 10 . the inspec model outperforms the random baseline on three out of the four datasets . the difference is most prevalent in the nus dataset ( 7 . 8 % ) where inspec is the only model that performs better than random on all datasets but is slightly worse than semeval . across all datasets , random is slightly better than inspec except for the krapivin dataset ( 3 . 4 % ) on average . we notice that the difference between inspec / semeval and random is less pronounced for nus , but still suggests some reliance on superficial cues .
table 3 shows the f @ k9 scores of the basernn , bigrnn and transformer models . our model achieves the best results with a f @ 5 score of 0 . 358 on the five - sentence test set . the transformer is slightly better than the basernn in terms of f @ 10 .
the results are shown in table 3 . our elmo - lstm - crf model outperforms all the other methods apart from the case of the prescription prescription dataset . detection accuracy is relatively consistent across all three domains , with the exception of the chemical – disease category , where it is slightly worse than the other two . also , we notice a drop in performance between the original lstm model ( hochreiter et al . 2008 ) and the current state - of - the - art model ( rahm - based elmo model , in which it relies on synthetic neural models trained on unlabelled word embeddings . when trained on supervised and unsupervised datasets , the effectiveness of our model drops significantly .
the results are presented in table 1 . diagnosis detection improvement is marked by an f1 score improvement of 1 . 7 points over the previous state of the art . prescription reasons support also improves . however , the biggest improvement is seen in the areas of drug – disease relations . this improvement is more than 9 % in the chemical – disease relations category . this indicates that the support provided by the label can help the system interpret the data more interpretably . further , the improvement is larger for drug - disorder relations . support for both groups is beneficial , however it is less significant than for support for both .
corpus statistics and label distributions of the friends and emotionpush datasets are shown in table 2 . in general terms , we observe that the training set has the best performance , with an average of 14 . 25 % utterances per dialogue . however , out - of - domain training data have the highest percentage of negative utterances , so we observe a lower percentage of positive utterances in training .
table 1 compares the performance of our method with the baseline on paraphrase extraction in terms of precision . our method outperforms the fsa baseline by a significant margin .
classification test scores for classifying r vs u in the br , us , and combined br + us dataset . the results are shown in table 5 . our system outperforms the best previous methods with a gap of 10 % in br classification test set .
table 2 shows that the impact of the additional data type on our model is minimal , however we see a noticeable increase in performance for datasets of geth and ethanos . adding the data type " difficulties " and " headers " improves the results for both datasets , difficulties are slightly less common in geth but are beneficial for ethanos , adding headers improves results for geth as well . we see that the number of concatenated tokens in a given hash is the most important factor in model performance , however it is slightly less significant for trie nodes . finally , we see that combining all the data types impacted our model results in a drop of more than 2 % .
table 2 shows the results for the fast sync geth and compact sync ethanos datasets . the results are broken down in terms of data type and number of tokens in the body , as those are the only ones that can be seen in the compact sync dataset . in general terms , we see that the performance of our approach is comparable across all data types , with the exception of the difficult ones . difficulties are rare in our setup , however we do not have significant performance drop .
table 2 shows the results of the best performing models . our transformer - word embeddings outperform the other models in all metrics . in particular , we see that our transformer word embedding model can significantly improve the results in mt02 – 08 bleu and he . when trained on the multi - news dataset , it achieves the best average mt02 score of 3 . 28 on the he dataset . this confirms the effectiveness of our model . we also observe that our model can improve its performance in the more realistic mt02 scenario when trained on a single dataset .
table 3 presents the results of task completion on the task slc and flc datasets . our multi - granularity model outperforms all the other models apart from the case of the sigmoid case when we include the multi - task requirement . it achieves 60 . 41 % on task flc f1 score , which shows the extent to which the granu model can rely on syntactic and semantic information in combination with the task requirement . moreover , it improves upon the performance of the joint model by 3 . 36 % on task flc r .
the results on cqa dev - random - split with cos - e used during training are shown in table 2 . our approach improves the performance by 9 % over the baseline .
the addition of cos - e - open - ended during training dramatically improves performance . our model achieves gains of 10 % over the previous state - of - the - art on cqa v1 . 0 .
table 4 shows the results on cqa dev - random - split using different variants of cos - e for both training and validation . the improved performance on validation set ups indicates that the selection process can be further improved with an increase of accuracy in the training phase . further improving performance by high margins
table 6 shows the results for explanation transfer from cqa to out - of - domain swag and sotry cloze tasks . our bert model significantly outperforms the best previous methods across all three domains .
we report the mean , standard deviation , and ensembled f1 score for citation detection on the fa split and the lqn split in table 1 . the bert model outperforms bert with a gap of 0 . 818 points from the mean of the standard deviation . however , bert ' s puc score improves by 0 . 18 points over the baseline model , which shows that bert has better generalization ability .
we include the number of synsets in the training set as well as the maximum depth of the hierarchy as a metric for scalability in table 1 . the top synset contains the most basic level concepts , and is the only one that can be expanded beyond the basic level . it also contains the maximum number of synnsets , which means that the system can easily distinguish between the true response and negative responses .
we ranked all the features in order of importance , which shows that the gloss - length and word - length features are of high importance . additionally , we ranked the sub - top features as the most important ones for each sub - category .
table 3 shows the accuracy and κ of our model making predictions in a new domain , with or without normalization . the frequency and structure features normalized with the training data have the best performance , so we do not need to normalize . however , we do need to consider the effect of the additional cost term on our model , as this affects the overall performance . our results show that the addition of the frequency features normalizes the results slightly , but does not improve significantly . fruit - based features normalize the predictions , but do not improve the results . we find that the use of the features of musical features improves results , however , not enough to improve significantly over the baseline .
table 1 shows the distribution of the event mentions per pos in all datasets of the eventi corpus . overall event tokens are 18 , 735 , 314 are for training and 4 , 068 are for test . pos statistics are presented in table 1 .
corpus statistics are shown in table 2 . overall , there are 17 , 528 events mentions per class in all datasets ( training , test , and validation set ) and 3 , 798 overall events mentions ( testing , validation , and state ) .
the relaxed evaluation set is presented in table 1 . we show the results for both the standard and relaxed set . in both sets , we see that the adapted embeddings perform better in the more relaxed setting . the difference is most prevalent in the relaxed set , where ilc - itwack performs better than dh - fbk on both the strict and relaxed evaluation sets . relaxed evaluation is more difficult than the strict set , but we find that it is comparable to the task when the relaxed evaluation is considered .
table 3 presents the results for intra - dist and inter - dist dist - 2 data . our model obtains the best results on both intra - and interdist datasets . it closely matches the performance of seqgan with only 0 . 012 bleu r from the last published results ( a ) and 0 . 262 f1 score . on the interdist dataset , it achieves the best score of 0 . 763 . we observe that our model significantly outperforms other methods across all three datasets in both language translations . the results reconfirm that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement .
the results reported in table 5 show that the models trained on the dailydialog dataset are comparable in difficulty to the best previous models . dialogwae - gmp model achieved the best overall performance with a percentage of 29 . 6 % on the diversity metric .
table 2 compares the bleu scores of our proposed methods and the baselines . our rl look - ahead model achieves the highest score for all three aspects : empathy , relevance , and fluency . additionally , multiseq model achieves a significant improvement in fluency score as well . however , the difference in accuracy between our proposed method and the existing baselines is less pronounced for empathy , relevance and fluency , which indicates that the accuracy obtained in our proposed approach can be further improved with a reduction of bias .
we report the best performance observed in 5 runs on the development sets of both sparc and cosql , since their test sets are not public . our model improves significantly over the baselines with p < 0 . 005 . we also conduct wilcoxon signed - rank tests to show that our bert model significantly outperforms the previous best state - of - the - art models across all three domains .
our proposed spon improves upon the results of bless with a boost in accuracy . it achieves state - of - the - art results , outperforming traditional approaches like bless , with an average precision drop of . 18 points .
table 4 shows the performance of our proposed spon model . relu and relu both receive positive and negative activation signals , making them more useful for hypernym detection . relu also benefits from a residual connection , indicating that the activation layer can take negative values as well . we find that relu is comparable to tanh in that it requires a negative activation signal to take positive values . it achieves a comparable performance with relu only when using relu with a positive activation signal .
as shown in table 5 , spon significantly improves the performance in the unsupervised hypernym detection task for bless dataset . with 13 , 089 test instances , the improvement in average precision values obtained by spon as compared against smoothed box model is statistically significant with two - tailed p value equals 0 . 00116 .
limited length rouge recall results on the nyt50 test set . we show in table 2 that our model obtains a limited recall improvement of 2 . 53 % over the best previous state - of - the - art model .
table 7 shows the results of the domain specific hypernym discovery task on the music map map and p @ 5 metrics . our system crim is the best on all three domains . the medical map metric is significantly better than spon , confirming the effectiveness of our model . for the scientific datasets , medical and physics , our system spon is slightly worse than crim but still superior on the domains specific datasets . we observe that the medical dataset is more than 5x larger than the other two domains , indicating that medical datasets are more specialized .
table 1 shows the average embedding similarity scores between the output and the target output in terms of real target output list . the pre - trained greedy embeddings outperform the rl greedy ones , indicating that the training set can be further improved with an increase in the training size . rl beamsearch also exhibits a drop in similarity scores as measured by the real target list size .
we also evaluated the models in terms of roc metric , where sopa outperforms all the other models except bilstm when trained on the amazon dataset . sopa achieves 60 . 8 % improvement over the baseline on average over both the easy and hard subsets . the difference is most prevalent in the sopa dataset , which shows that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . moreover , for the hard subset , sopa improves 8 . 6 % over ( hochreiter et al . , 2018 ) on average .
the results reported in table 4 show that our proposed method significantly improves the interpretability by increasing the average kappa score of the annotators and the observed agreement ( κ ) by 0 . 67 points .
experimental results are shown in table 3 . the best result for wikipedia was achieved by our joint model , which outperforms all the other models apart from the case of the inception fixed case . our joint model beat all the baseline models except doc2vec by 3 . 79 % in the percentage of responses scored on wikipedia , a result which shows the extent to which the translation quality of embeddings can be improved with a reasonable selection of the lexical resource from which the submission was derived . inceptionfixed also outperforms other models in terms of percentage of response , but this time it was due to larger variation in the training data set .
confusion matrix of the joint model on wikipedia . it is clear from table 4 that there is a significant imbalance in the quality of the start and finish positions , which indicates that the proposed method can rely on incorrect predictions . however , this imbalance is less pronounced for the start position , indicating that the quality prediction ability of our method can be further improved with a reasonable selection of the correct quality classes .
table 1 shows the classification performance of these large - scale text classification data sets . as a baseline , we also include the training and test set of each class for each language for english and chinese . for english , the training set is 120k documents , while the test set is 60k documents . the total number of classes for each task is 5k .
the results of models trained on the sogou dataset are shown in table 4 . our model shows marked improvement over the previous state of the art on all metrics . it closely matches the performance of sememnn - ct with only 0 . 5 % absolute difference . in addition , the model performs better on three out of the four metrics when trained on a larger data set . the difference is most prevalent in ag dataset , which shows that the semantic information injected into the model by the additional cost term is significant enough to result in a measurable improvement . it should also be noted that our model relies on word embeddings pretrained on unsupervised learner - generated sentences . these pretrained models perform similarly on both semantic and syntactic information .
in table ii , we report the average bleu scores of all the models trained on the multi - news dataset . the results show that all the pre - trained models perform well , but the smaller size and type of prefixing model seem to have a significant impact on performance , leading to a drop of more than 2 % .
we show the results for english . the results are broken down in table 1 . our pre - trained model improves upon the strong lemma baseline by 3 . 4 points in ai2 score . hosseini , et al . ( 2017 ) and kushman , et . al . ( 2016 ) obtain significantly better results on both ai2 and mawps . in particular , the improvement is much larger under the cc metric , which shows the advantage of finetuning word embeddings during training . the results of pre - training are slightly less clear than the results of training , but still show a considerable improvement over the baseline . we notice that the transformation step that pre - taught our model requires is crucial for the improvement of our model , as it helps the model to learn the task to a high degree .
table 1 : joint goal accuracy on the evaluation dataset of woz 2 . 0 corpus . we show that the slot - dependent sumbt model achieves a joint goal accuracy of 0 . 891 / 0 . 952 ( t - test , p < 0 . 010 ) better than the bert + rnn model ( 0 . 892 / 0 . 011 ) and bert - rnn + ontology ( baseline 2 ) . slot - independent sumbt , on the other hand , achieves an accuracy drop of 1 . 3 % ( p - value 0 . 013 ) compared to our proposed method . ( table 1 )
table 2 shows the joint goal accuracy on the evaluation dataset of multiwoz corpus . sumbt outperforms the benchmark model by a significant margin . ( joint goal accuracy is 0 . 4240 / 0 . 3557 , p < 0 . 0187 )
table 3 shows the results of fine - tuning our bert model . our hubert model achieved the best results with a gain of 2 . 53 % on the transfer bert metric .
table 3 shows the results of fine - tuning our bert model . as a baseline , we also consider whether the model can transfer the data from one slot to another without losing accuracy . our model achieves the best performance with a gain of 8 . 67 % on average .
table 3 presents the results of the best hubert transfer models . our model outperforms all the other methods with a gap of 10 . 35 points in accuracy .
we show the precision numbers for the primary ccs top - 1 , top - 5 and top - 10 recalls as well as the auroc in table 2 . the results are broken down in terms of recall : we report the average number of tokens in the bag - of - words and the number of times the bag is weighted over concatenated multiple tokens to obtain the correct recall number . for example , unigrams and bigrams alone results in significantly better precision ( p < 0 . 001 ) than any combination of tokens , but do not exceed the upper boundary of the expected range . we observe that combining all features , especially the bigrams , helps the model to improve precision .
the results are shown in table 2 . precision is relatively consistent across all metrics , with bert12 achieving 3 . 9 % improvement on average over the previous state of the art model .
the results are shown in table 2 . our model outperforms the previous state - of - the - art models on all three structure tasks . adabert - sst - 2 outperforms all the other models apart from the case of the random task . it closely matches the performance of the best previous models with only 0 . 4 % absolute difference .
we observe that for sst - 2 , the β = 0 implies that the model can rely on only one type of efficiency loss term , and that this loss term has no effect on performance . for mrpc , the effect is less pronounced , but still significant ( 7 . 8 % ) and we observe that it is important to consider the extent to which the loss term can be used to improve interpretability .
we observe that the knowledge loss terms have a generally positive effect on our model , improving the results across all three domains . the best performance is achieved on the synthetic dataset in table 4 .
we show the sentiment prediction results on cmu - mosi . our model significantly outperforms the current state of the art model across all evaluation metrics . it improves upon the performance of bert and m - bert by 5 . 53 points in the accuracy metric on average . further improving performance by high margins in the mae metric also indicates that our model can significantly improve its performance in the task formulation .
we show the inference times for each sentence as well as the attention span in table 1 . gaussian mask only masking results in significantly faster inference time than rl model . inference times in ms are 2 . 86 times faster than that in rt - time . attention span is faster than sentence length , indicating that the model is more accurate in selecting the relevant features and its output is more interpretable . gaussian masking also reduces the time to encode sentences .
table 3 shows the number of images after which the me score falls below threshold . our model obtains a me score of 0 . 05 for the omniglot dataset , which means that the model can classify most of the images in the dataset without losing accuracy .
the results are shown in table 3 . first , we can see that the automatic metrics increase the success rate for both en - de and en - fr tasks . moreover , the model also improves when trained with soft - att instead of random , indicating that the reliance on the self - attractive key - values leads to a better model performance . the second group of results show that the ability to select the correct salience of the relevant objects leads to an improvement in the accuracy of the model . in particular , the improvement is much larger when the model is trained with the soft - att attention layer instead of the hard - attened key - value layer . when trained with both the automatic and automatic metrics , the accuracy increases consistently , which shows that the model can rely less on superficial cues . we can further see that automatic metrics also improve results . ame model significantly outperforms transformer in both sets .
in the en - de model , the results are slightly worse than the random variation but still superior to the soft - att approach . the results reconfirm that the ability to select compact regions induces the model to rely less on superficial cues . when using the hotflip transformation , the model achieves the best results . it closely matches the performance of the original model with only 0 . 40 % absolute difference . in en - fr , the improvement is much larger . it achieves 82 . 43 % on average compared to the previous state of the art . regularization reduces recall , as the effect of the flipped targets is less pronounced in en - e . g . , when compared to random , but still results in significantly better results . we notice that the automatic thresholding step is crucial for the improvement of the model , as it helps to reduce false positive responses .
in the en - de model , the results are presented in table 2 . the smaller performance gap between random and soft - att modes indicates that the ability to select compact regions induces the model to rely less on superficial cues . moreover , for the french blstm model , automatic metrics results are slightly worse than the others but still superior to random on average . selective attention mechanisms like the hotflip and attract reduce repetition , as measured by the average number of frames in each attention layer , and thereby improving the generalization ability of the model . in addition , the automatic metrics reduce false positive responses , as the improvement is less pronounced for en - fr than for random .
we observe that the degradation of sacrebleu improves as a function of the proportion of bitext data that is noised . the percentage of noised data in our feed - forward model decreases as the performance increases , but remains relatively stable at the level of the original embeddings .
table 4 compares the scores of our approach with previous approaches on the wmt16 enro dataset . it . - 2 bt improves over the bitext model by 3 . 9 points in terms of sacrebleu score , while it . - 3 improves by 2 points over bitext .
table 5 shows the results for the best performing bitext model on wmt15 enfr , with bitext , bt , noisedbt , and taggedbt . bitext improves upon the baseline with gains in accuracy over both the baseline and the competition . it closely matches the performance of the best previous model with only 0 . 5 % absolute difference . in particular , the improvement is much larger under the gold - two - mention case when using the bitext embeddings . this corroborates our intuition that the use of selective attention leads to better interpretability .
we report the attention sink ratio on the first and last token and entropy ( at decoder layer 5 ) for the models in table 3 . for asr , data is treated as if it were bt ( noised and / or tagged , resp . ) , whereas for entropy the natural text is used . the model achieves 7 . 33 % attention sink on average compared to the bitext baseline . we note that the difference is less pronounced for asr0 than for entropy , indicating that the taggedbt tags have a greater impact on decoding performance . in addition , the model exhibits a drop in precision as well .
in table 7 , we compare the results of standard decoding and noisedbt decoding . we observe that the former performs better than the latter under all three metrics . the difference is most prevalent in avg 13 - 17 , the largest of the three datasets , where standard decoding underperforms the noise - free variant by 3 . 3 points . noise - free decoding under - performs both the standard and the noised variant , but does not exceed the upper boundary of the noise - free range , in which again demonstrating the advantage of finetuning word embeddings during training .
table 9 shows that the source - target overlap for both back - translated data with decoding newstest as if it were bitext or bt data . our model decodes have an overlap of 8 . 9 % with respect to src - tgt unigram overlap , which means that the tagged decode has an accuracy drop of 0 . 4 % over the standard decode .
we show the distribution of the sample distribution over the classes in the reuters - 8 dataset in table . it shows that most of the samples in the corpus belong to one of the three classes that are common to both oilfields and grains , namely , crude , grain , and synthetic . the remaining samples are divided into categories that are more specific of the oilfield , such as synthetic , fertiliser and petrochemical .
in the en - de news commentary , we compare our proposed approach against 3 baseline models - tf - msm , mnb , tf - ism and tf - nsp . due to space limitations , we show performance on w2v only when using the best performing model ( tfidfbow ) . in this easier setup , we observed that our proposed model outperforms all the other models apart from the case of the accuracy in which it was trained .
table 2 presents the results on errant . our system outperforms all the other systems with a gap of 10 . 5 conll iterations from the last published results . errant scores significantly outperform other systems as measured by f1 scores . for example , errant achieves a f1 score of 43 . 14 on the conll - 2014 dataset , which means that it performs better than 62 . 5 % of other systems . on the other hand , it is better than 58 . 63 % and 62 . 14 % on the conell - 2014 data set . this gap is mostly due to high overlap between the training and the test set of the two sets , as reported in the previous literature .
we managed to classify 9 , 992 messages in total ( out of the total of 5 , 898 messages in the hackforums corpus ) as messages that were sent to the original poster ( hence , there are no errors in our system ) . the distribution of the messages is presented in table iii . it can be observed that most messages in hackforums are classified into one of three categories : buy , sell or sell .
table 1 shows the results for the second variation of experiment 2 . we show that our proposed method significantly improves recall and accuracy . our proposed method results in significantly better recall with a drop of more than 10 % in precision for recalls .
table 1 compares the results of our method with other pooling methods . our proposed method outperforms all the baselines except syntactic information in semantic similarity task . in syntactic information task , it achieves the best performance with a mean score of 68 . 7 . in semantic similarity , it improves by 3 . 6 points over the previous state of the art . syntactic information is the most difficult task to predict , as it requires semantic information that is not easily found in dictionaries . this is reflected in the average score of our model ( 68 . 7 % ) in the supplementary material . we find that the semantic information is closely related to the task performance in which it is applied , but is inferior in semantic terms . this suggests that there are ways of improving the semantic features without sacrificing accuracy . we consider applying semantic features to improve the results in the future .
we show precision @ k9 for the insuranceqa dataset in table 1 . as a baseline , we also consider whether the model can be fine - tuned in the in - domain setting . we find that the pre - trained bert model can significantly improve its performance on wikipassageqa . its map metric improves from 53 . 2 % to 60 . 7 % over the baseline on bm25 .
we compare our model with previous works on the peyma word embeddings . the results in table 3 compare our beheshti - ner model with other models trained on the hidden test set of hotpotqa in the distractor and fullwiki setting . our model improves upon the previous state - of - the - art on every metric by 3 . 59 points . on the other hand , it does not improve significantly over other works .
the results are presented in table 4 . morphobert outperforms all the other models apart from beheshti - ner - 1 in the in domain and out domain tests . it achieves state - of - the - art results in both in - domain and out - ofdomain test sets . the total number of test data for each team is reported in table 4 . in in domain , the average number of iterations per team is 69 . 2 , out of 81 . 6 total test data . out - ofdomain test data is 82 . 8 , but the difference is less pronounced than in in domain . our model ictrc - nlpgroup shows that it can rely on superficial cues to predict answers based only on test data without understanding the task . its performance in the outdomain test set is much better than the team ' s performance in in - domain .
table 3 shows the scores of all the medical devices that our system interacts with . our system performs well in all three domains . the average score for each medical device term is reported in table 3 . we notice that the transfer learning method is more accurate in the sports rehab domain , as the average score of our system decreases when using a non - contact term .
table 1 shows the results for the optical test set . our approach outperforms all the base lines with a gap of 10 . 5 % on average . we observe that the average number of frames per second for each network model is significantly less than the competition , which indicates that the performance gain comes from a better model design .
table 3 presents the results for target → source / sys . we show that mt consistently outperforms the average of the baseline on all metrics , with a gap of 10 . 6 % on average compared to the previous best state .
in the en - de news commentary section , we compare our approach against previous approaches . we observe that our approach significantly outperforms the previous approaches across all three systems . our avg model achieves 62 . 6 % improvement over the previous state of the art on average compared to the baseline model .
the accuracy ( % ) of the ml models for nlu . according to table 2 , our hmm model obtains 94 . 45 % accuracy on the intent detection task and slot filling task , which shows that it can easily distinguish between the true response and negative responses . additionally , the model shows a slight improvement in slot filling accuracy as well .
the results are shown in table 4 . it can be seen that the gui of was suitable for reading the provided answers , and was able to provide answers to my questions in a reasonable time ( i . e . , in less than a minute ) . although the accuracy was slightly lower than the expected , it was still superior to using a manual typewriter . further , the speed with which the answers could be provided was superior than expected , confirming the effectiveness of the gui . we can also see that the selection of the correct word embeddings for each question was crucial for the success of the task , as it reduced my need to google a specific information twice as often . moreover , the accuracy remained the same across the five groups , confirming that the use of the provided features improved the interpretability of my questions .
table 2 presents the results of unsupervised and supervised ir baselines . we show that the models trained on wikipediapassageqa outperform their supervised counterparts on every metric by a significant margin . for example , tf * idf model achieves a joint score of 32 . 00 / 61 . 71 on average compared to the previous best state - of - the - art model ' s 25 . 2 / 35 . 38 . on the other hand , bm25 is only slightly better than the previous state of the art on five of the 10 categories , but it obtains a slight improvement on the remaining four .
thematic rankers outperform syntactic rankers when trained and tested on a single entity dataset . they show that the ability to select the correct syntactic and thematic tokens induces the generation of better ranking .
induced hierarchies are shown in table 4 . they can be seen as a result of the fact that the shared vocabulary setup puts the agent in a better position to pick out the most interesting ones . when the relevant topic is selected , the agent can apply the topical stress test on the relevant dataset . this helps the model to more precisely detect instances of conflict resolution .
the results of cross - lingual evaluation are shown in table 5 . our system outperforms the best previous approaches with a large margin . the difference is most prevalent in the ub category , which shows that the translation quality of the model can be significantly impacted by the language translation .
we compare our approach against 2 baseline models - egfr , ndcg and kras . as the results of applying our multi - factor metric improves the results for both models , we observe that the accuracy drops significantly for the egfr model . however , the improvement is less pronounced for kras , indicating that precision is relatively higher for the kras model .
we report the macro - f1 scores of our bilstm model in schulz et al . ( 2019a ) for each of the epistemic activities . it achieves a performance improvement over the best previous state - of - the - art model , flair ( hochreiter et al . , 2017 ) , by 1 . 71 points .
for brevity we also compare against the baseline bert model without context tags . in this easier setup , we observe that bert logits with tagged spans are comparable to the best previous state - of - the - art models on both development and reference set . however , the difference is much narrower with respect to features : on the development set , our model achieves 0 . 71 improvement on average compared to the previous state of the art . this indicates that the semantic information injected into the model by the tagged spans helps the model to further improve its performance in the development setting .
the results for the second variation of experiment 2 where we conditioned on the word " democracy " . we see similar results for development and test set . the results are broken down in table 2 . name calling , labeling and negation are the most difficult aspects of the task for our model to solve . they yield significantly lower f scores than in experiment 1 . further , we see that our approach relies on superficial cues , such as the fact that the flag - waving technique is flag - based , and that it can be seen as appealing to authority . it is clear from table 2 that this approach fails to account for the many ways in which flag waving can contribute to the wrong interpretation of a statement .
we also ranked the entity scores for kras validation and pik3ca testing in table 4 . the best performance on map metric is achieved by our model , ndcg @ 10 , which significantly boosts the precision of our model with respect to rocchio scores .
table 1 compares the performance of our linspector web with previous approaches . our model outperforms all the other approaches with a large margin . we have 28 tasks in total , divided into 9 categories and divided into task - and task - type - based tasks . wst and dt tasks are similar in that they consist of word similarity tasks , but are narrower in scope than pt tasks . nayak et al . ( 2014a ) and dyer ( 2014b ) additionally define a new task type , which we use here for our evaluation . the number of supported languages is limited , but we have 10 tasks per language , which is comparable to the number of tasks in the previous evaluation set . we also include the number and type of tasks for each language , as those consist of multiple tasks , and are divided into 10 tasks divided into tasks according to the type of similarity task . our approach verifies the effectiveness of our model .
we show the precision numbers for each variation in table 6 . for example , we report h1047r for the glioblastoma and liposarcoma variants and t790m for adenocarcin . in both cases , the results show that the drug used to treat the cancer has significantly better precision .
the results of the difficulty prediction approaches are shown in table 1 . as a baseline , we also consider the results of original and new data . for the new data , we use the best performing model , bilstm . it shows that mlp can significantly outperform ( p < . 01 ) all the other approaches that do not use mlp ,
table 2 shows the results for both strategies for both corpora with randomly sampled target difficulties for each corpora . as expected , the smaller size of the target makes it easier for the sel strategies to pick out targets with lower difficulty .
table 3 shows the mean error rates for different text types . as hard as the standard def is , it deviates significantly from the easy ( dec ) and hard ( inc ) sel sizes . results marked with ∗ deviate significantly from def
we show that without lexicon , precision onaccamb is relatively consistent , but with lexicon is significantly worse than onacc . with lexicon in the lexica , precision is only consistent with that of the baseline , meaning that precision is relatively less important than lexica in predicting errors . we observe that the semantic information injected into the model by the lexicon helps the model to improve its performance .
our model outperforms all the base lines with a gap of 10 . 2 bleu points from the last published results ( ouge ) , while surpassing our strong lemma by 3 points . the results reconfirm that the lemma generated by our model can be further improved with a reasonable selection of the lexical features that the model considers .
table 1 shows the percentage of incorrect summaries produced by recent summarization systems on the cnn - dm test set , evaluated on a subset of 100 summaries . the most representative system is pgc see17 , which makes use of word embeddings and multi - task learning . it outsperformed all the other systems except fas chen18 by a large margin .
table 5 shows that infersent outperforms the original model when trained and tested on the multi - news dataset . the results are slightly encouraging for future work in this direction as we see that infersent sentiment can significantly improve interpretability without a drop in performance . it should also be noted that the percentage of responses scored for each label is slightly different depending on the underlying data type . for example , randomized val is 50 . 1 % and sse is 45 . 8 % , but for test , it is only 39 . 7 % . multi - news datasets generally outperform esim ,
in french , the contraction rules are laid out in table 2 . as the table shows , the majority of the time , the presence of the correct pronoun helps the voc to reproduce the correct answer .
table 5 provides the setup information for the disjoint dbless , wbless and bibless setups . it shows that the pre - trained system is well - equipped to handle multiple tasks at once . the sg scores for both datasets are relatively consistent , with the boost being more pronounced on the sg dataset . with the help of word embeddings , we can setup the system to work on a single dataset with a minimum of 80 % accuracy . using the multi - task setup gives a significant boost in performance on both datasets with a gap of 10 . 5 % in the sg scores . when using the shared vocabulary setup , the difference between sg and sg scores drops significantly ( p < . 005 ) .
table 2 shows the performance of our model in cross - lingual transfer . results are shown for both postle models ( dffn and adv ) , two target languages ( spanish and french ) and three methods for inducing bilingual vector spaces : ar [ artetxe , 2018acl ] , co [ conneau , 2018iclr ] , and sm [ smith : 2017iclr ] .
the performance of our model compared to recent state of the art coreference models on the conll test set is presented in table 4 . we compare our model with the best performing model , peng et al . ( 2012 ) . the results show that our model significantly outperforms the previous state - of - the - art model on all metrics by a significant margin .
the quality of the regression model ’ s predictions on the test set is shown in table 1 . it follows the r2 scores of experiment 1 and experiment 2 , where random regression caused the model to make incorrect predictions . experiment 3 , in comparison , shows that random regression actually improves the model ' s performance .
the performance of our model compared to the pos - cnn model on the ccat10 , blogs50 and pos - han datasets is presented in table iii . our model obtains substantial gains in accuracy over the baselines across all three syntactic representations . it closely matches the performance of the best previous models with only 0 . 5 % absolute difference .
we show the performance of our model in the standard task formulation on the ccat10 , ccat50 and blogs50 test sets . syntactic and lexical - han models perform comparably to each other , but do not have the advantage of using automatic metrics . our combined model ( syntactic + lexical ) shows a considerable drop in performance compared to the previous state of the art . we observe that the accuracy drop is most prevalent in the syntactic category , which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the word embedding pattern was derived . in our particular case , this helps to more precisely detect instances of grammatical gender - neutral slurs .
table v shows the performance of our method with respect to the different fusion approaches . our approach improves the joint f1 score by 1 . 36 points over the baseline ccat10 score .
in table vi , we report the test results on the ccat10 , blogs50 and ccat50 datasets . our model obtains 86 . 53 % accuracy on average compared to the previous best state - of - the - art models . syntax - cnn , on the other hand , obtains a performance improvement of 3 . 08 % over the previous state of the art model . this confirms the effectiveness of our model selection process . we additionally test our model in the unsupervised setting , as this easier setup allows more data to be input into the model without sacrificing accuracy . we observe that our style - han model significantly outperforms other models with different classifiers in terms of accuracy . for example , it achieves 90 . 73 % on blogs10 and 82 . 53 % ) on the blog50 dataset . this indicates that the model can rely on word embeddings with fewer errors . additionally , our model exhibits a marked improvement in accuracy over the strong baselines across all datasets .
the results of experiment 1 are shown in table 2 . we observe that the lstm c + lstm r = improves upon the strong lemma baseline by 3 . 59 points in the standard task formulation . moreover , the improvement is much larger when the model is trained with the conditional supervision . this corroborates our intuition that the semantic information injected into the model by the additional supervision helps the model to learn more about the task .
the results are presented in table 4 . we observe that the lstm c + lstm r = improves upon the strong lemma baseline by 3 . 08 points in the standard task formulation . moreover , the improvement is much larger when the model is trained with the conditional supervision . this corroborates our intuition that the semantic information injected into the model by the additional supervision helps the model to improve its interpretability .
table 3 shows that the multi - factor approach further boosts precision and recall , and improves the f - score over random variation . ame model significantly outperforms t - a model in all aspects ( except precision in recall metric ) . ame improves recall with a drop of 9 . 2 % in precision compared to t + a . this confirms the value of word embeddings adaptation . we further find that ame models benefit from a reduction in error rate as well . in our model , we achieve 71 . 4 % f - score improvement over the strong monolingual baseline on average .
table 3 shows that the multi - modality approach improves recall and precision in the single - domain task . the multi - factor approach outperforms the naive approach in all metrics ( except precision in case of a , a + v ) . in particular , the improvement is much larger in the case of precision in which precision is relatively high than recall . this corroborates our intuition that the ability to select compact regions induces the generation of compact regions leads to a better model .
table 3 shows that the speaker dependentness baseline can be further improved with the addition of context features . the results show that the task completion ability of the task is further improved when the context is added , improving the f - score by 3 . 8 points in the standard task formulation and to parity in the gold - two - mention case . however , the biggest performance increase is seen in the precision improvement in the recall task . this corroborates our intuition that context information can help the model to more precisely detect events of armed conflicts termination .
