table 2 shows the performance of the treelstm model on our recursive framework , fold ’ s folding technique , with the large movie review dataset . the recursive approach performs the best on inference with efficient parallel execution of tree nodes , while the folding technique shows better performance on training thanks to its gpu exploitation . on the other hand , the iterative approach outperforms the recursive framework on training and in inference , but not on training .
the balanced dataset exhibits highest throughput thanks to the high degree of parallelization , but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25 , because there is only a small room of performance improvement left , w . r . t parallelization . table 1 shows the performance of the treernn model implemented with recursive dataflow graphs , using datasets of varying tree balancedness . we observe that the balanced dataset outperforms the linear datasets in terms of performance on all the tasks . the performance of all the datasets on all tasks on all of the tasks on the tree balanced dataset is significantly higher than that of the linear dataset .
the max pooling strategy consistently performs better in all model variations . for example , the best performance is seen in conll08 , ud v1 . 3 , sb , softplus , and sigmoid models . the best performance in all three models is observed in ud , sb and softplus models . the best performing model is ud , which outperforms all the other models with different representation . in ud , we observe that it outperforms the best - performing model with the best representation .
table 1 shows the results of using the shortest dependency path on each relation type . the results are shown in table 1 . the best f1 ( in 5 - fold ) with sdp and macro - averaged models outperforms the models with the shortest dependencies path on all relation types except part_whole . the results show that the best - performing models are those that use the shortest possible dependency path .
table 3 shows the performance of the f1 100 % and r - f1 50 % models . the results are shown in table 3 . we observe that the results of both models are significantly better than those of the other two models . the results of the two models are presented in table 4 . y - 3 : y < italic > r is the best performing of all the models . it outperforms all the other models by a large margin . it is also the best - performing of all models .
as expected , mst - parser outperforms the other two parsers by a large margin . it outperforms paragraph level r - f1 and essay level f1 by a significant margin . the difference between paragraph level f1 and the other parsers can be seen in the following table . the difference in the performance of the three parsers is significant .
table 4 shows the performance of the two systems on the essay vs . paragraph level . note that the mean performances are lower than the majority performances over the runs given in table 2 . the performance of stagblcc and lstm - parser is shown in table 4 .
as expected , the performance of sc - lstm outperforms that of tgen + and tgen − by a significant margin . the difference in performance between the two systems can be seen in the following table . the difference between the original and the clean - up performance is shown in table 1 . we see that the cleanup performance of the original system outperforms the cleaned system by a large margin . in particular , the clean up performance of tgen + outperforms tgen and tgen − by significant margins .
table 1 shows the results for the original e2e data and our cleaned version . the results are shown in table 1 . we find that our trained version outperforms the original data by a significant margin . the difference between the original and the cleaned version is shown in the following table . our trained version has the highest number of distinct mrs , total number of textual references , and ser as measured by our slot matching script , see section 3 .
we see that the performance of sc - lstm outperforms that of tgen + and tgen − by a significant margin . the difference in performance between the two systems is significant , with the difference between the original and the original model being less than 0 . 5 points . the difference between original and original model is also significant . we observe that the difference in the performance between tgen and tgen + is less than 1 . 0 points .
table 4 shows the results of manual error analysis of tgen on a sample of 100 instances from the original test set : total absolute numbers of errors we found ( added , missed , wrong values , slight disfluencies ) . the results are shown in table 4 . we found that tgen has a significant improvement in the accuracy of the training data compared to the test set . the accuracy of our training data is significantly higher than that of our test set on the original set .
table 3 shows the performance of the three models in terms of dcgcn ( ensemble ) and all models ( single ) over the last two years . the results are shown in table 3 . the model with the best performance is the graphlstm model , which outperforms the other two models by a large margin . the other models with the worst performance are pbmt ( pourdamghani et al . , 2016 ) and tree2str ( flanigan and cohen , 2016 ) .
table 2 shows the model size in terms of parameters ; “ s ” and “ e ” denote single and ensemble models , respectively . the model size is shown in table 2 . we observe that seq2seq ( beck et al . , 2018 ) achieves 24 . 5 bleu points . gcnseq ( damonte and cohen , 2019 ) , on the other hand , achieves 23 . 3 points . we also observe that dcgcn ( our ) achieves 22 . 5 points . our model achieves 25 . 1 points . in contrast , ggnnseq achieves 26 . 3 point .
table 1 shows the performance of the models in table 1 . the results are shown in table 2 . we see that the best performing models are the ones with the best performance in terms of accuracy . the best performing model is the birnn + gcn ( bastings et al . , 2017 ) , which has the highest accuracy in english - german and english - czech . the worst performing model , however , is the ggnn2seq ( beck et al , 2018 ) which has a lower accuracy in both english and czech .
table 5 shows the effect of the number of layers inside the dc on the performance of the model . the number of layer layers inside dc is shown in table 5 . we observe that the number - of - layer layers inside a dc has a significant effect on our performance . our results are shown in the table 5 table . the effect of layer number on performance of dc is seen in the following table .
table 6 shows the performance of gcns with residual connections on the baselines . the results are shown in table 6 . gcn + rc + la ( 2 ) and gcn + la + rla ( 4 ) outperform gcn - la ( 6 ) on all baselines except dcgcn1 , which outperforms gcn2 ( 18 ) by a significant margin . the difference between the two baselines can be seen in the table 6 results .
we find that dcgcn ( 1 ) outperforms dcgcngn ( 2 ) by a significant margin , and dcgcgn ( 3 ) by an even larger margin . we also observe that the model that achieves the best performance has the highest performance in terms of the number of points in the model , and the lowest number of errors .
table 8 shows the results of our ablation study for density of connections on the dev set of amr15 . - { i } dense block denotes removing the dense connections in the i - th block . dcgcn4 outperforms the other two models in terms of the number of connections removed . the results are shown in table 8 .
table 9 shows the results of our ablation study for the graph encoder and the lstm decoder . the results are shown in table 9 . the results of the ablation study show that the decoder and encoder modules used in the graph decoder outperform the global node and global node models . the decoder modules outperform both the global node and global node models by a significant margin .
table 7 shows the scores for initialization strategies on probing tasks . we find that somo and glorot outperform our paper by a significant margin . somo outperforms our paper in all three of the probing tasks , and we find that the accuracy of somo is higher than our paper on all of the tasks . our paper outperforms somo in all but one of the three tasks .
we find that h - cmow / 400 outperforms h - cbow by a large margin . it outperforms both cmow and cbow by more than 10 points . it also outperforms cmow by 6 points . we see that the best performance of the two approaches is achieved when we compare the results of both methods on the same dataset . the best performance is obtained when we use the best of all three approaches .
we find that the hybrid model outperforms the cmow model by a significant margin . cmow / 784 outperforms sick - r by a large margin . the hybrid model performs better than cmow on all three of the test domains except for sst2 , sst5 , and sts - b .
table 3 shows the results on unsupervised downstream tasks attained by our models . rows starting with “ cmp . ” show the relative change with respect to hybrid . we observe that our models outperform hybrid on all the tasks except for one task , where we observe that hybrid outperforms cmow on all tasks except one task . our models also outperform the hybrid model on all other tasks . the results are shown in table 3 .
table 8 shows the scores for initialization strategies on supervised downstream tasks . our paper outperforms glorot and trec in terms of sst2 , sst5 , sts - b , and sick - r . we also outperform our paper in sst3 and sst4 . our paper also outperforms our paper on the supervised downstream task . the results are shown in table 8 .
table 6 shows the results for different training objectives on the unsupervised downstream tasks . the results are shown in table 6 . we observe that sts12 and sts14 outperform cmow - r on the downstream tasks , but sts16 outperforms sts15 on the supervised downstream tasks by a significant margin . the difference between the two training objectives can be seen in the following table .
we see that the cmow - r model outperforms the cbow - c model by a significant margin . the cmow model has a better performance than the cbows model in terms of accuracy and accuracy , but it has a lower performance when it comes to accuracy and precision . the cbow model also outperforms cmow by a large margin , but not by much .
we find that the cmow - r model outperforms the sick - e model by a significant margin . the cmow - r model performs better than sick - e on all three of the test domains except for sst2 and sts - b , where the results are shown in table 1 .
table 2 shows the performance of the system and the results of the supervised learning model . the results are shown in table 2 . the results of supervised learning model outperform all other models in terms of accuracy , performance , and accuracy . in particular , it outperforms all the other models by a large margin . it outperforms the system by a significant margin in all three domains , except for the name matching model .
table 2 shows the results on the test set under two settings . the results of both settings are shown in table 2 . we observe that the model with the highest f1 scores outperforms the one with the lowest f1 score by a significant margin . we also observe that both models have higher τmil - nd scores than those of the other two . in particular , we observe a significant difference in the performance of the two models on the f1 test set . for example , the results of mil - nd ( model 1 ) and supervised learning ( model 2 ) show a significant improvement over the results obtained by the other model .
table 6 shows that g2s - gat outperforms all other models in terms of the number of features that are included in the model . it also shows that the model that has more features in it outperforms the other models by a significant margin . for example , the model with the most features in its model has the highest percentage of features in the models with the least features in them . the model with more features is the one with the highest number of feature in it . it has the lowest percentage of feature - related errors .
table 3 shows the results of our model on the ldc2015e86 and ldc2017t10 models . the results are shown in table 3 . we observe that g2s - gin outperforms gat and s2s by a significant margin . we also observe a significant difference in the performance of the model with respect to meteor and bleu . the difference between the two models is significant . the difference in performance between the model and the model is also significant .
table 3 shows the results on ldc2015e86 test set when models are trained with additional gigaword data . the results are shown in table 3 . we observe that the g2s - ggnn model outperforms the external models on the test set . it outperforms both the external and internal models in terms of performance . we see that the external model performs better than the internal model in all three cases .
table 4 shows the results of the ablation study on the ldc2017t10 development set . we find that bilstm outperforms get and meteor in terms of the size of the model and the number of models in the model .
we find that g2s - gin outperforms all other models in terms of s2s and gat . we observe that the model with the best model is the one with the highest model diameter . the model that has the highest diameter has the best performance on all the models . this is shown in table 1 . our model has a higher diameter than all the other models with the lowest diameter , but it also has a lower diameter on all models .
the token lemmas are used in the comparison . gold refers to the reference sentences . s2s and g2s - ggnn are used for the test set of ldc2017t10 . we find that gold outperforms the other models in terms of the fraction of elements in the output that are not present in the input ( added ) and that are missing in the generated sentence ( miss ) . we also find that the model with the best performance is the one with the lowest fraction of missing elements . in table 8 , we compare the performance of the two models .
table 4 shows the performance of the 4th nmt encoding layer trained with different target languages on a smaller parallel corpus ( 200k sentences ) . the results are shown in table 4 . we observe that the pos tagging accuracy is significantly higher than the sem tagging accuracy on the smaller corpus .
table 2 shows the pos and sem tagging accuracy with baselines and an upper bound . the results are shown in table 2 . we observe that the best tag classifier is word2tag , while the worst tag is unsupemb , which has a lower accuracy than the best tagging classifier .
table 4 shows the results of our model on the pos tagging accuracy and sem tagging accuracy . the results are shown in table 4 . our model outperforms all the other models by a significant margin . the best performing model is the one with the highest score in terms of accuracy . it has a score of 92 . 1 % and a score in the mid - 90s .
table 5 shows the pos and sem tagging accuracy with features from different layers of 4 - layer uni / bidirectional / residual nmt encoders , averaged over all non - english target languages . the results are shown in table 5 . we observe that the pos tagging accuracy is significantly better than the res tagging accuracy , and that the bi tagging accuracy outperforms the uni tagging accuracy on all target languages except english .
table 8 shows the performance of the attacker on different datasets . results are on a training set 10 % held - out . δ is the difference between the attacker score and the corresponding adversary ’ s accuracy . the results are shown in table 8 . we observe that the attacker performs better than the adversary on all datasets except pan16 .
table 1 shows the performance of pan16 when training directly towards a single task . the results are shown in table 1 . we observe that pan16 outperforms pan16 in terms of accuracy when trained directly towards the task . pan16 has a higher accuracy than pan16 on all tasks , except for gender .
table 2 shows the results of the protected attribute leakage model in table 2 . the results are shown in table 1 . we observe that the best performance is achieved when the data are split into two groups . the best performance of the two groups is obtained when the task is split into three groups , and the worst performance is observed when the tasks are split in two groups , such as gender and age .
table 3 shows the performance on different datasets with an adversarial training . the performance on pan16 and pan16 is shown in table 3 . we observe that pan16 outperforms pan16 by a significant margin . we also observe that the performance of pan16 over pan16 has a significant impact on the accuracy of the task .
table 6 shows the accuracies of the protected attribute with different encoders . the results are shown in table 6 . we see that guarded and leaky embeddings outperform guarded embedders by a significant margin .
table 3 shows the results of our model and model - based work . the results are shown in table 3 . we observe that our model outperforms all other models in terms of performance . our model performs better than all the other models , except for lrn , which outperforms lrn and lrn + finetune by a significant margin . this is due to the fact that lrn + finetune outperforms the other two models by a large margin .
table 1 shows the results of the model and the work performed by rocktäschel et al . ( 2016 ) . the results are shown in table 1 . the results of our model and work are presented in table 2 . we see that our model outperforms all other models by a significant margin . our model performs significantly better than all the other models in terms of the number of params and the time it takes to perform the task . in particular , our model performs much better than other models on all the parameters .
the results of zhang et al . ( 2015 ) are shown in table 1 . the results are presented in table 2 . our model outperforms yelppolar err and yelptime err by a significant margin . we observe that the model performs significantly better than yelptime and yelperr in terms of the number of params . the model also outperforms the model by a large margin . we observe a significant difference in the performance between the two models .
table 3 shows the performance of our model on wmt14 english - german translation task on tesla p100 . decode : time in milliseconds used to decode one sentence measured on newstest2014 dataset . train : time in seconds per training batch measured from 0 . 1k to 0 . 2k training steps on the same dataset , and the time in seconds used to train one sentence on the other dataset , measured on the newstests2014 dataset , as shown in table 3 . we observe that gnmt outperforms olrn and sru in the case - insensitive tokenized bleu score .
table 4 shows the results of our model on the squad dataset . the results of rnet and rnet * are shown in table 4 . we observe that rnet outperforms all other models in terms of the parameter number of base . “ # params ” shows that the model has a higher parameter number than other models . in particular , the model with the highest parameter number is the one with the lowest parameter number . we see that the model with the lower parameter number has a lower f1 - score than all the other models with the same parameter number , except for rnet .
table 6 shows the f1 score on conll - 2003 english ner task . lstm * denotes the reported result lample et al . ( 2016 ) and lrn and sru are shown in table 6 . “ # params ” denotes the parameter number in ner tasks . the model with the highest number of parameter number is the one with the best performance on the task . lrn , sru , gru , and atr have the highest f1 scores on the ner task .
table 7 shows the results of test accuracy on snli task with base + ln setting and test perplexity on ptb task with base setting . the results are shown in table 7 . we observe that the accuracy of the snli model outperforms the ptb model on both tasks . the accuracy of lrn and elrn outperforms that of glrn .
table 2 shows the performance of oracle retrieval on the human - based system . the results are shown in table 2 . we observe that the oracle system outperforms the human system by a significant margin . our results show that our system performs better on the human - based model than on the oracle model . in particular , we observe that it outperforms our system on the mtr model .
table 4 shows the results of human evaluation on grammaticality ( gram ) , appropriateness ( appr ) , and content richness ( cont . ) , on a scale of 1 to 5 ( best ) . the best result among automatic systems is highlighted in bold , with statistical significance marked with ∗ ( approximation randomization test , p < 0 . 0005 ) . the highest standard deviation among all is 1 . 0 . top - 1 / 2 : % of evaluations a system being ranked in top 1 or 2 for overall quality . the best results among all automatic systems are shown in table 4 . the best performance among human evaluation is seen in table 5 . the highest performance among automatic evaluation is observed in table 6 .
as expected , the performance of europarl outperforms ted talks by a significant margin . the performance of ted talks is also significantly better than that of the other two models . the difference between the two models can be seen in the following table . we see that the performance gap between ted talks and the other models is significantly smaller than the difference between those of the two other models .
as expected , the performance of europarl outperforms ted talks by a significant margin . it outperforms both ted talks and hclust by a large margin . the difference between the two models is significant . the performance of ted talks is significantly worse than that of hclust . the difference in performance between the three models can be seen in the following table . we see that the performance gap between the best and worst - performing models is much smaller than expected .
as expected , the performance of europarl outperforms ted talks by a significant margin . the performance of ted talks is significantly worse than that of the other two models . the difference in performance between the two models can be seen in the following table . we see that the performance gap between ted talks and the other models is less than 0 . 5 points . the performance gap is also smaller than expected .
we see that europarl outperforms all the other systems in the dataset . the best performance is seen in the number of numberrels and totalroots , which are shown in table 1 . we observe that the best performance of all the systems is in the numberrels category . the worst performance in the totalroots category is observed in the maxdepth category , which is shown in figure 2 .
we find that europarl outperforms all the other systems in terms of numberrels , totalroots , and totalterms . in particular , we see that the numberrel scores are significantly higher than those of all other systems . we also observe that the depthcohesion scores are higher than the average of all the systems in our dataset .
table 1 shows the performance ( ndcg % ) comparison for the experiments of applying our principles on the validation set of visdial v1 . 0 . r0 , r1 , r2 , r3 denote regressive loss , weighted softmax loss , binary sigmoid loss , and generalized ranking loss , respectively . qt , s and d denote question type , answer score sampling , and hidden dictionary learning , while p1 , p2 , p3 and p1 + p1 are the enhanced versions of the model . lf is the enhanced version as we mentioned . the performance of the enhanced model is shown in table 1 . ncdcg % is shown as the difference between the baseline and enhanced version of the same model .
table 2 shows the performance ( ndcg % ) of ablative studies on different models on visdial v1 . 0 validation set . p2 indicates the most effective one ( i . e . , hidden dictionary learning ) shown in table 1 . note that only applying p2 is implemented by the implementations in section 5 with the history shortcut . in table 2 , we see that p2 outperforms both p1 and p2 on the visdial validation set in all cases except for the case where p1 outperforms p2 .
table 5 shows the performance of hmd - recall + bert and wmd - unigram on hard and soft alignments . the results are shown in table 5 . we observe that the performance on hard alignments is comparable to that of the other two alignments , but not as good as the results on soft alignment . the performance of wmd unigram + wmd is comparable with that of all the other alignments except for hmd , which outperforms all the alignments but not hmd .
the results are shown in table 1 . we see that the sent - mover model outperforms the baselines by a significant margin . the average score of the two baselines is 0 . 624 points higher than the baselines . we observe a significant difference in the performance between the two sets of baselines . the baselines < c > ruse ( * ) and bertscore - f1 ( * ) outperform the baseline by a large margin . we observe that ruse ( * ) has a higher average score than the baseline .
table 2 shows the performance of the bertscore - f1 and bleu - 2 models on the sent - mover and w2v models . the results are shown in table 2 . we see that the results of both models outperform the baselines by a significant margin . in particular , we see that bert score - f1 outperforms the baselines by 0 . 074 points . our results also show that the performance gap between the two models is less than 0 . 05 points .
we see that the results of the word - mover setting are similar to those of the meteor setting . the difference between the two sets of settings is significant . we observe that the word - mover setting has a significant impact on the performance of the sent - movers . for example , we observe that word - movement is significantly higher than that of smd + bert + elmo + p and wmd - 1 + w2v .
as expected , the performance of the shen - 1 model outperforms that of the para + para + lang model by a large margin . we observe that m0 [ italic ] + cyc + para + lang outperforms m1 and m2 by a significant margin . the difference between the two models can be seen in the following table . the difference in performance between m0 and m1 is shown in table 1 .
table 3 shows the performance of the models in table 3 . the results are shown in table 4 . we see that yelp outperforms yelp in terms of transfer quality and transfer quality in all three of our models . we observe that yelp performs better than yelp in the transfer quality category , but yelp performs worse in the semantic preservation category .
table 5 shows the results of human sentence - level validation of metrics on yelp and yelp lit . the results are shown in table 5 . we observe that human sentence level validation improves the accuracy of our model by 0 . 79 points . our model outperforms both machine and human ratings of semantic preservation and fluency . in addition , we observe that our model performs significantly better than our human - level model in terms of accuracy . this is seen in table 6 .
as expected , the performance of our model is significantly better than that of the other models on the table . we observe that our model outperforms all other models in terms of accuracy and performance on para and para + para + lang . however , our model does not perform as well on the para - para - lang model . our model performs significantly worse on the para + para model than the other two models . the results are shown in table 1 . we see that m0 [ italic ] + cyc and para + para are the only two models that outperform all the others on the model .
table 6 shows the results on yelp sentiment transfer , where bleu is between 1000 transferred sentences and human references , and acc is restricted to the same 1000 sentences . our best models ( right table ) achieve higher bleu than prior work at similar levels of acc ∗ , but untransferred sentences achieve the highest bleu . the best models on yelp transfer are those with the highest acc and the best models with the lowest acc . the results of our best models are shown in table 6 . we find that our best model achieves higher acc than the best model with lower acc and lower acc with respect to human references . our best model also achieves better acc with higher accuracy than our best classifiers with higher acc and higher acc with lower accuracy . in addition , we find that the accuracy of our model is higher than that of the best classifier with low acc .
table 2 shows the percentage of reparandum tokens that were correctly predicted as disfluent . we find that nested disfluencies are significantly more likely to be disfluential than those that are not nested . we also observe that the accuracy of the disfluency prediction is significantly lower than that of the accuracy prediction for repetition tokens . we observe that nesteddisfluencies outperform repetition tokens by a significant margin . our results are shown in table 2 . the accuracy of our disfluence prediction is higher than the performance of our rephrase prediction .
table 3 shows the relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair ( content - content ) or in neither . percentages in parentheses show the fraction of tokens belong to each category . reparandum , repair , and function - function are shown in table 3 . we observe that the accuracy of disfluency prediction is significantly higher in the first two categories than in the third . the accuracy of the third category is significantly lower than the first .
the results are shown in table 1 . we see that the model with the best test performance is the one with the most innovations in it . the model that has the best innovations in the model has the highest test performance in terms of test performance . the model with best innovations has the lowest test performance of all the models . we observe that the models with the highest innovations in them are the ones that have the best performance on the test set . in particular , we observe that text + innovations outperforms innovations by a large margin .
table 2 shows the performance of the state - of - art algorithms on the fnc - 1 test dataset . our model outperforms the state of the art algorithms in terms of accuracy . the accuracy of our model is significantly higher than the accuracy of cnn - based sentence embedding and self - attention sentence embeddings . our model also outperforms rnn - based and selfattention - based algorithms in regards to accuracy .
table 2 shows that the unified model outperforms all previous models on the apw and nyt datasets for the document dating problem ( higher is better ) . the unified model significantly outperforms the other methods on the nyt dataset for the date dating problem . it outperforms both oe - gcn and acgcn , and outperforms neuraldater . it also outperforms burstysimdater , which outperforms maxent - joint .
table 3 shows the performance of neuraldater with and without attention . this results show the effectiveness of both word attention and graph attention for this task . the accuracy of the component models is shown in table 3 . we observe that the accuracy of our component models with attention is significantly higher than those with attention . please see section 6 . 2 .
as shown in table 1 , the performance of all the models in our model is shown in the table . the performance of the model on all stages is shown . we see that the model with the best performance is the one with the highest level of accuracy . the model that has the highest accuracy is the dmcnn model , which outperforms all the other models on all three stages .
table 3 shows the results of our cross - event model . the results are shown in table 3 . we observe that our model outperforms all other models by a significant margin . in particular , we observe a significant difference between our model and the other models in terms of the accuracy of our model . the difference between the two models can be seen in table 4 .
the results are shown in table 2 . the results of the test perp and test wer are presented in table 1 . we observe that the performance of the best - performing models is significantly better than the worst - performing ones . the best performing models are the ones with the highest dev acc and test acc score . the worst performing models have the lowest dev acc score , while the best performing ones have the highest test acc scores .
table 4 shows the results on the dev set and on the test set using discriminative training with only subsets of the code - switched data . the results are shown in table 4 . we observe that fine - tuned training outperforms fine - tuned training on both the dev and test sets . the results of fine - tuned training outperform the results of cs - only training in both the test and dev sets .
table 5 shows the accuracy on the dev set and on the test set , according to the type of the gold sentence in the set : code - switched ( cs ) vs . monolingual ( mono ) and code - only - disc vs . code - synchronous ( cs - only ) and fine - tuned - disc ( fine - tuned - disc ) . the results are shown in table 5 . we observe that the performance of the dev and test sets is significantly different . the dev set is significantly better than the test sets . the test set is slightly worse than the standard set .
table 7 shows the performance of the type - aggregated gaze features trained on all three eye - tracking datasets and tested on the conll - 2003 dataset ( * marks statistically significant improvement ) . the accuracy and recall of type combined gaze features is shown in table 7 . the accuracy of the three eye tracking datasets is shown by the f1 - score ( f1 ) score . we observe that type combined features perform better than type combined in all three datasets .
table 5 shows the performance of type - aggregated gaze features on the conll - 2003 dataset compared to the baseline dataset . we observe that type combined features perform better than the type combined gaze features in terms of precision , recall , and f1 - score ( f1 score ) on the conll dataset .
table 1 shows the results on belinkov2014exploring ’ s ppa test set . hpcd ( full ) is from the original paper , and it uses syntactic skipgram . glove - extended and ontolstm - pp embeddings are from the second paper ( faruqui et al . , 2015 ) and the third paper ( schütze et al . 2015 ) respectively . the results on the first paper ( hpcd ) are shown in table 1 . the results of the second and third papers are presented in table 2 . in both cases , the results are significantly better than those of the first two papers . the difference between the two papers can be seen in the following table .
table 2 shows the results of all the models in table 2 . the results are shown in table 1 . we see that the results from all of the models are significantly better than those from the other models . in particular , we observe a significant improvement in the performance of the hpcd and ontolstm - pp models over the other two models .
table 3 shows the effect of removing sense priors and context sensitivity ( attention ) from the model . the results are shown in table 3 . we find that context sensitivity improves the performance of the model by a significant amount .
table 2 shows the results of adding subtitle data and domain tuning for image caption translation ( bleu % scores ) . all results with marian amun . the domain tuning results are shown in table 2 . we observe that the domain tuning improves the performance of the domain - tuned model over the en - fr and en - de models , but not the sub - subsfull model . our domain tuning model outperforms all other models in terms of bleu % score . moreover , our domain tuning models outperform all the other models except for the multi30k model .
we find that the domain - tuned subs1m outperforms the domains1m in terms of performance in all three domains , except for the h + ms - coco , which outperforms all other domains except the lm + ms . we also observe a significant difference in the performance of the domain - tuned and domain - untuned sub - sets . the domain - untuned sub - sets outperform all other sub - sub - subsets in all the domains except for those that are domain - untuned . we observe that domain - tuned sub - sub - sub1m performs significantly better than domain - upsized sub - sub1m .
table 4 shows the results of adding automatic image captions ( only the best one or all 5 ) . the table shows bleu scores in % . all results with marian amun are shown in table 4 , except for the results with flickr16 and mscoco17 , which are shown with en - fr and flickr17 , respectively . the results of the automatic captions are shown on the left . the best captions have the highest bleu scores . we find that the best caption is the one with all 5 captions , and the worst one has the lowest score . in addition , the worst captions do not have a significant impact on the performance of the captions .
table 5 shows the results of all the strategies for integrating visual information ( bleu % scores ) . all results using transformer , multi30k + ms - coco + subs3mlm , detectron mask surface , and mscoco17 are shown in table 5 . the results of the best - performing strategies are seen in the table . the best performing strategies are those using en - de , en - fr , and mscoco . the best performance of enc - gate + dec - gate is shown in the table 5 , and the worst performing strategy is enc - de . we observe that the best performing strategy for integrating the visual information is the one with the highest bleu % score .
we find that the performance of our model is significantly better than that of all the other models in terms of the number of features in the dataset . for example , we find that our model outperforms all other models except for those that have a single - lingual feature in the text - only domain . however , our model does not outperform all the models in the other domains . for instance , it outperforms our model in the context of the text only domain , but not in the visual features domain .
table 2 shows the performance of the yule ’ s i and yule ' s i models on the mtld model . the results are shown in table 2 . the results of the ttr and mtld models are presented in table 3 . the ttr model outperforms all other models except for en - fr - smt - back and en - rnn - back , which outperform all the others except en - es - ht . ttr models outperform the other models by a significant margin .
table 1 shows the number of parallel sentences in the train , test and development splits for the language pairs we used . the results are shown in table 1 . we see that the parallel sentences we used in the test and test splits are significantly smaller than those in the development splits .
table 2 shows the results of the training vocabularies for the english , french and spanish data used for our models . the results of our models are shown in table 2 . the results show that our models perform significantly better in english and french than in spanish .
table 5 shows the results of the automatic evaluation scores ( bleu and ter ) for the rev systems . the results are shown in table 5 . bleu scores are shown on the left and ter scores on the right . we observe that ter scores are significantly higher than those of the system reference . ter scores outperform those of rev scores on both the automatic and manual evaluation scores .
table 2 shows the results on flickr . the row labeled vgs is the visually supervised model from chrupala2017representations . com , and the row labeled rsaimage is the rsaimage model from rsaimage . com .
table 1 shows the results on synthetically spoken coco . the row labeled vgs is the visually supervised model from chrupala2017representations . we observe that vgs outperforms rsaimage by a significant margin . the results on rsaimage are shown in table 1 . as expected , vgs performs better than rsaimage in terms of accuracy and recall .
we report further examples in the appendix . in table 1 , we show the results of the different classifiers compared to the original on sst - 2 . the results of cnn and dan are shown in table 1 . we observe that cnn turns in a screenplay screenplay screenplay that is at the edges , while dan turns in one that is on the edges .
table 2 shows the results of fine - tuning in sst - 2 . the symbols are purely analytic without any notion of goodness . the numbers indicate the changes in percentage points with respect to the original sentence . the last row indicates the overlap with the original sentences , and the last row shows the number of occurrences of each word in the final sentence . for example , , , and indicate that the total number of words has increased , decreased or stayed the same through finetuning respectively . a score of 0 thus means that fine - tuning has not changed the amount of words . the number of instances of the word " punctuation " has also increased or decreased , but not by a significant amount .
the last two rows correspond to the case where negative labels are flipped to positive and vice versa . the numbers indicate the changes in percentage points with respect to the original sentence . and indicate that the score increases in positive and negative sentiment . the results are shown in table 3 . the results of the sst - 2 model are shown on the left and the results of cnn on the right .
table 2 shows the results of the sst - 2 and corr models in table 1 . the results of both models are shown in table 2 . the results are shown for both pubmed and sift models .
